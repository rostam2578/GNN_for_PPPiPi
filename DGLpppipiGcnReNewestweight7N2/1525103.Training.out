0: gpu016.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-2135d612-642f-4ad0-ea96-14ef624f2286)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Wed Aug  3 02:59:34 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:B3:00.0 Off |                    0 |
| N/A   35C    P0    45W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2b8532fa0fa0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m17.641s
user	0m2.842s
sys	0m1.153s
[02:59:52] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch
/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./TrainingBha.py:126: RuntimeWarning: invalid value encountered in true_divide
  effres = efficiency.sum(axis = 0) / (efficiency!=0).sum(axis = 0)
/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./TrainingBha.py:128: RuntimeWarning: invalid value encountered in true_divide
  eval_effres = eval_efficiency.sum(axis = 0) / (eval_efficiency!=0).sum(axis = 0)




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 1.6691],
        [ 0.5897],
        [ 0.4056],
        ...,
        [ 0.7991],
        [-0.9913],
        [ 0.6389]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-95.4832, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (80000, 6796) (80000, 6796)
sum 5574226 8401300
shape torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-0.1155, -0.0332, -0.1168, -0.1018,  0.0390, -0.1505,  0.0995, -0.0321,
         -0.1284, -0.0125,  0.1372,  0.0311,  0.0465, -0.0138, -0.0765, -0.0873,
          0.0032, -0.1057, -0.1054, -0.1198, -0.0413, -0.0161, -0.1042,  0.1240,
          0.0603, -0.0420,  0.0815, -0.1410, -0.0695,  0.1057, -0.0265, -0.0682,
          0.1249,  0.0407, -0.0881,  0.0982,  0.0518,  0.0267,  0.0543,  0.0429,
         -0.0471,  0.1502,  0.1183,  0.0012,  0.1183, -0.0761, -0.0653, -0.0599,
          0.0638, -0.1215, -0.0079, -0.0735,  0.0458, -0.1144, -0.1124,  0.0970,
          0.1414, -0.0348, -0.1150,  0.0562, -0.0473,  0.1261,  0.0066,  0.0548,
          0.0051, -0.0413, -0.0324, -0.1304, -0.1478, -0.0443, -0.0553, -0.0679,
          0.1509, -0.0815,  0.1069, -0.0105,  0.1064,  0.1196,  0.1179, -0.0426,
          0.0644,  0.0126,  0.1323, -0.0437,  0.1488, -0.1129, -0.0638,  0.1044,
         -0.1246,  0.1517,  0.0714,  0.1191,  0.0220,  0.0788, -0.0798,  0.0473,
         -0.0293, -0.0199,  0.0602, -0.1027,  0.1326, -0.1152,  0.0683,  0.1024,
         -0.0178, -0.0878, -0.0324, -0.1419, -0.0577, -0.0902, -0.0294, -0.0116,
         -0.0250,  0.0640, -0.0697,  0.0782, -0.0839, -0.0227, -0.0994,  0.0232,
          0.0829, -0.1251,  0.1305, -0.1055, -0.1329,  0.0384, -0.0732, -0.1503,
         -0.1340,  0.0166,  0.0615,  0.1003, -0.0503,  0.0960,  0.0268,  0.0033,
         -0.0350, -0.0829, -0.0253,  0.0433,  0.1097,  0.0205, -0.1375, -0.1255,
          0.0587,  0.0839, -0.0794, -0.0358,  0.0778, -0.0729, -0.0645, -0.1367,
          0.1178,  0.0405, -0.0746, -0.0384, -0.0978, -0.0558, -0.0782,  0.0652,
         -0.0399, -0.0205,  0.1325, -0.0143,  0.1176, -0.0981,  0.0318, -0.0218,
          0.0311, -0.0623, -0.0236,  0.0762, -0.1222, -0.0647,  0.0739,  0.0297,
         -0.0077, -0.0487, -0.0735, -0.0779,  0.0265,  0.0506, -0.1130, -0.0204,
          0.1475, -0.0562, -0.0940,  0.0977, -0.0014, -0.0020, -0.1301, -0.0812,
          0.0789, -0.0272,  0.1470,  0.0010,  0.0572,  0.0655,  0.1334,  0.1508,
         -0.0543,  0.1105,  0.0121, -0.0721,  0.0170,  0.0186,  0.0881, -0.0654,
          0.1283, -0.0387, -0.1015,  0.0719, -0.1225, -0.0276, -0.0461,  0.0421,
          0.0003, -0.1074, -0.0537, -0.0172,  0.0314, -0.0093,  0.0069, -0.1371,
          0.1178, -0.1299,  0.0353,  0.1343, -0.0078,  0.0355,  0.0662,  0.0697,
          0.0179,  0.1202,  0.0774,  0.0366, -0.0010, -0.0592,  0.0826, -0.0805,
          0.0680,  0.0859,  0.0747,  0.1514,  0.1045, -0.0154,  0.1470,  0.0790,
         -0.0543, -0.0337, -0.1484, -0.1070, -0.1207,  0.0838, -0.1439, -0.1487]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1155, -0.0332, -0.1168, -0.1018,  0.0390, -0.1505,  0.0995, -0.0321,
         -0.1284, -0.0125,  0.1372,  0.0311,  0.0465, -0.0138, -0.0765, -0.0873,
          0.0032, -0.1057, -0.1054, -0.1198, -0.0413, -0.0161, -0.1042,  0.1240,
          0.0603, -0.0420,  0.0815, -0.1410, -0.0695,  0.1057, -0.0265, -0.0682,
          0.1249,  0.0407, -0.0881,  0.0982,  0.0518,  0.0267,  0.0543,  0.0429,
         -0.0471,  0.1502,  0.1183,  0.0012,  0.1183, -0.0761, -0.0653, -0.0599,
          0.0638, -0.1215, -0.0079, -0.0735,  0.0458, -0.1144, -0.1124,  0.0970,
          0.1414, -0.0348, -0.1150,  0.0562, -0.0473,  0.1261,  0.0066,  0.0548,
          0.0051, -0.0413, -0.0324, -0.1304, -0.1478, -0.0443, -0.0553, -0.0679,
          0.1509, -0.0815,  0.1069, -0.0105,  0.1064,  0.1196,  0.1179, -0.0426,
          0.0644,  0.0126,  0.1323, -0.0437,  0.1488, -0.1129, -0.0638,  0.1044,
         -0.1246,  0.1517,  0.0714,  0.1191,  0.0220,  0.0788, -0.0798,  0.0473,
         -0.0293, -0.0199,  0.0602, -0.1027,  0.1326, -0.1152,  0.0683,  0.1024,
         -0.0178, -0.0878, -0.0324, -0.1419, -0.0577, -0.0902, -0.0294, -0.0116,
         -0.0250,  0.0640, -0.0697,  0.0782, -0.0839, -0.0227, -0.0994,  0.0232,
          0.0829, -0.1251,  0.1305, -0.1055, -0.1329,  0.0384, -0.0732, -0.1503,
         -0.1340,  0.0166,  0.0615,  0.1003, -0.0503,  0.0960,  0.0268,  0.0033,
         -0.0350, -0.0829, -0.0253,  0.0433,  0.1097,  0.0205, -0.1375, -0.1255,
          0.0587,  0.0839, -0.0794, -0.0358,  0.0778, -0.0729, -0.0645, -0.1367,
          0.1178,  0.0405, -0.0746, -0.0384, -0.0978, -0.0558, -0.0782,  0.0652,
         -0.0399, -0.0205,  0.1325, -0.0143,  0.1176, -0.0981,  0.0318, -0.0218,
          0.0311, -0.0623, -0.0236,  0.0762, -0.1222, -0.0647,  0.0739,  0.0297,
         -0.0077, -0.0487, -0.0735, -0.0779,  0.0265,  0.0506, -0.1130, -0.0204,
          0.1475, -0.0562, -0.0940,  0.0977, -0.0014, -0.0020, -0.1301, -0.0812,
          0.0789, -0.0272,  0.1470,  0.0010,  0.0572,  0.0655,  0.1334,  0.1508,
         -0.0543,  0.1105,  0.0121, -0.0721,  0.0170,  0.0186,  0.0881, -0.0654,
          0.1283, -0.0387, -0.1015,  0.0719, -0.1225, -0.0276, -0.0461,  0.0421,
          0.0003, -0.1074, -0.0537, -0.0172,  0.0314, -0.0093,  0.0069, -0.1371,
          0.1178, -0.1299,  0.0353,  0.1343, -0.0078,  0.0355,  0.0662,  0.0697,
          0.0179,  0.1202,  0.0774,  0.0366, -0.0010, -0.0592,  0.0826, -0.0805,
          0.0680,  0.0859,  0.0747,  0.1514,  0.1045, -0.0154,  0.1470,  0.0790,
         -0.0543, -0.0337, -0.1484, -0.1070, -0.1207,  0.0838, -0.1439, -0.1487]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 9.1356e-02,  8.4973e-02,  7.9144e-02,  ...,  5.3187e-02,
         -1.0770e-03,  5.8612e-02],
        [-6.8769e-02, -9.8873e-02, -2.0191e-05,  ...,  4.7426e-02,
         -2.0001e-02, -6.1249e-02],
        [ 1.1618e-01, -9.8205e-02, -4.9532e-02,  ...,  1.1039e-01,
          1.2950e-02, -1.1189e-01],
        ...,
        [ 8.8622e-02,  1.2886e-02, -9.6428e-02,  ...,  7.9198e-02,
          8.6712e-02,  1.0832e-01],
        [ 1.1542e-02, -1.1166e-01,  4.5124e-02,  ...,  4.8466e-03,
         -1.0305e-01,  3.0275e-02],
        [-1.6737e-03,  1.8360e-02,  2.6726e-02,  ...,  1.0524e-01,
          1.1977e-01, -8.5829e-02]], device='cuda:0') 
 Parameter containing:
tensor([[ 9.1356e-02,  8.4973e-02,  7.9144e-02,  ...,  5.3187e-02,
         -1.0770e-03,  5.8612e-02],
        [-6.8769e-02, -9.8873e-02, -2.0191e-05,  ...,  4.7426e-02,
         -2.0001e-02, -6.1249e-02],
        [ 1.1618e-01, -9.8205e-02, -4.9532e-02,  ...,  1.1039e-01,
          1.2950e-02, -1.1189e-01],
        ...,
        [ 8.8622e-02,  1.2886e-02, -9.6428e-02,  ...,  7.9198e-02,
          8.6712e-02,  1.0832e-01],
        [ 1.1542e-02, -1.1166e-01,  4.5124e-02,  ...,  4.8466e-03,
         -1.0305e-01,  3.0275e-02],
        [-1.6737e-03,  1.8360e-02,  2.6726e-02,  ...,  1.0524e-01,
          1.1977e-01, -8.5829e-02]], device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.1001, -0.0426,  0.1613,  ...,  0.1132, -0.0084, -0.1416],
        [-0.0707,  0.0166,  0.1646,  ...,  0.1529, -0.0713,  0.0513],
        [-0.0999, -0.0178, -0.0928,  ...,  0.0447, -0.0151,  0.0269],
        ...,
        [ 0.0036, -0.0737,  0.1027,  ...,  0.0023,  0.0113, -0.1589],
        [ 0.1704,  0.0701, -0.0756,  ..., -0.0690,  0.0855, -0.0442],
        [ 0.0514,  0.0169,  0.1478,  ..., -0.0029, -0.0329, -0.1217]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1001, -0.0426,  0.1613,  ...,  0.1132, -0.0084, -0.1416],
        [-0.0707,  0.0166,  0.1646,  ...,  0.1529, -0.0713,  0.0513],
        [-0.0999, -0.0178, -0.0928,  ...,  0.0447, -0.0151,  0.0269],
        ...,
        [ 0.0036, -0.0737,  0.1027,  ...,  0.0023,  0.0113, -0.1589],
        [ 0.1704,  0.0701, -0.0756,  ..., -0.0690,  0.0855, -0.0442],
        [ 0.0514,  0.0169,  0.1478,  ..., -0.0029, -0.0329, -0.1217]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.0945, -0.0723,  0.2236,  ..., -0.0444, -0.1112,  0.1564],
        [ 0.0910, -0.0342,  0.1018,  ..., -0.2026, -0.1988, -0.2419],
        [-0.2170, -0.0019,  0.2369,  ...,  0.2434, -0.1259, -0.1411],
        ...,
        [ 0.1639,  0.1892,  0.2379,  ..., -0.0670,  0.2160, -0.0535],
        [ 0.0621,  0.2084,  0.1307,  ..., -0.0247,  0.1876, -0.1191],
        [ 0.0857, -0.0815, -0.1144,  ..., -0.1837,  0.0133,  0.2148]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0945, -0.0723,  0.2236,  ..., -0.0444, -0.1112,  0.1564],
        [ 0.0910, -0.0342,  0.1018,  ..., -0.2026, -0.1988, -0.2419],
        [-0.2170, -0.0019,  0.2369,  ...,  0.2434, -0.1259, -0.1411],
        ...,
        [ 0.1639,  0.1892,  0.2379,  ..., -0.0670,  0.2160, -0.0535],
        [ 0.0621,  0.2084,  0.1307,  ..., -0.0247,  0.1876, -0.1191],
        [ 0.0857, -0.0815, -0.1144,  ..., -0.1837,  0.0133,  0.2148]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.4251],
        [ 0.0375],
        [ 0.1828],
        [-0.2602],
        [-0.3422],
        [-0.1732],
        [ 0.1727],
        [-0.3463],
        [-0.1557],
        [ 0.1564],
        [ 0.0358],
        [-0.1280],
        [-0.2013],
        [-0.2065],
        [-0.3548],
        [ 0.2257],
        [-0.3322],
        [-0.2236],
        [ 0.3727],
        [ 0.2696],
        [ 0.3636],
        [-0.0534],
        [-0.2133],
        [-0.1085],
        [-0.1527],
        [ 0.3920],
        [-0.2471],
        [ 0.3465],
        [-0.3605],
        [ 0.2850],
        [-0.0809],
        [ 0.2385]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.4251],
        [ 0.0375],
        [ 0.1828],
        [-0.2602],
        [-0.3422],
        [-0.1732],
        [ 0.1727],
        [-0.3463],
        [-0.1557],
        [ 0.1564],
        [ 0.0358],
        [-0.1280],
        [-0.2013],
        [-0.2065],
        [-0.3548],
        [ 0.2257],
        [-0.3322],
        [-0.2236],
        [ 0.3727],
        [ 0.2696],
        [ 0.3636],
        [-0.0534],
        [-0.2133],
        [-0.1085],
        [-0.1527],
        [ 0.3920],
        [-0.2471],
        [ 0.3465],
        [-0.3605],
        [ 0.2850],
        [-0.0809],
        [ 0.2385]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 1.5087e-01, -1.2536e-01, -4.1319e-02, -9.7148e-02, -2.0934e-02,
         -9.6708e-02,  1.1013e-01, -7.1454e-02, -7.6677e-02, -5.5606e-02,
          4.9487e-02,  1.1529e-01,  1.1871e-03, -8.7390e-02,  1.3898e-01,
          1.2274e-02,  6.8330e-02,  1.2624e-01, -1.0726e-01, -1.4234e-01,
          1.2266e-01,  5.8905e-02, -1.0867e-01,  2.5755e-02, -9.8949e-02,
         -3.0083e-02,  1.2233e-01, -5.6237e-02, -7.2757e-02,  1.3731e-01,
         -1.2898e-01,  2.3378e-02, -1.4765e-01, -4.5728e-02,  1.1365e-01,
         -1.3844e-01, -1.2193e-01,  5.4459e-02,  4.3561e-02, -6.4675e-02,
         -7.9669e-02, -2.5446e-02,  6.7484e-02, -5.0975e-02,  5.0730e-02,
         -1.4982e-01, -1.8169e-02, -7.9421e-02,  1.0246e-01,  1.2048e-01,
         -1.3208e-01,  6.9117e-02,  4.0985e-02, -9.1037e-02,  4.0469e-03,
          5.2658e-02, -5.2874e-02,  1.4330e-01, -5.1652e-02, -7.9024e-02,
         -1.5257e-01, -1.2345e-01,  8.5785e-03,  5.2779e-02, -3.1223e-02,
         -1.4128e-01,  4.1210e-02, -5.6092e-02,  4.0753e-02, -1.4707e-02,
         -1.5203e-01,  4.9562e-02, -5.6304e-02,  5.0738e-02, -3.6188e-02,
          2.6633e-02,  9.8731e-02, -6.2250e-03, -6.0007e-02,  7.3412e-03,
          6.6236e-03,  1.2460e-01,  1.0107e-01, -1.4286e-01,  3.5461e-02,
          1.3866e-01, -3.6445e-02, -6.9605e-03, -8.2349e-02,  7.6278e-02,
         -4.3305e-02, -3.3575e-02, -7.5308e-02,  1.3328e-01, -4.5167e-02,
          4.7607e-02, -2.0462e-02, -4.3947e-03, -2.2134e-02,  5.2546e-02,
          7.2212e-02,  3.7988e-02,  3.8618e-02, -1.5045e-01,  1.3173e-01,
         -1.2677e-01, -1.0240e-01, -1.3379e-01,  5.9098e-03,  1.6772e-02,
          1.3672e-01,  3.9569e-02,  1.1262e-01,  7.2014e-02,  1.3130e-01,
          6.0551e-02,  5.7794e-02,  1.1677e-01,  1.1801e-01,  6.4914e-02,
          1.2007e-01,  1.3493e-01,  1.6998e-02, -1.2539e-01, -9.9247e-02,
          8.1666e-02,  1.2111e-01, -6.6087e-02,  6.3702e-02, -9.9548e-02,
          3.0738e-02,  6.3757e-02, -1.3137e-01,  4.9033e-02, -9.3073e-02,
          9.0857e-02, -1.1771e-01, -1.3877e-01, -7.4885e-02,  2.2064e-03,
          7.1625e-02, -1.2307e-01, -8.7099e-02,  5.0512e-02,  1.3693e-01,
          5.4536e-02, -1.5144e-01, -1.3537e-01,  6.9584e-02,  1.1292e-02,
         -1.0503e-01, -1.4253e-01,  4.5631e-02,  1.0130e-01,  1.0377e-01,
          1.3475e-01,  7.2872e-02,  1.0472e-01,  3.3097e-03, -5.8283e-02,
          3.7955e-03, -4.4639e-02,  4.8930e-02,  1.3404e-01,  5.9811e-02,
          3.5477e-02,  1.1642e-01, -4.2048e-02,  5.7453e-02,  1.9726e-02,
          2.7473e-02,  9.1449e-03,  6.5079e-02,  5.6316e-02, -1.1672e-01,
          1.2473e-01, -1.5169e-01,  8.4650e-02, -1.0800e-01,  1.4987e-04,
         -9.6164e-02, -8.1759e-02,  1.4911e-01,  8.8877e-02,  1.2469e-01,
          3.6353e-02,  1.1099e-02,  7.2296e-02,  8.6134e-02, -1.0956e-01,
         -6.5789e-02, -7.2797e-02,  5.3652e-02, -9.8836e-02, -1.0353e-01,
         -1.3672e-01, -2.1681e-02,  1.3253e-01, -1.4582e-01,  1.2365e-01,
         -1.5047e-02, -1.0333e-02, -1.1006e-01,  9.3942e-02, -2.5155e-02,
          3.4494e-02, -1.1611e-01, -6.8593e-02, -1.3953e-01,  7.3057e-03,
          1.6727e-02, -8.0383e-02,  1.3828e-01,  5.4163e-02, -1.1114e-01,
         -1.3767e-01, -8.6445e-02, -6.3683e-02,  7.6838e-04, -2.6727e-02,
          6.0289e-02,  3.2895e-02, -7.3711e-02,  1.4332e-01, -3.0252e-02,
         -1.4331e-01, -4.5512e-02,  1.2238e-01, -8.8810e-02, -1.1547e-01,
          1.1627e-01, -1.0778e-01, -6.0068e-04,  1.3399e-01,  9.5901e-02,
         -1.3756e-01, -1.9533e-02, -7.9085e-02,  6.2420e-02, -2.7271e-02,
          1.1250e-01,  1.5010e-01, -4.6734e-02, -9.8034e-02, -3.9884e-02,
         -4.1871e-03,  3.5833e-02, -1.0568e-01, -1.4801e-02,  5.3514e-02,
         -3.6051e-02,  7.0837e-02,  1.0268e-01,  1.2780e-01,  9.6226e-02,
         -4.4475e-02]], device='cuda:0') 
 Parameter containing:
tensor([[ 1.5087e-01, -1.2536e-01, -4.1319e-02, -9.7148e-02, -2.0934e-02,
         -9.6708e-02,  1.1013e-01, -7.1454e-02, -7.6677e-02, -5.5606e-02,
          4.9487e-02,  1.1529e-01,  1.1871e-03, -8.7390e-02,  1.3898e-01,
          1.2274e-02,  6.8330e-02,  1.2624e-01, -1.0726e-01, -1.4234e-01,
          1.2266e-01,  5.8905e-02, -1.0867e-01,  2.5755e-02, -9.8949e-02,
         -3.0083e-02,  1.2233e-01, -5.6237e-02, -7.2757e-02,  1.3731e-01,
         -1.2898e-01,  2.3378e-02, -1.4765e-01, -4.5728e-02,  1.1365e-01,
         -1.3844e-01, -1.2193e-01,  5.4459e-02,  4.3561e-02, -6.4675e-02,
         -7.9669e-02, -2.5446e-02,  6.7484e-02, -5.0975e-02,  5.0730e-02,
         -1.4982e-01, -1.8169e-02, -7.9421e-02,  1.0246e-01,  1.2048e-01,
         -1.3208e-01,  6.9117e-02,  4.0985e-02, -9.1037e-02,  4.0469e-03,
          5.2658e-02, -5.2874e-02,  1.4330e-01, -5.1652e-02, -7.9024e-02,
         -1.5257e-01, -1.2345e-01,  8.5785e-03,  5.2779e-02, -3.1223e-02,
         -1.4128e-01,  4.1210e-02, -5.6092e-02,  4.0753e-02, -1.4707e-02,
         -1.5203e-01,  4.9562e-02, -5.6304e-02,  5.0738e-02, -3.6188e-02,
          2.6633e-02,  9.8731e-02, -6.2250e-03, -6.0007e-02,  7.3412e-03,
          6.6236e-03,  1.2460e-01,  1.0107e-01, -1.4286e-01,  3.5461e-02,
          1.3866e-01, -3.6445e-02, -6.9605e-03, -8.2349e-02,  7.6278e-02,
         -4.3305e-02, -3.3575e-02, -7.5308e-02,  1.3328e-01, -4.5167e-02,
          4.7607e-02, -2.0462e-02, -4.3947e-03, -2.2134e-02,  5.2546e-02,
          7.2212e-02,  3.7988e-02,  3.8618e-02, -1.5045e-01,  1.3173e-01,
         -1.2677e-01, -1.0240e-01, -1.3379e-01,  5.9098e-03,  1.6772e-02,
          1.3672e-01,  3.9569e-02,  1.1262e-01,  7.2014e-02,  1.3130e-01,
          6.0551e-02,  5.7794e-02,  1.1677e-01,  1.1801e-01,  6.4914e-02,
          1.2007e-01,  1.3493e-01,  1.6998e-02, -1.2539e-01, -9.9247e-02,
          8.1666e-02,  1.2111e-01, -6.6087e-02,  6.3702e-02, -9.9548e-02,
          3.0738e-02,  6.3757e-02, -1.3137e-01,  4.9033e-02, -9.3073e-02,
          9.0857e-02, -1.1771e-01, -1.3877e-01, -7.4885e-02,  2.2064e-03,
          7.1625e-02, -1.2307e-01, -8.7099e-02,  5.0512e-02,  1.3693e-01,
          5.4536e-02, -1.5144e-01, -1.3537e-01,  6.9584e-02,  1.1292e-02,
         -1.0503e-01, -1.4253e-01,  4.5631e-02,  1.0130e-01,  1.0377e-01,
          1.3475e-01,  7.2872e-02,  1.0472e-01,  3.3097e-03, -5.8283e-02,
          3.7955e-03, -4.4639e-02,  4.8930e-02,  1.3404e-01,  5.9811e-02,
          3.5477e-02,  1.1642e-01, -4.2048e-02,  5.7453e-02,  1.9726e-02,
          2.7473e-02,  9.1449e-03,  6.5079e-02,  5.6316e-02, -1.1672e-01,
          1.2473e-01, -1.5169e-01,  8.4650e-02, -1.0800e-01,  1.4987e-04,
         -9.6164e-02, -8.1759e-02,  1.4911e-01,  8.8877e-02,  1.2469e-01,
          3.6353e-02,  1.1099e-02,  7.2296e-02,  8.6134e-02, -1.0956e-01,
         -6.5789e-02, -7.2797e-02,  5.3652e-02, -9.8836e-02, -1.0353e-01,
         -1.3672e-01, -2.1681e-02,  1.3253e-01, -1.4582e-01,  1.2365e-01,
         -1.5047e-02, -1.0333e-02, -1.1006e-01,  9.3942e-02, -2.5155e-02,
          3.4494e-02, -1.1611e-01, -6.8593e-02, -1.3953e-01,  7.3057e-03,
          1.6727e-02, -8.0383e-02,  1.3828e-01,  5.4163e-02, -1.1114e-01,
         -1.3767e-01, -8.6445e-02, -6.3683e-02,  7.6838e-04, -2.6727e-02,
          6.0289e-02,  3.2895e-02, -7.3711e-02,  1.4332e-01, -3.0252e-02,
         -1.4331e-01, -4.5512e-02,  1.2238e-01, -8.8810e-02, -1.1547e-01,
          1.1627e-01, -1.0778e-01, -6.0068e-04,  1.3399e-01,  9.5901e-02,
         -1.3756e-01, -1.9533e-02, -7.9085e-02,  6.2420e-02, -2.7271e-02,
          1.1250e-01,  1.5010e-01, -4.6734e-02, -9.8034e-02, -3.9884e-02,
         -4.1871e-03,  3.5833e-02, -1.0568e-01, -1.4801e-02,  5.3514e-02,
         -3.6051e-02,  7.0837e-02,  1.0268e-01,  1.2780e-01,  9.6226e-02,
         -4.4475e-02]], device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0370, -0.0050,  0.0052,  ...,  0.0348,  0.0354,  0.0509],
        [ 0.1189,  0.0099, -0.0887,  ..., -0.0100,  0.1162,  0.1102],
        [-0.0330, -0.0911, -0.0931,  ..., -0.0212,  0.0092,  0.0391],
        ...,
        [ 0.0812,  0.0989, -0.0511,  ..., -0.0575, -0.0418,  0.0142],
        [ 0.0910,  0.0722,  0.1122,  ...,  0.0070,  0.1047,  0.0245],
        [-0.0357, -0.0586, -0.0530,  ...,  0.0034,  0.0920, -0.0061]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0370, -0.0050,  0.0052,  ...,  0.0348,  0.0354,  0.0509],
        [ 0.1189,  0.0099, -0.0887,  ..., -0.0100,  0.1162,  0.1102],
        [-0.0330, -0.0911, -0.0931,  ..., -0.0212,  0.0092,  0.0391],
        ...,
        [ 0.0812,  0.0989, -0.0511,  ..., -0.0575, -0.0418,  0.0142],
        [ 0.0910,  0.0722,  0.1122,  ...,  0.0070,  0.1047,  0.0245],
        [-0.0357, -0.0586, -0.0530,  ...,  0.0034,  0.0920, -0.0061]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.1465, -0.0532,  0.1627,  ..., -0.0137, -0.0602, -0.1436],
        [-0.1230,  0.0702, -0.1173,  ..., -0.1498,  0.1496, -0.0731],
        [-0.0024, -0.1456, -0.0092,  ..., -0.0716,  0.0745,  0.0931],
        ...,
        [-0.0094, -0.0517, -0.1579,  ..., -0.0240, -0.1030, -0.1456],
        [-0.0122,  0.1036,  0.1762,  ..., -0.0271,  0.0632,  0.1223],
        [-0.0105, -0.1543,  0.0890,  ..., -0.0768,  0.1564,  0.1759]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1465, -0.0532,  0.1627,  ..., -0.0137, -0.0602, -0.1436],
        [-0.1230,  0.0702, -0.1173,  ..., -0.1498,  0.1496, -0.0731],
        [-0.0024, -0.1456, -0.0092,  ..., -0.0716,  0.0745,  0.0931],
        ...,
        [-0.0094, -0.0517, -0.1579,  ..., -0.0240, -0.1030, -0.1456],
        [-0.0122,  0.1036,  0.1762,  ..., -0.0271,  0.0632,  0.1223],
        [-0.0105, -0.1543,  0.0890,  ..., -0.0768,  0.1564,  0.1759]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.1620,  0.1240, -0.2049,  ...,  0.0466,  0.2288,  0.0541],
        [-0.0745,  0.0239, -0.0637,  ...,  0.1871, -0.0266,  0.2264],
        [ 0.0033, -0.2330, -0.1349,  ...,  0.0487,  0.0483, -0.1766],
        ...,
        [-0.2286,  0.0369, -0.1212,  ..., -0.0887, -0.0689,  0.0384],
        [-0.0432,  0.1345,  0.1238,  ...,  0.0310,  0.1996, -0.0177],
        [ 0.1605, -0.0008,  0.1762,  ...,  0.0365,  0.2469,  0.0805]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1620,  0.1240, -0.2049,  ...,  0.0466,  0.2288,  0.0541],
        [-0.0745,  0.0239, -0.0637,  ...,  0.1871, -0.0266,  0.2264],
        [ 0.0033, -0.2330, -0.1349,  ...,  0.0487,  0.0483, -0.1766],
        ...,
        [-0.2286,  0.0369, -0.1212,  ..., -0.0887, -0.0689,  0.0384],
        [-0.0432,  0.1345,  0.1238,  ...,  0.0310,  0.1996, -0.0177],
        [ 0.1605, -0.0008,  0.1762,  ...,  0.0365,  0.2469,  0.0805]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.2522],
        [-0.3478],
        [ 0.2031],
        [-0.4103],
        [-0.0756],
        [ 0.4067],
        [ 0.2087],
        [-0.2146],
        [-0.2571],
        [ 0.0234],
        [-0.0917],
        [ 0.1798],
        [-0.1853],
        [ 0.4071],
        [ 0.1008],
        [-0.4250],
        [-0.1829],
        [ 0.3174],
        [ 0.1074],
        [ 0.3272],
        [ 0.4223],
        [ 0.2850],
        [-0.0774],
        [ 0.3673],
        [-0.1057],
        [-0.0273],
        [-0.3826],
        [-0.2669],
        [ 0.3392],
        [-0.1480],
        [-0.2400],
        [-0.1149]], device='cuda:0') 
 Parameter containing:
tensor([[-0.2522],
        [-0.3478],
        [ 0.2031],
        [-0.4103],
        [-0.0756],
        [ 0.4067],
        [ 0.2087],
        [-0.2146],
        [-0.2571],
        [ 0.0234],
        [-0.0917],
        [ 0.1798],
        [-0.1853],
        [ 0.4071],
        [ 0.1008],
        [-0.4250],
        [-0.1829],
        [ 0.3174],
        [ 0.1074],
        [ 0.3272],
        [ 0.4223],
        [ 0.2850],
        [-0.0774],
        [ 0.3673],
        [-0.1057],
        [-0.0273],
        [-0.3826],
        [-0.2669],
        [ 0.3392],
        [-0.1480],
        [-0.2400],
        [-0.1149]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(10.4432, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(3.2740, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(3.3602, device='cuda:0')



h[100].sum tensor(0.0231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0.0237, device='cuda:0')



h[200].sum tensor(1.3403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(1.3756, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(2858.9158, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0016, 0.0019, 0.0000,  ..., 0.0014, 0.0000, 0.0003],
        [0.0086, 0.0102, 0.0000,  ..., 0.0073, 0.0000, 0.0015],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(16405.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(334.1547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(26.7315, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(98.9158, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(7.9130, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-12.5387, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0209],
        [0.0256],
        [0.0369],
        ...,
        [0.0059],
        [0.0059],
        [0.0047]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(527.1641, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[0.0209],
        [0.0256],
        [0.0369],
        ...,
        [0.0059],
        [0.0059],
        [0.0047]], device='cuda:0', grad_fn=<ReluBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(112.3486, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-15.6062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(-15.4798, device='cuda:0')



h[100].sum tensor(-4.0179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(-3.9853, device='cuda:0')



h[200].sum tensor(-5.2944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(-5.2515, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(15380.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0140,  ..., 0.0301, 0.0000, 0.0080],
        [0.0000, 0.0000, 0.0029,  ..., 0.0063, 0.0000, 0.0017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(98689.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-173.0770, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-60.1572, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(166.9307, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(11.7443, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0572],
        [0.0351],
        [0.0215],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(3357.8384, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[0.0209],
        [0.0256],
        [0.0369],
        ...,
        [0.0059],
        [0.0059],
        [0.0047]], device='cuda:0', grad_fn=<ReluBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]
=> loading checkpoint from /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/saved_checkpoint.pth.tar



load_model True 
TraEvN 1998 
BatchSize 5 
EpochNum 2 
epoch_save 5 
LrVal 0.0001 
weight_decay 5e-05 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.0333,  0.1189,  0.0852,  0.1029, -0.0396,  0.0802, -0.1099, -0.0570,
         -0.0195,  0.0929, -0.1255,  0.0683,  0.0826,  0.0601,  0.0515, -0.0191,
          0.1339, -0.0391, -0.0922, -0.1408,  0.0193,  0.0141, -0.0017,  0.0040,
          0.0841,  0.0188, -0.0174, -0.0586, -0.0512, -0.0611, -0.1157, -0.1381,
         -0.1479, -0.0785,  0.1087, -0.0810, -0.1153,  0.1369, -0.0808,  0.0002,
          0.0429,  0.1261,  0.1207, -0.1513,  0.0020,  0.1209,  0.1127,  0.0837,
          0.1341,  0.1017,  0.0408,  0.0199,  0.0067, -0.0680, -0.0990,  0.0142,
         -0.1019, -0.1031,  0.0735,  0.0620,  0.1143,  0.1380, -0.1097,  0.0533,
         -0.0174, -0.0680, -0.0647, -0.0280, -0.1246, -0.0683, -0.1098, -0.0228,
         -0.1208, -0.0773,  0.0953, -0.0375,  0.0276, -0.0249,  0.1355,  0.0592,
         -0.0236,  0.0031,  0.1377, -0.0354,  0.1315,  0.0961,  0.0445,  0.0931,
          0.0313, -0.1245,  0.0971,  0.0051,  0.0470, -0.1456,  0.0623,  0.0624,
          0.1078, -0.0973,  0.1375, -0.0752,  0.0166,  0.0859, -0.0758,  0.0914,
         -0.1043,  0.1166,  0.1518, -0.0064, -0.0164, -0.1103,  0.1428, -0.1279,
         -0.1036,  0.0791, -0.1304,  0.0259, -0.1181,  0.0752, -0.1512,  0.1083,
          0.0252, -0.0218, -0.0380,  0.0805, -0.1309,  0.0607, -0.0092,  0.1283,
          0.0979,  0.0387, -0.1189,  0.1409,  0.0909, -0.0267,  0.0698, -0.1286,
          0.1516, -0.1238, -0.1421,  0.1453, -0.0626,  0.0258, -0.0918, -0.0816,
          0.0523,  0.1050, -0.1129,  0.0397,  0.1026,  0.0243,  0.0626,  0.0128,
         -0.0604,  0.0637,  0.0521,  0.0661,  0.0241,  0.1181, -0.0590, -0.1064,
          0.1272, -0.1437,  0.1073,  0.0792,  0.0493,  0.1429,  0.1122,  0.1227,
         -0.0679, -0.0588,  0.0747, -0.0625,  0.0911, -0.0269,  0.0304,  0.0090,
          0.1134, -0.0237,  0.1347,  0.0705,  0.0242, -0.1015,  0.0677,  0.1275,
         -0.0583,  0.0154, -0.1486, -0.0004, -0.1272, -0.1498,  0.0032,  0.0402,
          0.0014, -0.0876,  0.1294, -0.0175, -0.1026,  0.0725, -0.0860,  0.0250,
          0.0334, -0.0683, -0.1059, -0.0671, -0.1302, -0.0903, -0.0477,  0.1159,
         -0.0937, -0.0770, -0.0691,  0.1049, -0.0370,  0.0671, -0.1418, -0.1441,
          0.0092,  0.0661, -0.0700, -0.0824,  0.1423, -0.0340, -0.0881, -0.0014,
          0.1217,  0.0391,  0.0620,  0.0552, -0.1151, -0.0713, -0.0417,  0.0952,
          0.1006,  0.0784,  0.0025, -0.1275,  0.0743, -0.1528, -0.0520,  0.1310,
          0.1162,  0.0156,  0.0626, -0.0810, -0.0101, -0.1151,  0.1077, -0.1082,
         -0.0224,  0.0240,  0.0459, -0.0308, -0.0527,  0.1148,  0.1030,  0.1105]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0659, -0.0035,  0.1202,  ..., -0.0062,  0.1244,  0.0828],
        [ 0.0170,  0.0311, -0.0383,  ...,  0.0658, -0.0169, -0.1019],
        [-0.0943,  0.0645,  0.0132,  ...,  0.1248, -0.0522,  0.1077],
        ...,
        [ 0.0512, -0.0966,  0.0234,  ..., -0.0599,  0.0682,  0.0674],
        [ 0.0556, -0.1048, -0.1007,  ...,  0.0381, -0.1161,  0.1045],
        [-0.0324,  0.1121,  0.1241,  ..., -0.0553, -0.0183, -0.1009]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1109, -0.0636, -0.0047,  ...,  0.0591, -0.0413,  0.0326],
        [-0.1514, -0.0516, -0.1396,  ..., -0.0372, -0.0051, -0.1551],
        [ 0.1405, -0.1682,  0.0980,  ..., -0.0075,  0.1311, -0.1112],
        ...,
        [ 0.0428, -0.0675,  0.0396,  ..., -0.0397, -0.0721, -0.0117],
        [-0.0742,  0.1222, -0.0438,  ...,  0.1687,  0.0382,  0.0595],
        [ 0.0974,  0.0246, -0.1531,  ...,  0.1450,  0.0414, -0.0990]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0618,  0.2068, -0.2283,  ...,  0.0588, -0.2283, -0.1271],
        [ 0.0446, -0.1056,  0.0597,  ..., -0.1907, -0.0333, -0.2397],
        [-0.0568, -0.1511, -0.1832,  ...,  0.0175, -0.0020, -0.1857],
        ...,
        [ 0.0385, -0.0152, -0.1747,  ...,  0.0537,  0.0631, -0.1039],
        [ 0.2297, -0.2158,  0.0996,  ...,  0.0412, -0.0184, -0.2048],
        [ 0.0619,  0.2073,  0.0188,  ..., -0.0706, -0.0997,  0.1064]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1578],
        [ 0.3258],
        [-0.2485],
        [ 0.3051],
        [ 0.3718],
        [-0.3557],
        [-0.1657],
        [ 0.0119],
        [-0.1382],
        [-0.1053],
        [ 0.1298],
        [-0.3583],
        [ 0.3528],
        [-0.2261],
        [-0.2761],
        [ 0.3954],
        [-0.0902],
        [-0.2065],
        [-0.1576],
        [ 0.1105],
        [ 0.1392],
        [-0.2798],
        [ 0.0187],
        [-0.4256],
        [-0.1466],
        [-0.2664],
        [ 0.2699],
        [ 0.4095],
        [-0.1622],
        [ 0.0230],
        [ 0.0107],
        [-0.2747]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.0333,  0.1189,  0.0852,  0.1029, -0.0396,  0.0802, -0.1099, -0.0570,
         -0.0195,  0.0929, -0.1255,  0.0683,  0.0826,  0.0601,  0.0515, -0.0191,
          0.1339, -0.0391, -0.0922, -0.1408,  0.0193,  0.0141, -0.0017,  0.0040,
          0.0841,  0.0188, -0.0174, -0.0586, -0.0512, -0.0611, -0.1157, -0.1381,
         -0.1479, -0.0785,  0.1087, -0.0810, -0.1153,  0.1369, -0.0808,  0.0002,
          0.0429,  0.1261,  0.1207, -0.1513,  0.0020,  0.1209,  0.1127,  0.0837,
          0.1341,  0.1017,  0.0408,  0.0199,  0.0067, -0.0680, -0.0990,  0.0142,
         -0.1019, -0.1031,  0.0735,  0.0620,  0.1143,  0.1380, -0.1097,  0.0533,
         -0.0174, -0.0680, -0.0647, -0.0280, -0.1246, -0.0683, -0.1098, -0.0228,
         -0.1208, -0.0773,  0.0953, -0.0375,  0.0276, -0.0249,  0.1355,  0.0592,
         -0.0236,  0.0031,  0.1377, -0.0354,  0.1315,  0.0961,  0.0445,  0.0931,
          0.0313, -0.1245,  0.0971,  0.0051,  0.0470, -0.1456,  0.0623,  0.0624,
          0.1078, -0.0973,  0.1375, -0.0752,  0.0166,  0.0859, -0.0758,  0.0914,
         -0.1043,  0.1166,  0.1518, -0.0064, -0.0164, -0.1103,  0.1428, -0.1279,
         -0.1036,  0.0791, -0.1304,  0.0259, -0.1181,  0.0752, -0.1512,  0.1083,
          0.0252, -0.0218, -0.0380,  0.0805, -0.1309,  0.0607, -0.0092,  0.1283,
          0.0979,  0.0387, -0.1189,  0.1409,  0.0909, -0.0267,  0.0698, -0.1286,
          0.1516, -0.1238, -0.1421,  0.1453, -0.0626,  0.0258, -0.0918, -0.0816,
          0.0523,  0.1050, -0.1129,  0.0397,  0.1026,  0.0243,  0.0626,  0.0128,
         -0.0604,  0.0637,  0.0521,  0.0661,  0.0241,  0.1181, -0.0590, -0.1064,
          0.1272, -0.1437,  0.1073,  0.0792,  0.0493,  0.1429,  0.1122,  0.1227,
         -0.0679, -0.0588,  0.0747, -0.0625,  0.0911, -0.0269,  0.0304,  0.0090,
          0.1134, -0.0237,  0.1347,  0.0705,  0.0242, -0.1015,  0.0677,  0.1275,
         -0.0583,  0.0154, -0.1486, -0.0004, -0.1272, -0.1498,  0.0032,  0.0402,
          0.0014, -0.0876,  0.1294, -0.0175, -0.1026,  0.0725, -0.0860,  0.0250,
          0.0334, -0.0683, -0.1059, -0.0671, -0.1302, -0.0903, -0.0477,  0.1159,
         -0.0937, -0.0770, -0.0691,  0.1049, -0.0370,  0.0671, -0.1418, -0.1441,
          0.0092,  0.0661, -0.0700, -0.0824,  0.1423, -0.0340, -0.0881, -0.0014,
          0.1217,  0.0391,  0.0620,  0.0552, -0.1151, -0.0713, -0.0417,  0.0952,
          0.1006,  0.0784,  0.0025, -0.1275,  0.0743, -0.1528, -0.0520,  0.1310,
          0.1162,  0.0156,  0.0626, -0.0810, -0.0101, -0.1151,  0.1077, -0.1082,
         -0.0224,  0.0240,  0.0459, -0.0308, -0.0527,  0.1148,  0.1030,  0.1105]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0659, -0.0035,  0.1202,  ..., -0.0062,  0.1244,  0.0828],
        [ 0.0170,  0.0311, -0.0383,  ...,  0.0658, -0.0169, -0.1019],
        [-0.0943,  0.0645,  0.0132,  ...,  0.1248, -0.0522,  0.1077],
        ...,
        [ 0.0512, -0.0966,  0.0234,  ..., -0.0599,  0.0682,  0.0674],
        [ 0.0556, -0.1048, -0.1007,  ...,  0.0381, -0.1161,  0.1045],
        [-0.0324,  0.1121,  0.1241,  ..., -0.0553, -0.0183, -0.1009]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1109, -0.0636, -0.0047,  ...,  0.0591, -0.0413,  0.0326],
        [-0.1514, -0.0516, -0.1396,  ..., -0.0372, -0.0051, -0.1551],
        [ 0.1405, -0.1682,  0.0980,  ..., -0.0075,  0.1311, -0.1112],
        ...,
        [ 0.0428, -0.0675,  0.0396,  ..., -0.0397, -0.0721, -0.0117],
        [-0.0742,  0.1222, -0.0438,  ...,  0.1687,  0.0382,  0.0595],
        [ 0.0974,  0.0246, -0.1531,  ...,  0.1450,  0.0414, -0.0990]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0618,  0.2068, -0.2283,  ...,  0.0588, -0.2283, -0.1271],
        [ 0.0446, -0.1056,  0.0597,  ..., -0.1907, -0.0333, -0.2397],
        [-0.0568, -0.1511, -0.1832,  ...,  0.0175, -0.0020, -0.1857],
        ...,
        [ 0.0385, -0.0152, -0.1747,  ...,  0.0537,  0.0631, -0.1039],
        [ 0.2297, -0.2158,  0.0996,  ...,  0.0412, -0.0184, -0.2048],
        [ 0.0619,  0.2073,  0.0188,  ..., -0.0706, -0.0997,  0.1064]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.1578],
        [ 0.3258],
        [-0.2485],
        [ 0.3051],
        [ 0.3718],
        [-0.3557],
        [-0.1657],
        [ 0.0119],
        [-0.1382],
        [-0.1053],
        [ 0.1298],
        [-0.3583],
        [ 0.3528],
        [-0.2261],
        [-0.2761],
        [ 0.3954],
        [-0.0902],
        [-0.2065],
        [-0.1576],
        [ 0.1105],
        [ 0.1392],
        [-0.2798],
        [ 0.0187],
        [-0.4256],
        [-0.1466],
        [-0.2664],
        [ 0.2699],
        [ 0.4095],
        [-0.1622],
        [ 0.0230],
        [ 0.0107],
        [-0.2747]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.0001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 5e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0.0049, 0.0174, 0.0125,  ..., 0.0168, 0.0151, 0.0162],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(333.8151, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(9.7553, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.7751, device='cuda:0')



h[100].sum tensor(4.8514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5198, device='cuda:0')



h[200].sum tensor(9.7719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.1971, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0185, 0.0659, 0.0472,  ..., 0.0637, 0.0571, 0.0613],
        [0.0152, 0.0543, 0.0388,  ..., 0.0524, 0.0470, 0.0504],
        [0.0036, 0.0127, 0.0091,  ..., 0.0123, 0.0110, 0.0118],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31990.1426, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2189, 0.0000,  ..., 0.0667, 0.0000, 0.1005],
        [0.0000, 0.1876, 0.0000,  ..., 0.0571, 0.0000, 0.0861],
        [0.0000, 0.1506, 0.0000,  ..., 0.0459, 0.0000, 0.0691],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(155098.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-154.2490, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(30.9205, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-235.2440, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[2.3270e-01],
        [2.5179e-01],
        [2.7784e-01],
        ...,
        [2.7955e-07],
        [3.6729e-08],
        [0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(4336.8042, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365930., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(67.0317, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [1.0001],
        [0.9999],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365923.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.0000e-04, -1.0000e-04,  1.0000e-04,  ...,  1.0000e-04,
         -1.0000e-04, -1.0000e-04],
        [ 1.0000e-04, -1.0000e-04,  1.0000e-04,  ...,  1.0000e-04,
         -1.0000e-04, -1.0000e-04],
        [ 1.0000e-04, -1.0000e-04,  1.0000e-04,  ...,  1.0000e-04,
         -1.0000e-04, -1.0000e-04],
        ...,
        [ 1.0000e-04, -1.0000e-04,  1.0000e-04,  ...,  1.0000e-04,
         -1.0000e-04, -1.0000e-04],
        [ 1.0000e-04, -1.0000e-04,  1.0000e-04,  ...,  1.0000e-04,
         -1.0000e-04, -1.0000e-04],
        [ 1.0000e-04, -1.0000e-04,  1.0000e-04,  ...,  1.0000e-04,
         -1.0000e-04, -1.0000e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(321.3105, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(12.4231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.6311, device='cuda:0')



h[100].sum tensor(7.8998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8376, device='cuda:0')



h[200].sum tensor(5.5885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.7790, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0004, 0.0000, 0.0004,  ..., 0.0004, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0004,  ..., 0.0004, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0004,  ..., 0.0004, 0.0000, 0.0000],
        ...,
        [0.0004, 0.0000, 0.0004,  ..., 0.0004, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0004,  ..., 0.0004, 0.0000, 0.0000],
        [0.0004, 0.0000, 0.0004,  ..., 0.0004, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32659.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[9.6605e-05, 0.0000e+00, 4.3556e-04,  ..., 0.0000e+00, 0.0000e+00,
         2.6317e-03],
        [9.6601e-05, 0.0000e+00, 4.3554e-04,  ..., 0.0000e+00, 0.0000e+00,
         2.6316e-03],
        [6.9445e-05, 0.0000e+00, 3.1311e-04,  ..., 0.0000e+00, 0.0000e+00,
         2.7375e-03],
        ...,
        [9.5805e-05, 0.0000e+00, 4.3195e-04,  ..., 0.0000e+00, 0.0000e+00,
         2.6131e-03],
        [9.5805e-05, 0.0000e+00, 4.3195e-04,  ..., 0.0000e+00, 0.0000e+00,
         2.6131e-03],
        [9.5805e-05, 0.0000e+00, 4.3195e-04,  ..., 0.0000e+00, 0.0000e+00,
         2.6131e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(172951.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(13.7316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-164.6914, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(17.4235, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-246.4464, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0371],
        [0.0496],
        [0.0685],
        ...,
        [0.0133],
        [0.0132],
        [0.0132]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(29235.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [1.0001],
        [0.9999],
        ...,
        [0.9999],
        [0.9999],
        [0.9999]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365923.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [1.0002],
        [0.9999],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365913.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 4.4247e-05, -4.8142e-05,  3.1000e-05,  ...,  3.2876e-05,
         -4.8468e-05, -1.0884e-04],
        [ 4.4247e-05, -4.8142e-05,  3.1000e-05,  ...,  3.2876e-05,
         -4.8468e-05, -1.0884e-04],
        [ 4.4247e-05, -4.8142e-05,  3.1000e-05,  ...,  3.2876e-05,
         -4.8468e-05, -1.0884e-04],
        ...,
        [ 4.4247e-05, -4.8142e-05,  3.1000e-05,  ...,  3.2876e-05,
         -4.8468e-05, -1.0884e-04],
        [ 4.4247e-05, -4.8142e-05,  3.1000e-05,  ...,  3.2876e-05,
         -4.8468e-05, -1.0884e-04],
        [ 4.4247e-05, -4.8142e-05,  3.1000e-05,  ...,  3.2876e-05,
         -4.8468e-05, -1.0884e-04]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(212.5720, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(7.9062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5039, device='cuda:0')



h[100].sum tensor(9.9837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.5697, device='cuda:0')



h[200].sum tensor(5.1498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.7409, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0166, 0.0585, 0.0422,  ..., 0.0568, 0.0507, 0.0542],
        [0.0111, 0.0388, 0.0280,  ..., 0.0377, 0.0336, 0.0360],
        [0.0092, 0.0321, 0.0232,  ..., 0.0313, 0.0278, 0.0297],
        ...,
        [0.0002, 0.0000, 0.0001,  ..., 0.0001, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0001,  ..., 0.0001, 0.0000, 0.0000],
        [0.0002, 0.0000, 0.0001,  ..., 0.0001, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(24102.0664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1812, 0.0000,  ..., 0.0532, 0.0000, 0.0868],
        [0.0000, 0.1451, 0.0000,  ..., 0.0424, 0.0000, 0.0700],
        [0.0000, 0.1100, 0.0000,  ..., 0.0319, 0.0000, 0.0536],
        ...,
        [0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0017],
        [0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0017],
        [0.0002, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0017]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(132272.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(5.7304, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-118.7535, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1.1467, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(36.0977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-173.3210, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.1974],
        [0.1835],
        [0.1602],
        ...,
        [0.0003],
        [0.0003],
        [0.0003]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(11759.7324, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [1.0002],
        [0.9999],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365913.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0003],
        [1.0000],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365908.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 1.7843e-03,  6.4955e-03,  4.6059e-03,  ...,  6.2124e-03,
          5.6251e-03,  5.9623e-03],
        [ 4.3674e-03,  1.5725e-02,  1.1210e-02,  ...,  1.5117e-02,
          1.3618e-02,  1.4541e-02],
        [ 3.1878e-03,  1.1510e-02,  8.1940e-03,  ...,  1.1051e-02,
          9.9679e-03,  1.0623e-02],
        ...,
        [-2.9448e-05,  1.5203e-05, -3.0939e-05,  ..., -4.0056e-05,
          1.2796e-05, -6.0865e-05],
        [-2.9448e-05,  1.5203e-05, -3.0939e-05,  ..., -4.0056e-05,
          1.2796e-05, -6.0865e-05],
        [-2.9448e-05,  1.5203e-05, -3.0939e-05,  ..., -4.0056e-05,
          1.2796e-05, -6.0865e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(223.5480, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(6.0127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2430, device='cuda:0')



h[100].sum tensor(10.5969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1231, device='cuda:0')



h[200].sum tensor(8.1409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.7023, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[1.5582e-02, 5.6153e-02, 4.0012e-02,  ..., 5.3959e-02, 4.8630e-02,
         5.1892e-02],
        [1.0014e-02, 3.6260e-02, 2.5778e-02,  ..., 3.4766e-02, 3.1402e-02,
         3.3402e-02],
        [8.4186e-03, 3.0455e-02, 2.1655e-02,  ..., 2.9205e-02, 2.6374e-02,
         2.8068e-02],
        ...,
        [0.0000e+00, 6.0793e-05, 0.0000e+00,  ..., 0.0000e+00, 5.1167e-05,
         0.0000e+00],
        [0.0000e+00, 6.0793e-05, 0.0000e+00,  ..., 0.0000e+00, 5.1167e-05,
         0.0000e+00],
        [0.0000e+00, 6.0793e-05, 0.0000e+00,  ..., 0.0000e+00, 5.1167e-05,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(24332.0977, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.2445e-01, 0.0000e+00,  ..., 3.9189e-02, 0.0000e+00,
         5.6339e-02],
        [0.0000e+00, 1.1117e-01, 0.0000e+00,  ..., 3.5048e-02, 0.0000e+00,
         5.0378e-02],
        [0.0000e+00, 9.5439e-02, 0.0000e+00,  ..., 3.0060e-02, 0.0000e+00,
         4.3384e-02],
        ...,
        [1.1588e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         5.1534e-04],
        [1.1589e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         5.1533e-04],
        [1.1589e-04, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         5.1533e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(127184.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0.5732, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-113.8974, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-4.2425, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(92.1250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-169.9001, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0311],
        [0.0293],
        [0.0255],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(269.9002, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0003],
        [1.0000],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365908.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [1.0003],
        [1.0000],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365903.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.4415e-05,  5.9415e-05, -8.4129e-05,  ..., -9.5785e-05,
          4.3419e-05, -3.5676e-05],
        [-8.4415e-05,  5.9415e-05, -8.4129e-05,  ..., -9.5785e-05,
          4.3419e-05, -3.5676e-05],
        [-8.4415e-05,  5.9415e-05, -8.4129e-05,  ..., -9.5785e-05,
          4.3419e-05, -3.5676e-05],
        ...,
        [-8.4415e-05,  5.9415e-05, -8.4129e-05,  ..., -9.5785e-05,
          4.3419e-05, -3.5676e-05],
        [-8.4415e-05,  5.9415e-05, -8.4129e-05,  ..., -9.5785e-05,
          4.3419e-05, -3.5676e-05],
        [-8.4415e-05,  5.9415e-05, -8.4129e-05,  ..., -9.5785e-05,
          4.3419e-05, -3.5676e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(185.5021, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(3.6270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9175, device='cuda:0')



h[100].sum tensor(11.2562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7013, device='cuda:0')



h[200].sum tensor(9.2204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.2075, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0002, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0002, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(22828.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.2729e-05, 0.0000e+00,
         2.5662e-04],
        [0.0000e+00, 1.1390e-03, 0.0000e+00,  ..., 4.8720e-04, 0.0000e+00,
         8.3155e-04],
        [0.0000e+00, 6.6055e-03, 0.0000e+00,  ..., 2.4181e-03, 0.0000e+00,
         3.2343e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.4625e-05, 0.0000e+00,
         2.5216e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.4631e-05, 0.0000e+00,
         2.5215e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.4633e-05, 0.0000e+00,
         2.5214e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(114968.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-100.3522, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-10.2809, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(58.8940, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-154.6106, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [1.0003],
        [1.0000],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365903.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0004],
        [1.0000],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365898.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.9461e-03,  1.1137e-02,  7.7546e-03,  ...,  1.0489e-02,
          9.6315e-03,  1.0248e-02],
        [-1.3087e-04,  9.6784e-05, -1.2909e-04,  ..., -1.4289e-04,
          6.9302e-05, -1.4386e-05],
        [-1.3087e-04,  9.6784e-05, -1.2909e-04,  ..., -1.4289e-04,
          6.9302e-05, -1.4386e-05],
        ...,
        [-1.3087e-04,  9.6784e-05, -1.2909e-04,  ..., -1.4289e-04,
          6.9302e-05, -1.4386e-05],
        [-1.3087e-04,  9.6784e-05, -1.2909e-04,  ..., -1.4289e-04,
          6.9302e-05, -1.4386e-05],
        [-1.3087e-04,  9.6784e-05, -1.2909e-04,  ..., -1.4289e-04,
          6.9302e-05, -1.4386e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(274.9189, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(4.9794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.6755, device='cuda:0')



h[100].sum tensor(13.4816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.1699, device='cuda:0')



h[200].sum tensor(13.5375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.9569, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0112, 0.0416, 0.0292,  ..., 0.0394, 0.0360, 0.0383],
        [0.0029, 0.0114, 0.0078,  ..., 0.0105, 0.0098, 0.0103],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0003, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0003, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0003, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0003, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35378.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.1446e-01, 0.0000e+00,  ..., 3.9149e-02, 0.0000e+00,
         4.7592e-02],
        [0.0000e+00, 5.0920e-02, 0.0000e+00,  ..., 1.7975e-02, 0.0000e+00,
         2.1323e-02],
        [0.0000e+00, 1.3885e-02, 0.0000e+00,  ..., 5.3820e-03, 0.0000e+00,
         5.9332e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3800e-04, 0.0000e+00,
         3.4958e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3799e-04, 0.0000e+00,
         3.4947e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3799e-04, 0.0000e+00,
         3.4946e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(187797.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-154.1315, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-2.9676, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(8.2459, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-242.2966, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0004],
        [1.0000],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365898.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0004],
        [1.0000],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365898.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-1.3087e-04,  9.6784e-05, -1.2909e-04,  ..., -1.4289e-04,
          6.9302e-05, -1.4386e-05],
        [-1.3087e-04,  9.6784e-05, -1.2909e-04,  ..., -1.4289e-04,
          6.9302e-05, -1.4386e-05],
        [-1.3087e-04,  9.6784e-05, -1.2909e-04,  ..., -1.4289e-04,
          6.9302e-05, -1.4386e-05],
        ...,
        [-1.3087e-04,  9.6784e-05, -1.2909e-04,  ..., -1.4289e-04,
          6.9302e-05, -1.4386e-05],
        [-1.3087e-04,  9.6784e-05, -1.2909e-04,  ..., -1.4289e-04,
          6.9302e-05, -1.4386e-05],
        [-1.3087e-04,  9.6784e-05, -1.2909e-04,  ..., -1.4289e-04,
          6.9302e-05, -1.4386e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(237.4439, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(3.9408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6597, device='cuda:0')



h[100].sum tensor(12.9676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2103, device='cuda:0')



h[200].sum tensor(12.4871, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.5557, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0003, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0003, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0003, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0003, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0003, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0003, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31280.4570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.9089e-02, 0.0000e+00,  ..., 7.3023e-03, 0.0000e+00,
         8.0908e-03],
        [0.0000e+00, 3.6834e-03, 0.0000e+00,  ..., 1.9798e-03, 0.0000e+00,
         1.6922e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4028e-04, 0.0000e+00,
         3.9915e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3800e-04, 0.0000e+00,
         3.4958e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3799e-04, 0.0000e+00,
         3.4947e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3799e-04, 0.0000e+00,
         3.4946e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(159346.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-134.2532, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-6.6195, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9.4778, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-212.5262, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0004],
        [1.0000],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365898.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0004],
        [1.0001],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365893.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 5.4813e-03,  2.0444e-02,  1.4327e-02,  ...,  1.9365e-02,
          1.7687e-02,  1.8887e-02],
        [-1.7092e-04,  1.2899e-04, -1.6784e-04,  ..., -1.8349e-04,
          9.1610e-05,  3.9635e-06],
        [-1.7092e-04,  1.2899e-04, -1.6784e-04,  ..., -1.8349e-04,
          9.1610e-05,  3.9635e-06],
        ...,
        [-1.7092e-04,  1.2899e-04, -1.6784e-04,  ..., -1.8349e-04,
          9.1610e-05,  3.9635e-06],
        [-1.7092e-04,  1.2899e-04, -1.6784e-04,  ..., -1.8349e-04,
          9.1610e-05,  3.9635e-06],
        [-1.7092e-04,  1.2899e-04, -1.6784e-04,  ..., -1.8349e-04,
          9.1610e-05,  3.9635e-06]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(157.5510, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0.7472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9896, device='cuda:0')



h[100].sum tensor(12.7278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7243, device='cuda:0')



h[200].sum tensor(11.7989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.2887, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[9.8131e-03, 3.7015e-02, 2.5706e-02,  ..., 3.4755e-02, 3.1980e-02,
         3.3943e-02],
        [5.4837e-03, 2.0840e-02, 1.4333e-02,  ..., 1.9374e-02, 1.7970e-02,
         1.8908e-02],
        [0.0000e+00, 5.1619e-04, 0.0000e+00,  ..., 0.0000e+00, 3.6660e-04,
         1.5861e-05],
        ...,
        [0.0000e+00, 5.1566e-04, 0.0000e+00,  ..., 0.0000e+00, 3.6622e-04,
         1.5844e-05],
        [0.0000e+00, 5.1566e-04, 0.0000e+00,  ..., 0.0000e+00, 3.6622e-04,
         1.5844e-05],
        [0.0000e+00, 5.1566e-04, 0.0000e+00,  ..., 0.0000e+00, 3.6622e-04,
         1.5844e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(25586.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1617, 0.0000,  ..., 0.0560, 0.0000, 0.0656],
        [0.0000, 0.0804, 0.0000,  ..., 0.0286, 0.0000, 0.0327],
        [0.0000, 0.0260, 0.0000,  ..., 0.0103, 0.0000, 0.0105],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(133030.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-100.5057, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-16.7584, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-166.6690, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0004],
        [1.0001],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365893.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0005],
        [1.0001],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365887.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.0589e-04,  1.5712e-04, -2.0167e-04,  ..., -2.1894e-04,
          1.1109e-04,  1.9988e-05],
        [-2.0589e-04,  1.5712e-04, -2.0167e-04,  ..., -2.1894e-04,
          1.1109e-04,  1.9988e-05],
        [-2.0589e-04,  1.5712e-04, -2.0167e-04,  ..., -2.1894e-04,
          1.1109e-04,  1.9988e-05],
        ...,
        [-2.0589e-04,  1.5712e-04, -2.0167e-04,  ..., -2.1894e-04,
          1.1109e-04,  1.9988e-05],
        [-2.0589e-04,  1.5712e-04, -2.0167e-04,  ..., -2.1894e-04,
          1.1109e-04,  1.9988e-05],
        [-2.0589e-04,  1.5712e-04, -2.0167e-04,  ..., -2.1894e-04,
          1.1109e-04,  1.9988e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(101.0741, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.6556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.8455, device='cuda:0')



h[100].sum tensor(12.7111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.7238, device='cuda:0')



h[200].sum tensor(11.5825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.7428, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[7.5993e-03, 2.9465e-02, 2.0157e-02,  ..., 2.7292e-02, 2.5421e-02,
         2.6885e-02],
        [3.7995e-03, 1.5046e-02, 1.0078e-02,  ..., 1.3645e-02, 1.2932e-02,
         1.3482e-02],
        [6.7684e-03, 2.5733e-02, 1.7697e-02,  ..., 2.3922e-02, 2.2188e-02,
         2.3415e-02],
        ...,
        [0.0000e+00, 6.2803e-04, 0.0000e+00,  ..., 0.0000e+00, 4.4406e-04,
         7.9896e-05],
        [0.0000e+00, 6.2803e-04, 0.0000e+00,  ..., 0.0000e+00, 4.4406e-04,
         7.9896e-05],
        [0.0000e+00, 6.2803e-04, 0.0000e+00,  ..., 0.0000e+00, 4.4406e-04,
         7.9896e-05]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(21962.8926, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0745, 0.0000,  ..., 0.0279, 0.0000, 0.0287],
        [0.0000, 0.0669, 0.0000,  ..., 0.0251, 0.0000, 0.0259],
        [0.0000, 0.0814, 0.0000,  ..., 0.0300, 0.0000, 0.0318],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(115793.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-76.9185, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-23.8842, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-136.6276, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0005],
        [1.0001],
        ...,
        [0.9998],
        [0.9998],
        [0.9998]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365887.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0005],
        [1.0001],
        ...,
        [0.9998],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365881.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.3671e-04,  1.8191e-04, -2.3150e-04,  ..., -2.5019e-04,
          1.2827e-04,  3.4115e-05],
        [ 2.0100e-03,  8.2794e-03,  5.5383e-03,  ...,  7.5322e-03,
          7.1420e-03,  7.5612e-03],
        [-2.3671e-04,  1.8191e-04, -2.3150e-04,  ..., -2.5019e-04,
          1.2827e-04,  3.4115e-05],
        ...,
        [-2.3671e-04,  1.8191e-04, -2.3150e-04,  ..., -2.5019e-04,
          1.2827e-04,  3.4115e-05],
        [-2.3671e-04,  1.8191e-04, -2.3150e-04,  ..., -2.5019e-04,
          1.2827e-04,  3.4115e-05],
        [-2.3671e-04,  1.8191e-04, -2.3150e-04,  ..., -2.5019e-04,
          1.2827e-04,  3.4115e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(71.7940, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.2071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.3284, device='cuda:0')



h[100].sum tensor(12.9770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.2411, device='cuda:0')



h[200].sum tensor(11.9643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.0318, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0020, 0.0088, 0.0055,  ..., 0.0075, 0.0075, 0.0077],
        [0.0029, 0.0122, 0.0079,  ..., 0.0108, 0.0104, 0.0108],
        [0.0099, 0.0398, 0.0269,  ..., 0.0366, 0.0344, 0.0365],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0005, 0.0001],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0005, 0.0001],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0005, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(21303.3008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0311, 0.0000,  ..., 0.0134, 0.0000, 0.0108],
        [0.0000, 0.0465, 0.0000,  ..., 0.0190, 0.0000, 0.0164],
        [0.0000, 0.0724, 0.0000,  ..., 0.0285, 0.0000, 0.0257],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(115364.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-68.3803, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-27.9617, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-128.1752, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0005],
        [1.0001],
        ...,
        [0.9998],
        [0.9998],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365881.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0005],
        [1.0001],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365875.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-2.6408e-04,  2.0393e-04, -2.5799e-04,  ..., -2.7794e-04,
          1.4351e-04,  4.6655e-05],
        [ 7.1542e-03,  2.6971e-02,  1.8805e-02,  ...,  2.5435e-02,
          2.3329e-02,  2.4929e-02],
        [ 7.2510e-03,  2.7321e-02,  1.9053e-02,  ...,  2.5770e-02,
          2.3631e-02,  2.5254e-02],
        ...,
        [-2.6408e-04,  2.0393e-04, -2.5799e-04,  ..., -2.7794e-04,
          1.4351e-04,  4.6655e-05],
        [-2.6408e-04,  2.0393e-04, -2.5799e-04,  ..., -2.7794e-04,
          1.4351e-04,  4.6655e-05],
        [-2.6408e-04,  2.0393e-04, -2.5799e-04,  ..., -2.7794e-04,
          1.4351e-04,  4.6655e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(143.1750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.9914, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2055, device='cuda:0')



h[100].sum tensor(14.4914, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1112, device='cuda:0')



h[200].sum tensor(14.9472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.6600, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0092, 0.0360, 0.0246,  ..., 0.0333, 0.0311, 0.0329],
        [0.0128, 0.0498, 0.0341,  ..., 0.0462, 0.0430, 0.0457],
        [0.0274, 0.1034, 0.0720,  ..., 0.0975, 0.0895, 0.0956],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0006, 0.0002],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0006, 0.0002],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0006, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28178.6133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1137, 0.0000,  ..., 0.0431, 0.0000, 0.0416],
        [0.0000, 0.1777, 0.0000,  ..., 0.0657, 0.0000, 0.0660],
        [0.0000, 0.2694, 0.0000,  ..., 0.0980, 0.0000, 0.1010],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(145245.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-96.7539, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-24.5639, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-175.2555, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0005],
        [1.0001],
        ...,
        [0.9998],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365875.7188, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 10.0 event: 50 loss: tensor(550.6757, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0006],
        [1.0001],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365869.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.3312e-03,  9.6860e-03,  6.4538e-03,  ...,  8.7827e-03,
          8.3535e-03,  8.8540e-03],
        [ 2.8834e-03,  1.1680e-02,  7.8734e-03,  ...,  1.0698e-02,
          1.0081e-02,  1.0708e-02],
        [ 4.9308e-03,  1.9076e-02,  1.3137e-02,  ...,  1.7798e-02,
          1.6487e-02,  1.7583e-02],
        ...,
        [-2.8850e-04,  2.2357e-04, -2.8162e-04,  ..., -3.0270e-04,
          1.5712e-04,  5.7848e-05],
        [-2.8850e-04,  2.2357e-04, -2.8162e-04,  ..., -3.0270e-04,
          1.5712e-04,  5.7848e-05],
        [-2.8850e-04,  2.2357e-04, -2.8162e-04,  ..., -3.0270e-04,
          1.5712e-04,  5.7848e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(244.8374, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(0.0481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.9913, device='cuda:0')



h[100].sum tensor(16.3116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5886, device='cuda:0')



h[200].sum tensor(18.5916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.4409, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0125, 0.0503, 0.0340,  ..., 0.0462, 0.0434, 0.0461],
        [0.0136, 0.0540, 0.0367,  ..., 0.0498, 0.0467, 0.0496],
        [0.0090, 0.0377, 0.0251,  ..., 0.0341, 0.0325, 0.0344],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0000, 0.0006, 0.0002],
        [0.0000, 0.0009, 0.0000,  ..., 0.0000, 0.0006, 0.0002],
        [0.0000, 0.0009, 0.0000,  ..., 0.0000, 0.0006, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36725.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0989, 0.0000,  ..., 0.0391, 0.0000, 0.0344],
        [0.0000, 0.1269, 0.0000,  ..., 0.0493, 0.0000, 0.0445],
        [0.0000, 0.1557, 0.0000,  ..., 0.0597, 0.0000, 0.0549],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0024, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(178516.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-133.7469, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-19.1289, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-235.0932, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0006],
        [1.0001],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365869.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0006],
        [1.0001],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365863.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.1039e-04,  2.4118e-04, -3.0280e-04,  ..., -3.2489e-04,
          1.6931e-04,  6.7878e-05],
        [-3.1039e-04,  2.4118e-04, -3.0280e-04,  ..., -3.2489e-04,
          1.6931e-04,  6.7878e-05],
        [-3.1039e-04,  2.4118e-04, -3.0280e-04,  ..., -3.2489e-04,
          1.6931e-04,  6.7878e-05],
        ...,
        [-3.1039e-04,  2.4118e-04, -3.0280e-04,  ..., -3.2489e-04,
          1.6931e-04,  6.7878e-05],
        [-3.1039e-04,  2.4118e-04, -3.0280e-04,  ..., -3.2489e-04,
          1.6931e-04,  6.7878e-05],
        [-3.1039e-04,  2.4118e-04, -3.0280e-04,  ..., -3.2489e-04,
          1.6931e-04,  6.7878e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(381.8137, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(3.0125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.5984, device='cuda:0')



h[100].sum tensor(18.5002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.9637, device='cuda:0')



h[200].sum tensor(23.0290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.4036, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0010, 0.0000,  ..., 0.0000, 0.0007, 0.0003],
        [0.0000, 0.0010, 0.0000,  ..., 0.0000, 0.0007, 0.0003],
        [0.0033, 0.0141, 0.0090,  ..., 0.0123, 0.0121, 0.0125],
        ...,
        [0.0000, 0.0010, 0.0000,  ..., 0.0000, 0.0007, 0.0003],
        [0.0000, 0.0010, 0.0000,  ..., 0.0000, 0.0007, 0.0003],
        [0.0000, 0.0010, 0.0000,  ..., 0.0000, 0.0007, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48530.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0021, 0.0000,  ..., 0.0035, 0.0000, 0.0006],
        [0.0000, 0.0106, 0.0000,  ..., 0.0067, 0.0000, 0.0033],
        [0.0000, 0.0349, 0.0000,  ..., 0.0159, 0.0000, 0.0114],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(236594.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-186.7311, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-10.4561, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-318.7205, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0006],
        [1.0001],
        ...,
        [0.9997],
        [0.9997],
        [0.9997]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365863.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0006],
        [1.0001],
        ...,
        [0.9997],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365856.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 7.1085e-03,  2.7172e-02,  1.8821e-02,  ...,  2.5478e-02,
          2.3495e-02,  2.5097e-02],
        [ 1.0485e-02,  3.9389e-02,  2.7509e-02,  ...,  3.7198e-02,
          3.4077e-02,  3.6454e-02],
        [ 7.4010e-03,  2.8231e-02,  1.9573e-02,  ...,  2.6493e-02,
          2.4412e-02,  2.6081e-02],
        ...,
        [-3.3007e-04,  2.5701e-04, -3.2185e-04,  ..., -3.4485e-04,
          1.8028e-04,  7.6897e-05],
        [-3.3007e-04,  2.5701e-04, -3.2185e-04,  ..., -3.4485e-04,
          1.8028e-04,  7.6897e-05],
        [-3.3007e-04,  2.5701e-04, -3.2185e-04,  ..., -3.4485e-04,
          1.8028e-04,  7.6897e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(237.0792, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.3163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5006, device='cuda:0')



h[100].sum tensor(17.0261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7506, device='cuda:0')



h[200].sum tensor(19.8606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0153, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0320, 0.1217, 0.0846,  ..., 0.1144, 0.1053, 0.1125],
        [0.0376, 0.1420, 0.0990,  ..., 0.1338, 0.1228, 0.1313],
        [0.0310, 0.1179, 0.0818,  ..., 0.1107, 0.1019, 0.1089],
        ...,
        [0.0000, 0.0010, 0.0000,  ..., 0.0000, 0.0007, 0.0003],
        [0.0000, 0.0010, 0.0000,  ..., 0.0000, 0.0007, 0.0003],
        [0.0000, 0.0010, 0.0000,  ..., 0.0000, 0.0007, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38780.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2922, 0.0000,  ..., 0.1092, 0.0000, 0.1048],
        [0.0000, 0.3555, 0.0000,  ..., 0.1321, 0.0000, 0.1280],
        [0.0000, 0.3436, 0.0000,  ..., 0.1281, 0.0000, 0.1234],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0029, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(194287.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-136.4774, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-21.6904, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-245.4977, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0006],
        [1.0001],
        ...,
        [0.9997],
        [0.9997],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365856.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0006],
        [1.0001],
        ...,
        [0.9997],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365850.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-3.4782e-04,  2.7128e-04, -3.3902e-04,  ..., -3.6283e-04,
          1.9016e-04,  8.5027e-05],
        [-3.4782e-04,  2.7128e-04, -3.3902e-04,  ..., -3.6283e-04,
          1.9016e-04,  8.5027e-05],
        [-3.4782e-04,  2.7128e-04, -3.3902e-04,  ..., -3.6283e-04,
          1.9016e-04,  8.5027e-05],
        ...,
        [-3.4782e-04,  2.7128e-04, -3.3902e-04,  ..., -3.6283e-04,
          1.9016e-04,  8.5027e-05],
        [-3.4782e-04,  2.7128e-04, -3.3902e-04,  ..., -3.6283e-04,
          1.9016e-04,  8.5027e-05],
        [-3.4782e-04,  2.7128e-04, -3.3902e-04,  ..., -3.6283e-04,
          1.9016e-04,  8.5027e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(219.4394, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.2786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0809, device='cuda:0')



h[100].sum tensor(17.1442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2989, device='cuda:0')



h[200].sum tensor(20.0125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.4142, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0011, 0.0000,  ..., 0.0000, 0.0008, 0.0003],
        [0.0000, 0.0011, 0.0000,  ..., 0.0000, 0.0008, 0.0003],
        [0.0000, 0.0011, 0.0000,  ..., 0.0000, 0.0008, 0.0003],
        ...,
        [0.0000, 0.0011, 0.0000,  ..., 0.0000, 0.0008, 0.0003],
        [0.0000, 0.0011, 0.0000,  ..., 0.0000, 0.0008, 0.0003],
        [0.0000, 0.0011, 0.0000,  ..., 0.0000, 0.0008, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38819.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(202222.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-133.5446, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-23.5056, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-243.8999, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0006],
        [1.0001],
        ...,
        [0.9997],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365850.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [1.0001],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365843.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.9007e-03,  1.2113e-02,  8.0527e-03,  ...,  1.0963e-02,
          1.0446e-02,  1.1089e-02],
        [-3.6384e-04,  2.8417e-04, -3.5453e-04,  ..., -3.7908e-04,
          1.9909e-04,  9.2372e-05],
        [ 5.1903e-03,  2.0410e-02,  1.3949e-02,  ...,  1.8917e-02,
          1.7632e-02,  1.8801e-02],
        ...,
        [-3.6384e-04,  2.8417e-04, -3.5453e-04,  ..., -3.7908e-04,
          1.9909e-04,  9.2372e-05],
        [-3.6384e-04,  2.8417e-04, -3.5453e-04,  ..., -3.7908e-04,
          1.9909e-04,  9.2372e-05],
        [-3.6384e-04,  2.8417e-04, -3.5453e-04,  ..., -3.7908e-04,
          1.9909e-04,  9.2372e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(177.3990, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.8117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.6651, device='cuda:0')



h[100].sum tensor(16.9250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5302, device='cuda:0')



h[200].sum tensor(19.4672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.6895, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0023, 0.0108, 0.0065,  ..., 0.0089, 0.0092, 0.0094],
        [0.0127, 0.0524, 0.0350,  ..., 0.0477, 0.0452, 0.0481],
        [0.0068, 0.0298, 0.0193,  ..., 0.0264, 0.0256, 0.0270],
        ...,
        [0.0000, 0.0011, 0.0000,  ..., 0.0000, 0.0008, 0.0004],
        [0.0000, 0.0011, 0.0000,  ..., 0.0000, 0.0008, 0.0004],
        [0.0000, 0.0011, 0.0000,  ..., 0.0000, 0.0008, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36611.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0460, 0.0000,  ..., 0.0211, 0.0000, 0.0139],
        [0.0000, 0.0942, 0.0000,  ..., 0.0396, 0.0000, 0.0295],
        [0.0000, 0.0968, 0.0000,  ..., 0.0409, 0.0000, 0.0297],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(191618.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-119.7860, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-27.9555, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-225.2127, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [1.0001],
        ...,
        [0.9996],
        [0.9996],
        [0.9996]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365843.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [1.0001],
        ...,
        [0.9996],
        [0.9996],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365836.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 2.5109e-03,  1.0772e-02,  7.0746e-03,  ...,  9.6475e-03,
          9.2816e-03,  9.8374e-03],
        [-3.7835e-04,  2.9584e-04, -3.6856e-04,  ..., -3.9379e-04,
          2.0717e-04,  9.9019e-05],
        [ 2.5109e-03,  1.0772e-02,  7.0746e-03,  ...,  9.6475e-03,
          9.2816e-03,  9.8374e-03],
        ...,
        [-3.7835e-04,  2.9584e-04, -3.6856e-04,  ..., -3.9379e-04,
          2.0717e-04,  9.9019e-05],
        [-3.7835e-04,  2.9584e-04, -3.6856e-04,  ..., -3.9379e-04,
          2.0717e-04,  9.9019e-05],
        [-3.7835e-04,  2.9584e-04, -3.6856e-04,  ..., -3.9379e-04,
          2.0717e-04,  9.9019e-05]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(96.1353, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.2753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2456, device='cuda:0')



h[100].sum tensor(16.1987, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8057, device='cuda:0')



h[200].sum tensor(17.8656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.5774, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0020, 0.0097, 0.0057,  ..., 0.0078, 0.0082, 0.0084],
        [0.0090, 0.0393, 0.0256,  ..., 0.0349, 0.0338, 0.0358],
        [0.0020, 0.0097, 0.0057,  ..., 0.0078, 0.0082, 0.0084],
        ...,
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0008, 0.0004],
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0008, 0.0004],
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0008, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29150.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0312, 0.0000,  ..., 0.0158, 0.0000, 0.0087],
        [0.0000, 0.0537, 0.0000,  ..., 0.0245, 0.0000, 0.0157],
        [0.0000, 0.0312, 0.0000,  ..., 0.0158, 0.0000, 0.0087],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(153920.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.8065, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.6809, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-170.3673, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [1.0001],
        ...,
        [0.9996],
        [0.9996],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365836.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [1.0001],
        ...,
        [0.9996],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365830., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0059,  0.0230,  0.0158,  ...,  0.0214,  0.0199,  0.0212],
        [ 0.0054,  0.0213,  0.0145,  ...,  0.0197,  0.0184,  0.0196],
        [ 0.0085,  0.0327,  0.0226,  ...,  0.0306,  0.0282,  0.0302],
        ...,
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(145.1648, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.4278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9276, device='cuda:0')



h[100].sum tensor(17.0530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9773, device='cuda:0')



h[200].sum tensor(19.5801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.7300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0234, 0.0918, 0.0628,  ..., 0.0852, 0.0793, 0.0846],
        [0.0308, 0.1187, 0.0819,  ..., 0.1109, 0.1026, 0.1096],
        [0.0375, 0.1428, 0.0991,  ..., 0.1341, 0.1235, 0.1321],
        ...,
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0009, 0.0004],
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0009, 0.0004],
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0009, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32556.8008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.7855e-01, 0.0000e+00,  ..., 1.0835e-01, 0.0000e+00,
         9.3735e-02],
        [0.0000e+00, 3.3344e-01, 0.0000e+00,  ..., 1.2828e-01, 0.0000e+00,
         1.1350e-01],
        [0.0000e+00, 3.9627e-01, 0.0000e+00,  ..., 1.5101e-01, 0.0000e+00,
         1.3609e-01],
        ...,
        [0.0000e+00, 4.8003e-06, 0.0000e+00,  ..., 3.6682e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 4.8463e-06, 0.0000e+00,  ..., 3.6680e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 4.8702e-06, 0.0000e+00,  ..., 3.6679e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(165898.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-95.7844, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-34.0660, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-193.5813, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [1.0001],
        ...,
        [0.9996],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365830., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [1.0001],
        ...,
        [0.9996],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365823., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0018,  0.0083,  0.0053,  ...,  0.0072,  0.0072,  0.0075],
        [ 0.0017,  0.0078,  0.0049,  ...,  0.0067,  0.0067,  0.0071],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        ...,
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(327.2500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.2918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.2407, device='cuda:0')



h[100].sum tensor(19.4798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.8953, device='cuda:0')



h[200].sum tensor(24.6050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.6167, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0155, 0.0632, 0.0424,  ..., 0.0577, 0.0546, 0.0580],
        [0.0031, 0.0154, 0.0092,  ..., 0.0127, 0.0131, 0.0136],
        [0.0072, 0.0319, 0.0205,  ..., 0.0281, 0.0274, 0.0289],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.0004],
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.0004],
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50540.3633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.5789e-01, 0.0000e+00,  ..., 6.4387e-02, 0.0000e+00,
         5.0098e-02],
        [0.0000e+00, 9.3678e-02, 0.0000e+00,  ..., 4.0348e-02, 0.0000e+00,
         2.7994e-02],
        [0.0000e+00, 1.0318e-01, 0.0000e+00,  ..., 4.4135e-02, 0.0000e+00,
         3.0935e-02],
        ...,
        [0.0000e+00, 2.4962e-05, 0.0000e+00,  ..., 3.8168e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.5010e-05, 0.0000e+00,  ..., 3.8166e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.5034e-05, 0.0000e+00,  ..., 3.8165e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(265214.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-180.0259, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-19.1761, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-322.3674, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [1.0001],
        ...,
        [0.9996],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365823., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [1.0001],
        ...,
        [0.9996],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365823., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        ...,
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(202.8482, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.3604, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.7773, device='cuda:0')



h[100].sum tensor(17.9741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2023, device='cuda:0')



h[200].sum tensor(21.4450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.0717, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.0004],
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.0004],
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.0004],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.0004],
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.0004],
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38593.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.3631e-05, 0.0000e+00,  ..., 3.8535e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.3725e-05, 0.0000e+00,  ..., 3.8528e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.3820e-05, 0.0000e+00,  ..., 3.8624e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 2.4962e-05, 0.0000e+00,  ..., 3.8168e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.5010e-05, 0.0000e+00,  ..., 3.8166e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.5034e-05, 0.0000e+00,  ..., 3.8165e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(202452.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-122.2238, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-30.6855, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-234.8768, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [1.0001],
        ...,
        [0.9996],
        [0.9995],
        [0.9995]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365823., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [1.0001],
        ...,
        [0.9995],
        [0.9995],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365815.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0023,  0.0103,  0.0067,  ...,  0.0091,  0.0089,  0.0094],
        [ 0.0025,  0.0108,  0.0070,  ...,  0.0096,  0.0093,  0.0099],
        [ 0.0023,  0.0103,  0.0067,  ...,  0.0091,  0.0089,  0.0094],
        ...,
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(267.1801, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.1297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.4220, device='cuda:0')



h[100].sum tensor(18.9384, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6802, device='cuda:0')



h[200].sum tensor(23.4100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.3100, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0111, 0.0476, 0.0312,  ..., 0.0426, 0.0410, 0.0435],
        [0.0107, 0.0462, 0.0303,  ..., 0.0413, 0.0398, 0.0422],
        [0.0043, 0.0200, 0.0124,  ..., 0.0170, 0.0171, 0.0178],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.0005],
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.0005],
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45470.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 9.0582e-02, 0.0000e+00,  ..., 3.9697e-02, 0.0000e+00,
         2.5963e-02],
        [0.0000e+00, 9.1593e-02, 0.0000e+00,  ..., 4.0154e-02, 0.0000e+00,
         2.6109e-02],
        [0.0000e+00, 7.1214e-02, 0.0000e+00,  ..., 3.2317e-02, 0.0000e+00,
         1.9545e-02],
        ...,
        [0.0000e+00, 4.3401e-05, 0.0000e+00,  ..., 3.9520e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 4.3450e-05, 0.0000e+00,  ..., 3.9517e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 4.3474e-05, 0.0000e+00,  ..., 3.9516e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(238655.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-153.4643, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-24.9841, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-284.3750, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [1.0001],
        ...,
        [0.9995],
        [0.9995],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365815.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 20.0 event: 100 loss: tensor(555.5646, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [1.0001],
        ...,
        [0.9995],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365808.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0017,  0.0079,  0.0050,  ...,  0.0068,  0.0068,  0.0072],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        ...,
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(127.1420, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.8564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1489, device='cuda:0')



h[100].sum tensor(17.4386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7296, device='cuda:0')



h[200].sum tensor(20.2018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.8518, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0072, 0.0305, 0.0199,  ..., 0.0271, 0.0262, 0.0276],
        [0.0017, 0.0089, 0.0050,  ..., 0.0068, 0.0075, 0.0075],
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.0005],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.0005],
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.0005],
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32639.7051, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.0808e-01, 0.0000e+00,  ..., 4.6142e-02, 0.0000e+00,
         3.2908e-02],
        [0.0000e+00, 5.4492e-02, 0.0000e+00,  ..., 2.5441e-02, 0.0000e+00,
         1.5628e-02],
        [0.0000e+00, 1.7416e-02, 0.0000e+00,  ..., 1.0934e-02, 0.0000e+00,
         4.7591e-03],
        ...,
        [0.0000e+00, 6.0266e-05, 0.0000e+00,  ..., 4.0749e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.0316e-05, 0.0000e+00,  ..., 4.0746e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.0340e-05, 0.0000e+00,  ..., 4.0745e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(170298.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-90.1869, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.6168, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-190.4393, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [1.0001],
        ...,
        [0.9995],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365808.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [1.0001],
        ...,
        [0.9995],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365808.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        ...,
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(83.4688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.9150, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0540, device='cuda:0')



h[100].sum tensor(16.9195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7448, device='cuda:0')



h[200].sum tensor(19.1102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.3614, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.0005],
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.0005],
        [0.0046, 0.0195, 0.0125,  ..., 0.0170, 0.0167, 0.0174],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.0005],
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.0005],
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28382.3555, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 5.2784e-03, 0.0000e+00,  ..., 6.2448e-03, 0.0000e+00,
         6.7900e-04],
        [0.0000e+00, 1.6798e-02, 0.0000e+00,  ..., 1.0643e-02, 0.0000e+00,
         4.5841e-03],
        [0.0000e+00, 5.2384e-02, 0.0000e+00,  ..., 2.4279e-02, 0.0000e+00,
         1.5863e-02],
        ...,
        [0.0000e+00, 6.0266e-05, 0.0000e+00,  ..., 4.0749e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.0316e-05, 0.0000e+00,  ..., 4.0746e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.0340e-05, 0.0000e+00,  ..., 4.0745e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(146332.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-70.2336, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.0159, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-160.2875, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [1.0001],
        ...,
        [0.9995],
        [0.9994],
        [0.9994]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365808.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [1.0001],
        ...,
        [0.9995],
        [0.9994],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365801.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        ...,
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0004,  0.0002,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(207.2643, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.2121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.2412, device='cuda:0')



h[100].sum tensor(18.5427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3499, device='cuda:0')



h[200].sum tensor(22.4734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.5949, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0010, 0.0005],
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0010, 0.0005],
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0010, 0.0005],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0009, 0.0005],
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0009, 0.0005],
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0009, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40462.8789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.6066e-02, 0.0000e+00,  ..., 1.4438e-02, 0.0000e+00,
         7.1129e-03],
        [0.0000e+00, 7.0233e-03, 0.0000e+00,  ..., 6.9814e-03, 0.0000e+00,
         1.4879e-03],
        [0.0000e+00, 1.5512e-03, 0.0000e+00,  ..., 4.8643e-03, 0.0000e+00,
         2.2154e-05],
        ...,
        [0.0000e+00, 7.5692e-05, 0.0000e+00,  ..., 4.1868e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.5743e-05, 0.0000e+00,  ..., 4.1865e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.5767e-05, 0.0000e+00,  ..., 4.1864e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(212110.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-125.9986, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-31.6650, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-245.8307, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0007],
        [1.0001],
        ...,
        [0.9995],
        [0.9994],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365801.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0001],
        ...,
        [0.9994],
        [0.9994],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365794.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0013,  0.0069,  0.0042,  ...,  0.0058,  0.0059,  0.0062],
        [ 0.0018,  0.0086,  0.0055,  ...,  0.0075,  0.0074,  0.0078],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0005,  0.0002,  0.0001],
        ...,
        [ 0.0025,  0.0110,  0.0072,  ...,  0.0098,  0.0095,  0.0101],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0005,  0.0002,  0.0001],
        [-0.0004,  0.0003, -0.0004,  ..., -0.0005,  0.0002,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(163.9981, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.5213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.5662, device='cuda:0')



h[100].sum tensor(18.1722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.4987, device='cuda:0')



h[200].sum tensor(21.6466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.5780, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0152, 0.0631, 0.0421,  ..., 0.0572, 0.0544, 0.0579],
        [0.0059, 0.0261, 0.0167,  ..., 0.0228, 0.0224, 0.0235],
        [0.0018, 0.0097, 0.0055,  ..., 0.0075, 0.0082, 0.0082],
        ...,
        [0.0199, 0.0786, 0.0535,  ..., 0.0725, 0.0678, 0.0723],
        [0.0175, 0.0697, 0.0472,  ..., 0.0641, 0.0602, 0.0641],
        [0.0126, 0.0504, 0.0340,  ..., 0.0461, 0.0435, 0.0461]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35174.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1441, 0.0000,  ..., 0.0609, 0.0000, 0.0433],
        [0.0000, 0.0958, 0.0000,  ..., 0.0420, 0.0000, 0.0280],
        [0.0000, 0.0486, 0.0000,  ..., 0.0236, 0.0000, 0.0132],
        ...,
        [0.0000, 0.3328, 0.0000,  ..., 0.1305, 0.0000, 0.1094],
        [0.0000, 0.2671, 0.0000,  ..., 0.1060, 0.0000, 0.0869],
        [0.0000, 0.1857, 0.0000,  ..., 0.0753, 0.0000, 0.0596]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(181571.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-99.5836, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-36.7500, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-207.4807, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0001],
        ...,
        [0.9994],
        [0.9994],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365794.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0001],
        ...,
        [0.9994],
        [0.9993],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365786.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0004,  0.0004, -0.0004,  ..., -0.0005,  0.0002,  0.0001],
        [ 0.0030,  0.0130,  0.0085,  ...,  0.0116,  0.0112,  0.0119],
        [ 0.0030,  0.0130,  0.0085,  ...,  0.0116,  0.0112,  0.0119],
        ...,
        [-0.0004,  0.0004, -0.0004,  ..., -0.0005,  0.0002,  0.0001],
        [-0.0004,  0.0004, -0.0004,  ..., -0.0005,  0.0002,  0.0001],
        [-0.0004,  0.0004, -0.0004,  ..., -0.0005,  0.0002,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(114.8856, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.9332, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2938, device='cuda:0')



h[100].sum tensor(17.7272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4574, device='cuda:0')



h[200].sum tensor(20.6653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.8874, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0054, 0.0244, 0.0154,  ..., 0.0211, 0.0209, 0.0219],
        [0.0078, 0.0346, 0.0223,  ..., 0.0304, 0.0298, 0.0314],
        [0.0119, 0.0497, 0.0330,  ..., 0.0448, 0.0428, 0.0454],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0010, 0.0005],
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0010, 0.0005],
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0010, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32158.3242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.1261e-01, 0.0000e+00,  ..., 4.8410e-02, 0.0000e+00,
         3.3756e-02],
        [0.0000e+00, 1.6096e-01, 0.0000e+00,  ..., 6.6875e-02, 0.0000e+00,
         4.9660e-02],
        [0.0000e+00, 2.2044e-01, 0.0000e+00,  ..., 8.9288e-02, 0.0000e+00,
         6.9881e-02],
        ...,
        [0.0000e+00, 1.0270e-04, 0.0000e+00,  ..., 4.3814e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.0276e-04, 0.0000e+00,  ..., 4.3812e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.0278e-04, 0.0000e+00,  ..., 4.3811e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(167838.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-84.1591, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.0236, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-185.2693, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0001],
        ...,
        [0.9994],
        [0.9993],
        [0.9993]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365786.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0001],
        ...,
        [0.9993],
        [0.9993],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365779.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0004,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0004,  ..., -0.0005,  0.0003,  0.0001],
        [ 0.0017,  0.0082,  0.0052,  ...,  0.0071,  0.0071,  0.0075],
        ...,
        [-0.0005,  0.0004, -0.0004,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0004,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0004,  ..., -0.0005,  0.0003,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(126.5094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.8862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3108, device='cuda:0')



h[100].sum tensor(17.9751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7811, device='cuda:0')



h[200].sum tensor(21.1478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.0344, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0010, 0.0005],
        [0.0017, 0.0093, 0.0052,  ..., 0.0071, 0.0078, 0.0079],
        [0.0041, 0.0195, 0.0119,  ..., 0.0164, 0.0167, 0.0173],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0010, 0.0005],
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0010, 0.0005],
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0010, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34830.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0137, 0.0000,  ..., 0.0101, 0.0000, 0.0024],
        [0.0000, 0.0440, 0.0000,  ..., 0.0222, 0.0000, 0.0109],
        [0.0000, 0.0849, 0.0000,  ..., 0.0383, 0.0000, 0.0234],
        ...,
        [0.0000, 0.0001, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0045, 0.0000, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0045, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(186878.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-95.2813, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.0089, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-202.8813, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0001],
        ...,
        [0.9993],
        [0.9993],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365779.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0001],
        ...,
        [0.9993],
        [0.9992],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365771.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056,  0.0224,  0.0152,  ...,  0.0206,  0.0193,  0.0206],
        [ 0.0058,  0.0232,  0.0157,  ...,  0.0214,  0.0200,  0.0214],
        [ 0.0048,  0.0196,  0.0132,  ...,  0.0180,  0.0169,  0.0180],
        ...,
        [-0.0005,  0.0004, -0.0004,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0004,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0004,  ..., -0.0005,  0.0003,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81.4881, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1554, device='cuda:0')



h[100].sum tensor(17.5631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7770, device='cuda:0')



h[200].sum tensor(20.2416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.4757, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0210, 0.0845, 0.0571,  ..., 0.0776, 0.0730, 0.0778],
        [0.0227, 0.0909, 0.0616,  ..., 0.0836, 0.0785, 0.0837],
        [0.0285, 0.1117, 0.0764,  ..., 0.1036, 0.0966, 0.1031],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0010, 0.0005],
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0010, 0.0005],
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0010, 0.0005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29717.0586, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.2704e-01, 0.0000e+00,  ..., 9.2782e-02, 0.0000e+00,
         7.0728e-02],
        [0.0000e+00, 2.4746e-01, 0.0000e+00,  ..., 1.0063e-01, 0.0000e+00,
         7.7359e-02],
        [0.0000e+00, 2.6936e-01, 0.0000e+00,  ..., 1.0896e-01, 0.0000e+00,
         8.4669e-02],
        ...,
        [0.0000e+00, 1.2529e-04, 0.0000e+00,  ..., 4.5429e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.2535e-04, 0.0000e+00,  ..., 4.5426e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.2537e-04, 0.0000e+00,  ..., 4.5425e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(157304.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-69.9990, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.8978, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-165.6916, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0001],
        ...,
        [0.9993],
        [0.9992],
        [0.9992]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365771.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0000],
        ...,
        [0.9993],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365763.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [ 0.0019,  0.0088,  0.0056,  ...,  0.0076,  0.0076,  0.0080],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(125.1809, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9468, device='cuda:0')



h[100].sum tensor(18.1495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6652, device='cuda:0')



h[200].sum tensor(21.4461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.6238, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0082, 0.0383, 0.0243,  ..., 0.0333, 0.0329, 0.0348],
        [0.0014, 0.0084, 0.0045,  ..., 0.0061, 0.0070, 0.0070],
        [0.0019, 0.0100, 0.0056,  ..., 0.0076, 0.0084, 0.0084],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0010, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0010, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0010, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33616.6055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0930, 0.0000,  ..., 0.0421, 0.0000, 0.0250],
        [0.0000, 0.0561, 0.0000,  ..., 0.0273, 0.0000, 0.0142],
        [0.0000, 0.0331, 0.0000,  ..., 0.0181, 0.0000, 0.0075],
        ...,
        [0.0000, 0.0001, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0046, 0.0000, 0.0000],
        [0.0000, 0.0001, 0.0000,  ..., 0.0046, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(176853.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-87.3735, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.2774, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-192.9108, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0000],
        ...,
        [0.9993],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365763.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0000],
        ...,
        [0.9992],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365755.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(186.8508, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.1044, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.8381, device='cuda:0')



h[100].sum tensor(18.9187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9035, device='cuda:0')



h[200].sum tensor(23.0408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.0125, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0010, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0010, 0.0006],
        [0.0031, 0.0163, 0.0096,  ..., 0.0132, 0.0139, 0.0144],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0010, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0010, 0.0006],
        [0.0024, 0.0121, 0.0071,  ..., 0.0097, 0.0102, 0.0104]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38392.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0068, 0.0000, 0.0005],
        [0.0000, 0.0277, 0.0000,  ..., 0.0158, 0.0000, 0.0069],
        [0.0000, 0.0958, 0.0000,  ..., 0.0425, 0.0000, 0.0275],
        ...,
        [0.0000, 0.0020, 0.0000,  ..., 0.0054, 0.0000, 0.0002],
        [0.0000, 0.0199, 0.0000,  ..., 0.0125, 0.0000, 0.0052],
        [0.0000, 0.0584, 0.0000,  ..., 0.0278, 0.0000, 0.0159]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(200363.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-108.8891, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.6050, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-226.6705, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0000],
        ...,
        [0.9992],
        [0.9992],
        [0.9991]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365755.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0000],
        ...,
        [0.9992],
        [0.9991],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365748.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62.9803, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9043, device='cuda:0')



h[100].sum tensor(17.6067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.3789, device='cuda:0')



h[200].sum tensor(20.2398, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.0648, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0010, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0010, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0010, 0.0006],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0010, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0010, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0010, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28375.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.6694e-03, 0.0000e+00,  ..., 5.4103e-03, 0.0000e+00,
         3.8734e-05],
        [0.0000e+00, 1.5109e-04, 0.0000e+00,  ..., 4.7887e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.5119e-04, 0.0000e+00,  ..., 4.8006e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 1.5248e-04, 0.0000e+00,  ..., 4.7348e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.5262e-04, 0.0000e+00,  ..., 4.7345e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.5265e-04, 0.0000e+00,  ..., 4.7344e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(150494.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-60.9368, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.6769, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-154.5167, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0000],
        ...,
        [0.9992],
        [0.9991],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365748.1562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 30.0 event: 150 loss: tensor(540.0342, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0000],
        ...,
        [0.9991],
        [0.9991],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365740.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(131.3219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2605, device='cuda:0')



h[100].sum tensor(18.4258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7651, device='cuda:0')



h[200].sum tensor(21.9462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.9777, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34139.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(179847.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-87.3532, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.3617, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-195.0462, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0000],
        ...,
        [0.9991],
        [0.9991],
        [0.9990]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365740.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0000],
        ...,
        [0.9991],
        [0.9990],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365732.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [ 0.0021,  0.0096,  0.0061,  ...,  0.0084,  0.0083,  0.0088],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(180.4157, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.6410, device='cuda:0')



h[100].sum tensor(19.0149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5225, device='cuda:0')



h[200].sum tensor(23.1693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.6624, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0035, 0.0179, 0.0107,  ..., 0.0147, 0.0153, 0.0158],
        [0.0098, 0.0426, 0.0277,  ..., 0.0378, 0.0366, 0.0388],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38743.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0326, 0.0000,  ..., 0.0181, 0.0000, 0.0076],
        [0.0000, 0.0850, 0.0000,  ..., 0.0390, 0.0000, 0.0229],
        [0.0000, 0.1590, 0.0000,  ..., 0.0678, 0.0000, 0.0463],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(206651.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-108.7392, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.1700, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-228.4821, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0000],
        ...,
        [0.9991],
        [0.9990],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365732.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0000],
        ...,
        [0.9991],
        [0.9990],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365724.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0001]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(64.6921, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8155, device='cuda:0')



h[100].sum tensor(17.8012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.3507, device='cuda:0')



h[200].sum tensor(20.5794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.9646, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28991.0898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0002, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(155222.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-61.2615, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.7812, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-156.9991, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0000],
        ...,
        [0.9991],
        [0.9990],
        [0.9989]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365724.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0000],
        ...,
        [0.9990],
        [0.9989],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365716.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0089,  0.0346,  0.0238,  ...,  0.0322,  0.0299,  0.0320],
        [ 0.0095,  0.0369,  0.0254,  ...,  0.0344,  0.0319,  0.0341],
        [ 0.0075,  0.0296,  0.0202,  ...,  0.0274,  0.0256,  0.0273],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(285.2527, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.7681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.8764, device='cuda:0')



h[100].sum tensor(20.2196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.5066, device='cuda:0')



h[200].sum tensor(25.6798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.6947, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0372, 0.1443, 0.0993,  ..., 0.1345, 0.1248, 0.1333],
        [0.0384, 0.1486, 0.1023,  ..., 0.1386, 0.1285, 0.1373],
        [0.0423, 0.1627, 0.1123,  ..., 0.1521, 0.1407, 0.1504],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47828.7109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 4.2193e-01, 0.0000e+00,  ..., 1.6758e-01, 0.0000e+00,
         1.3417e-01],
        [0.0000e+00, 4.5923e-01, 0.0000e+00,  ..., 1.8151e-01, 0.0000e+00,
         1.4678e-01],
        [0.0000e+00, 4.8270e-01, 0.0000e+00,  ..., 1.9014e-01, 0.0000e+00,
         1.5483e-01],
        ...,
        [0.0000e+00, 2.0641e-04, 0.0000e+00,  ..., 4.9195e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.0657e-04, 0.0000e+00,  ..., 4.9191e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.0660e-04, 0.0000e+00,  ..., 4.9190e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(260517.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-151.4754, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-30.3520, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-293.8713, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0000],
        ...,
        [0.9990],
        [0.9989],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365716.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0000],
        ...,
        [0.9990],
        [0.9989],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365708.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0020,  0.0094,  0.0059,  ...,  0.0081,  0.0081,  0.0086],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(72.1444, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3247, device='cuda:0')



h[100].sum tensor(17.9645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.5127, device='cuda:0')



h[200].sum tensor(20.8871, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.5389, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0015, 0.0089, 0.0048,  ..., 0.0066, 0.0075, 0.0075],
        [0.0020, 0.0106, 0.0059,  ..., 0.0081, 0.0089, 0.0090],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29753.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0282, 0.0000,  ..., 0.0166, 0.0000, 0.0057],
        [0.0000, 0.0220, 0.0000,  ..., 0.0140, 0.0000, 0.0041],
        [0.0000, 0.0070, 0.0000,  ..., 0.0079, 0.0000, 0.0010],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(160280.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-63.6435, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.6734, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-161.9966, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [1.0000],
        ...,
        [0.9990],
        [0.9989],
        [0.9988]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365708.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9999],
        ...,
        [0.9989],
        [0.9988],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365699.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0040,  0.0169,  0.0112,  ...,  0.0153,  0.0146,  0.0155],
        [ 0.0047,  0.0193,  0.0129,  ...,  0.0176,  0.0167,  0.0177],
        [ 0.0140,  0.0534,  0.0371,  ...,  0.0501,  0.0462,  0.0494],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(222.5223, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4310, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.8338, device='cuda:0')



h[100].sum tensor(19.5915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2203, device='cuda:0')



h[200].sum tensor(24.3172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.1354, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0335, 0.1308, 0.0897,  ..., 0.1216, 0.1131, 0.1208],
        [0.0382, 0.1481, 0.1019,  ..., 0.1381, 0.1280, 0.1368],
        [0.0225, 0.0907, 0.0612,  ..., 0.0832, 0.0783, 0.0835],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43011.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.8726e-01, 0.0000e+00,  ..., 1.5501e-01, 0.0000e+00,
         1.2174e-01],
        [0.0000e+00, 4.1466e-01, 0.0000e+00,  ..., 1.6526e-01, 0.0000e+00,
         1.3096e-01],
        [0.0000e+00, 3.4441e-01, 0.0000e+00,  ..., 1.3879e-01, 0.0000e+00,
         1.0748e-01],
        ...,
        [0.0000e+00, 2.2689e-04, 0.0000e+00,  ..., 4.9887e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.2705e-04, 0.0000e+00,  ..., 4.9883e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.2708e-04, 0.0000e+00,  ..., 4.9882e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(232368.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-126.9430, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.4969, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-258.1043, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9999],
        ...,
        [0.9989],
        [0.9988],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365699.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9999],
        ...,
        [0.9989],
        [0.9988],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365691.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(203.1100, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.9796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.1465, device='cuda:0')



h[100].sum tensor(19.4037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0016, device='cuda:0')



h[200].sum tensor(23.9050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.3603, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0052, 0.0241, 0.0150,  ..., 0.0205, 0.0206, 0.0215],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40373.1484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0095, 0.0000,  ..., 0.0088, 0.0000, 0.0017],
        [0.0000, 0.0192, 0.0000,  ..., 0.0129, 0.0000, 0.0040],
        [0.0000, 0.0594, 0.0000,  ..., 0.0293, 0.0000, 0.0144],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(217115.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-113.7021, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.7792, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-237.9322, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9999],
        ...,
        [0.9989],
        [0.9988],
        [0.9987]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365691.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9999],
        ...,
        [0.9988],
        [0.9987],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365683.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0021,  0.0097,  0.0061,  ...,  0.0084,  0.0083,  0.0088],
        [ 0.0011,  0.0064,  0.0038,  ...,  0.0052,  0.0055,  0.0057],
        [ 0.0037,  0.0157,  0.0104,  ...,  0.0141,  0.0135,  0.0144],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(74.8225, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2387, device='cuda:0')



h[100].sum tensor(18.0778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4853, device='cuda:0')



h[200].sum tensor(21.0820, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.4419, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0041, 0.0201, 0.0121,  ..., 0.0166, 0.0171, 0.0178],
        [0.0119, 0.0524, 0.0340,  ..., 0.0465, 0.0451, 0.0478],
        [0.0069, 0.0322, 0.0203,  ..., 0.0277, 0.0277, 0.0291],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30463.4180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0781, 0.0000,  ..., 0.0368, 0.0000, 0.0200],
        [0.0000, 0.1159, 0.0000,  ..., 0.0520, 0.0000, 0.0310],
        [0.0000, 0.1048, 0.0000,  ..., 0.0476, 0.0000, 0.0275],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0002, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(164876.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-66.4524, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.4001, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-166.8561, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9999],
        ...,
        [0.9988],
        [0.9987],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365683.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9999],
        ...,
        [0.9988],
        [0.9987],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365674.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(361.6343, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.8566, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.6756, device='cuda:0')



h[100].sum tensor(21.0710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.7155, device='cuda:0')



h[200].sum tensor(27.4141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.9794, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0008, 0.0065, 0.0030,  ..., 0.0041, 0.0053, 0.0052]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49378.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.3669e-04, 0.0000e+00,  ..., 5.1376e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.3689e-04, 0.0000e+00,  ..., 5.1362e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.5618e-03, 0.0000e+00,  ..., 6.0842e-03, 0.0000e+00,
         2.7302e-04],
        ...,
        [0.0000e+00, 4.2853e-03, 0.0000e+00,  ..., 6.7945e-03, 0.0000e+00,
         7.9223e-05],
        [0.0000e+00, 1.0191e-02, 0.0000e+00,  ..., 9.3181e-03, 0.0000e+00,
         9.1227e-04],
        [0.0000e+00, 1.9841e-02, 0.0000e+00,  ..., 1.3442e-02, 0.0000e+00,
         2.4983e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(251084.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-156.2519, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-30.0142, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-303.9467, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9999],
        ...,
        [0.9988],
        [0.9987],
        [0.9986]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365674.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9999],
        ...,
        [0.9987],
        [0.9986],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365666.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(171.7986, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.9864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1019, device='cuda:0')



h[100].sum tensor(19.1122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3510, device='cuda:0')



h[200].sum tensor(23.2510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.0544, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0045, 0.0198, 0.0124,  ..., 0.0168, 0.0168, 0.0175],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36755.9258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0674, 0.0000,  ..., 0.0319, 0.0000, 0.0186],
        [0.0000, 0.0361, 0.0000,  ..., 0.0194, 0.0000, 0.0091],
        [0.0000, 0.0207, 0.0000,  ..., 0.0134, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(194462.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-95.4930, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.4012, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-211.5433, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9999],
        ...,
        [0.9987],
        [0.9986],
        [0.9985]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365666.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 200 loss: tensor(630.5695, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9998],
        ...,
        [0.9987],
        [0.9986],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365658., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0067,  0.0040,  ...,  0.0055,  0.0057,  0.0060],
        [ 0.0012,  0.0067,  0.0040,  ...,  0.0055,  0.0057,  0.0060],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(136.0466, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6175, device='cuda:0')



h[100].sum tensor(18.7530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5605, device='cuda:0')



h[200].sum tensor(22.4793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.2525, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0130, 0.0071,  ..., 0.0099, 0.0110, 0.0113],
        [0.0021, 0.0130, 0.0071,  ..., 0.0099, 0.0110, 0.0113],
        [0.0021, 0.0130, 0.0071,  ..., 0.0099, 0.0110, 0.0113],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34019.7461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0489, 0.0000,  ..., 0.0258, 0.0000, 0.0094],
        [0.0000, 0.0374, 0.0000,  ..., 0.0210, 0.0000, 0.0066],
        [0.0000, 0.0278, 0.0000,  ..., 0.0169, 0.0000, 0.0045],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(179523.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-82.0046, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.0839, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.5220, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9998],
        ...,
        [0.9987],
        [0.9986],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365658., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9998],
        ...,
        [0.9986],
        [0.9985],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365649.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0073,  0.0289,  0.0197,  ...,  0.0267,  0.0249,  0.0266],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(277.3192, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.9940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.1409, device='cuda:0')



h[100].sum tensor(20.1921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9544, device='cuda:0')



h[200].sum tensor(25.5220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.7374, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0112, 0.0463, 0.0307,  ..., 0.0417, 0.0399, 0.0422],
        [0.0143, 0.0573, 0.0385,  ..., 0.0522, 0.0494, 0.0524],
        [0.0308, 0.1213, 0.0828,  ..., 0.1123, 0.1048, 0.1119],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44773.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1245, 0.0000,  ..., 0.0540, 0.0000, 0.0367],
        [0.0000, 0.1658, 0.0000,  ..., 0.0701, 0.0000, 0.0497],
        [0.0000, 0.2032, 0.0000,  ..., 0.0846, 0.0000, 0.0620],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(235881.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-133.2139, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.3871, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-269.2275, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9998],
        ...,
        [0.9986],
        [0.9985],
        [0.9984]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365649.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9998],
        ...,
        [0.9986],
        [0.9985],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365640.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0122,  0.0468,  0.0324,  ...,  0.0439,  0.0405,  0.0433],
        [ 0.0115,  0.0443,  0.0306,  ...,  0.0415,  0.0383,  0.0410],
        [ 0.0117,  0.0448,  0.0310,  ...,  0.0420,  0.0388,  0.0415],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(202.6059, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.1300, device='cuda:0')



h[100].sum tensor(19.4329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.6781, device='cuda:0')



h[200].sum tensor(23.9035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.2138, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0461, 0.1772, 0.1224,  ..., 0.1658, 0.1532, 0.1639],
        [0.0504, 0.1928, 0.1335,  ..., 0.1807, 0.1667, 0.1784],
        [0.0494, 0.1890, 0.1308,  ..., 0.1771, 0.1635, 0.1748],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38893.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 5.3443e-01, 0.0000e+00,  ..., 2.1096e-01, 0.0000e+00,
         1.6993e-01],
        [0.0000e+00, 5.5173e-01, 0.0000e+00,  ..., 2.1742e-01, 0.0000e+00,
         1.7577e-01],
        [0.0000e+00, 5.2250e-01, 0.0000e+00,  ..., 2.0655e-01, 0.0000e+00,
         1.6590e-01],
        ...,
        [0.0000e+00, 2.7520e-04, 0.0000e+00,  ..., 5.1482e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.7537e-04, 0.0000e+00,  ..., 5.1478e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.7540e-04, 0.0000e+00,  ..., 5.1477e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(206663.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-104.5916, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.3459, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-226.0011, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9998],
        ...,
        [0.9986],
        [0.9985],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365640.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9998],
        ...,
        [0.9985],
        [0.9984],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365632.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(263.5946, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.7889, device='cuda:0')



h[100].sum tensor(20.0394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.8424, device='cuda:0')



h[200].sum tensor(25.1827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.3404, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42491.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0185, 0.0000,  ..., 0.0127, 0.0000, 0.0037],
        [0.0000, 0.0042, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(223533.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-122.3911, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.2084, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-253.2466, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9998],
        ...,
        [0.9985],
        [0.9984],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365632.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9998],
        ...,
        [0.9985],
        [0.9984],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365632.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0029,  0.0129,  0.0083,  ...,  0.0114,  0.0111,  0.0117],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(110.1429, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7957, device='cuda:0')



h[100].sum tensor(18.5044, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.9808, device='cuda:0')



h[200].sum tensor(21.9264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.1979, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0054, 0.0233, 0.0149,  ..., 0.0202, 0.0199, 0.0208],
        [0.0029, 0.0141, 0.0083,  ..., 0.0114, 0.0119, 0.0122],
        [0.0090, 0.0382, 0.0249,  ..., 0.0339, 0.0328, 0.0347],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32168.7754, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1000, 0.0000,  ..., 0.0449, 0.0000, 0.0282],
        [0.0000, 0.0939, 0.0000,  ..., 0.0426, 0.0000, 0.0261],
        [0.0000, 0.1306, 0.0000,  ..., 0.0569, 0.0000, 0.0379],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(175178.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-72.6090, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.9318, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-178.0698, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9998],
        ...,
        [0.9985],
        [0.9984],
        [0.9983]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365632.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9998],
        ...,
        [0.9985],
        [0.9984],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365623.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0037,  0.0158,  0.0104,  ...,  0.0142,  0.0136,  0.0145],
        [ 0.0015,  0.0079,  0.0048,  ...,  0.0066,  0.0068,  0.0071],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(307.5721, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.6891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.5674, device='cuda:0')



h[100].sum tensor(20.4666, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4083, device='cuda:0')



h[200].sum tensor(26.0823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.3462, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0160, 0.0676, 0.0448,  ..., 0.0610, 0.0583, 0.0620],
        [0.0098, 0.0412, 0.0270,  ..., 0.0367, 0.0354, 0.0374],
        [0.0015, 0.0091, 0.0048,  ..., 0.0066, 0.0076, 0.0076],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45140.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1944, 0.0000,  ..., 0.0825, 0.0000, 0.0565],
        [0.0000, 0.1437, 0.0000,  ..., 0.0624, 0.0000, 0.0411],
        [0.0000, 0.0902, 0.0000,  ..., 0.0415, 0.0000, 0.0243],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(233770.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-134.2859, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.5181, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-271.4423, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9998],
        ...,
        [0.9985],
        [0.9984],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365623.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9998],
        ...,
        [0.9984],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365614.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(119.9436, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0751, device='cuda:0')



h[100].sum tensor(18.6012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0697, device='cuda:0')



h[200].sum tensor(22.1181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.5130, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31572.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0687, 0.0000,  ..., 0.0322, 0.0000, 0.0198],
        [0.0000, 0.0190, 0.0000,  ..., 0.0127, 0.0000, 0.0046],
        [0.0000, 0.0045, 0.0000,  ..., 0.0070, 0.0000, 0.0003],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(164550.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-68.9470, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.8535, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-173.3559, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0007],
        [0.9998],
        ...,
        [0.9984],
        [0.9983],
        [0.9982]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365614.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0006],
        [0.9997],
        ...,
        [0.9984],
        [0.9982],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365606.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0019,  0.0091,  0.0057,  ...,  0.0078,  0.0078,  0.0083],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(182.7545, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.3876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6682, device='cuda:0')



h[100].sum tensor(19.2134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2130, device='cuda:0')



h[200].sum tensor(23.4114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.5653, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0041, 0.0185, 0.0115,  ..., 0.0156, 0.0158, 0.0164],
        [0.0019, 0.0104, 0.0057,  ..., 0.0078, 0.0087, 0.0088],
        [0.0043, 0.0209, 0.0127,  ..., 0.0174, 0.0178, 0.0186],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35407.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0545, 0.0000,  ..., 0.0272, 0.0000, 0.0135],
        [0.0000, 0.0568, 0.0000,  ..., 0.0284, 0.0000, 0.0135],
        [0.0000, 0.0827, 0.0000,  ..., 0.0390, 0.0000, 0.0206],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(182971.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-87.5117, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.4596, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-201.0238, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0006],
        [0.9997],
        ...,
        [0.9984],
        [0.9982],
        [0.9981]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365606.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0006],
        [0.9997],
        ...,
        [0.9983],
        [0.9982],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365597.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(142.0558, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1788, device='cuda:0')



h[100].sum tensor(18.8085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4209, device='cuda:0')



h[200].sum tensor(22.5468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.7577, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0031, 0.0148, 0.0088,  ..., 0.0121, 0.0126, 0.0129],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32643.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0058, 0.0000,  ..., 0.0076, 0.0000, 0.0002],
        [0.0000, 0.0137, 0.0000,  ..., 0.0107, 0.0000, 0.0028],
        [0.0000, 0.0422, 0.0000,  ..., 0.0222, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(171237.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-73.6908, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.2745, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-180.6488, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0006],
        [0.9997],
        ...,
        [0.9983],
        [0.9982],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365597.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0006],
        [0.9997],
        ...,
        [0.9983],
        [0.9981],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365588.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0043,  0.0180,  0.0119,  ...,  0.0163,  0.0155,  0.0165],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(134.9656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7521, device='cuda:0')



h[100].sum tensor(18.7324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2851, device='cuda:0')



h[200].sum tensor(22.3806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.2764, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0058, 0.0267, 0.0168,  ..., 0.0229, 0.0228, 0.0240],
        [0.0034, 0.0160, 0.0097,  ..., 0.0132, 0.0136, 0.0140],
        [0.0205, 0.0840, 0.0563,  ..., 0.0766, 0.0725, 0.0772],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33577.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0798, 0.0000,  ..., 0.0375, 0.0000, 0.0205],
        [0.0000, 0.0805, 0.0000,  ..., 0.0375, 0.0000, 0.0217],
        [0.0000, 0.1432, 0.0000,  ..., 0.0620, 0.0000, 0.0414],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(183173.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-78.0291, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.5534, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-187.2142, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0006],
        [0.9997],
        ...,
        [0.9983],
        [0.9981],
        [0.9980]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365588.4688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 50.0 event: 250 loss: tensor(580.2721, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0006],
        [0.9997],
        ...,
        [0.9982],
        [0.9981],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365579.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0024,  0.0112,  0.0071,  ...,  0.0098,  0.0096,  0.0102],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(650.9832, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.4099, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(47.0265, device='cuda:0')



h[100].sum tensor(23.6801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.9638, device='cuda:0')



h[200].sum tensor(32.8775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(53.0367, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0035, 0.0183, 0.0108,  ..., 0.0149, 0.0156, 0.0162],
        [0.0091, 0.0404, 0.0260,  ..., 0.0355, 0.0347, 0.0367],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68998.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0290, 0.0000,  ..., 0.0172, 0.0000, 0.0058],
        [0.0000, 0.0743, 0.0000,  ..., 0.0355, 0.0000, 0.0185],
        [0.0000, 0.1349, 0.0000,  ..., 0.0597, 0.0000, 0.0366],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(363190.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-247.2877, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-14.0142, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-443.8535, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0006],
        [0.9997],
        ...,
        [0.9982],
        [0.9981],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365579.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0008],
        [1.0006],
        [0.9997],
        ...,
        [0.9982],
        [0.9980],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365570.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(235.0421, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.6256, device='cuda:0')



h[100].sum tensor(19.6699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8358, device='cuda:0')



h[200].sum tensor(24.3621, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.7728, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42170.4414, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.3352e-02, 0.0000e+00,  ..., 1.0876e-02, 0.0000e+00,
         1.4637e-03],
        [0.0000e+00, 9.8296e-03, 0.0000e+00,  ..., 9.3754e-03, 0.0000e+00,
         8.8094e-04],
        [0.0000e+00, 3.2763e-03, 0.0000e+00,  ..., 6.6006e-03, 0.0000e+00,
         2.9553e-05],
        ...,
        [0.0000e+00, 3.0311e-04, 0.0000e+00,  ..., 5.2351e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.0328e-04, 0.0000e+00,  ..., 5.2347e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.0331e-04, 0.0000e+00,  ..., 5.2345e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(235689.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-120.0731, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.2386, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-250.1218, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0008],
        [1.0006],
        [0.9997],
        ...,
        [0.9982],
        [0.9980],
        [0.9979]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365570.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0006],
        [0.9996],
        ...,
        [0.9981],
        [0.9980],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365561.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0054,  0.0221,  0.0149,  ...,  0.0202,  0.0191,  0.0204],
        [ 0.0025,  0.0113,  0.0072,  ...,  0.0099,  0.0097,  0.0103],
        [ 0.0025,  0.0113,  0.0072,  ...,  0.0099,  0.0097,  0.0103],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(202.3400, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.3651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.9538, device='cuda:0')



h[100].sum tensor(19.3436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3039, device='cuda:0')



h[200].sum tensor(23.6659, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.8874, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0137, 0.0591, 0.0387,  ..., 0.0527, 0.0509, 0.0540],
        [0.0142, 0.0610, 0.0400,  ..., 0.0546, 0.0526, 0.0558],
        [0.0044, 0.0214, 0.0130,  ..., 0.0179, 0.0183, 0.0191],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37917.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1332, 0.0000,  ..., 0.0589, 0.0000, 0.0366],
        [0.0000, 0.1196, 0.0000,  ..., 0.0534, 0.0000, 0.0325],
        [0.0000, 0.0737, 0.0000,  ..., 0.0353, 0.0000, 0.0185],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(204509.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-98.4271, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.7846, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-218.4869, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0006],
        [0.9996],
        ...,
        [0.9981],
        [0.9980],
        [0.9978]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365561.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0006],
        [0.9996],
        ...,
        [0.9981],
        [0.9979],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365552.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056,  0.0227,  0.0153,  ...,  0.0208,  0.0196,  0.0209],
        [ 0.0037,  0.0159,  0.0105,  ...,  0.0143,  0.0137,  0.0145],
        [ 0.0055,  0.0223,  0.0150,  ...,  0.0204,  0.0192,  0.0205],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(191.0375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3723, device='cuda:0')



h[100].sum tensor(19.2221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.1188, device='cuda:0')



h[200].sum tensor(23.4048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.2316, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0131, 0.0571, 0.0373,  ..., 0.0508, 0.0492, 0.0522],
        [0.0150, 0.0638, 0.0420,  ..., 0.0572, 0.0550, 0.0584],
        [0.0091, 0.0404, 0.0259,  ..., 0.0354, 0.0347, 0.0367],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36163.9570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1811, 0.0000,  ..., 0.0782, 0.0000, 0.0503],
        [0.0000, 0.1761, 0.0000,  ..., 0.0760, 0.0000, 0.0492],
        [0.0000, 0.1463, 0.0000,  ..., 0.0642, 0.0000, 0.0401],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(191835.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-89.9379, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.5574, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-205.5831, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0006],
        [0.9996],
        ...,
        [0.9981],
        [0.9979],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365552.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0006],
        [0.9996],
        ...,
        [0.9980],
        [0.9979],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365543.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(140.4762, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2018, device='cuda:0')



h[100].sum tensor(18.7340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1100, device='cuda:0')



h[200].sum tensor(22.3655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.6558, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32570.3867, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(175084.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-73.0559, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.4241, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-180.1709, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0006],
        [0.9996],
        ...,
        [0.9980],
        [0.9979],
        [0.9977]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365543.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0006],
        [0.9996],
        ...,
        [0.9980],
        [0.9978],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365534.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(220.5345, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.2146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.6001, device='cuda:0')



h[100].sum tensor(19.4657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5095, device='cuda:0')



h[200].sum tensor(23.9163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.6163, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35665.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0107, 0.0000,  ..., 0.0095, 0.0000, 0.0023],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(180536.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-87.8212, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.3542, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-202.9635, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0006],
        [0.9996],
        ...,
        [0.9980],
        [0.9978],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365534.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0006],
        [0.9995],
        ...,
        [0.9979],
        [0.9977],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365525.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(386.1604, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.1477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.9222, device='cuda:0')



h[100].sum tensor(20.9781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.1576, device='cuda:0')



h[200].sum tensor(27.1247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.0020, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48346.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0252, 0.0000,  ..., 0.0158, 0.0000, 0.0043],
        [0.0000, 0.0168, 0.0000,  ..., 0.0123, 0.0000, 0.0023],
        [0.0000, 0.0110, 0.0000,  ..., 0.0099, 0.0000, 0.0013],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(251163.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-148.2851, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-33.6984, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-293.5086, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0006],
        [0.9995],
        ...,
        [0.9979],
        [0.9977],
        [0.9976]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365525.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0006],
        [0.9995],
        ...,
        [0.9978],
        [0.9977],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365516.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0077,  0.0305,  0.0208,  ...,  0.0283,  0.0264,  0.0282],
        [ 0.0018,  0.0088,  0.0055,  ...,  0.0075,  0.0076,  0.0080],
        [ 0.0067,  0.0268,  0.0182,  ...,  0.0247,  0.0231,  0.0247],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(558.8774, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.9851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.5089, device='cuda:0')



h[100].sum tensor(22.5359, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.8899, device='cuda:0')



h[200].sum tensor(30.4303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(45.6861, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0108, 0.0466, 0.0303,  ..., 0.0413, 0.0401, 0.0424],
        [0.0282, 0.1121, 0.0763,  ..., 0.1035, 0.0969, 0.1034],
        [0.0172, 0.0719, 0.0478,  ..., 0.0650, 0.0620, 0.0660],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62669.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1877, 0.0000,  ..., 0.0801, 0.0000, 0.0540],
        [0.0000, 0.2833, 0.0000,  ..., 0.1172, 0.0000, 0.0849],
        [0.0000, 0.2919, 0.0000,  ..., 0.1208, 0.0000, 0.0873],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0081, 0.0000, 0.0012],
        [0.0000, 0.0285, 0.0000,  ..., 0.0165, 0.0000, 0.0072]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(343416.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-216.8078, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-19.8755, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-398.2371, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0006],
        [0.9995],
        ...,
        [0.9978],
        [0.9977],
        [0.9975]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365516.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0006],
        [0.9995],
        ...,
        [0.9978],
        [0.9976],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365506.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(349.3070, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.9971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.5478, device='cuda:0')



h[100].sum tensor(20.5859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4021, device='cuda:0')



h[200].sum tensor(26.2881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.3241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45071.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(239603.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-132.5184, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-36.4804, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-270.1008, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0006],
        [0.9995],
        ...,
        [0.9978],
        [0.9976],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365506.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9995],
        ...,
        [0.9977],
        [0.9976],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365497.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(184.0283, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1373, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1045, device='cuda:0')



h[100].sum tensor(19.0611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7154, device='cuda:0')



h[200].sum tensor(23.0487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.8017, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35612.3867, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(191087.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-86.8342, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.1827, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-201.5189, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9995],
        ...,
        [0.9977],
        [0.9976],
        [0.9974]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365497.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 60.0 event: 300 loss: tensor(636.7463, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9995],
        ...,
        [0.9977],
        [0.9975],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365488.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(271.1632, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.3722, device='cuda:0')



h[100].sum tensor(19.8292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0734, device='cuda:0')



h[200].sum tensor(24.6781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.6149, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0018, 0.0101, 0.0055,  ..., 0.0076, 0.0085, 0.0085],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39081.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0021, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0139, 0.0000,  ..., 0.0110, 0.0000, 0.0025],
        [0.0000, 0.0390, 0.0000,  ..., 0.0214, 0.0000, 0.0082],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(200995.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-103.8455, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.7815, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-227.0203, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9995],
        ...,
        [0.9977],
        [0.9975],
        [0.9973]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365488.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9994],
        ...,
        [0.9976],
        [0.9974],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365479.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050,  0.0206,  0.0138,  ...,  0.0187,  0.0177,  0.0189],
        [ 0.0068,  0.0270,  0.0184,  ...,  0.0249,  0.0234,  0.0249],
        [ 0.0031,  0.0136,  0.0088,  ...,  0.0120,  0.0117,  0.0124],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(94.1226, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.8908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1690, device='cuda:0')



h[100].sum tensor(18.2214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.1449, device='cuda:0')



h[200].sum tensor(21.2624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.2354, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0310, 0.1222, 0.0834,  ..., 0.1131, 0.1056, 0.1127],
        [0.0255, 0.1023, 0.0693,  ..., 0.0940, 0.0883, 0.0942],
        [0.0214, 0.0855, 0.0579,  ..., 0.0786, 0.0738, 0.0787],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29776.8320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 4.1854e-01, 0.0000e+00,  ..., 1.6836e-01, 0.0000e+00,
         1.2989e-01],
        [0.0000e+00, 3.7875e-01, 0.0000e+00,  ..., 1.5307e-01, 0.0000e+00,
         1.1691e-01],
        [0.0000e+00, 2.9797e-01, 0.0000e+00,  ..., 1.2173e-01, 0.0000e+00,
         9.1365e-02],
        ...,
        [0.0000e+00, 3.1865e-04, 0.0000e+00,  ..., 5.2765e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.1884e-04, 0.0000e+00,  ..., 5.2761e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.1886e-04, 0.0000e+00,  ..., 5.2759e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(164274.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-58.7542, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.6233, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-159.0927, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9994],
        ...,
        [0.9976],
        [0.9974],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365479.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9994],
        ...,
        [0.9976],
        [0.9974],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365479.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0069,  0.0041,  ...,  0.0056,  0.0059,  0.0062],
        [ 0.0038,  0.0162,  0.0107,  ...,  0.0146,  0.0140,  0.0149],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(448.4998, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.3963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.6823, device='cuda:0')



h[100].sum tensor(21.3955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.0359, device='cuda:0')



h[200].sum tensor(28.0025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.1148, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0134, 0.0583, 0.0381,  ..., 0.0520, 0.0502, 0.0533],
        [0.0052, 0.0264, 0.0160,  ..., 0.0221, 0.0226, 0.0237],
        [0.0047, 0.0227, 0.0139,  ..., 0.0191, 0.0194, 0.0203],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47447.5273, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1203, 0.0000,  ..., 0.0543, 0.0000, 0.0314],
        [0.0000, 0.0919, 0.0000,  ..., 0.0430, 0.0000, 0.0227],
        [0.0000, 0.0683, 0.0000,  ..., 0.0332, 0.0000, 0.0166],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(233695.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-144.5627, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-33.1684, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-288.9012, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9994],
        ...,
        [0.9976],
        [0.9974],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365479.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9994],
        ...,
        [0.9976],
        [0.9974],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365469.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(145.7300, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6978, device='cuda:0')



h[100].sum tensor(18.6682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.9496, device='cuda:0')



h[200].sum tensor(22.2098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.0874, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31383.4180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 4.1047e-03, 0.0000e+00,  ..., 6.9945e-03, 0.0000e+00,
         7.8176e-05],
        [0.0000e+00, 7.4489e-04, 0.0000e+00,  ..., 5.5577e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.9874e-04, 0.0000e+00,  ..., 5.3794e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 3.1957e-04, 0.0000e+00,  ..., 5.2784e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.1976e-04, 0.0000e+00,  ..., 5.2780e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.1978e-04, 0.0000e+00,  ..., 5.2779e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(166717.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-66.5733, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.9632, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-171.0554, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9994],
        ...,
        [0.9976],
        [0.9974],
        [0.9972]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365469.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9994],
        ...,
        [0.9975],
        [0.9973],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365460.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0101,  0.0393,  0.0270,  ...,  0.0366,  0.0340,  0.0363],
        [ 0.0045,  0.0187,  0.0125,  ...,  0.0170,  0.0162,  0.0172],
        ...,
        [ 0.0033,  0.0144,  0.0094,  ...,  0.0128,  0.0124,  0.0132],
        [ 0.0022,  0.0102,  0.0065,  ...,  0.0089,  0.0088,  0.0093],
        [ 0.0033,  0.0144,  0.0094,  ...,  0.0128,  0.0124,  0.0132]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(232.3829, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4686, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1960, device='cuda:0')



h[100].sum tensor(19.4194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3809, device='cuda:0')



h[200].sum tensor(23.8038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.1605, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0337, 0.1321, 0.0904,  ..., 0.1226, 0.1142, 0.1220],
        [0.0122, 0.0518, 0.0340,  ..., 0.0463, 0.0446, 0.0473],
        [0.0340, 0.1332, 0.0911,  ..., 0.1236, 0.1151, 0.1229],
        ...,
        [0.0048, 0.0228, 0.0140,  ..., 0.0191, 0.0194, 0.0203],
        [0.0140, 0.0602, 0.0394,  ..., 0.0538, 0.0518, 0.0551],
        [0.0108, 0.0485, 0.0312,  ..., 0.0426, 0.0417, 0.0442]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34713.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3270, 0.0000,  ..., 0.1328, 0.0000, 0.1005],
        [0.0000, 0.2494, 0.0000,  ..., 0.1033, 0.0000, 0.0750],
        [0.0000, 0.3359, 0.0000,  ..., 0.1364, 0.0000, 0.1032],
        ...,
        [0.0000, 0.0659, 0.0000,  ..., 0.0319, 0.0000, 0.0167],
        [0.0000, 0.1071, 0.0000,  ..., 0.0484, 0.0000, 0.0289],
        [0.0000, 0.1176, 0.0000,  ..., 0.0527, 0.0000, 0.0319]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(175830.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-83.1665, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.4914, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-195.8106, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9994],
        ...,
        [0.9975],
        [0.9973],
        [0.9971]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365460.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9994],
        ...,
        [0.9974],
        [0.9973],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365450.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(153.4666, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8790, device='cuda:0')



h[100].sum tensor(18.7049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0073, device='cuda:0')



h[200].sum tensor(22.2853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.2917, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32087.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0362, 0.0000,  ..., 0.0199, 0.0000, 0.0090],
        [0.0000, 0.0130, 0.0000,  ..., 0.0106, 0.0000, 0.0023],
        [0.0000, 0.0057, 0.0000,  ..., 0.0076, 0.0000, 0.0003],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(171418.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-70.4397, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.1638, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-176.4028, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9994],
        ...,
        [0.9974],
        [0.9973],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365450.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9993],
        ...,
        [0.9974],
        [0.9972],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365441.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(215.0769, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8407, device='cuda:0')



h[100].sum tensor(19.2274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9497, device='cuda:0')



h[200].sum tensor(23.3939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.6320, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36167.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0200, 0.0000,  ..., 0.0132, 0.0000, 0.0052],
        [0.0000, 0.0057, 0.0000,  ..., 0.0075, 0.0000, 0.0010],
        [0.0000, 0.0072, 0.0000,  ..., 0.0082, 0.0000, 0.0008],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(191788.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-89.3324, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.9262, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-205.2684, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9993],
        ...,
        [0.9974],
        [0.9972],
        [0.9970]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365441.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9993],
        ...,
        [0.9973],
        [0.9971],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365431.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(81.3417, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.3145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1517, device='cuda:0')



h[100].sum tensor(18.0453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.8212, device='cuda:0')



h[200].sum tensor(20.8826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.0881, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0010, 0.0072, 0.0034,  ..., 0.0048, 0.0060, 0.0058],
        [0.0020, 0.0128, 0.0069,  ..., 0.0096, 0.0108, 0.0110],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26514.0195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0227, 0.0000,  ..., 0.0148, 0.0000, 0.0040],
        [0.0000, 0.0383, 0.0000,  ..., 0.0213, 0.0000, 0.0074],
        [0.0000, 0.0520, 0.0000,  ..., 0.0270, 0.0000, 0.0109],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(141312.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-44.0628, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-52.9005, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-136.5359, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9993],
        ...,
        [0.9973],
        [0.9971],
        [0.9969]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365431.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9993],
        ...,
        [0.9973],
        [0.9971],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365422.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0037,  0.0157,  0.0103,  ...,  0.0141,  0.0135,  0.0144],
        [ 0.0012,  0.0067,  0.0040,  ...,  0.0055,  0.0058,  0.0060],
        [ 0.0019,  0.0094,  0.0058,  ...,  0.0080,  0.0081,  0.0085],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(224.5613, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0748, device='cuda:0')



h[100].sum tensor(19.2705, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0242, device='cuda:0')



h[200].sum tensor(23.4835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.8960, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0054, 0.0269, 0.0164,  ..., 0.0226, 0.0231, 0.0242],
        [0.0135, 0.0587, 0.0384,  ..., 0.0524, 0.0506, 0.0537],
        [0.0059, 0.0269, 0.0169,  ..., 0.0231, 0.0231, 0.0242],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34692.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0927, 0.0000,  ..., 0.0433, 0.0000, 0.0230],
        [0.0000, 0.1210, 0.0000,  ..., 0.0545, 0.0000, 0.0317],
        [0.0000, 0.0978, 0.0000,  ..., 0.0450, 0.0000, 0.0254],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(179815.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-83.4862, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.3236, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-195.9271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9993],
        ...,
        [0.9973],
        [0.9971],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365422.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9993],
        ...,
        [0.9972],
        [0.9970],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365412.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(203.0187, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2453, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9585, device='cuda:0')



h[100].sum tensor(19.0654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6690, device='cuda:0')



h[200].sum tensor(23.0472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.6370, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35560.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0171, 0.0000,  ..., 0.0124, 0.0000, 0.0030],
        [0.0000, 0.0048, 0.0000,  ..., 0.0073, 0.0000, 0.0004],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(192822.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-86.3582, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.5125, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-200.7996, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9993],
        ...,
        [0.9972],
        [0.9970],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365412.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9993],
        ...,
        [0.9972],
        [0.9970],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365412.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0057,  0.0230,  0.0155,  ...,  0.0210,  0.0199,  0.0212],
        [ 0.0077,  0.0305,  0.0208,  ...,  0.0283,  0.0264,  0.0282],
        [ 0.0101,  0.0392,  0.0270,  ...,  0.0366,  0.0339,  0.0363],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(373.1572, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.2539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.0940, device='cuda:0')



h[100].sum tensor(20.5274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.2577, device='cuda:0')



h[200].sum tensor(26.1520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.8123, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0257, 0.1031, 0.0698,  ..., 0.0948, 0.0890, 0.0950],
        [0.0283, 0.1126, 0.0766,  ..., 0.1039, 0.0973, 0.1038],
        [0.0341, 0.1335, 0.0914,  ..., 0.1239, 0.1154, 0.1233],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43753.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.5733e-01, 0.0000e+00,  ..., 1.0673e-01, 0.0000e+00,
         7.7396e-02],
        [0.0000e+00, 3.0713e-01, 0.0000e+00,  ..., 1.2583e-01, 0.0000e+00,
         9.3487e-02],
        [0.0000e+00, 3.5097e-01, 0.0000e+00,  ..., 1.4272e-01, 0.0000e+00,
         1.0732e-01],
        ...,
        [0.0000e+00, 3.2372e-04, 0.0000e+00,  ..., 5.2854e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.2391e-04, 0.0000e+00,  ..., 5.2850e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.2393e-04, 0.0000e+00,  ..., 5.2848e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(224156.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-126.0033, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.8504, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-260.3155, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9993],
        ...,
        [0.9972],
        [0.9970],
        [0.9968]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365412.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9992],
        ...,
        [0.9971],
        [0.9969],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365403.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(243.3378, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.7969, device='cuda:0')



h[100].sum tensor(19.3910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2539, device='cuda:0')



h[200].sum tensor(23.7381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.7104, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0043, 0.0212, 0.0128,  ..., 0.0176, 0.0181, 0.0188],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37233.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0655, 0.0000,  ..., 0.0324, 0.0000, 0.0151],
        [0.0000, 0.0297, 0.0000,  ..., 0.0176, 0.0000, 0.0060],
        [0.0000, 0.0125, 0.0000,  ..., 0.0105, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(199082.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-94.4969, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.8880, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-213.0275, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0005],
        [0.9992],
        ...,
        [0.9971],
        [0.9969],
        [0.9967]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365403.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0004],
        [0.9992],
        ...,
        [0.9971],
        [0.9969],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365393.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0029,  0.0130,  0.0084,  ...,  0.0115,  0.0112,  0.0119],
        [ 0.0020,  0.0095,  0.0059,  ...,  0.0081,  0.0082,  0.0086],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(246.9262, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.8582, device='cuda:0')



h[100].sum tensor(19.4008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2734, device='cuda:0')



h[200].sum tensor(23.7581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.7795, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0071, 0.0314, 0.0201,  ..., 0.0274, 0.0270, 0.0283],
        [0.0063, 0.0284, 0.0180,  ..., 0.0245, 0.0244, 0.0256],
        [0.0150, 0.0638, 0.0420,  ..., 0.0573, 0.0550, 0.0585],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38567.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0883, 0.0000,  ..., 0.0408, 0.0000, 0.0238],
        [0.0000, 0.0951, 0.0000,  ..., 0.0438, 0.0000, 0.0253],
        [0.0000, 0.1365, 0.0000,  ..., 0.0604, 0.0000, 0.0374],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(208317.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-100.7437, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.5739, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-222.8825, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0004],
        [0.9992],
        ...,
        [0.9971],
        [0.9969],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365393.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0004],
        [0.9992],
        ...,
        [0.9970],
        [0.9968],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365383.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0011,  0.0064,  0.0037,  ...,  0.0051,  0.0054,  0.0057],
        [ 0.0011,  0.0064,  0.0037,  ...,  0.0051,  0.0054,  0.0057],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(272.3948, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.1505, device='cuda:0')



h[100].sum tensor(19.5951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.6846, device='cuda:0')



h[200].sum tensor(24.1702, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.2370, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0019, 0.0125, 0.0066,  ..., 0.0092, 0.0105, 0.0107],
        [0.0019, 0.0125, 0.0066,  ..., 0.0092, 0.0105, 0.0107],
        [0.0019, 0.0125, 0.0066,  ..., 0.0092, 0.0105, 0.0107],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37635.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0266, 0.0000,  ..., 0.0166, 0.0000, 0.0038],
        [0.0000, 0.0414, 0.0000,  ..., 0.0228, 0.0000, 0.0077],
        [0.0000, 0.0742, 0.0000,  ..., 0.0357, 0.0000, 0.0178],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(198532.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-96.2260, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.4127, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-216.1389, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0004],
        [0.9992],
        ...,
        [0.9970],
        [0.9968],
        [0.9966]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365383.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0004],
        [0.9992],
        ...,
        [0.9970],
        [0.9967],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365374.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(163.4559, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5139, device='cuda:0')



h[100].sum tensor(18.6582, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8911, device='cuda:0')



h[200].sum tensor(22.1799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.8800, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30609.2227, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(160155.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-63.8839, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.1144, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-166.2494, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0004],
        [0.9992],
        ...,
        [0.9970],
        [0.9967],
        [0.9965]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365374.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0007],
        [1.0004],
        [0.9991],
        ...,
        [0.9969],
        [0.9967],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365364.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(238.0201, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0927, device='cuda:0')



h[100].sum tensor(19.2642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0298, device='cuda:0')



h[200].sum tensor(23.4665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.9162, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0056, 0.0260, 0.0162,  ..., 0.0222, 0.0222, 0.0233],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38874.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0761, 0.0000,  ..., 0.0363, 0.0000, 0.0187],
        [0.0000, 0.0295, 0.0000,  ..., 0.0174, 0.0000, 0.0059],
        [0.0000, 0.0106, 0.0000,  ..., 0.0097, 0.0000, 0.0012],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(221197.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-102.7622, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.6513, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-226.0044, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0007],
        [1.0004],
        [0.9991],
        ...,
        [0.9969],
        [0.9967],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365364.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0004],
        [0.9991],
        ...,
        [0.9968],
        [0.9966],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365354.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(190.1785, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.6887, device='cuda:0')



h[100].sum tensor(18.8468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2649, device='cuda:0')



h[200].sum tensor(22.5796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.2050, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0027, 0.0134, 0.0078,  ..., 0.0107, 0.0113, 0.0116],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34113.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0419, 0.0000,  ..., 0.0222, 0.0000, 0.0102],
        [0.0000, 0.0111, 0.0000,  ..., 0.0098, 0.0000, 0.0021],
        [0.0000, 0.0023, 0.0000,  ..., 0.0062, 0.0000, 0.0001],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(188302.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-79.5921, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.5681, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-190.7212, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0004],
        [0.9991],
        ...,
        [0.9968],
        [0.9966],
        [0.9964]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365354.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0004],
        [0.9991],
        ...,
        [0.9968],
        [0.9966],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365344.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(144.2985, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.5455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4475, device='cuda:0')



h[100].sum tensor(18.4501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.5518, device='cuda:0')



h[200].sum tensor(21.7367, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.6774, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0062, 0.0282, 0.0178,  ..., 0.0243, 0.0241, 0.0253],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30619.2930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0081, 0.0000, 0.0012],
        [0.0000, 0.0303, 0.0000,  ..., 0.0175, 0.0000, 0.0075],
        [0.0000, 0.0882, 0.0000,  ..., 0.0409, 0.0000, 0.0235],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(167316.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-63.0741, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.6339, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-165.5853, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0004],
        [0.9991],
        ...,
        [0.9968],
        [0.9966],
        [0.9963]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365344.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0004],
        [0.9991],
        ...,
        [0.9967],
        [0.9965],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365335.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0016,  0.0081,  0.0049,  ...,  0.0068,  0.0070,  0.0073],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(146.5617, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.5432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3407, device='cuda:0')



h[100].sum tensor(18.4531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.5178, device='cuda:0')



h[200].sum tensor(21.7427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.5569, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0270, 0.0170,  ..., 0.0232, 0.0231, 0.0243],
        [0.0016, 0.0094, 0.0049,  ..., 0.0068, 0.0078, 0.0078],
        [0.0104, 0.0436, 0.0287,  ..., 0.0390, 0.0375, 0.0397],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30529.8320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1170, 0.0000,  ..., 0.0525, 0.0000, 0.0317],
        [0.0000, 0.0962, 0.0000,  ..., 0.0439, 0.0000, 0.0262],
        [0.0000, 0.1413, 0.0000,  ..., 0.0612, 0.0000, 0.0411],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(164778.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-62.5087, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.9195, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.6250, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0004],
        [0.9991],
        ...,
        [0.9967],
        [0.9965],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365335.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0004],
        [0.9990],
        ...,
        [0.9967],
        [0.9964],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365325.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(167.0993, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2903, device='cuda:0')



h[100].sum tensor(18.6053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8199, device='cuda:0')



h[200].sum tensor(22.0657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.6278, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32120.5645, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0146, 0.0000,  ..., 0.0113, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(173151.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-70.8641, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.7954, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-177.1545, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0004],
        [0.9990],
        ...,
        [0.9967],
        [0.9964],
        [0.9962]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365325.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 80.0 event: 400 loss: tensor(1115.5414, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0004],
        [0.9990],
        ...,
        [0.9966],
        [0.9964],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365315.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0039,  0.0166,  0.0109,  ...,  0.0149,  0.0143,  0.0152],
        [ 0.0017,  0.0086,  0.0053,  ...,  0.0073,  0.0074,  0.0078],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(259.5141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.2677, device='cuda:0')



h[100].sum tensor(19.3399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0856, device='cuda:0')



h[200].sum tensor(23.6254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.1136, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0052, 0.0245, 0.0152,  ..., 0.0207, 0.0209, 0.0219],
        [0.0056, 0.0277, 0.0169,  ..., 0.0233, 0.0237, 0.0249],
        [0.0236, 0.0954, 0.0644,  ..., 0.0874, 0.0824, 0.0878],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34243.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0788, 0.0000,  ..., 0.0372, 0.0000, 0.0201],
        [0.0000, 0.1354, 0.0000,  ..., 0.0598, 0.0000, 0.0369],
        [0.0000, 0.2469, 0.0000,  ..., 0.1031, 0.0000, 0.0726],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(175657.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.0453, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.4543, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.6535, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0004],
        [0.9990],
        ...,
        [0.9966],
        [0.9964],
        [0.9961]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365315.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0003],
        [0.9990],
        ...,
        [0.9965],
        [0.9963],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365305.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(346.5518, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.3155, device='cuda:0')



h[100].sum tensor(20.0230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3736, device='cuda:0')



h[200].sum tensor(25.0760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.6787, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43157.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(230066.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-122.8658, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.4335, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-255.9523, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0003],
        [0.9990],
        ...,
        [0.9965],
        [0.9963],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365305.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0003],
        [0.9990],
        ...,
        [0.9965],
        [0.9962],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365295.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(450.0597, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.6895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.7555, device='cuda:0')



h[100].sum tensor(20.8312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.7863, device='cuda:0')



h[200].sum tensor(26.7923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.6861, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0011, 0.0075, 0.0036,  ..., 0.0050, 0.0062, 0.0061],
        [0.0021, 0.0133, 0.0072,  ..., 0.0101, 0.0112, 0.0115],
        [0.0011, 0.0075, 0.0036,  ..., 0.0050, 0.0062, 0.0061],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45883.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0356, 0.0000,  ..., 0.0204, 0.0000, 0.0062],
        [0.0000, 0.0409, 0.0000,  ..., 0.0226, 0.0000, 0.0074],
        [0.0000, 0.0392, 0.0000,  ..., 0.0218, 0.0000, 0.0072],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(236863.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-136.3151, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.5135, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-276.3913, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0003],
        [0.9990],
        ...,
        [0.9965],
        [0.9962],
        [0.9960]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365295.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0003],
        [0.9989],
        ...,
        [0.9964],
        [0.9962],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365285.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(227.1102, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6966, device='cuda:0')



h[100].sum tensor(19.0204, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5856, device='cuda:0')



h[200].sum tensor(22.9462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.3416, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34575.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0582, 0.0000,  ..., 0.0288, 0.0000, 0.0150],
        [0.0000, 0.0353, 0.0000,  ..., 0.0196, 0.0000, 0.0085],
        [0.0000, 0.0253, 0.0000,  ..., 0.0155, 0.0000, 0.0056],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(185349.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-82.0762, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.1426, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-194.0898, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0003],
        [0.9989],
        ...,
        [0.9964],
        [0.9962],
        [0.9959]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365285.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0003],
        [0.9989],
        ...,
        [0.9963],
        [0.9961],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365275.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0120,  0.0460,  0.0318,  ...,  0.0431,  0.0398,  0.0426],
        [ 0.0085,  0.0332,  0.0227,  ...,  0.0308,  0.0287,  0.0306],
        [ 0.0042,  0.0177,  0.0117,  ...,  0.0160,  0.0153,  0.0162],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(221.7400, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1164, device='cuda:0')



h[100].sum tensor(18.9593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4010, device='cuda:0')



h[200].sum tensor(22.8162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.6873, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0363, 0.1415, 0.0971,  ..., 0.1316, 0.1223, 0.1307],
        [0.0331, 0.1301, 0.0890,  ..., 0.1207, 0.1125, 0.1201],
        [0.0270, 0.1079, 0.0732,  ..., 0.0994, 0.0932, 0.0994],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32559.7441, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.5921e-01, 0.0000e+00,  ..., 1.4536e-01, 0.0000e+00,
         1.1088e-01],
        [0.0000e+00, 3.1398e-01, 0.0000e+00,  ..., 1.2801e-01, 0.0000e+00,
         9.6115e-02],
        [0.0000e+00, 2.5514e-01, 0.0000e+00,  ..., 1.0577e-01, 0.0000e+00,
         7.6463e-02],
        ...,
        [0.0000e+00, 3.2837e-04, 0.0000e+00,  ..., 5.2843e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.2857e-04, 0.0000e+00,  ..., 5.2838e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.2859e-04, 0.0000e+00,  ..., 5.2837e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(170377.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-72.9233, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.6884, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-179.9078, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0003],
        [0.9989],
        ...,
        [0.9963],
        [0.9961],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365275.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0003],
        [0.9989],
        ...,
        [0.9963],
        [0.9960],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365265.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0135,  0.0517,  0.0358,  ...,  0.0485,  0.0447,  0.0478],
        [ 0.0038,  0.0160,  0.0106,  ...,  0.0144,  0.0138,  0.0147],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(220.1282, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1537, device='cuda:0')



h[100].sum tensor(18.9285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4129, device='cuda:0')



h[200].sum tensor(22.7506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.7294, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0215, 0.0878, 0.0590,  ..., 0.0802, 0.0758, 0.0807],
        [0.0264, 0.1038, 0.0708,  ..., 0.0960, 0.0896, 0.0956],
        [0.0101, 0.0425, 0.0279,  ..., 0.0379, 0.0365, 0.0386],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33540.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2632, 0.0000,  ..., 0.1090, 0.0000, 0.0790],
        [0.0000, 0.2627, 0.0000,  ..., 0.1086, 0.0000, 0.0791],
        [0.0000, 0.1931, 0.0000,  ..., 0.0817, 0.0000, 0.0568],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(178597.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-77.0005, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.0380, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.6548, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0003],
        [0.9989],
        ...,
        [0.9963],
        [0.9960],
        [0.9958]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365265.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0003],
        [0.9988],
        ...,
        [0.9962],
        [0.9960],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365255.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(461.5798, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.7386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.2363, device='cuda:0')



h[100].sum tensor(20.8112, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.9393, device='cuda:0')



h[200].sum tensor(26.7491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.2283, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46079.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(243587.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-136.5800, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.5384, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-277.5273, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0003],
        [0.9988],
        ...,
        [0.9962],
        [0.9960],
        [0.9957]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365255.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0003],
        [0.9988],
        ...,
        [0.9961],
        [0.9959],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365245.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0046,  0.0190,  0.0126,  ...,  0.0172,  0.0164,  0.0174],
        [ 0.0101,  0.0391,  0.0269,  ...,  0.0364,  0.0338,  0.0361],
        [ 0.0079,  0.0313,  0.0214,  ...,  0.0290,  0.0270,  0.0289],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(398.0436, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.8144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5738, device='cuda:0')



h[100].sum tensor(20.2862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7739, device='cuda:0')



h[200].sum tensor(25.6340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0978, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0240, 0.0967, 0.0653,  ..., 0.0887, 0.0835, 0.0890],
        [0.0256, 0.1026, 0.0695,  ..., 0.0943, 0.0886, 0.0945],
        [0.0321, 0.1264, 0.0863,  ..., 0.1171, 0.1092, 0.1166],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43277.4922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2161, 0.0000,  ..., 0.0907, 0.0000, 0.0640],
        [0.0000, 0.2636, 0.0000,  ..., 0.1094, 0.0000, 0.0788],
        [0.0000, 0.2976, 0.0000,  ..., 0.1226, 0.0000, 0.0896],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(225729.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-123.1223, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.4100, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-256.7437, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0003],
        [0.9988],
        ...,
        [0.9961],
        [0.9959],
        [0.9956]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365245.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0003],
        [0.9988],
        ...,
        [0.9961],
        [0.9958],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365235.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0015,  0.0077,  0.0046,  ...,  0.0064,  0.0066,  0.0069],
        [ 0.0061,  0.0244,  0.0165,  ...,  0.0224,  0.0211,  0.0225],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(576.5582, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.0206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.4476, device='cuda:0')



h[100].sum tensor(21.6523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.2794, device='cuda:0')



h[200].sum tensor(28.5353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.9779, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0190, 0.0787, 0.0525,  ..., 0.0715, 0.0679, 0.0723],
        [0.0071, 0.0332, 0.0209,  ..., 0.0286, 0.0285, 0.0300],
        [0.0121, 0.0534, 0.0346,  ..., 0.0473, 0.0460, 0.0488],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50958.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2104, 0.0000,  ..., 0.0893, 0.0000, 0.0606],
        [0.0000, 0.1627, 0.0000,  ..., 0.0707, 0.0000, 0.0455],
        [0.0000, 0.1425, 0.0000,  ..., 0.0627, 0.0000, 0.0394],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(259711.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-160.1387, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-31.3462, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-312.3961, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0003],
        [0.9988],
        ...,
        [0.9961],
        [0.9958],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365235.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0003],
        [0.9988],
        ...,
        [0.9960],
        [0.9958],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365225.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0081,  0.0050,  ...,  0.0068,  0.0070,  0.0073],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(534.2400, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.7574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.1905, device='cuda:0')



h[100].sum tensor(21.2928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.5612, device='cuda:0')



h[200].sum tensor(27.7717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.4324, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0016, 0.0094, 0.0050,  ..., 0.0068, 0.0078, 0.0078],
        [0.0061, 0.0277, 0.0174,  ..., 0.0238, 0.0237, 0.0249],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50357.5820, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0169, 0.0000,  ..., 0.0122, 0.0000, 0.0034],
        [0.0000, 0.0466, 0.0000,  ..., 0.0245, 0.0000, 0.0105],
        [0.0000, 0.0866, 0.0000,  ..., 0.0408, 0.0000, 0.0216],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(263577.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-157.3885, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-31.7650, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-308.2811, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0003],
        [0.9988],
        ...,
        [0.9960],
        [0.9958],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365225.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0003],
        [0.9988],
        ...,
        [0.9960],
        [0.9958],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365225.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(283.8953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.2031, device='cuda:0')



h[100].sum tensor(19.3525, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0650, device='cuda:0')



h[200].sum tensor(23.6507, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.0407, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39341.9727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 7.3294e-03, 0.0000e+00,  ..., 8.3819e-03, 0.0000e+00,
         2.4497e-04],
        [0.0000e+00, 4.4162e-03, 0.0000e+00,  ..., 7.1589e-03, 0.0000e+00,
         9.2853e-06],
        [0.0000e+00, 1.5338e-02, 0.0000e+00,  ..., 1.1623e-02, 0.0000e+00,
         2.5507e-03],
        ...,
        [0.0000e+00, 3.2927e-04, 0.0000e+00,  ..., 5.2813e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.2947e-04, 0.0000e+00,  ..., 5.2808e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.2949e-04, 0.0000e+00,  ..., 5.2807e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(222749.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-104.3608, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.0406, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-228.2943, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0003],
        [0.9988],
        ...,
        [0.9960],
        [0.9958],
        [0.9955]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365225.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0002],
        [0.9987],
        ...,
        [0.9960],
        [0.9957],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365215.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(220.5899, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7351, device='cuda:0')



h[100].sum tensor(18.8447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2797, device='cuda:0')



h[200].sum tensor(22.5720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.2573, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32267.5586, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0199, 0.0000,  ..., 0.0132, 0.0000, 0.0050],
        [0.0000, 0.0121, 0.0000,  ..., 0.0102, 0.0000, 0.0019],
        [0.0000, 0.0283, 0.0000,  ..., 0.0170, 0.0000, 0.0060],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(169080.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-71.1875, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.0366, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-177.6883, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0002],
        [0.9987],
        ...,
        [0.9960],
        [0.9957],
        [0.9954]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365215.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0002],
        [0.9987],
        ...,
        [0.9959],
        [0.9956],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365204.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0018,  0.0088,  0.0055,  ...,  0.0075,  0.0076,  0.0080],
        [ 0.0013,  0.0071,  0.0042,  ...,  0.0059,  0.0061,  0.0064],
        [ 0.0036,  0.0155,  0.0102,  ...,  0.0139,  0.0134,  0.0142],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(152.9056, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.8664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5613, device='cuda:0')



h[100].sum tensor(18.3084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.2698, device='cuda:0')



h[200].sum tensor(21.4329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.6779, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0058, 0.0267, 0.0168,  ..., 0.0229, 0.0229, 0.0240],
        [0.0129, 0.0565, 0.0368,  ..., 0.0503, 0.0487, 0.0517],
        [0.0053, 0.0267, 0.0162,  ..., 0.0224, 0.0229, 0.0240],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29078.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0839, 0.0000,  ..., 0.0397, 0.0000, 0.0208],
        [0.0000, 0.1184, 0.0000,  ..., 0.0536, 0.0000, 0.0307],
        [0.0000, 0.0913, 0.0000,  ..., 0.0428, 0.0000, 0.0225],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(157785.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-55.4406, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-51.1074, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-154.3744, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0002],
        [0.9987],
        ...,
        [0.9959],
        [0.9956],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365204.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0002],
        [0.9987],
        ...,
        [0.9958],
        [0.9956],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365194.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0020,  0.0097,  0.0060,  ...,  0.0083,  0.0083,  0.0088],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(376.4824, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.4040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.4948, device='cuda:0')



h[100].sum tensor(20.0010, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4306, device='cuda:0')



h[200].sum tensor(25.0277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.8810, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0288, 0.0187,  ..., 0.0254, 0.0247, 0.0259],
        [0.0020, 0.0109, 0.0060,  ..., 0.0083, 0.0092, 0.0093],
        [0.0033, 0.0177, 0.0103,  ..., 0.0142, 0.0150, 0.0155],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40227.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0915, 0.0000,  ..., 0.0414, 0.0000, 0.0263],
        [0.0000, 0.0584, 0.0000,  ..., 0.0288, 0.0000, 0.0150],
        [0.0000, 0.0714, 0.0000,  ..., 0.0344, 0.0000, 0.0178],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(211046.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-109.5874, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.6175, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-235.5131, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0002],
        [0.9987],
        ...,
        [0.9958],
        [0.9956],
        [0.9953]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365194.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0006],
        [1.0002],
        [0.9986],
        ...,
        [0.9958],
        [0.9955],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365184.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0021,  0.0101,  0.0063,  ...,  0.0087,  0.0086,  0.0091],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(426.9429, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.6668, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.1970, device='cuda:0')



h[100].sum tensor(20.3617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9722, device='cuda:0')



h[200].sum tensor(25.7938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.8007, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0021, 0.0113, 0.0063,  ..., 0.0087, 0.0095, 0.0096],
        [0.0016, 0.0095, 0.0051,  ..., 0.0070, 0.0080, 0.0080],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44575.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0095, 0.0000,  ..., 0.0093, 0.0000, 0.0011],
        [0.0000, 0.0260, 0.0000,  ..., 0.0162, 0.0000, 0.0045],
        [0.0000, 0.0392, 0.0000,  ..., 0.0217, 0.0000, 0.0078],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(239287.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-130.2466, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-36.8822, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-266.6160, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0006],
        [1.0002],
        [0.9986],
        ...,
        [0.9958],
        [0.9955],
        [0.9952]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365184.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0002],
        [0.9986],
        ...,
        [0.9957],
        [0.9954],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365174.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(563.9417, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.5983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.5206, device='cuda:0')



h[100].sum tensor(21.3730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.6662, device='cuda:0')



h[200].sum tensor(27.9416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.8046, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49387.1289, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(252515.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-152.7304, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-32.4457, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-301.4744, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0002],
        [0.9986],
        ...,
        [0.9957],
        [0.9954],
        [0.9951]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365174.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0002],
        [0.9986],
        ...,
        [0.9956],
        [0.9953],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365163.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0017,  0.0083,  0.0051,  ...,  0.0070,  0.0072,  0.0075],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(284.9701, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9170, device='cuda:0')



h[100].sum tensor(19.2456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9740, device='cuda:0')



h[200].sum tensor(23.4232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.7181, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0062, 0.0282, 0.0178,  ..., 0.0243, 0.0241, 0.0253],
        [0.0017, 0.0096, 0.0051,  ..., 0.0070, 0.0080, 0.0080],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35173.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0890, 0.0000,  ..., 0.0417, 0.0000, 0.0225],
        [0.0000, 0.0468, 0.0000,  ..., 0.0245, 0.0000, 0.0108],
        [0.0000, 0.0184, 0.0000,  ..., 0.0128, 0.0000, 0.0037],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(182847.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-84.6301, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.3666, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-198.7414, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0002],
        [0.9986],
        ...,
        [0.9956],
        [0.9953],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365163.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0002],
        [0.9986],
        ...,
        [0.9956],
        [0.9953],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365153.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0121,  0.0465,  0.0322,  ...,  0.0436,  0.0403,  0.0431],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0070,  0.0278,  0.0189,  ...,  0.0256,  0.0240,  0.0256],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(359.3134, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.6520, device='cuda:0')



h[100].sum tensor(19.7836, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8442, device='cuda:0')



h[200].sum tensor(24.5656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.8026, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0207, 0.0829, 0.0560,  ..., 0.0760, 0.0715, 0.0762],
        [0.0303, 0.1198, 0.0817,  ..., 0.1108, 0.1035, 0.1105],
        [0.0056, 0.0240, 0.0153,  ..., 0.0208, 0.0205, 0.0214],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39264.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2827, 0.0000,  ..., 0.1156, 0.0000, 0.0866],
        [0.0000, 0.2281, 0.0000,  ..., 0.0945, 0.0000, 0.0695],
        [0.0000, 0.1062, 0.0000,  ..., 0.0470, 0.0000, 0.0312],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(208268.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-104.7915, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.7012, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-228.2871, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0002],
        [0.9986],
        ...,
        [0.9956],
        [0.9953],
        [0.9950]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365153.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0002],
        [0.9985],
        ...,
        [0.9955],
        [0.9952],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365143.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0030,  0.0133,  0.0087,  ...,  0.0118,  0.0115,  0.0122],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(463.5188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.3075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.6497, device='cuda:0')



h[100].sum tensor(20.5387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1163, device='cuda:0')



h[200].sum tensor(26.1694, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.3112, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0030, 0.0146, 0.0087,  ..., 0.0118, 0.0124, 0.0127],
        [0.0113, 0.0466, 0.0308,  ..., 0.0419, 0.0401, 0.0424],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46512.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0337, 0.0000,  ..., 0.0189, 0.0000, 0.0082],
        [0.0000, 0.0983, 0.0000,  ..., 0.0443, 0.0000, 0.0277],
        [0.0000, 0.2082, 0.0000,  ..., 0.0871, 0.0000, 0.0625],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(244899.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-139.2927, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-34.8637, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-280.9696, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0002],
        [0.9985],
        ...,
        [0.9955],
        [0.9952],
        [0.9949]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365143.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0002],
        [0.9985],
        ...,
        [0.9954],
        [0.9951],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365132.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(155.4737, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1449, device='cuda:0')



h[100].sum tensor(18.2307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.1373, device='cuda:0')



h[200].sum tensor(21.2673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.2083, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0072, 0.0297, 0.0193,  ..., 0.0262, 0.0254, 0.0267],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28520.2227, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0073, 0.0000, 0.0010],
        [0.0000, 0.0208, 0.0000,  ..., 0.0135, 0.0000, 0.0055],
        [0.0000, 0.0706, 0.0000,  ..., 0.0330, 0.0000, 0.0202],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(153522.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.3845, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-51.2818, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-150.7781, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0002],
        [0.9985],
        ...,
        [0.9954],
        [0.9951],
        [0.9948]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365132.9062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 100.0 event: 500 loss: tensor(1186.8400, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0001],
        [0.9985],
        ...,
        [0.9954],
        [0.9951],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365122.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0083,  0.0051,  ...,  0.0070,  0.0071,  0.0075],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(185.0506, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1717, device='cuda:0')



h[100].sum tensor(18.4355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4640, device='cuda:0')



h[200].sum tensor(21.7023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.3663, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0016, 0.0095, 0.0051,  ..., 0.0070, 0.0080, 0.0080],
        [0.0012, 0.0081, 0.0040,  ..., 0.0056, 0.0067, 0.0066],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29767.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0079, 0.0000, 0.0006],
        [0.0000, 0.0194, 0.0000,  ..., 0.0135, 0.0000, 0.0030],
        [0.0000, 0.0262, 0.0000,  ..., 0.0164, 0.0000, 0.0044],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0055, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(159809.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-59.2262, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.3585, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-159.5078, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0001],
        [0.9985],
        ...,
        [0.9954],
        [0.9951],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365122.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0001],
        [0.9984],
        ...,
        [0.9953],
        [0.9950],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365112.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0018,  0.0089,  0.0055,  ...,  0.0076,  0.0077,  0.0081],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0018,  0.0089,  0.0055,  ...,  0.0076,  0.0077,  0.0081],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(543.5736, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.2556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.3839, device='cuda:0')



h[100].sum tensor(21.0535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.3045, device='cuda:0')



h[200].sum tensor(27.2626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.5227, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0061, 0.0279, 0.0176,  ..., 0.0240, 0.0239, 0.0250],
        [0.0098, 0.0450, 0.0287,  ..., 0.0393, 0.0387, 0.0410],
        [0.0031, 0.0148, 0.0088,  ..., 0.0120, 0.0126, 0.0129],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49803.3164, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0973, 0.0000,  ..., 0.0451, 0.0000, 0.0244],
        [0.0000, 0.0971, 0.0000,  ..., 0.0453, 0.0000, 0.0238],
        [0.0000, 0.0587, 0.0000,  ..., 0.0295, 0.0000, 0.0134],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(261260.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-154.4575, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-32.7138, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-303.6698, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0001],
        [0.9984],
        ...,
        [0.9953],
        [0.9950],
        [0.9947]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365112.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0001],
        [0.9984],
        ...,
        [0.9952],
        [0.9949],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365101.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(406.6057, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.2325, device='cuda:0')



h[100].sum tensor(20.0260, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3471, device='cuda:0')



h[200].sum tensor(25.0803, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.5850, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0014, 0.0088, 0.0046,  ..., 0.0063, 0.0074, 0.0073],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40690.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.1225e-02, 0.0000e+00,  ..., 1.0098e-02, 0.0000e+00,
         3.7190e-04],
        [0.0000e+00, 1.4438e-02, 0.0000e+00,  ..., 1.1322e-02, 0.0000e+00,
         1.6928e-03],
        [0.0000e+00, 3.0814e-02, 0.0000e+00,  ..., 1.7999e-02, 0.0000e+00,
         6.2959e-03],
        ...,
        [0.0000e+00, 1.9959e-03, 0.0000e+00,  ..., 5.9601e-03, 0.0000e+00,
         1.6643e-05],
        [0.0000e+00, 3.3105e-04, 0.0000e+00,  ..., 5.2716e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3107e-04, 0.0000e+00,  ..., 5.2715e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(211176.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-111.1097, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.4106, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-238.5717, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0001],
        [0.9984],
        ...,
        [0.9952],
        [0.9949],
        [0.9946]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365101.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0001],
        [0.9984],
        ...,
        [0.9952],
        [0.9949],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365091.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0027,  0.0124,  0.0080,  ...,  0.0109,  0.0106,  0.0113],
        [ 0.0023,  0.0107,  0.0068,  ...,  0.0093,  0.0092,  0.0097],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(141.4916, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.3483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1259, device='cuda:0')



h[100].sum tensor(18.0763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.8131, device='cuda:0')



h[200].sum tensor(20.9392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.0591, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0028, 0.0136, 0.0080,  ..., 0.0109, 0.0115, 0.0118],
        [0.0067, 0.0300, 0.0191,  ..., 0.0261, 0.0258, 0.0271],
        [0.0171, 0.0718, 0.0477,  ..., 0.0649, 0.0619, 0.0659],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27671.2129, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0739, 0.0000,  ..., 0.0350, 0.0000, 0.0196],
        [0.0000, 0.1081, 0.0000,  ..., 0.0487, 0.0000, 0.0296],
        [0.0000, 0.1683, 0.0000,  ..., 0.0727, 0.0000, 0.0476],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(149610.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-49.1534, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-52.1293, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-144.5430, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0001],
        [0.9984],
        ...,
        [0.9952],
        [0.9949],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365091.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0001],
        [0.9984],
        ...,
        [0.9951],
        [0.9948],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365080.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0037,  0.0157,  0.0103,  ...,  0.0141,  0.0135,  0.0144],
        [ 0.0066,  0.0264,  0.0179,  ...,  0.0243,  0.0228,  0.0243],
        [ 0.0069,  0.0275,  0.0186,  ...,  0.0253,  0.0237,  0.0253],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(251.7750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7822, device='cuda:0')



h[100].sum tensor(18.8622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2947, device='cuda:0')



h[200].sum tensor(22.6084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.3104, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0201, 0.0825, 0.0552,  ..., 0.0751, 0.0712, 0.0758],
        [0.0274, 0.1093, 0.0742,  ..., 0.1007, 0.0944, 0.1007],
        [0.0240, 0.0967, 0.0653,  ..., 0.0887, 0.0835, 0.0890],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33992.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2606, 0.0000,  ..., 0.1089, 0.0000, 0.0763],
        [0.0000, 0.2758, 0.0000,  ..., 0.1144, 0.0000, 0.0822],
        [0.0000, 0.2323, 0.0000,  ..., 0.0972, 0.0000, 0.0688],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(187005.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-79.5183, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.3692, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-190.2776, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0001],
        [0.9984],
        ...,
        [0.9951],
        [0.9948],
        [0.9945]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365080.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0001],
        [0.9983],
        ...,
        [0.9950],
        [0.9947],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365070.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049,  0.0202,  0.0135,  ...,  0.0184,  0.0174,  0.0185],
        [ 0.0027,  0.0120,  0.0077,  ...,  0.0106,  0.0103,  0.0110],
        [ 0.0107,  0.0415,  0.0286,  ...,  0.0387,  0.0359,  0.0383],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(280.9185, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7831, device='cuda:0')



h[100].sum tensor(19.0562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6132, device='cuda:0')



h[200].sum tensor(23.0205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.4393, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0139, 0.0600, 0.0393,  ..., 0.0536, 0.0517, 0.0549],
        [0.0260, 0.1043, 0.0707,  ..., 0.0960, 0.0901, 0.0961],
        [0.0108, 0.0468, 0.0305,  ..., 0.0415, 0.0403, 0.0426],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34258.9102, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1361, 0.0000,  ..., 0.0597, 0.0000, 0.0384],
        [0.0000, 0.1875, 0.0000,  ..., 0.0795, 0.0000, 0.0551],
        [0.0000, 0.1512, 0.0000,  ..., 0.0654, 0.0000, 0.0435],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(181760.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.3837, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.3173, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.8997, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0001],
        [0.9983],
        ...,
        [0.9950],
        [0.9947],
        [0.9944]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365070.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0001],
        [0.9983],
        ...,
        [0.9950],
        [0.9946],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365059.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(240.4348, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9614, device='cuda:0')



h[100].sum tensor(18.7491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0335, device='cuda:0')



h[200].sum tensor(22.3681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.3847, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0020, 0.0109, 0.0060,  ..., 0.0083, 0.0092, 0.0092],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31904.6895, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0359, 0.0000,  ..., 0.0202, 0.0000, 0.0072],
        [0.0000, 0.0135, 0.0000,  ..., 0.0109, 0.0000, 0.0022],
        [0.0000, 0.0032, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(168644.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-69.1119, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.5066, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-174.8043, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0001],
        [0.9983],
        ...,
        [0.9950],
        [0.9946],
        [0.9943]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365059.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0001],
        [0.9983],
        ...,
        [0.9949],
        [0.9946],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365049.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0069,  0.0041,  ...,  0.0056,  0.0059,  0.0062],
        [ 0.0012,  0.0069,  0.0041,  ...,  0.0056,  0.0059,  0.0062],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(694.9208, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.3651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.3923, device='cuda:0')



h[100].sum tensor(21.9782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.8982, device='cuda:0')



h[200].sum tensor(29.2267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.1711, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0091, 0.0427, 0.0270,  ..., 0.0370, 0.0367, 0.0388],
        [0.0051, 0.0259, 0.0156,  ..., 0.0216, 0.0222, 0.0232],
        [0.0112, 0.0501, 0.0323,  ..., 0.0441, 0.0431, 0.0457],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(56585.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1277, 0.0000,  ..., 0.0578, 0.0000, 0.0323],
        [0.0000, 0.1150, 0.0000,  ..., 0.0527, 0.0000, 0.0287],
        [0.0000, 0.1339, 0.0000,  ..., 0.0600, 0.0000, 0.0350],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(297793.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-187.8961, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-25.1370, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-354.6281, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0001],
        [0.9983],
        ...,
        [0.9949],
        [0.9946],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365049.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0000],
        [0.9982],
        ...,
        [0.9948],
        [0.9945],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365038.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(402.0906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.3480, device='cuda:0')



h[100].sum tensor(19.8674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0657, device='cuda:0')



h[200].sum tensor(24.7434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.5875, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0073, 0.0302, 0.0197,  ..., 0.0267, 0.0259, 0.0272],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39877.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0172, 0.0000,  ..., 0.0123, 0.0000, 0.0030],
        [0.0000, 0.0359, 0.0000,  ..., 0.0196, 0.0000, 0.0092],
        [0.0000, 0.0888, 0.0000,  ..., 0.0404, 0.0000, 0.0252],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(207012.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-107.3403, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.2874, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-232.4424, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0000],
        [0.9982],
        ...,
        [0.9948],
        [0.9945],
        [0.9942]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365038.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0005],
        [1.0000],
        [0.9982],
        ...,
        [0.9948],
        [0.9944],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365028., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [ 0.0042,  0.0178,  0.0118,  ...,  0.0161,  0.0154,  0.0164],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(538.3564, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.7537, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.3534, device='cuda:0')



h[100].sum tensor(20.8111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6584, device='cuda:0')



h[200].sum tensor(26.7478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.2326, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0061, 0.0294, 0.0182,  ..., 0.0250, 0.0252, 0.0265],
        [0.0056, 0.0259, 0.0162,  ..., 0.0221, 0.0221, 0.0232],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48801.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0078, 0.0000, 0.0009],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.1041, 0.0000,  ..., 0.0476, 0.0000, 0.0271],
        [0.0000, 0.0759, 0.0000,  ..., 0.0360, 0.0000, 0.0195],
        [0.0000, 0.0256, 0.0000,  ..., 0.0156, 0.0000, 0.0060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(260403.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-149.8918, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-32.9291, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-297.3172, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0005],
        [1.0000],
        [0.9982],
        ...,
        [0.9948],
        [0.9944],
        [0.9941]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365028., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 110.0 event: 550 loss: tensor(606.0385, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0000],
        [0.9982],
        ...,
        [0.9947],
        [0.9944],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365017.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0069,  0.0041,  ...,  0.0056,  0.0059,  0.0062],
        [ 0.0017,  0.0087,  0.0053,  ...,  0.0074,  0.0074,  0.0078],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(406.1520, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.4899, device='cuda:0')



h[100].sum tensor(19.8558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.1108, device='cuda:0')



h[200].sum tensor(24.7187, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.7476, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0154, 0.0656, 0.0433,  ..., 0.0590, 0.0566, 0.0601],
        [0.0061, 0.0279, 0.0176,  ..., 0.0240, 0.0239, 0.0251],
        [0.0017, 0.0099, 0.0053,  ..., 0.0074, 0.0083, 0.0083],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39425.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1483, 0.0000,  ..., 0.0652, 0.0000, 0.0406],
        [0.0000, 0.0989, 0.0000,  ..., 0.0453, 0.0000, 0.0262],
        [0.0000, 0.0505, 0.0000,  ..., 0.0259, 0.0000, 0.0123],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(206304.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-104.9496, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.8851, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-228.9129, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0000],
        [0.9982],
        ...,
        [0.9947],
        [0.9944],
        [0.9940]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365017.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0000],
        [0.9981],
        ...,
        [0.9946],
        [0.9943],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(365006.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0012,  0.0069,  0.0041,  ...,  0.0056,  0.0059,  0.0062],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(581.8678, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.2291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.0170, device='cuda:0')



h[100].sum tensor(21.0677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.1878, device='cuda:0')



h[200].sum tensor(27.2927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.1089, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0009, 0.0069, 0.0032,  ..., 0.0045, 0.0057, 0.0055],
        [0.0059, 0.0288, 0.0177,  ..., 0.0243, 0.0246, 0.0259],
        [0.0113, 0.0485, 0.0317,  ..., 0.0432, 0.0417, 0.0442],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50157.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0471, 0.0000,  ..., 0.0248, 0.0000, 0.0102],
        [0.0000, 0.0953, 0.0000,  ..., 0.0444, 0.0000, 0.0236],
        [0.0000, 0.1334, 0.0000,  ..., 0.0596, 0.0000, 0.0351],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(264416.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-156.1890, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-32.2812, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-306.3068, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0000],
        [0.9981],
        ...,
        [0.9946],
        [0.9943],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(365006.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0000],
        [0.9981],
        ...,
        [0.9945],
        [0.9942],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364996.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(283.0860, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1825, device='cuda:0')



h[100].sum tensor(18.9573, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4220, device='cuda:0')



h[200].sum tensor(22.8103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.7619, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34220.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(181722., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.5868, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.1801, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.8605, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0000],
        [0.9981],
        ...,
        [0.9945],
        [0.9942],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364996.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0000],
        [0.9981],
        ...,
        [0.9945],
        [0.9942],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364996.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0083,  0.0050,  ...,  0.0070,  0.0071,  0.0075],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(397.3195, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.9163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.8807, device='cuda:0')



h[100].sum tensor(19.7545, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9170, device='cuda:0')



h[200].sum tensor(24.5035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.0606, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0016, 0.0095, 0.0050,  ..., 0.0070, 0.0079, 0.0080],
        [0.0027, 0.0133, 0.0078,  ..., 0.0106, 0.0113, 0.0115],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40155.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0104, 0.0000,  ..., 0.0096, 0.0000, 0.0017],
        [0.0000, 0.0343, 0.0000,  ..., 0.0195, 0.0000, 0.0071],
        [0.0000, 0.0565, 0.0000,  ..., 0.0287, 0.0000, 0.0128],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(212927.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-108.1510, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.3665, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-234.0076, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0000],
        [0.9981],
        ...,
        [0.9945],
        [0.9942],
        [0.9939]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364996.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0000],
        [0.9981],
        ...,
        [0.9945],
        [0.9941],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364985.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(352.8640, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.9668, device='cuda:0')



h[100].sum tensor(19.4264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3080, device='cuda:0')



h[200].sum tensor(23.8066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.9020, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0030, 0.0163, 0.0094,  ..., 0.0129, 0.0139, 0.0143],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36018.1914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0072, 0.0000, 0.0004],
        [0.0000, 0.0178, 0.0000,  ..., 0.0127, 0.0000, 0.0033],
        [0.0000, 0.0507, 0.0000,  ..., 0.0263, 0.0000, 0.0111],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(188399.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-88.8226, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.8812, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-204.4339, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0000],
        [0.9981],
        ...,
        [0.9945],
        [0.9941],
        [0.9938]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364985.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0000],
        [0.9980],
        ...,
        [0.9944],
        [0.9941],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364974.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0110,  0.0423,  0.0292,  ...,  0.0395,  0.0366,  0.0392],
        [ 0.0024,  0.0109,  0.0069,  ...,  0.0095,  0.0094,  0.0099],
        [ 0.0088,  0.0345,  0.0236,  ...,  0.0320,  0.0298,  0.0319],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(707.4072, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.6059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.6885, device='cuda:0')



h[100].sum tensor(21.8611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.6742, device='cuda:0')



h[200].sum tensor(28.9778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.3774, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0185, 0.0769, 0.0513,  ..., 0.0697, 0.0663, 0.0706],
        [0.0442, 0.1706, 0.1177,  ..., 0.1594, 0.1476, 0.1578],
        [0.0273, 0.1087, 0.0738,  ..., 0.1002, 0.0939, 0.1002],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55230.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.8072e-01, 0.0000e+00,  ..., 1.1584e-01, 0.0000e+00,
         8.3777e-02],
        [0.0000e+00, 3.7715e-01, 0.0000e+00,  ..., 1.5243e-01, 0.0000e+00,
         1.1608e-01],
        [0.0000e+00, 3.4487e-01, 0.0000e+00,  ..., 1.4025e-01, 0.0000e+00,
         1.0555e-01],
        ...,
        [0.0000e+00, 3.3209e-04, 0.0000e+00,  ..., 5.2618e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3230e-04, 0.0000e+00,  ..., 5.2613e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3232e-04, 0.0000e+00,  ..., 5.2611e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(286949.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-180.7457, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-27.4767, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-343.2892, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0000],
        [0.9980],
        ...,
        [0.9944],
        [0.9941],
        [0.9937]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364974.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [1.0000],
        [0.9980],
        ...,
        [0.9943],
        [0.9940],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364963.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0061,  0.0246,  0.0167,  ...,  0.0226,  0.0213,  0.0227],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(304.8992, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9843, device='cuda:0')



h[100].sum tensor(19.0608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6772, device='cuda:0')



h[200].sum tensor(23.0301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.6662, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0061, 0.0259, 0.0167,  ..., 0.0226, 0.0222, 0.0232],
        [0.0049, 0.0214, 0.0135,  ..., 0.0184, 0.0183, 0.0191],
        [0.0220, 0.0897, 0.0603,  ..., 0.0820, 0.0774, 0.0825],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34528.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0572, 0.0000,  ..., 0.0279, 0.0000, 0.0157],
        [0.0000, 0.0732, 0.0000,  ..., 0.0343, 0.0000, 0.0205],
        [0.0000, 0.1248, 0.0000,  ..., 0.0547, 0.0000, 0.0363],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(182694.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.6892, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.0959, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-193.8105, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [1.0000],
        [0.9980],
        ...,
        [0.9943],
        [0.9940],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364963.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9999],
        [0.9980],
        ...,
        [0.9943],
        [0.9939],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364953.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(324.7914, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3457, device='cuda:0')



h[100].sum tensor(19.1813, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7922, device='cuda:0')



h[200].sum tensor(23.2860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.0737, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34497.0195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(180776.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.1501, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.3969, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-193.2571, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9999],
        [0.9980],
        ...,
        [0.9943],
        [0.9939],
        [0.9936]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364953.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9999],
        [0.9979],
        ...,
        [0.9942],
        [0.9939],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364942.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0012,  0.0069,  0.0041,  ...,  0.0056,  0.0059,  0.0062],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(275.5172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7114, device='cuda:0')



h[100].sum tensor(18.8286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2721, device='cuda:0')



h[200].sum tensor(22.5368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.2306, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0022, 0.0134, 0.0073,  ..., 0.0101, 0.0113, 0.0116],
        [0.0030, 0.0183, 0.0103,  ..., 0.0143, 0.0156, 0.0162],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31440.0820, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0152, 0.0000,  ..., 0.0116, 0.0000, 0.0026],
        [0.0000, 0.0435, 0.0000,  ..., 0.0235, 0.0000, 0.0088],
        [0.0000, 0.0623, 0.0000,  ..., 0.0314, 0.0000, 0.0132],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(162639.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-67.5877, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.4073, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-172.1698, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9999],
        [0.9979],
        ...,
        [0.9942],
        [0.9939],
        [0.9935]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364942.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9999],
        [0.9979],
        ...,
        [0.9941],
        [0.9938],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364931.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0082,  0.0050,  ...,  0.0069,  0.0070,  0.0074],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0052,  0.0213,  0.0143,  ...,  0.0194,  0.0183,  0.0195],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(464.3987, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.8324, device='cuda:0')



h[100].sum tensor(20.0977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5380, device='cuda:0')



h[200].sum tensor(25.2324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.2616, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0016, 0.0094, 0.0050,  ..., 0.0069, 0.0079, 0.0079],
        [0.0061, 0.0276, 0.0173,  ..., 0.0237, 0.0236, 0.0247],
        ...,
        [0.0051, 0.0222, 0.0141,  ..., 0.0192, 0.0190, 0.0198],
        [0.0041, 0.0185, 0.0114,  ..., 0.0156, 0.0157, 0.0163],
        [0.0185, 0.0765, 0.0510,  ..., 0.0695, 0.0660, 0.0703]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42471.7930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0302, 0.0000,  ..., 0.0176, 0.0000, 0.0065],
        [0.0000, 0.0512, 0.0000,  ..., 0.0264, 0.0000, 0.0119],
        [0.0000, 0.0869, 0.0000,  ..., 0.0409, 0.0000, 0.0217],
        ...,
        [0.0000, 0.0483, 0.0000,  ..., 0.0244, 0.0000, 0.0127],
        [0.0000, 0.0621, 0.0000,  ..., 0.0299, 0.0000, 0.0168],
        [0.0000, 0.1062, 0.0000,  ..., 0.0474, 0.0000, 0.0301]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(225066.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-118.8809, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.4642, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-250.4505, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9999],
        [0.9979],
        ...,
        [0.9941],
        [0.9938],
        [0.9934]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364931.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 120.0 event: 600 loss: tensor(581.3310, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9999],
        [0.9979],
        ...,
        [0.9941],
        [0.9937],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364920.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(346.3238, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0926, device='cuda:0')



h[100].sum tensor(19.2785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0298, device='cuda:0')



h[200].sum tensor(23.4923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.9161, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37738.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0042, 0.0000,  ..., 0.0070, 0.0000, 0.0005],
        [0.0000, 0.0005, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(203608.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-97.4782, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.0640, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-217.1983, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9999],
        [0.9979],
        ...,
        [0.9941],
        [0.9937],
        [0.9933]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364920.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9999],
        [0.9978],
        ...,
        [0.9940],
        [0.9936],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364910., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(266.4101, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7852, device='cuda:0')



h[100].sum tensor(18.7234, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.9774, device='cuda:0')



h[200].sum tensor(22.3134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.1860, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0016, 0.0095, 0.0050,  ..., 0.0069, 0.0079, 0.0079],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0038, 0.0173, 0.0106,  ..., 0.0144, 0.0147, 0.0152],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33233.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0221, 0.0000,  ..., 0.0146, 0.0000, 0.0037],
        [0.0000, 0.0179, 0.0000,  ..., 0.0126, 0.0000, 0.0030],
        [0.0000, 0.0391, 0.0000,  ..., 0.0211, 0.0000, 0.0094],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(180302.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-75.5112, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.2740, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-184.4676, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9999],
        [0.9978],
        ...,
        [0.9940],
        [0.9936],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364910., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9999],
        [0.9978],
        ...,
        [0.9939],
        [0.9936],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364899.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(448.8443, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5507, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.6280, device='cuda:0')



h[100].sum tensor(19.9336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.1548, device='cuda:0')



h[200].sum tensor(24.8838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.9033, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39717.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0019, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0101, 0.0000,  ..., 0.0096, 0.0000, 0.0010],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(206907.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-105.7783, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.1582, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-230.2615, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9999],
        [0.9978],
        ...,
        [0.9939],
        [0.9936],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364899.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9999],
        [0.9978],
        ...,
        [0.9939],
        [0.9936],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364899.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(317.9542, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7832, device='cuda:0')



h[100].sum tensor(19.0552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6132, device='cuda:0')



h[200].sum tensor(23.0182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.4394, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0170, 0.0695, 0.0466,  ..., 0.0633, 0.0600, 0.0638],
        [0.0025, 0.0126, 0.0073,  ..., 0.0100, 0.0107, 0.0109],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34014.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1847, 0.0000,  ..., 0.0786, 0.0000, 0.0539],
        [0.0000, 0.1135, 0.0000,  ..., 0.0506, 0.0000, 0.0319],
        [0.0000, 0.0815, 0.0000,  ..., 0.0378, 0.0000, 0.0224],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(179219.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-78.8349, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.7704, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-189.8459, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9999],
        [0.9978],
        ...,
        [0.9939],
        [0.9936],
        [0.9932]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364899.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9999],
        [0.9978],
        ...,
        [0.9938],
        [0.9935],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364888.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(326.0571, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0402, device='cuda:0')



h[100].sum tensor(19.0940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6950, device='cuda:0')



h[200].sum tensor(23.1005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.7292, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35613.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 9.1445e-03, 0.0000e+00,  ..., 9.0708e-03, 0.0000e+00,
         1.3119e-03],
        [0.0000e+00, 2.4142e-03, 0.0000e+00,  ..., 6.2634e-03, 0.0000e+00,
         7.5538e-05],
        [0.0000e+00, 3.0741e-04, 0.0000e+00,  ..., 5.4010e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 3.3285e-04, 0.0000e+00,  ..., 5.2547e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3307e-04, 0.0000e+00,  ..., 5.2542e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3309e-04, 0.0000e+00,  ..., 5.2540e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(192525.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-86.8802, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.4230, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-201.2197, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9999],
        [0.9978],
        ...,
        [0.9938],
        [0.9935],
        [0.9931]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364888.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9998],
        [0.9978],
        ...,
        [0.9938],
        [0.9934],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364877.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(349.0984, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.7241, device='cuda:0')



h[100].sum tensor(19.2319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9126, device='cuda:0')



h[200].sum tensor(23.3933, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.5005, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35134.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0276, 0.0000,  ..., 0.0167, 0.0000, 0.0055],
        [0.0000, 0.0200, 0.0000,  ..., 0.0136, 0.0000, 0.0038],
        [0.0000, 0.0122, 0.0000,  ..., 0.0104, 0.0000, 0.0017],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(183954.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-83.8444, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.0612, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-197.4506, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9998],
        [0.9978],
        ...,
        [0.9938],
        [0.9934],
        [0.9930]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364877.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0004],
        [0.9998],
        [0.9977],
        ...,
        [0.9937],
        [0.9933],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364866.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(337.8443, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1722, device='cuda:0')



h[100].sum tensor(19.1413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7369, device='cuda:0')



h[200].sum tensor(23.2010, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.8780, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34019.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(177904.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-79.5889, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.3772, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-190.4462, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0004],
        [0.9998],
        [0.9977],
        ...,
        [0.9937],
        [0.9933],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364866.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9977],
        ...,
        [0.9936],
        [0.9933],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364855.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(339.8174, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1318, device='cuda:0')



h[100].sum tensor(19.1388, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7241, device='cuda:0')



h[200].sum tensor(23.1957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.8325, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35072.4102, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0045, 0.0000,  ..., 0.0071, 0.0000, 0.0003],
        [0.0000, 0.0183, 0.0000,  ..., 0.0127, 0.0000, 0.0038],
        [0.0000, 0.0347, 0.0000,  ..., 0.0193, 0.0000, 0.0081],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(187492.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-83.9904, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.9662, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-197.2386, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9977],
        ...,
        [0.9936],
        [0.9933],
        [0.9929]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364855.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9977],
        ...,
        [0.9936],
        [0.9932],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364844.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(340.5240, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1992, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3476, device='cuda:0')



h[100].sum tensor(19.1281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7928, device='cuda:0')



h[200].sum tensor(23.1729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.0759, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34314.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0074, 0.0000, 0.0003],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0076, 0.0000, 0.0008],
        [0.0000, 0.0180, 0.0000,  ..., 0.0124, 0.0000, 0.0037],
        [0.0000, 0.0238, 0.0000,  ..., 0.0148, 0.0000, 0.0049]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(178832.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.8794, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.2675, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-192.2705, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9977],
        ...,
        [0.9936],
        [0.9932],
        [0.9928]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364844.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9976],
        ...,
        [0.9935],
        [0.9931],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364833.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0048,  0.0200,  0.0134,  ...,  0.0182,  0.0173,  0.0184],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(770.0806, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.4780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.1521, device='cuda:0')



h[100].sum tensor(21.9240, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.8217, device='cuda:0')



h[200].sum tensor(29.1114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.9002, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0087, 0.0372, 0.0242,  ..., 0.0329, 0.0320, 0.0337],
        [0.0087, 0.0372, 0.0242,  ..., 0.0329, 0.0320, 0.0337],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52605.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0516, 0.0000,  ..., 0.0262, 0.0000, 0.0126],
        [0.0000, 0.0986, 0.0000,  ..., 0.0446, 0.0000, 0.0274],
        [0.0000, 0.1241, 0.0000,  ..., 0.0547, 0.0000, 0.0354],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(270237.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-168.1571, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-29.4968, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-324.8051, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9976],
        ...,
        [0.9935],
        [0.9931],
        [0.9927]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364833.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 130.0 event: 650 loss: tensor(576.2130, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9976],
        ...,
        [0.9934],
        [0.9930],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364822.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0013,  0.0069,  0.0041,  ...,  0.0057,  0.0059,  0.0062],
        [ 0.0073,  0.0291,  0.0198,  ...,  0.0269,  0.0251,  0.0268],
        [ 0.0117,  0.0449,  0.0310,  ...,  0.0420,  0.0388,  0.0415],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(541.0333, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.5436, device='cuda:0')



h[100].sum tensor(20.4045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0825, device='cuda:0')



h[200].sum tensor(25.8840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.1916, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0237, 0.0956, 0.0645,  ..., 0.0876, 0.0826, 0.0880],
        [0.0267, 0.1068, 0.0725,  ..., 0.0984, 0.0923, 0.0984],
        [0.0361, 0.1411, 0.0967,  ..., 0.1311, 0.1219, 0.1303],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45035.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.1416e-01, 0.0000e+00,  ..., 1.2891e-01, 0.0000e+00,
         9.4940e-02],
        [0.0000e+00, 3.3960e-01, 0.0000e+00,  ..., 1.3857e-01, 0.0000e+00,
         1.0317e-01],
        [0.0000e+00, 3.8069e-01, 0.0000e+00,  ..., 1.5415e-01, 0.0000e+00,
         1.1683e-01],
        ...,
        [0.0000e+00, 3.3341e-04, 0.0000e+00,  ..., 5.2493e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3363e-04, 0.0000e+00,  ..., 5.2489e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3365e-04, 0.0000e+00,  ..., 5.2487e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(239062.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-131.8800, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-36.5734, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-269.7399, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9976],
        ...,
        [0.9934],
        [0.9930],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364822.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9976],
        ...,
        [0.9934],
        [0.9930],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364811.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0040,  0.0169,  0.0111,  ...,  0.0152,  0.0145,  0.0155],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0118,  0.0455,  0.0314,  ...,  0.0425,  0.0393,  0.0420],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(743.1199, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.9447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.6107, device='cuda:0')



h[100].sum tensor(21.6960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.3313, device='cuda:0')



h[200].sum tensor(28.6270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.1618, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0032, 0.0151, 0.0090,  ..., 0.0123, 0.0128, 0.0132],
        [0.0250, 0.1005, 0.0680,  ..., 0.0924, 0.0868, 0.0926],
        [0.0237, 0.0937, 0.0637,  ..., 0.0864, 0.0809, 0.0862],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49251.9805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0925, 0.0000,  ..., 0.0420, 0.0000, 0.0256],
        [0.0000, 0.2097, 0.0000,  ..., 0.0879, 0.0000, 0.0622],
        [0.0000, 0.2640, 0.0000,  ..., 0.1089, 0.0000, 0.0796],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0079, 0.0000, 0.0007],
        [0.0000, 0.0161, 0.0000,  ..., 0.0119, 0.0000, 0.0027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(239546.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-152.2493, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-32.3585, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-300.8347, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9976],
        ...,
        [0.9934],
        [0.9930],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364811.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9976],
        ...,
        [0.9934],
        [0.9930],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364811.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0039,  0.0164,  0.0108,  ...,  0.0147,  0.0141,  0.0150],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(226.7390, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.8036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6819, device='cuda:0')



h[100].sum tensor(18.3440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.3081, device='cuda:0')



h[200].sum tensor(21.5076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.8139, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0049, 0.0235, 0.0144,  ..., 0.0198, 0.0201, 0.0210],
        [0.0040, 0.0218, 0.0128,  ..., 0.0177, 0.0186, 0.0194],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29403.2070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0187, 0.0000,  ..., 0.0130, 0.0000, 0.0037],
        [0.0000, 0.0571, 0.0000,  ..., 0.0289, 0.0000, 0.0131],
        [0.0000, 0.0727, 0.0000,  ..., 0.0356, 0.0000, 0.0165],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(157534.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-57.5535, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.6764, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-156.8077, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9976],
        ...,
        [0.9934],
        [0.9930],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364811.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9976],
        ...,
        [0.9934],
        [0.9930],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364811.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0074,  0.0292,  0.0199,  ...,  0.0270,  0.0253,  0.0270],
        [ 0.0074,  0.0294,  0.0201,  ...,  0.0272,  0.0254,  0.0271],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(513.6091, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.9932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.3865, device='cuda:0')



h[100].sum tensor(20.2062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7143, device='cuda:0')



h[200].sum tensor(25.4627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.8866, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0303, 0.1199, 0.0817,  ..., 0.1109, 0.1036, 0.1106],
        [0.0218, 0.0868, 0.0588,  ..., 0.0797, 0.0749, 0.0798],
        [0.0193, 0.0798, 0.0533,  ..., 0.0726, 0.0689, 0.0733],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42571.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2704, 0.0000,  ..., 0.1119, 0.0000, 0.0813],
        [0.0000, 0.2715, 0.0000,  ..., 0.1125, 0.0000, 0.0811],
        [0.0000, 0.2582, 0.0000,  ..., 0.1077, 0.0000, 0.0760],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(223062.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-120.1208, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.7808, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-251.9918, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9976],
        ...,
        [0.9934],
        [0.9930],
        [0.9926]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364811.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9975],
        ...,
        [0.9933],
        [0.9929],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364800.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0066,  0.0039,  ...,  0.0054,  0.0057,  0.0059],
        [ 0.0018,  0.0089,  0.0055,  ...,  0.0076,  0.0077,  0.0081],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(290.3905, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7821, device='cuda:0')



h[100].sum tensor(18.7437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.9764, device='cuda:0')



h[200].sum tensor(22.3564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.1825, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0132, 0.0574, 0.0374,  ..., 0.0511, 0.0494, 0.0525],
        [0.0040, 0.0199, 0.0119,  ..., 0.0164, 0.0170, 0.0176],
        [0.0018, 0.0101, 0.0055,  ..., 0.0076, 0.0085, 0.0086],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32185.7988, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1502, 0.0000,  ..., 0.0659, 0.0000, 0.0413],
        [0.0000, 0.0825, 0.0000,  ..., 0.0389, 0.0000, 0.0210],
        [0.0000, 0.0382, 0.0000,  ..., 0.0210, 0.0000, 0.0084],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(171310.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-70.4239, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.3907, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-176.6061, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9998],
        [0.9975],
        ...,
        [0.9933],
        [0.9929],
        [0.9925]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364800.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9975],
        ...,
        [0.9932],
        [0.9928],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364789.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(333.8920, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7029, device='cuda:0')



h[100].sum tensor(19.0104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5876, device='cuda:0')



h[200].sum tensor(22.9228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.3487, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0024, 0.0123, 0.0070,  ..., 0.0096, 0.0103, 0.0105],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34578.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0207, 0.0000,  ..., 0.0138, 0.0000, 0.0042],
        [0.0000, 0.0342, 0.0000,  ..., 0.0192, 0.0000, 0.0080],
        [0.0000, 0.0601, 0.0000,  ..., 0.0298, 0.0000, 0.0148],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(188445.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-82.2208, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.0715, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-194.1273, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9975],
        ...,
        [0.9932],
        [0.9928],
        [0.9924]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364789.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9975],
        ...,
        [0.9931],
        [0.9927],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364778.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(330.5874, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1062, device='cuda:0')



h[100].sum tensor(18.9747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.3978, device='cuda:0')



h[200].sum tensor(22.8472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.6759, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33576.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 5.6271e-03, 0.0000e+00,  ..., 7.6724e-03, 0.0000e+00,
         2.3910e-05],
        [0.0000e+00, 2.1674e-03, 0.0000e+00,  ..., 6.1960e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.8722e-04, 0.0000e+00,  ..., 5.5229e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 3.3378e-04, 0.0000e+00,  ..., 5.2458e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3400e-04, 0.0000e+00,  ..., 5.2453e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3402e-04, 0.0000e+00,  ..., 5.2451e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(177123.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-77.4553, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.0567, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.7386, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9975],
        ...,
        [0.9931],
        [0.9927],
        [0.9923]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364778.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9974],
        ...,
        [0.9931],
        [0.9927],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364767.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0034,  0.0146,  0.0096,  ...,  0.0131,  0.0126,  0.0134],
        [ 0.0039,  0.0165,  0.0109,  ...,  0.0148,  0.0142,  0.0151],
        [ 0.0040,  0.0169,  0.0111,  ...,  0.0152,  0.0145,  0.0155],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(352.1021, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9924, device='cuda:0')



h[100].sum tensor(19.0980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6797, device='cuda:0')



h[200].sum tensor(23.1089, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.6753, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0122, 0.0537, 0.0348,  ..., 0.0476, 0.0463, 0.0491],
        [0.0145, 0.0622, 0.0409,  ..., 0.0557, 0.0536, 0.0570],
        [0.0189, 0.0781, 0.0521,  ..., 0.0709, 0.0674, 0.0717],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34776.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2391, 0.0000,  ..., 0.1007, 0.0000, 0.0691],
        [0.0000, 0.2179, 0.0000,  ..., 0.0926, 0.0000, 0.0619],
        [0.0000, 0.1930, 0.0000,  ..., 0.0828, 0.0000, 0.0543],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(185284.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-82.9126, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.9954, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-195.4062, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9974],
        ...,
        [0.9931],
        [0.9927],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364767.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9974],
        ...,
        [0.9930],
        [0.9926],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364756.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0036,  0.0155,  0.0102,  ...,  0.0139,  0.0134,  0.0142],
        [ 0.0012,  0.0068,  0.0040,  ...,  0.0055,  0.0058,  0.0061],
        [ 0.0019,  0.0091,  0.0057,  ...,  0.0078,  0.0078,  0.0083],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(299.3679, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9774, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8972, device='cuda:0')



h[100].sum tensor(18.7479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0131, device='cuda:0')



h[200].sum tensor(22.3653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.3124, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0053, 0.0266, 0.0162,  ..., 0.0222, 0.0228, 0.0239],
        [0.0132, 0.0575, 0.0375,  ..., 0.0512, 0.0495, 0.0526],
        [0.0058, 0.0266, 0.0167,  ..., 0.0228, 0.0228, 0.0239],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31256., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0914, 0.0000,  ..., 0.0428, 0.0000, 0.0225],
        [0.0000, 0.1188, 0.0000,  ..., 0.0538, 0.0000, 0.0309],
        [0.0000, 0.0830, 0.0000,  ..., 0.0393, 0.0000, 0.0207],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(163876., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-66.7382, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.5838, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-170.8274, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9974],
        ...,
        [0.9930],
        [0.9926],
        [0.9922]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364756.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9973],
        ...,
        [0.9929],
        [0.9925],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364745.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0014,  0.0074,  0.0044,  ...,  0.0061,  0.0063,  0.0066],
        [ 0.0038,  0.0161,  0.0106,  ...,  0.0145,  0.0139,  0.0148],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(647.1263, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.4949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.5701, device='cuda:0')



h[100].sum tensor(20.9385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.0456, device='cuda:0')



h[200].sum tensor(27.0181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.6049, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0129, 0.0562, 0.0366,  ..., 0.0500, 0.0484, 0.0514],
        [0.0052, 0.0265, 0.0161,  ..., 0.0221, 0.0227, 0.0237],
        [0.0048, 0.0230, 0.0141,  ..., 0.0194, 0.0197, 0.0205],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51723.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1192, 0.0000,  ..., 0.0540, 0.0000, 0.0309],
        [0.0000, 0.0900, 0.0000,  ..., 0.0423, 0.0000, 0.0221],
        [0.0000, 0.0635, 0.0000,  ..., 0.0313, 0.0000, 0.0153],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(283081.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-164.4253, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-30.2649, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-318.4467, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9973],
        ...,
        [0.9929],
        [0.9925],
        [0.9921]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364745.0312, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 140.0 event: 700 loss: tensor(484.4425, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9973],
        ...,
        [0.9928],
        [0.9924],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364733.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0083,  0.0325,  0.0222,  ...,  0.0301,  0.0281,  0.0300],
        [ 0.0170,  0.0645,  0.0449,  ...,  0.0607,  0.0558,  0.0597],
        [ 0.0083,  0.0327,  0.0223,  ...,  0.0303,  0.0282,  0.0302],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(287.5675, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1834, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4233, device='cuda:0')



h[100].sum tensor(18.6472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8622, device='cuda:0')



h[200].sum tensor(22.1514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.7778, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0408, 0.1582, 0.1089,  ..., 0.1475, 0.1368, 0.1462],
        [0.0378, 0.1473, 0.1011,  ..., 0.1370, 0.1273, 0.1360],
        [0.0413, 0.1599, 0.1101,  ..., 0.1491, 0.1383, 0.1478],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30792.2695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.4501e-01, 0.0000e+00,  ..., 1.4011e-01, 0.0000e+00,
         1.0563e-01],
        [0.0000e+00, 3.6610e-01, 0.0000e+00,  ..., 1.4834e-01, 0.0000e+00,
         1.1229e-01],
        [0.0000e+00, 3.6478e-01, 0.0000e+00,  ..., 1.4792e-01, 0.0000e+00,
         1.1172e-01],
        ...,
        [0.0000e+00, 3.3415e-04, 0.0000e+00,  ..., 5.2422e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3437e-04, 0.0000e+00,  ..., 5.2417e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3439e-04, 0.0000e+00,  ..., 5.2415e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(162932.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-64.1700, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.2908, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-167.0545, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9973],
        ...,
        [0.9928],
        [0.9924],
        [0.9920]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364733.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9973],
        ...,
        [0.9928],
        [0.9924],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364722.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0083,  0.0051,  ...,  0.0070,  0.0071,  0.0075],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(333.2982, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9658, device='cuda:0')



h[100].sum tensor(18.9221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.3531, device='cuda:0')



h[200].sum tensor(22.7353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.5175, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0092, 0.0427, 0.0270,  ..., 0.0371, 0.0367, 0.0388],
        [0.0012, 0.0081, 0.0041,  ..., 0.0056, 0.0067, 0.0067],
        [0.0016, 0.0095, 0.0051,  ..., 0.0070, 0.0080, 0.0080],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32494.5742, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0811, 0.0000,  ..., 0.0388, 0.0000, 0.0195],
        [0.0000, 0.0400, 0.0000,  ..., 0.0219, 0.0000, 0.0084],
        [0.0000, 0.0351, 0.0000,  ..., 0.0198, 0.0000, 0.0075],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(168791.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-72.7428, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.4609, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-179.7845, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9997],
        [0.9973],
        ...,
        [0.9928],
        [0.9924],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364722.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0003],
        [0.9996],
        [0.9972],
        ...,
        [0.9927],
        [0.9923],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364711.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0050,  0.0207,  0.0139,  ...,  0.0189,  0.0179,  0.0190],
        [ 0.0078,  0.0308,  0.0210,  ...,  0.0285,  0.0266,  0.0284],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(440.6774, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.2727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7131, device='cuda:0')



h[100].sum tensor(19.5810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5455, device='cuda:0')



h[200].sum tensor(24.1347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7437, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0090, 0.0401, 0.0257,  ..., 0.0351, 0.0344, 0.0364],
        [0.0135, 0.0565, 0.0373,  ..., 0.0508, 0.0487, 0.0517],
        [0.0209, 0.0854, 0.0573,  ..., 0.0779, 0.0737, 0.0786],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37291.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2036, 0.0000,  ..., 0.0863, 0.0000, 0.0589],
        [0.0000, 0.2403, 0.0000,  ..., 0.1006, 0.0000, 0.0708],
        [0.0000, 0.2852, 0.0000,  ..., 0.1183, 0.0000, 0.0851],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(192902.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-94.9701, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.5941, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-213.7993, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0003],
        [0.9996],
        [0.9972],
        ...,
        [0.9927],
        [0.9923],
        [0.9919]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364711.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [0.9996],
        [0.9972],
        ...,
        [0.9926],
        [0.9922],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364700.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0018,  0.0089,  0.0055,  ...,  0.0075,  0.0076,  0.0080],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(343.8805, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2677, device='cuda:0')



h[100].sum tensor(18.9609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4492, device='cuda:0')



h[200].sum tensor(22.8178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.8580, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0018, 0.0101, 0.0055,  ..., 0.0075, 0.0085, 0.0085],
        [0.0116, 0.0477, 0.0316,  ..., 0.0429, 0.0411, 0.0435],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33475.6602, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0308, 0.0000,  ..., 0.0178, 0.0000, 0.0073],
        [0.0000, 0.0713, 0.0000,  ..., 0.0341, 0.0000, 0.0184],
        [0.0000, 0.1464, 0.0000,  ..., 0.0636, 0.0000, 0.0414],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(177653.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-76.9250, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.8530, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.4462, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [0.9996],
        [0.9972],
        ...,
        [0.9926],
        [0.9922],
        [0.9918]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364700.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [0.9996],
        [0.9972],
        ...,
        [0.9926],
        [0.9921],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364689.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(181.9878, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.6282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.3070, device='cuda:0')



h[100].sum tensor(17.9411, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.5525, device='cuda:0')



h[200].sum tensor(20.6518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.1355, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0095, 0.0400, 0.0262,  ..., 0.0356, 0.0344, 0.0364],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27337.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0891, 0.0000,  ..., 0.0408, 0.0000, 0.0249],
        [0.0000, 0.0298, 0.0000,  ..., 0.0171, 0.0000, 0.0075],
        [0.0000, 0.0112, 0.0000,  ..., 0.0097, 0.0000, 0.0024],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(151320.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-47.2817, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-52.6692, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-141.6951, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [0.9996],
        [0.9972],
        ...,
        [0.9926],
        [0.9921],
        [0.9917]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364689.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [0.9996],
        [0.9971],
        ...,
        [0.9925],
        [0.9921],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364677.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0040,  0.0170,  0.0112,  ...,  0.0153,  0.0146,  0.0156],
        [ 0.0086,  0.0338,  0.0232,  ...,  0.0314,  0.0292,  0.0312],
        [ 0.0128,  0.0489,  0.0338,  ...,  0.0458,  0.0423,  0.0452],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(227.9513, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0147, device='cuda:0')



h[100].sum tensor(18.2160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.0959, device='cuda:0')



h[200].sum tensor(21.2356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.0614, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0276, 0.1101, 0.0748,  ..., 0.1015, 0.0951, 0.1015],
        [0.0401, 0.1556, 0.1070,  ..., 0.1450, 0.1345, 0.1437],
        [0.0601, 0.2287, 0.1588,  ..., 0.2149, 0.1978, 0.2117],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27834.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.7340e-01, 0.0000e+00,  ..., 1.5126e-01, 0.0000e+00,
         1.1423e-01],
        [0.0000e+00, 5.2615e-01, 0.0000e+00,  ..., 2.0873e-01, 0.0000e+00,
         1.6525e-01],
        [0.0000e+00, 6.8483e-01, 0.0000e+00,  ..., 2.6825e-01, 0.0000e+00,
         2.1830e-01],
        ...,
        [0.0000e+00, 3.3461e-04, 0.0000e+00,  ..., 5.2377e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3483e-04, 0.0000e+00,  ..., 5.2372e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3485e-04, 0.0000e+00,  ..., 5.2370e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(148042.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-50.3674, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-51.7483, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-145.9545, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [0.9996],
        [0.9971],
        ...,
        [0.9925],
        [0.9921],
        [0.9916]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364677.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [0.9996],
        [0.9971],
        ...,
        [0.9924],
        [0.9920],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364666.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(360.0156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7656, device='cuda:0')



h[100].sum tensor(19.0198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6076, device='cuda:0')



h[200].sum tensor(22.9430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.4195, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0111, 0.0062,  ..., 0.0085, 0.0093, 0.0094],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0027, 0.0133, 0.0077,  ..., 0.0106, 0.0112, 0.0115],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34632.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0252, 0.0000,  ..., 0.0157, 0.0000, 0.0047],
        [0.0000, 0.0178, 0.0000,  ..., 0.0126, 0.0000, 0.0028],
        [0.0000, 0.0381, 0.0000,  ..., 0.0208, 0.0000, 0.0088],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(187127.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-82.7541, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.8881, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-194.7020, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [0.9996],
        [0.9971],
        ...,
        [0.9924],
        [0.9920],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364666.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [0.9996],
        [0.9971],
        ...,
        [0.9923],
        [0.9919],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364655.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0066,  0.0039,  ...,  0.0054,  0.0057,  0.0059],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(571.6400, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5050, device='cuda:0')



h[100].sum tensor(20.3065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7520, device='cuda:0')



h[200].sum tensor(25.6757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0202, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0049, 0.0233, 0.0143,  ..., 0.0196, 0.0199, 0.0208],
        [0.0092, 0.0411, 0.0264,  ..., 0.0361, 0.0353, 0.0373],
        [0.0164, 0.0672, 0.0449,  ..., 0.0610, 0.0580, 0.0616],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43213.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1488, 0.0000,  ..., 0.0650, 0.0000, 0.0415],
        [0.0000, 0.1970, 0.0000,  ..., 0.0837, 0.0000, 0.0569],
        [0.0000, 0.2500, 0.0000,  ..., 0.1041, 0.0000, 0.0745],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(225318.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-123.5220, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.9493, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-256.9764, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [0.9996],
        [0.9971],
        ...,
        [0.9923],
        [0.9919],
        [0.9915]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364655.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [0.9996],
        [0.9970],
        ...,
        [0.9923],
        [0.9918],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364644., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(258.8238, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8252, device='cuda:0')



h[100].sum tensor(18.3732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.3538, device='cuda:0')



h[200].sum tensor(21.5696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.9755, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0017, 0.0115, 0.0060,  ..., 0.0083, 0.0097, 0.0098],
        [0.0008, 0.0066, 0.0030,  ..., 0.0042, 0.0054, 0.0052],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30647.8242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0276, 0.0000,  ..., 0.0172, 0.0000, 0.0037],
        [0.0000, 0.0196, 0.0000,  ..., 0.0137, 0.0000, 0.0021],
        [0.0000, 0.0102, 0.0000,  ..., 0.0096, 0.0000, 0.0008],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(168903.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-63.0174, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.8062, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-165.4610, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [0.9996],
        [0.9970],
        ...,
        [0.9923],
        [0.9918],
        [0.9914]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364644., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [0.9995],
        [0.9970],
        ...,
        [0.9922],
        [0.9918],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364632.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0016,  0.0082,  0.0050,  ...,  0.0069,  0.0070,  0.0074],
        [ 0.0016,  0.0080,  0.0049,  ...,  0.0067,  0.0069,  0.0072],
        [ 0.0037,  0.0158,  0.0104,  ...,  0.0142,  0.0136,  0.0145],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(417.1076, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.2441, device='cuda:0')



h[100].sum tensor(19.3274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0780, device='cuda:0')



h[200].sum tensor(23.5961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.0869, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0271, 0.0170,  ..., 0.0232, 0.0232, 0.0243],
        [0.0123, 0.0543, 0.0352,  ..., 0.0481, 0.0467, 0.0496],
        [0.0054, 0.0270, 0.0165,  ..., 0.0227, 0.0232, 0.0243],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35743.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0942, 0.0000,  ..., 0.0437, 0.0000, 0.0242],
        [0.0000, 0.1350, 0.0000,  ..., 0.0602, 0.0000, 0.0359],
        [0.0000, 0.1208, 0.0000,  ..., 0.0546, 0.0000, 0.0315],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(186758.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-87.3020, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.1467, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-202.3447, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [0.9995],
        [0.9970],
        ...,
        [0.9922],
        [0.9918],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364632.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [0.9995],
        [0.9970],
        ...,
        [0.9922],
        [0.9918],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364632.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(373.6819, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0694, device='cuda:0')



h[100].sum tensor(19.0625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7042, device='cuda:0')



h[200].sum tensor(23.0336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.7621, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34397.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(182703.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.4207, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.5363, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-193.7948, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [0.9995],
        [0.9970],
        ...,
        [0.9922],
        [0.9918],
        [0.9913]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364632.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [0.9995],
        [0.9970],
        ...,
        [0.9921],
        [0.9917],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364621.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(254.9550, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.8374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6279, device='cuda:0')



h[100].sum tensor(18.3276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.2910, device='cuda:0')



h[200].sum tensor(21.4727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.7530, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0066, 0.0030,  ..., 0.0042, 0.0054, 0.0052],
        [0.0017, 0.0115, 0.0060,  ..., 0.0083, 0.0097, 0.0098],
        [0.0008, 0.0066, 0.0030,  ..., 0.0042, 0.0054, 0.0052],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30753.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0273, 0.0000,  ..., 0.0169, 0.0000, 0.0040],
        [0.0000, 0.0346, 0.0000,  ..., 0.0201, 0.0000, 0.0056],
        [0.0000, 0.0254, 0.0000,  ..., 0.0162, 0.0000, 0.0036],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(172402.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-63.3915, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.5673, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-166.4094, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [0.9995],
        [0.9970],
        ...,
        [0.9921],
        [0.9917],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364621.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [0.9995],
        [0.9970],
        ...,
        [0.9921],
        [0.9917],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364621.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0039,  0.0167,  0.0110,  ...,  0.0150,  0.0144,  0.0153],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(542.9286, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.4437, device='cuda:0')



h[100].sum tensor(20.0771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4143, device='cuda:0')



h[200].sum tensor(25.1886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.8232, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0019, 0.0105, 0.0058,  ..., 0.0079, 0.0088, 0.0089],
        [0.0116, 0.0497, 0.0325,  ..., 0.0443, 0.0428, 0.0453],
        [0.0183, 0.0741, 0.0498,  ..., 0.0676, 0.0639, 0.0680],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40711.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0842, 0.0000,  ..., 0.0392, 0.0000, 0.0223],
        [0.0000, 0.1841, 0.0000,  ..., 0.0784, 0.0000, 0.0534],
        [0.0000, 0.2687, 0.0000,  ..., 0.1113, 0.0000, 0.0806],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(208773.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-111.1889, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.3672, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-238.6744, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [0.9995],
        [0.9970],
        ...,
        [0.9921],
        [0.9917],
        [0.9912]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364621.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [0.9995],
        [0.9969],
        ...,
        [0.9920],
        [0.9916],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364610., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0024,  0.0113,  0.0072,  ...,  0.0098,  0.0097,  0.0102],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(263.0219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8989, device='cuda:0')



h[100].sum tensor(18.3656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.3772, device='cuda:0')



h[200].sum tensor(21.5534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.0587, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0084, 0.0362, 0.0235,  ..., 0.0320, 0.0311, 0.0328],
        [0.0024, 0.0125, 0.0072,  ..., 0.0098, 0.0105, 0.0107],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29580.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1337, 0.0000,  ..., 0.0586, 0.0000, 0.0380],
        [0.0000, 0.0634, 0.0000,  ..., 0.0308, 0.0000, 0.0164],
        [0.0000, 0.0211, 0.0000,  ..., 0.0139, 0.0000, 0.0043],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(158899.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-58.0360, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.4731, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-158.1446, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [0.9995],
        [0.9969],
        ...,
        [0.9920],
        [0.9916],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364610., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [0.9995],
        [0.9969],
        ...,
        [0.9920],
        [0.9915],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364598.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0074,  0.0294,  0.0200,  ...,  0.0272,  0.0254,  0.0271],
        [ 0.0088,  0.0344,  0.0236,  ...,  0.0319,  0.0297,  0.0317],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(286.2817, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4980, device='cuda:0')



h[100].sum tensor(18.4949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.5678, device='cuda:0')



h[200].sum tensor(21.8281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.7343, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0074, 0.0307, 0.0200,  ..., 0.0272, 0.0263, 0.0276],
        [0.0165, 0.0655, 0.0442,  ..., 0.0599, 0.0565, 0.0600],
        [0.0393, 0.1528, 0.1050,  ..., 0.1423, 0.1321, 0.1412],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31087.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1074, 0.0000,  ..., 0.0474, 0.0000, 0.0312],
        [0.0000, 0.1974, 0.0000,  ..., 0.0825, 0.0000, 0.0594],
        [0.0000, 0.3057, 0.0000,  ..., 0.1245, 0.0000, 0.0935],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(168207.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-65.8084, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.7052, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-169.6729, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [0.9995],
        [0.9969],
        ...,
        [0.9920],
        [0.9915],
        [0.9911]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364598.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [0.9995],
        [0.9969],
        ...,
        [0.9919],
        [0.9914],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364587.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(549.8054, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.6041, device='cuda:0')



h[100].sum tensor(20.0669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4654, device='cuda:0')



h[200].sum tensor(25.1669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.0042, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39076.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0257, 0.0000,  ..., 0.0159, 0.0000, 0.0053],
        [0.0000, 0.0084, 0.0000,  ..., 0.0088, 0.0000, 0.0009],
        [0.0000, 0.0015, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(196928.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-103.3004, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.9041, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-226.7672, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [0.9995],
        [0.9969],
        ...,
        [0.9919],
        [0.9914],
        [0.9910]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364587.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [0.9995],
        [0.9968],
        ...,
        [0.9918],
        [0.9914],
        [0.9909]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364575.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(333.3362, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0875, device='cuda:0')



h[100].sum tensor(18.7540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0736, device='cuda:0')



h[200].sum tensor(22.3784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.5270, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32101.7559, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0069, 0.0000, 0.0008],
        [0.0000, 0.0092, 0.0000,  ..., 0.0089, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(170117.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-70.0567, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.3700, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-176.1129, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [0.9995],
        [0.9968],
        ...,
        [0.9918],
        [0.9914],
        [0.9909]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364575.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0002],
        [0.9994],
        [0.9968],
        ...,
        [0.9917],
        [0.9913],
        [0.9908]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364564.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0019,  0.0094,  0.0058,  ...,  0.0080,  0.0081,  0.0085],
        [ 0.0053,  0.0216,  0.0145,  ...,  0.0197,  0.0186,  0.0198],
        [ 0.0068,  0.0272,  0.0184,  ...,  0.0250,  0.0235,  0.0250],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(953.4557, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.4214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.9319, device='cuda:0')



h[100].sum tensor(22.4406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.3881, device='cuda:0')



h[200].sum tensor(30.2084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.9075, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0099, 0.0417, 0.0274,  ..., 0.0372, 0.0359, 0.0379],
        [0.0224, 0.0911, 0.0613,  ..., 0.0833, 0.0787, 0.0838],
        [0.0275, 0.1096, 0.0744,  ..., 0.1010, 0.0946, 0.1010],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57964.8086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1595, 0.0000,  ..., 0.0689, 0.0000, 0.0456],
        [0.0000, 0.2582, 0.0000,  ..., 0.1074, 0.0000, 0.0768],
        [0.0000, 0.3274, 0.0000,  ..., 0.1341, 0.0000, 0.0993],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(294402.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-194.4366, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-23.5138, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-365.0945, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0002],
        [0.9994],
        [0.9968],
        ...,
        [0.9917],
        [0.9913],
        [0.9908]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364564.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [0.9994],
        [0.9967],
        ...,
        [0.9917],
        [0.9912],
        [0.9907]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364552.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0064,  0.0257,  0.0174,  ...,  0.0236,  0.0222,  0.0236],
        [ 0.0116,  0.0447,  0.0309,  ...,  0.0418,  0.0387,  0.0414],
        [ 0.0060,  0.0244,  0.0165,  ...,  0.0224,  0.0211,  0.0225],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(319.6378, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5218, device='cuda:0')



h[100].sum tensor(18.6484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8936, device='cuda:0')



h[200].sum tensor(22.1540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.8890, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0318, 0.1255, 0.0857,  ..., 0.1162, 0.1084, 0.1158],
        [0.0329, 0.1295, 0.0885,  ..., 0.1200, 0.1119, 0.1195],
        [0.0349, 0.1367, 0.0936,  ..., 0.1269, 0.1181, 0.1262],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30999.1055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2687, 0.0000,  ..., 0.1107, 0.0000, 0.0813],
        [0.0000, 0.3130, 0.0000,  ..., 0.1282, 0.0000, 0.0947],
        [0.0000, 0.3198, 0.0000,  ..., 0.1309, 0.0000, 0.0964],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(162546.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-65.2256, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.0579, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-168.5452, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [0.9994],
        [0.9967],
        ...,
        [0.9917],
        [0.9912],
        [0.9907]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364552.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [0.9994],
        [0.9967],
        ...,
        [0.9916],
        [0.9911],
        [0.9907]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364541.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0083,  0.0325,  0.0222,  ...,  0.0302,  0.0281,  0.0300],
        [ 0.0166,  0.0629,  0.0437,  ...,  0.0592,  0.0544,  0.0582],
        [ 0.0162,  0.0615,  0.0428,  ...,  0.0579,  0.0532,  0.0570],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(339.6502, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0192, device='cuda:0')



h[100].sum tensor(18.7552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0519, device='cuda:0')



h[200].sum tensor(22.3809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.4500, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0412, 0.1598, 0.1100,  ..., 0.1490, 0.1381, 0.1476],
        [0.0552, 0.2106, 0.1460,  ..., 0.1976, 0.1822, 0.1949],
        [0.0578, 0.2201, 0.1527,  ..., 0.2066, 0.1904, 0.2037],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32784.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.8898e-01, 0.0000e+00,  ..., 1.5587e-01, 0.0000e+00,
         1.2160e-01],
        [0.0000e+00, 5.0258e-01, 0.0000e+00,  ..., 1.9937e-01, 0.0000e+00,
         1.5824e-01],
        [0.0000e+00, 5.1941e-01, 0.0000e+00,  ..., 2.0600e-01, 0.0000e+00,
         1.6322e-01],
        ...,
        [0.0000e+00, 3.3570e-04, 0.0000e+00,  ..., 5.2269e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3593e-04, 0.0000e+00,  ..., 5.2265e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3594e-04, 0.0000e+00,  ..., 5.2263e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(175524.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-73.2156, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.5195, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-181.3770, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [0.9994],
        [0.9967],
        ...,
        [0.9916],
        [0.9911],
        [0.9907]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364541.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 160.0 event: 800 loss: tensor(1085.0101, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [0.9994],
        [0.9967],
        ...,
        [0.9915],
        [0.9911],
        [0.9906]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364530., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0031,  0.0137,  0.0089,  ...,  0.0121,  0.0118,  0.0125],
        [ 0.0082,  0.0323,  0.0221,  ...,  0.0299,  0.0279,  0.0298],
        [ 0.0070,  0.0277,  0.0188,  ...,  0.0256,  0.0240,  0.0256],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(590.9278, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.9533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.4456, device='cuda:0')



h[100].sum tensor(20.2258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7332, device='cuda:0')



h[200].sum tensor(25.5043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.9533, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0223, 0.0906, 0.0610,  ..., 0.0829, 0.0782, 0.0834],
        [0.0221, 0.0899, 0.0605,  ..., 0.0822, 0.0776, 0.0827],
        [0.0231, 0.0935, 0.0630,  ..., 0.0856, 0.0807, 0.0860],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41496.1133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1905, 0.0000,  ..., 0.0808, 0.0000, 0.0559],
        [0.0000, 0.2126, 0.0000,  ..., 0.0896, 0.0000, 0.0627],
        [0.0000, 0.2105, 0.0000,  ..., 0.0888, 0.0000, 0.0619],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(217208.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-115.1131, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.5636, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-244.4640, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [0.9994],
        [0.9967],
        ...,
        [0.9915],
        [0.9911],
        [0.9906]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364530., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [0.9994],
        [0.9966],
        ...,
        [0.9914],
        [0.9910],
        [0.9905]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364518.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0044,  0.0182,  0.0121,  ...,  0.0165,  0.0157,  0.0167],
        [ 0.0039,  0.0165,  0.0109,  ...,  0.0148,  0.0142,  0.0151],
        [ 0.0043,  0.0179,  0.0119,  ...,  0.0162,  0.0154,  0.0164],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0050,  0.0204,  0.0137,  ...,  0.0186,  0.0176,  0.0188],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(512.4001, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.9332, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.9082, device='cuda:0')



h[100].sum tensor(19.7469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9257, device='cuda:0')



h[200].sum tensor(24.4872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.0915, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0148, 0.0632, 0.0416,  ..., 0.0567, 0.0545, 0.0579],
        [0.0152, 0.0647, 0.0427,  ..., 0.0581, 0.0558, 0.0593],
        [0.0146, 0.0627, 0.0412,  ..., 0.0562, 0.0540, 0.0574],
        ...,
        [0.0049, 0.0214, 0.0135,  ..., 0.0183, 0.0182, 0.0190],
        [0.0039, 0.0177, 0.0109,  ..., 0.0149, 0.0151, 0.0156],
        [0.0176, 0.0733, 0.0488,  ..., 0.0664, 0.0632, 0.0673]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38106.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1625, 0.0000,  ..., 0.0708, 0.0000, 0.0449],
        [0.0000, 0.1612, 0.0000,  ..., 0.0701, 0.0000, 0.0449],
        [0.0000, 0.1583, 0.0000,  ..., 0.0689, 0.0000, 0.0440],
        ...,
        [0.0000, 0.0461, 0.0000,  ..., 0.0235, 0.0000, 0.0120],
        [0.0000, 0.0593, 0.0000,  ..., 0.0288, 0.0000, 0.0158],
        [0.0000, 0.1014, 0.0000,  ..., 0.0456, 0.0000, 0.0285]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(198142.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-98.2926, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.1320, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-219.2332, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [0.9994],
        [0.9966],
        ...,
        [0.9914],
        [0.9910],
        [0.9905]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364518.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [0.9994],
        [0.9966],
        ...,
        [0.9914],
        [0.9910],
        [0.9905]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364518.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(626.8304, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5566, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.3646, device='cuda:0')



h[100].sum tensor(20.4197, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0256, device='cuda:0')



h[200].sum tensor(25.9161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.9897, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42608.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.6947e-02, 0.0000e+00,  ..., 1.2353e-02, 0.0000e+00,
         2.8312e-03],
        [0.0000e+00, 7.5252e-03, 0.0000e+00,  ..., 8.4589e-03, 0.0000e+00,
         3.7990e-04],
        [0.0000e+00, 4.0738e-03, 0.0000e+00,  ..., 7.0094e-03, 0.0000e+00,
         7.6217e-05],
        ...,
        [0.0000e+00, 3.3588e-04, 0.0000e+00,  ..., 5.2252e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3611e-04, 0.0000e+00,  ..., 5.2247e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3613e-04, 0.0000e+00,  ..., 5.2245e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(217712.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-120.2894, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.0618, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-251.6916, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [0.9994],
        [0.9966],
        ...,
        [0.9914],
        [0.9910],
        [0.9905]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364518.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [0.9994],
        [0.9966],
        ...,
        [0.9914],
        [0.9909],
        [0.9904]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364507., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0038,  0.0163,  0.0107,  ...,  0.0146,  0.0140,  0.0149],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(579.0137, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.7980, device='cuda:0')



h[100].sum tensor(20.1217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5271, device='cuda:0')



h[200].sum tensor(25.2833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.2229, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0068, 0.0304, 0.0194,  ..., 0.0264, 0.0261, 0.0274],
        [0.0093, 0.0414, 0.0266,  ..., 0.0363, 0.0355, 0.0376],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0046, 0.0221, 0.0135,  ..., 0.0185, 0.0189, 0.0197]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39205.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0327, 0.0000,  ..., 0.0185, 0.0000, 0.0077],
        [0.0000, 0.0913, 0.0000,  ..., 0.0419, 0.0000, 0.0248],
        [0.0000, 0.1404, 0.0000,  ..., 0.0618, 0.0000, 0.0389],
        ...,
        [0.0000, 0.0092, 0.0000,  ..., 0.0089, 0.0000, 0.0015],
        [0.0000, 0.0273, 0.0000,  ..., 0.0163, 0.0000, 0.0058],
        [0.0000, 0.0695, 0.0000,  ..., 0.0336, 0.0000, 0.0170]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(198657.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-104.0516, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.8029, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-227.6362, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [0.9994],
        [0.9966],
        ...,
        [0.9914],
        [0.9909],
        [0.9904]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364507., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [0.9994],
        [0.9966],
        ...,
        [0.9913],
        [0.9908],
        [0.9903]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364495.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0034,  0.0149,  0.0097,  ...,  0.0133,  0.0128,  0.0136],
        [ 0.0100,  0.0389,  0.0267,  ...,  0.0362,  0.0336,  0.0359],
        [ 0.0013,  0.0069,  0.0041,  ...,  0.0057,  0.0059,  0.0062],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(337.2493, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7535, device='cuda:0')



h[100].sum tensor(18.6938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.9673, device='cuda:0')



h[200].sum tensor(22.2504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.1502, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0353, 0.1381, 0.0946,  ..., 0.1283, 0.1194, 0.1275],
        [0.0168, 0.0704, 0.0467,  ..., 0.0636, 0.0608, 0.0646],
        [0.0189, 0.0781, 0.0522,  ..., 0.0710, 0.0674, 0.0718],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29973.6133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2951, 0.0000,  ..., 0.1215, 0.0000, 0.0886],
        [0.0000, 0.2306, 0.0000,  ..., 0.0969, 0.0000, 0.0670],
        [0.0000, 0.1921, 0.0000,  ..., 0.0819, 0.0000, 0.0550],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(153938.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-60.4838, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.7841, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-161.4461, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [0.9994],
        [0.9966],
        ...,
        [0.9913],
        [0.9908],
        [0.9903]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364495.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [0.9993],
        [0.9965],
        ...,
        [0.9912],
        [0.9907],
        [0.9903]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364483.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0030,  0.0131,  0.0085,  ...,  0.0116,  0.0113,  0.0120],
        [ 0.0065,  0.0259,  0.0175,  ...,  0.0238,  0.0223,  0.0238],
        [ 0.0030,  0.0132,  0.0085,  ...,  0.0117,  0.0114,  0.0120],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(647.5727, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.7160, device='cuda:0')



h[100].sum tensor(20.4873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1374, device='cuda:0')



h[200].sum tensor(26.0598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.3860, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0214, 0.0856, 0.0579,  ..., 0.0786, 0.0739, 0.0787],
        [0.0209, 0.0854, 0.0573,  ..., 0.0779, 0.0737, 0.0786],
        [0.0211, 0.0845, 0.0572,  ..., 0.0776, 0.0729, 0.0777],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44727.2070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.3325e-01, 0.0000e+00,  ..., 1.3549e-01, 0.0000e+00,
         1.0240e-01],
        [0.0000e+00, 3.6425e-01, 0.0000e+00,  ..., 1.4766e-01, 0.0000e+00,
         1.1189e-01],
        [0.0000e+00, 3.4808e-01, 0.0000e+00,  ..., 1.4129e-01, 0.0000e+00,
         1.0710e-01],
        ...,
        [0.0000e+00, 3.3616e-04, 0.0000e+00,  ..., 5.2225e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3639e-04, 0.0000e+00,  ..., 5.2220e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3640e-04, 0.0000e+00,  ..., 5.2218e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(229881.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-130.7499, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-36.3975, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-268.1428, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [0.9993],
        [0.9965],
        ...,
        [0.9912],
        [0.9907],
        [0.9903]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364483.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [0.9993],
        [0.9965],
        ...,
        [0.9911],
        [0.9907],
        [0.9902]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364472.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(376.1445, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6740, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8636, device='cuda:0')



h[100].sum tensor(18.8962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.3206, device='cuda:0')



h[200].sum tensor(22.6803, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.4022, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33327.7617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0067, 0.0000, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(178575.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-75.8558, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.1856, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-185.0936, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [0.9993],
        [0.9965],
        ...,
        [0.9911],
        [0.9907],
        [0.9902]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364472.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [0.9993],
        [0.9965],
        ...,
        [0.9911],
        [0.9906],
        [0.9901]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364460.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(325.6228, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9484, device='cuda:0')



h[100].sum tensor(18.5922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7111, device='cuda:0')



h[200].sum tensor(22.0347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.2422, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30465.5410, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(162493.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-62.3844, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.6724, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.5485, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [0.9993],
        [0.9965],
        ...,
        [0.9911],
        [0.9906],
        [0.9901]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364460.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [0.9993],
        [0.9964],
        ...,
        [0.9910],
        [0.9905],
        [0.9900]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364449.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0044,  0.0184,  0.0122,  ...,  0.0167,  0.0159,  0.0169],
        [ 0.0078,  0.0307,  0.0209,  ...,  0.0284,  0.0265,  0.0283],
        [ 0.0051,  0.0210,  0.0141,  ...,  0.0192,  0.0182,  0.0193],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1134.2173, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.7945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(44.4323, device='cuda:0')



h[100].sum tensor(23.2356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.1383, device='cuda:0')



h[200].sum tensor(31.8971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(50.1109, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0121, 0.0514, 0.0337,  ..., 0.0459, 0.0443, 0.0469],
        [0.0179, 0.0744, 0.0495,  ..., 0.0674, 0.0642, 0.0683],
        [0.0361, 0.1408, 0.0966,  ..., 0.1309, 0.1217, 0.1301],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(69121.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.9529e-01, 0.0000e+00,  ..., 8.3118e-02, 0.0000e+00,
         5.5963e-02],
        [0.0000e+00, 2.7286e-01, 0.0000e+00,  ..., 1.1311e-01, 0.0000e+00,
         8.0981e-02],
        [0.0000e+00, 3.8226e-01, 0.0000e+00,  ..., 1.5481e-01, 0.0000e+00,
         1.1733e-01],
        ...,
        [0.0000e+00, 3.3643e-04, 0.0000e+00,  ..., 5.2198e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3666e-04, 0.0000e+00,  ..., 5.2193e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3667e-04, 0.0000e+00,  ..., 5.2191e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(390597.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-246.9560, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-14.0081, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-444.7832, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [0.9993],
        [0.9964],
        ...,
        [0.9910],
        [0.9905],
        [0.9900]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364449.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [0.9993],
        [0.9964],
        ...,
        [0.9909],
        [0.9904],
        [0.9899]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364437.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0038,  0.0162,  0.0107,  ...,  0.0146,  0.0140,  0.0148],
        [ 0.0014,  0.0076,  0.0046,  ...,  0.0063,  0.0065,  0.0068],
        [ 0.0070,  0.0279,  0.0189,  ...,  0.0257,  0.0241,  0.0257],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(596.8112, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.5704, device='cuda:0')



h[100].sum tensor(20.1258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4547, device='cuda:0')



h[200].sum tensor(25.2920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.9662, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0076, 0.0370, 0.0230,  ..., 0.0316, 0.0318, 0.0335],
        [0.0152, 0.0648, 0.0427,  ..., 0.0582, 0.0559, 0.0594],
        [0.0081, 0.0371, 0.0236,  ..., 0.0322, 0.0318, 0.0336],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44188.7617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1568, 0.0000,  ..., 0.0688, 0.0000, 0.0422],
        [0.0000, 0.1550, 0.0000,  ..., 0.0680, 0.0000, 0.0420],
        [0.0000, 0.1284, 0.0000,  ..., 0.0573, 0.0000, 0.0342],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(244821.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-127.6135, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.5703, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-263.2398, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [0.9993],
        [0.9964],
        ...,
        [0.9909],
        [0.9904],
        [0.9899]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364437.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 170.0 event: 850 loss: tensor(590.8610, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0001],
        [0.9993],
        [0.9963],
        ...,
        [0.9908],
        [0.9904],
        [0.9899]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364425.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0027,  0.0121,  0.0078,  ...,  0.0107,  0.0104,  0.0111],
        [ 0.0028,  0.0124,  0.0080,  ...,  0.0109,  0.0106,  0.0113],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(578.1365, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.4095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.1727, device='cuda:0')



h[100].sum tensor(20.0029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3281, device='cuda:0')



h[200].sum tensor(25.0308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.5176, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0156, 0.0663, 0.0438,  ..., 0.0597, 0.0572, 0.0608],
        [0.0048, 0.0231, 0.0142,  ..., 0.0194, 0.0198, 0.0206],
        [0.0028, 0.0136, 0.0080,  ..., 0.0109, 0.0115, 0.0118],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42281.1992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1341, 0.0000,  ..., 0.0590, 0.0000, 0.0376],
        [0.0000, 0.0751, 0.0000,  ..., 0.0356, 0.0000, 0.0197],
        [0.0000, 0.0394, 0.0000,  ..., 0.0213, 0.0000, 0.0094],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(233125.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-118.8897, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.9371, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-249.9658, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0001],
        [0.9993],
        [0.9963],
        ...,
        [0.9908],
        [0.9904],
        [0.9899]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364425.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [0.9992],
        [0.9963],
        ...,
        [0.9908],
        [0.9903],
        [0.9898]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364414.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0093,  0.0363,  0.0249,  ...,  0.0337,  0.0313,  0.0335],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(562.9551, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.4449, device='cuda:0')



h[100].sum tensor(19.9006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0965, device='cuda:0')



h[200].sum tensor(24.8137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.6968, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0202, 0.0830, 0.0556,  ..., 0.0756, 0.0717, 0.0763],
        [0.0070, 0.0309, 0.0197,  ..., 0.0269, 0.0265, 0.0279],
        [0.0211, 0.0863, 0.0579,  ..., 0.0787, 0.0745, 0.0793],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40878.6523, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1360, 0.0000,  ..., 0.0593, 0.0000, 0.0389],
        [0.0000, 0.1086, 0.0000,  ..., 0.0486, 0.0000, 0.0305],
        [0.0000, 0.1391, 0.0000,  ..., 0.0605, 0.0000, 0.0400],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(214575.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-112.4744, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.1335, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-239.9931, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [0.9992],
        [0.9963],
        ...,
        [0.9908],
        [0.9903],
        [0.9898]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364414.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [0.9992],
        [0.9963],
        ...,
        [0.9907],
        [0.9902],
        [0.9897]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364402.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0020,  0.0096,  0.0060,  ...,  0.0083,  0.0083,  0.0088],
        [ 0.0070,  0.0280,  0.0190,  ...,  0.0258,  0.0242,  0.0258],
        [ 0.0088,  0.0344,  0.0236,  ...,  0.0320,  0.0297,  0.0318],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(446.2704, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.6310, device='cuda:0')



h[100].sum tensor(19.2228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.8829, device='cuda:0')



h[200].sum tensor(23.3741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.3955, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0099, 0.0417, 0.0274,  ..., 0.0372, 0.0359, 0.0379],
        [0.0238, 0.0960, 0.0648,  ..., 0.0880, 0.0829, 0.0883],
        [0.0406, 0.1575, 0.1084,  ..., 0.1468, 0.1362, 0.1456],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35652.6445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.6504e-01, 0.0000e+00,  ..., 7.0722e-02, 0.0000e+00,
         4.7767e-02],
        [0.0000e+00, 2.7620e-01, 0.0000e+00,  ..., 1.1378e-01, 0.0000e+00,
         8.3204e-02],
        [0.0000e+00, 3.9672e-01, 0.0000e+00,  ..., 1.5998e-01, 0.0000e+00,
         1.2263e-01],
        ...,
        [0.0000e+00, 3.3679e-04, 0.0000e+00,  ..., 5.2162e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3702e-04, 0.0000e+00,  ..., 5.2157e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3704e-04, 0.0000e+00,  ..., 5.2155e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(190244.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-86.6346, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.3244, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-201.5297, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [0.9992],
        [0.9963],
        ...,
        [0.9907],
        [0.9902],
        [0.9897]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364402.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [0.9992],
        [0.9962],
        ...,
        [0.9906],
        [0.9901],
        [0.9896]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364391., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0033,  0.0145,  0.0095,  ...,  0.0129,  0.0125,  0.0133],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(422.5503, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0583, device='cuda:0')



h[100].sum tensor(19.0757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7007, device='cuda:0')



h[200].sum tensor(23.0616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.7496, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0301, 0.0191,  ..., 0.0261, 0.0258, 0.0271],
        [0.0033, 0.0157, 0.0095,  ..., 0.0129, 0.0134, 0.0138],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33602.4648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0957, 0.0000,  ..., 0.0436, 0.0000, 0.0265],
        [0.0000, 0.0538, 0.0000,  ..., 0.0270, 0.0000, 0.0134],
        [0.0000, 0.0213, 0.0000,  ..., 0.0141, 0.0000, 0.0035],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(175576.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-77.8784, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.6704, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-187.4060, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [0.9992],
        [0.9962],
        ...,
        [0.9906],
        [0.9901],
        [0.9896]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364391., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [0.9992],
        [0.9962],
        ...,
        [0.9905],
        [0.9900],
        [0.9895]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364379.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0037,  0.0157,  0.0103,  ...,  0.0141,  0.0135,  0.0144],
        [ 0.0013,  0.0071,  0.0042,  ...,  0.0058,  0.0061,  0.0064],
        [ 0.0018,  0.0090,  0.0056,  ...,  0.0077,  0.0078,  0.0082],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(372.8279, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2428, device='cuda:0')



h[100].sum tensor(18.7829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1230, device='cuda:0')



h[200].sum tensor(22.4397, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.7021, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0054, 0.0270, 0.0164,  ..., 0.0226, 0.0231, 0.0242],
        [0.0132, 0.0574, 0.0375,  ..., 0.0511, 0.0495, 0.0525],
        [0.0059, 0.0269, 0.0169,  ..., 0.0231, 0.0231, 0.0242],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31573.2852, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0915, 0.0000,  ..., 0.0429, 0.0000, 0.0227],
        [0.0000, 0.1194, 0.0000,  ..., 0.0540, 0.0000, 0.0311],
        [0.0000, 0.0849, 0.0000,  ..., 0.0400, 0.0000, 0.0211],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(167178.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-68.1578, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.4347, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-172.8401, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [0.9992],
        [0.9962],
        ...,
        [0.9905],
        [0.9900],
        [0.9895]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364379.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [0.9992],
        [0.9961],
        ...,
        [0.9905],
        [0.9900],
        [0.9894]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364367.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0016,  0.0081,  0.0049,  ...,  0.0068,  0.0069,  0.0073],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(421.9249, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7876, device='cuda:0')



h[100].sum tensor(19.0475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6146, device='cuda:0')



h[200].sum tensor(23.0017, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.4443, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0060, 0.0275, 0.0173,  ..., 0.0237, 0.0236, 0.0247],
        [0.0016, 0.0093, 0.0049,  ..., 0.0068, 0.0078, 0.0078],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34121.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0879, 0.0000,  ..., 0.0412, 0.0000, 0.0221],
        [0.0000, 0.0476, 0.0000,  ..., 0.0249, 0.0000, 0.0107],
        [0.0000, 0.0188, 0.0000,  ..., 0.0130, 0.0000, 0.0035],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(179488.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.0773, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.4193, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-190.8363, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [0.9992],
        [0.9961],
        ...,
        [0.9905],
        [0.9900],
        [0.9894]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364367.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [0.9992],
        [0.9961],
        ...,
        [0.9904],
        [0.9899],
        [0.9894]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364355.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0015,  0.0077,  0.0047,  ...,  0.0064,  0.0066,  0.0069],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(440.9660, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0035, device='cuda:0')



h[100].sum tensor(19.1420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6833, device='cuda:0')



h[200].sum tensor(23.2024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.6878, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0034, 0.0198, 0.0114,  ..., 0.0158, 0.0169, 0.0176],
        [0.0026, 0.0149, 0.0084,  ..., 0.0116, 0.0126, 0.0130],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36513.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0673, 0.0000,  ..., 0.0335, 0.0000, 0.0146],
        [0.0000, 0.0484, 0.0000,  ..., 0.0256, 0.0000, 0.0099],
        [0.0000, 0.0193, 0.0000,  ..., 0.0134, 0.0000, 0.0030],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(197714.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-91.6430, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.1592, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-208.2698, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [0.9992],
        [0.9961],
        ...,
        [0.9904],
        [0.9899],
        [0.9894]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364355.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [0.9992],
        [0.9961],
        ...,
        [0.9903],
        [0.9898],
        [0.9893]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364344.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0083,  0.0327,  0.0223,  ...,  0.0303,  0.0282,  0.0301],
        [ 0.0137,  0.0523,  0.0362,  ...,  0.0490,  0.0452,  0.0484],
        [ 0.0096,  0.0372,  0.0256,  ...,  0.0346,  0.0322,  0.0344],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(279.1382, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9507, device='cuda:0')



h[100].sum tensor(18.2261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.0755, device='cuda:0')



h[200].sum tensor(21.2571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.9892, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0361, 0.1409, 0.0966,  ..., 0.1309, 0.1218, 0.1301],
        [0.0468, 0.1799, 0.1242,  ..., 0.1682, 0.1556, 0.1664],
        [0.0495, 0.1901, 0.1314,  ..., 0.1779, 0.1644, 0.1758],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28172.3555, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.4844e-01, 0.0000e+00,  ..., 1.4083e-01, 0.0000e+00,
         1.0769e-01],
        [0.0000e+00, 4.3671e-01, 0.0000e+00,  ..., 1.7483e-01, 0.0000e+00,
         1.3601e-01],
        [0.0000e+00, 4.4965e-01, 0.0000e+00,  ..., 1.8008e-01, 0.0000e+00,
         1.3965e-01],
        ...,
        [0.0000e+00, 3.3724e-04, 0.0000e+00,  ..., 5.2117e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3747e-04, 0.0000e+00,  ..., 5.2113e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3749e-04, 0.0000e+00,  ..., 5.2111e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(149848.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-51.4772, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-51.5470, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-148.2352, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [0.9992],
        [0.9961],
        ...,
        [0.9903],
        [0.9898],
        [0.9893]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364344.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [0.9991],
        [0.9960],
        ...,
        [0.9902],
        [0.9897],
        [0.9892]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364332.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0037,  0.0158,  0.0104,  ...,  0.0141,  0.0136,  0.0145],
        [ 0.0015,  0.0078,  0.0047,  ...,  0.0066,  0.0067,  0.0071],
        [ 0.0017,  0.0084,  0.0051,  ...,  0.0071,  0.0072,  0.0076],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(509.9056, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4367, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1700, device='cuda:0')



h[100].sum tensor(19.5008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3727, device='cuda:0')



h[200].sum tensor(23.9646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.1312, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0054, 0.0270, 0.0164,  ..., 0.0226, 0.0231, 0.0242],
        [0.0124, 0.0547, 0.0355,  ..., 0.0485, 0.0471, 0.0500],
        [0.0059, 0.0269, 0.0169,  ..., 0.0231, 0.0231, 0.0242],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37926.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0887, 0.0000,  ..., 0.0418, 0.0000, 0.0218],
        [0.0000, 0.1135, 0.0000,  ..., 0.0516, 0.0000, 0.0293],
        [0.0000, 0.0802, 0.0000,  ..., 0.0381, 0.0000, 0.0200],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(199320.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-97.5221, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.2177, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-218.0157, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [0.9991],
        [0.9960],
        ...,
        [0.9902],
        [0.9897],
        [0.9892]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364332.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [0.9991],
        [0.9960],
        ...,
        [0.9902],
        [0.9896],
        [0.9891]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364320.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0063,  0.0253,  0.0171,  ...,  0.0232,  0.0218,  0.0233],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(560.0609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.7126, device='cuda:0')



h[100].sum tensor(19.7655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8635, device='cuda:0')



h[200].sum tensor(24.5267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.8710, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0235, 0.0931, 0.0633,  ..., 0.0858, 0.0804, 0.0857],
        [0.0154, 0.0637, 0.0424,  ..., 0.0577, 0.0549, 0.0583],
        [0.0086, 0.0386, 0.0247,  ..., 0.0337, 0.0332, 0.0350],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39198.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2932, 0.0000,  ..., 0.1207, 0.0000, 0.0885],
        [0.0000, 0.2462, 0.0000,  ..., 0.1028, 0.0000, 0.0730],
        [0.0000, 0.2005, 0.0000,  ..., 0.0855, 0.0000, 0.0575],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(204354.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-103.8871, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.0529, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-227.2714, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [0.9991],
        [0.9960],
        ...,
        [0.9902],
        [0.9896],
        [0.9891]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364320.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 180.0 event: 900 loss: tensor(654.5710, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [0.9991],
        [0.9960],
        ...,
        [0.9901],
        [0.9896],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364308.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(501.7638, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1437, device='cuda:0')



h[100].sum tensor(19.4289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3643, device='cuda:0')



h[200].sum tensor(23.8118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.1015, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0017, 0.0097, 0.0052,  ..., 0.0071, 0.0081, 0.0081],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37011.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0039, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0127, 0.0000,  ..., 0.0106, 0.0000, 0.0018],
        [0.0000, 0.0379, 0.0000,  ..., 0.0210, 0.0000, 0.0079],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(195361.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-94.2432, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.5382, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-212.0764, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [0.9991],
        [0.9960],
        ...,
        [0.9901],
        [0.9896],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364308.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [0.9991],
        [0.9959],
        ...,
        [0.9900],
        [0.9895],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364297.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0021,  0.0098,  0.0062,  ...,  0.0085,  0.0085,  0.0089],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0021,  0.0098,  0.0062,  ...,  0.0085,  0.0085,  0.0089],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1174.8325, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.0227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(43.1023, device='cuda:0')



h[100].sum tensor(23.1241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.7151, device='cuda:0')



h[200].sum tensor(31.6602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(48.6109, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0043, 0.0212, 0.0128,  ..., 0.0176, 0.0181, 0.0188],
        [0.0089, 0.0418, 0.0264,  ..., 0.0362, 0.0360, 0.0380],
        [0.0016, 0.0093, 0.0049,  ..., 0.0068, 0.0078, 0.0078],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(67767.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0784, 0.0000,  ..., 0.0376, 0.0000, 0.0188],
        [0.0000, 0.0892, 0.0000,  ..., 0.0421, 0.0000, 0.0217],
        [0.0000, 0.0544, 0.0000,  ..., 0.0278, 0.0000, 0.0122],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(365395.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-241.5804, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-13.7288, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-437.1084, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [0.9991],
        [0.9959],
        ...,
        [0.9900],
        [0.9895],
        [0.9890]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364297.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[1.0000],
        [0.9991],
        [0.9959],
        ...,
        [0.9899],
        [0.9894],
        [0.9889]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364285.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0015,  0.0077,  0.0047,  ...,  0.0065,  0.0066,  0.0070],
        [ 0.0015,  0.0077,  0.0047,  ...,  0.0065,  0.0066,  0.0070],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(532.5764, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.2913, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7545, device='cuda:0')



h[100].sum tensor(19.5719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5586, device='cuda:0')



h[200].sum tensor(24.1156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7904, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0048, 0.0250, 0.0150,  ..., 0.0207, 0.0214, 0.0224],
        [0.0035, 0.0200, 0.0115,  ..., 0.0159, 0.0171, 0.0177],
        [0.0026, 0.0150, 0.0084,  ..., 0.0116, 0.0127, 0.0130],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37079.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0847, 0.0000,  ..., 0.0407, 0.0000, 0.0192],
        [0.0000, 0.0715, 0.0000,  ..., 0.0353, 0.0000, 0.0154],
        [0.0000, 0.0531, 0.0000,  ..., 0.0276, 0.0000, 0.0108],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(193887.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-93.6871, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.8157, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-212.1696, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.0000],
        [0.9991],
        [0.9959],
        ...,
        [0.9899],
        [0.9894],
        [0.9889]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364285.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9991],
        [0.9958],
        ...,
        [0.9898],
        [0.9893],
        [0.9888]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364273.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0047,  0.0195,  0.0130,  ...,  0.0177,  0.0168,  0.0179],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(405.2357, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5952, device='cuda:0')



h[100].sum tensor(18.8614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2352, device='cuda:0')



h[200].sum tensor(22.6064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.0995, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0089, 0.0417, 0.0263,  ..., 0.0361, 0.0358, 0.0379],
        [0.0032, 0.0172, 0.0100,  ..., 0.0138, 0.0146, 0.0151],
        [0.0111, 0.0499, 0.0321,  ..., 0.0439, 0.0429, 0.0455],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32749.7402, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0996, 0.0000,  ..., 0.0464, 0.0000, 0.0245],
        [0.0000, 0.0893, 0.0000,  ..., 0.0422, 0.0000, 0.0217],
        [0.0000, 0.1166, 0.0000,  ..., 0.0531, 0.0000, 0.0298],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(174374.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-73.1716, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.7586, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-180.8388, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9991],
        [0.9958],
        ...,
        [0.9898],
        [0.9893],
        [0.9888]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364273.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9990],
        [0.9958],
        ...,
        [0.9898],
        [0.9892],
        [0.9887]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364261.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0042,  0.0178,  0.0118,  ...,  0.0161,  0.0154,  0.0163],
        [ 0.0037,  0.0158,  0.0104,  ...,  0.0141,  0.0136,  0.0144],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(531.5980, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.3572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7173, device='cuda:0')



h[100].sum tensor(19.5397, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5468, device='cuda:0')



h[200].sum tensor(24.0471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7485, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0171, 0.0717, 0.0476,  ..., 0.0648, 0.0618, 0.0658],
        [0.0072, 0.0316, 0.0202,  ..., 0.0275, 0.0271, 0.0285],
        [0.0037, 0.0170, 0.0104,  ..., 0.0141, 0.0144, 0.0149],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35568.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1325, 0.0000,  ..., 0.0582, 0.0000, 0.0375],
        [0.0000, 0.0884, 0.0000,  ..., 0.0407, 0.0000, 0.0241],
        [0.0000, 0.0515, 0.0000,  ..., 0.0260, 0.0000, 0.0130],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(178380.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-87.4195, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.6275, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-202.0144, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9990],
        [0.9958],
        ...,
        [0.9898],
        [0.9892],
        [0.9887]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364261.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9990],
        [0.9958],
        ...,
        [0.9897],
        [0.9892],
        [0.9886]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364249.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0014,  0.0074,  0.0044,  ...,  0.0061,  0.0063,  0.0066],
        [ 0.0041,  0.0174,  0.0115,  ...,  0.0157,  0.0150,  0.0159],
        [ 0.0020,  0.0096,  0.0060,  ...,  0.0082,  0.0082,  0.0087],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(365.9554, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2288, device='cuda:0')



h[100].sum tensor(18.6256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8004, device='cuda:0')



h[200].sum tensor(22.1055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.5584, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0113, 0.0507, 0.0327,  ..., 0.0447, 0.0436, 0.0462],
        [0.0061, 0.0316, 0.0192,  ..., 0.0264, 0.0271, 0.0285],
        [0.0115, 0.0513, 0.0331,  ..., 0.0453, 0.0441, 0.0468],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31076.8496, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.0049e-01, 0.0000e+00,  ..., 4.6561e-02, 0.0000e+00,
         2.5202e-02],
        [0.0000e+00, 1.0411e-01, 0.0000e+00,  ..., 4.8319e-02, 0.0000e+00,
         2.5427e-02],
        [0.0000e+00, 1.2799e-01, 0.0000e+00,  ..., 5.7703e-02, 0.0000e+00,
         3.2882e-02],
        ...,
        [0.0000e+00, 1.9708e-03, 0.0000e+00,  ..., 5.8622e-03, 0.0000e+00,
         4.1363e-05],
        [0.0000e+00, 3.3820e-04, 0.0000e+00,  ..., 5.2041e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3821e-04, 0.0000e+00,  ..., 5.2039e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(165891., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-65.3112, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.1958, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-168.7884, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9990],
        [0.9958],
        ...,
        [0.9897],
        [0.9892],
        [0.9886]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364249.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9990],
        [0.9957],
        ...,
        [0.9896],
        [0.9891],
        [0.9885]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364237.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(291.0945, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9143, device='cuda:0')



h[100].sum tensor(18.2094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.0639, device='cuda:0')



h[200].sum tensor(21.2217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.9482, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28671.1289, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0069, 0.0000, 0.0001],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(154924.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.0526, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-51.3451, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-151.4338, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9990],
        [0.9957],
        ...,
        [0.9896],
        [0.9891],
        [0.9885]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364237.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9990],
        [0.9957],
        ...,
        [0.9895],
        [0.9890],
        [0.9885]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364225.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(357.0370, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3683, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8169, device='cuda:0')



h[100].sum tensor(18.5569, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.6693, device='cuda:0')



h[200].sum tensor(21.9596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.0939, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30420.5039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(160650.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-62.4358, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.4397, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.5743, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9990],
        [0.9957],
        ...,
        [0.9895],
        [0.9890],
        [0.9885]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364225.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9990],
        [0.9957],
        ...,
        [0.9895],
        [0.9890],
        [0.9885]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364225.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0039,  0.0166,  0.0109,  ...,  0.0149,  0.0143,  0.0152],
        [ 0.0061,  0.0245,  0.0166,  ...,  0.0225,  0.0212,  0.0226],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(291.6614, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8614, device='cuda:0')



h[100].sum tensor(18.2036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.0471, device='cuda:0')



h[200].sum tensor(21.2092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.8885, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0103, 0.0471, 0.0301,  ..., 0.0412, 0.0405, 0.0429],
        [0.0092, 0.0390, 0.0254,  ..., 0.0346, 0.0335, 0.0353],
        [0.0088, 0.0375, 0.0244,  ..., 0.0332, 0.0322, 0.0340],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28072.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0982, 0.0000,  ..., 0.0454, 0.0000, 0.0252],
        [0.0000, 0.1142, 0.0000,  ..., 0.0514, 0.0000, 0.0308],
        [0.0000, 0.1282, 0.0000,  ..., 0.0570, 0.0000, 0.0349],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(151018.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-51.3025, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-51.6363, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-147.4868, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9990],
        [0.9957],
        ...,
        [0.9895],
        [0.9890],
        [0.9885]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364225.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9990],
        [0.9956],
        ...,
        [0.9895],
        [0.9889],
        [0.9884]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364214., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(406.6972, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1930, device='cuda:0')



h[100].sum tensor(18.8144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1072, device='cuda:0')



h[200].sum tensor(22.5066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.6460, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34422.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0039, 0.0000,  ..., 0.0069, 0.0000, 0.0002],
        [0.0000, 0.0116, 0.0000,  ..., 0.0102, 0.0000, 0.0014],
        [0.0000, 0.0155, 0.0000,  ..., 0.0118, 0.0000, 0.0021],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(188624.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.5829, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.6924, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-193.6439, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9990],
        [0.9956],
        ...,
        [0.9895],
        [0.9889],
        [0.9884]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364214., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 190.0 event: 950 loss: tensor(557.8589, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9990],
        [0.9956],
        ...,
        [0.9894],
        [0.9888],
        [0.9883]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364202.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056,  0.0229,  0.0154,  ...,  0.0210,  0.0198,  0.0211],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(421.0685, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5242, device='cuda:0')



h[100].sum tensor(18.8808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2126, device='cuda:0')



h[200].sum tensor(22.6477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.0194, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0181, 0.0717, 0.0486,  ..., 0.0658, 0.0618, 0.0657],
        [0.0102, 0.0425, 0.0279,  ..., 0.0380, 0.0365, 0.0386],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33951.5977, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1815, 0.0000,  ..., 0.0762, 0.0000, 0.0548],
        [0.0000, 0.1266, 0.0000,  ..., 0.0551, 0.0000, 0.0371],
        [0.0000, 0.0642, 0.0000,  ..., 0.0307, 0.0000, 0.0176],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(184924.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-79.2772, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.4255, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-189.8278, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9990],
        [0.9956],
        ...,
        [0.9894],
        [0.9888],
        [0.9883]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364202.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9989],
        [0.9956],
        ...,
        [0.9893],
        [0.9888],
        [0.9882]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364190.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0065,  0.0260,  0.0176,  ...,  0.0239,  0.0224,  0.0239],
        [ 0.0026,  0.0119,  0.0076,  ...,  0.0104,  0.0102,  0.0108],
        [ 0.0035,  0.0152,  0.0099,  ...,  0.0136,  0.0131,  0.0139],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(606.3569, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.2823, device='cuda:0')



h[100].sum tensor(19.8617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0448, device='cuda:0')



h[200].sum tensor(24.7311, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.5134, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0181, 0.0753, 0.0501,  ..., 0.0682, 0.0649, 0.0691],
        [0.0181, 0.0754, 0.0502,  ..., 0.0684, 0.0651, 0.0693],
        [0.0054, 0.0252, 0.0156,  ..., 0.0214, 0.0215, 0.0225],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40424.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2774, 0.0000,  ..., 0.1151, 0.0000, 0.0824],
        [0.0000, 0.2236, 0.0000,  ..., 0.0944, 0.0000, 0.0651],
        [0.0000, 0.1450, 0.0000,  ..., 0.0635, 0.0000, 0.0405],
        ...,
        [0.0000, 0.0029, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0085, 0.0000, 0.0004],
        [0.0000, 0.0105, 0.0000,  ..., 0.0096, 0.0000, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(212131.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-110.0872, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.7650, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-236.3707, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9989],
        [0.9956],
        ...,
        [0.9893],
        [0.9888],
        [0.9882]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364190.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9989],
        [0.9955],
        ...,
        [0.9892],
        [0.9887],
        [0.9881]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364178.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(443.2316, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3088, device='cuda:0')



h[100].sum tensor(18.9773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4622, device='cuda:0')



h[200].sum tensor(22.8527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.9044, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32889.0898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.9859e-02, 0.0000e+00,  ..., 2.1418e-02, 0.0000e+00,
         9.8460e-03],
        [0.0000e+00, 1.2252e-02, 0.0000e+00,  ..., 1.0374e-02, 0.0000e+00,
         1.6806e-03],
        [0.0000e+00, 3.0506e-03, 0.0000e+00,  ..., 6.5619e-03, 0.0000e+00,
         2.8602e-05],
        ...,
        [0.0000e+00, 3.3850e-04, 0.0000e+00,  ..., 5.1992e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3874e-04, 0.0000e+00,  ..., 5.1988e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3875e-04, 0.0000e+00,  ..., 5.1985e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(170344.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-74.5577, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.0286, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-182.6446, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9989],
        [0.9955],
        ...,
        [0.9892],
        [0.9887],
        [0.9881]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364178.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9989],
        [0.9955],
        ...,
        [0.9891],
        [0.9886],
        [0.9880]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364166.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0032,  0.0139,  0.0090,  ...,  0.0123,  0.0120,  0.0127],
        [ 0.0022,  0.0103,  0.0065,  ...,  0.0089,  0.0088,  0.0093],
        [ 0.0014,  0.0073,  0.0044,  ...,  0.0060,  0.0062,  0.0066],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(359.5621, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7895, device='cuda:0')



h[100].sum tensor(18.5211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.6606, device='cuda:0')



h[200].sum tensor(21.8837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.0630, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0107, 0.0484, 0.0311,  ..., 0.0426, 0.0417, 0.0442],
        [0.0072, 0.0356, 0.0220,  ..., 0.0303, 0.0306, 0.0323],
        [0.0054, 0.0288, 0.0172,  ..., 0.0238, 0.0247, 0.0259],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29792.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1099, 0.0000,  ..., 0.0503, 0.0000, 0.0281],
        [0.0000, 0.1100, 0.0000,  ..., 0.0507, 0.0000, 0.0270],
        [0.0000, 0.1009, 0.0000,  ..., 0.0472, 0.0000, 0.0238],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(158937.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-59.2640, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.1379, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-159.8201, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9989],
        [0.9955],
        ...,
        [0.9891],
        [0.9886],
        [0.9880]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364166.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9989],
        [0.9954],
        ...,
        [0.9891],
        [0.9885],
        [0.9880]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364154.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0048,  0.0198,  0.0132,  ...,  0.0180,  0.0171,  0.0182],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(697.9906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7889, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5514, device='cuda:0')



h[100].sum tensor(20.3061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7668, device='cuda:0')



h[200].sum tensor(25.6750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0725, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0048, 0.0210, 0.0132,  ..., 0.0180, 0.0180, 0.0187],
        [0.0165, 0.0657, 0.0444,  ..., 0.0602, 0.0566, 0.0602],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43695.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0288, 0.0000,  ..., 0.0167, 0.0000, 0.0071],
        [0.0000, 0.1037, 0.0000,  ..., 0.0458, 0.0000, 0.0305],
        [0.0000, 0.2297, 0.0000,  ..., 0.0945, 0.0000, 0.0706],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(230559.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-125.7666, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.1250, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-260.8633, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9989],
        [0.9954],
        ...,
        [0.9891],
        [0.9885],
        [0.9880]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364154.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9999],
        [0.9989],
        [0.9954],
        ...,
        [0.9890],
        [0.9884],
        [0.9879]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364142.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(446.8316, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5362, device='cuda:0')



h[100].sum tensor(18.9632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5346, device='cuda:0')



h[200].sum tensor(22.8227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.1608, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33447.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(181218.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-76.4893, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.2110, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-185.6515, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9999],
        [0.9989],
        [0.9954],
        ...,
        [0.9890],
        [0.9884],
        [0.9879]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364142.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9989],
        [0.9954],
        ...,
        [0.9889],
        [0.9884],
        [0.9878]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364130.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0058,  0.0234,  0.0158,  ...,  0.0214,  0.0202,  0.0216],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(255.4723, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.6228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.3784, device='cuda:0')



h[100].sum tensor(17.9438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.5752, device='cuda:0')



h[200].sum tensor(20.6574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.2160, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0162, 0.0663, 0.0443,  ..., 0.0602, 0.0572, 0.0608],
        [0.0104, 0.0434, 0.0286,  ..., 0.0389, 0.0374, 0.0395],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26432.1445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1821, 0.0000,  ..., 0.0774, 0.0000, 0.0532],
        [0.0000, 0.1180, 0.0000,  ..., 0.0520, 0.0000, 0.0339],
        [0.0000, 0.0377, 0.0000,  ..., 0.0203, 0.0000, 0.0099],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(142714.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-43.4514, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-53.0367, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-135.7144, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9989],
        [0.9954],
        ...,
        [0.9889],
        [0.9884],
        [0.9878]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364130.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9988],
        [0.9953],
        ...,
        [0.9888],
        [0.9883],
        [0.9877]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364118.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0082,  0.0050,  ...,  0.0069,  0.0070,  0.0074],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1199.4709, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.4896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(42.8199, device='cuda:0')



h[100].sum tensor(22.8959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.6252, device='cuda:0')



h[200].sum tensor(31.1756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(48.2925, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0118, 0.0524, 0.0339,  ..., 0.0463, 0.0451, 0.0478],
        [0.0012, 0.0080, 0.0040,  ..., 0.0055, 0.0067, 0.0066],
        [0.0118, 0.0504, 0.0330,  ..., 0.0449, 0.0434, 0.0460],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60290.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1736, 0.0000,  ..., 0.0749, 0.0000, 0.0488],
        [0.0000, 0.1537, 0.0000,  ..., 0.0668, 0.0000, 0.0433],
        [0.0000, 0.2167, 0.0000,  ..., 0.0912, 0.0000, 0.0634],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(304187.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-205.4184, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-22.0616, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-380.9110, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9988],
        [0.9953],
        ...,
        [0.9888],
        [0.9883],
        [0.9877]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364118.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9988],
        [0.9953],
        ...,
        [0.9888],
        [0.9882],
        [0.9876]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364106.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0076,  0.0301,  0.0205,  ...,  0.0278,  0.0260,  0.0278],
        [ 0.0083,  0.0326,  0.0223,  ...,  0.0302,  0.0281,  0.0301],
        [ 0.0077,  0.0304,  0.0207,  ...,  0.0281,  0.0262,  0.0280],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(605.8516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.8485, device='cuda:0')



h[100].sum tensor(19.7636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9067, device='cuda:0')



h[200].sum tensor(24.5227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.0242, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0287, 0.1141, 0.0776,  ..., 0.1053, 0.0985, 0.1052],
        [0.0334, 0.1310, 0.0896,  ..., 0.1215, 0.1133, 0.1210],
        [0.0347, 0.1360, 0.0931,  ..., 0.1263, 0.1175, 0.1256],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40187.9570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.8338e-01, 0.0000e+00,  ..., 1.5556e-01, 0.0000e+00,
         1.1773e-01],
        [0.0000e+00, 3.9090e-01, 0.0000e+00,  ..., 1.5836e-01, 0.0000e+00,
         1.2022e-01],
        [0.0000e+00, 3.7061e-01, 0.0000e+00,  ..., 1.5059e-01, 0.0000e+00,
         1.1345e-01],
        ...,
        [0.0000e+00, 3.3904e-04, 0.0000e+00,  ..., 5.1939e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3928e-04, 0.0000e+00,  ..., 5.1934e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.3929e-04, 0.0000e+00,  ..., 5.1932e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(213546.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-108.9238, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.0788, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-234.5076, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9988],
        [0.9953],
        ...,
        [0.9888],
        [0.9882],
        [0.9876]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364106.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9988],
        [0.9952],
        ...,
        [0.9887],
        [0.9881],
        [0.9875]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364094.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0088,  0.0344,  0.0235,  ...,  0.0319,  0.0297,  0.0317],
        [ 0.0060,  0.0241,  0.0163,  ...,  0.0221,  0.0208,  0.0222],
        [ 0.0016,  0.0083,  0.0051,  ...,  0.0070,  0.0072,  0.0075],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(523.2480, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3599, device='cuda:0')



h[100].sum tensor(19.3190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.1149, device='cuda:0')



h[200].sum tensor(23.5784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.2175, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0316, 0.1247, 0.0851,  ..., 0.1155, 0.1078, 0.1151],
        [0.0167, 0.0704, 0.0466,  ..., 0.0635, 0.0607, 0.0645],
        [0.0140, 0.0604, 0.0396,  ..., 0.0540, 0.0521, 0.0553],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35845.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3124, 0.0000,  ..., 0.1282, 0.0000, 0.0937],
        [0.0000, 0.2378, 0.0000,  ..., 0.1000, 0.0000, 0.0686],
        [0.0000, 0.1913, 0.0000,  ..., 0.0824, 0.0000, 0.0530],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(189946.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-87.9698, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.0151, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-203.0630, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9988],
        [0.9952],
        ...,
        [0.9887],
        [0.9881],
        [0.9875]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364094.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 200.0 event: 1000 loss: tensor(630.7459, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9988],
        [0.9952],
        ...,
        [0.9886],
        [0.9880],
        [0.9875]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364082.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0020,  0.0096,  0.0060,  ...,  0.0082,  0.0082,  0.0087],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(443.4042, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6836, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8873, device='cuda:0')



h[100].sum tensor(18.8915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.3281, device='cuda:0')



h[200].sum tensor(22.6704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.4290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0020, 0.0108, 0.0060,  ..., 0.0082, 0.0091, 0.0092],
        [0.0040, 0.0202, 0.0121,  ..., 0.0166, 0.0172, 0.0179],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34672.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0195, 0.0000,  ..., 0.0135, 0.0000, 0.0029],
        [0.0000, 0.0372, 0.0000,  ..., 0.0207, 0.0000, 0.0076],
        [0.0000, 0.0637, 0.0000,  ..., 0.0317, 0.0000, 0.0144],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(188291.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-82.3353, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.9039, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-194.8673, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9988],
        [0.9952],
        ...,
        [0.9886],
        [0.9880],
        [0.9875]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364082.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9988],
        [0.9952],
        ...,
        [0.9885],
        [0.9880],
        [0.9874]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364069.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0065,  0.0260,  0.0176,  ...,  0.0239,  0.0225,  0.0240],
        [ 0.0060,  0.0241,  0.0163,  ...,  0.0221,  0.0209,  0.0222],
        [ 0.0011,  0.0064,  0.0038,  ...,  0.0052,  0.0055,  0.0058],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(597.4285, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.0691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.2569, device='cuda:0')



h[100].sum tensor(19.6805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7185, device='cuda:0')



h[200].sum tensor(24.3462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.3570, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0296, 0.1172, 0.0798,  ..., 0.1083, 0.1013, 0.1081],
        [0.0212, 0.0865, 0.0580,  ..., 0.0789, 0.0746, 0.0795],
        [0.0273, 0.1090, 0.0740,  ..., 0.1004, 0.0941, 0.1004],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37539.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3322, 0.0000,  ..., 0.1361, 0.0000, 0.1005],
        [0.0000, 0.3087, 0.0000,  ..., 0.1272, 0.0000, 0.0926],
        [0.0000, 0.3088, 0.0000,  ..., 0.1273, 0.0000, 0.0927],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(194251.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-96.5056, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.3428, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-215.5452, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9988],
        [0.9952],
        ...,
        [0.9885],
        [0.9880],
        [0.9874]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364069.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9988],
        [0.9951],
        ...,
        [0.9884],
        [0.9879],
        [0.9873]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364057.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0021,  0.0101,  0.0063,  ...,  0.0087,  0.0087,  0.0091],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(522.3872, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1527, device='cuda:0')



h[100].sum tensor(19.2793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0490, device='cuda:0')



h[200].sum tensor(23.4941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.9839, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0113, 0.0063,  ..., 0.0087, 0.0095, 0.0096],
        [0.0016, 0.0095, 0.0051,  ..., 0.0070, 0.0080, 0.0080],
        [0.0090, 0.0420, 0.0265,  ..., 0.0364, 0.0361, 0.0381],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37073.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0321, 0.0000,  ..., 0.0186, 0.0000, 0.0062],
        [0.0000, 0.0486, 0.0000,  ..., 0.0255, 0.0000, 0.0103],
        [0.0000, 0.0816, 0.0000,  ..., 0.0392, 0.0000, 0.0191],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(199114.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-94.5314, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.4024, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-212.6286, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9988],
        [0.9951],
        ...,
        [0.9884],
        [0.9879],
        [0.9873]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364057.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9987],
        [0.9951],
        ...,
        [0.9884],
        [0.9878],
        [0.9872]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364045.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(592.5343, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.0314, device='cuda:0')



h[100].sum tensor(19.6297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.6467, device='cuda:0')



h[200].sum tensor(24.2383, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.1026, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35656.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0025, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(179581.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-87.3850, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.7500, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-202.3198, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9987],
        [0.9951],
        ...,
        [0.9884],
        [0.9878],
        [0.9872]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364045.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9987],
        [0.9950],
        ...,
        [0.9883],
        [0.9877],
        [0.9871]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364033.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0080,  0.0049,  ...,  0.0067,  0.0069,  0.0072],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(436.6293, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8398, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3290, device='cuda:0')



h[100].sum tensor(18.8151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1505, device='cuda:0')



h[200].sum tensor(22.5082, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.7993, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0016, 0.0092, 0.0049,  ..., 0.0067, 0.0077, 0.0077],
        [0.0059, 0.0269, 0.0169,  ..., 0.0231, 0.0231, 0.0242],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33195.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0732, 0.0000,  ..., 0.0347, 0.0000, 0.0193],
        [0.0000, 0.0868, 0.0000,  ..., 0.0405, 0.0000, 0.0225],
        [0.0000, 0.1103, 0.0000,  ..., 0.0502, 0.0000, 0.0289],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0086, 0.0000, 0.0013],
        [0.0000, 0.0270, 0.0000,  ..., 0.0161, 0.0000, 0.0059]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(177859.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-75.2523, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.2530, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-184.1363, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9987],
        [0.9950],
        ...,
        [0.9883],
        [0.9877],
        [0.9871]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364033.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9987],
        [0.9950],
        ...,
        [0.9882],
        [0.9876],
        [0.9870]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364021.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0017,  0.0084,  0.0052,  ...,  0.0071,  0.0072,  0.0076],
        [ 0.0015,  0.0080,  0.0048,  ...,  0.0067,  0.0068,  0.0072],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(487.8290, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6529, device='cuda:0')



h[100].sum tensor(19.0677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5717, device='cuda:0')



h[200].sum tensor(23.0445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.2924, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0094, 0.0437, 0.0278,  ..., 0.0381, 0.0376, 0.0398],
        [0.0046, 0.0224, 0.0137,  ..., 0.0188, 0.0191, 0.0200],
        [0.0015, 0.0092, 0.0048,  ..., 0.0067, 0.0077, 0.0077],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34226.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0909, 0.0000,  ..., 0.0429, 0.0000, 0.0219],
        [0.0000, 0.0654, 0.0000,  ..., 0.0324, 0.0000, 0.0151],
        [0.0000, 0.0345, 0.0000,  ..., 0.0197, 0.0000, 0.0068],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(181302.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.9235, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.9729, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-192.0602, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9987],
        [0.9950],
        ...,
        [0.9882],
        [0.9876],
        [0.9870]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364021.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9998],
        [0.9987],
        [0.9950],
        ...,
        [0.9881],
        [0.9875],
        [0.9870]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(364009.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(442.9613, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3864, device='cuda:0')



h[100].sum tensor(18.8274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1687, device='cuda:0')



h[200].sum tensor(22.5342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.8640, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32618.3301, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0127, 0.0000,  ..., 0.0107, 0.0000, 0.0009],
        [0.0000, 0.0110, 0.0000,  ..., 0.0100, 0.0000, 0.0007],
        [0.0000, 0.0122, 0.0000,  ..., 0.0104, 0.0000, 0.0008],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(173016.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-72.7929, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.6452, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-180.1595, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9998],
        [0.9987],
        [0.9950],
        ...,
        [0.9881],
        [0.9875],
        [0.9870]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(364009.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9987],
        [0.9949],
        ...,
        [0.9880],
        [0.9875],
        [0.9869]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363997.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(868.1918, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.3987, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.3301, device='cuda:0')



h[100].sum tensor(20.9856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.2874, device='cuda:0')



h[200].sum tensor(27.1181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.4620, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48178.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(251226.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-147.3246, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-32.8253, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-293.6607, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9987],
        [0.9949],
        ...,
        [0.9880],
        [0.9875],
        [0.9869]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363997.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9986],
        [0.9949],
        ...,
        [0.9880],
        [0.9874],
        [0.9868]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363985., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0019,  0.0094,  0.0059,  ...,  0.0081,  0.0081,  0.0085],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(412.7820, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4534, device='cuda:0')



h[100].sum tensor(18.6540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8718, device='cuda:0')



h[200].sum tensor(22.1658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.8118, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0041, 0.0205, 0.0123,  ..., 0.0169, 0.0175, 0.0182],
        [0.0019, 0.0107, 0.0059,  ..., 0.0081, 0.0090, 0.0090],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30681.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0680, 0.0000,  ..., 0.0332, 0.0000, 0.0164],
        [0.0000, 0.0357, 0.0000,  ..., 0.0200, 0.0000, 0.0078],
        [0.0000, 0.0104, 0.0000,  ..., 0.0095, 0.0000, 0.0018],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(161512.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-63.3871, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.2926, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-166.2950, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9986],
        [0.9949],
        ...,
        [0.9880],
        [0.9874],
        [0.9868]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363985., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9986],
        [0.9948],
        ...,
        [0.9879],
        [0.9873],
        [0.9867]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363972.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0040,  0.0169,  0.0111,  ...,  0.0152,  0.0145,  0.0155],
        [ 0.0034,  0.0147,  0.0096,  ...,  0.0131,  0.0126,  0.0134],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(498.0883, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.8766, device='cuda:0')



h[100].sum tensor(19.0769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6429, device='cuda:0')



h[200].sum tensor(23.0642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.5447, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0040, 0.0181, 0.0111,  ..., 0.0152, 0.0154, 0.0160],
        [0.0065, 0.0293, 0.0186,  ..., 0.0254, 0.0251, 0.0264],
        [0.0217, 0.0883, 0.0594,  ..., 0.0807, 0.0763, 0.0813],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32369.6934, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0537, 0.0000,  ..., 0.0267, 0.0000, 0.0140],
        [0.0000, 0.0980, 0.0000,  ..., 0.0443, 0.0000, 0.0274],
        [0.0000, 0.1816, 0.0000,  ..., 0.0770, 0.0000, 0.0537],
        ...,
        [0.0000, 0.0246, 0.0000,  ..., 0.0150, 0.0000, 0.0052],
        [0.0000, 0.0186, 0.0000,  ..., 0.0126, 0.0000, 0.0039],
        [0.0000, 0.0064, 0.0000,  ..., 0.0076, 0.0000, 0.0009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(165843.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-71.8420, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.6570, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-178.6366, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9986],
        [0.9948],
        ...,
        [0.9879],
        [0.9873],
        [0.9867]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363972.8125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 210.0 event: 1050 loss: tensor(614.3331, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9986],
        [0.9948],
        ...,
        [0.9878],
        [0.9872],
        [0.9866]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363960.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(261.8732, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.7689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.0348, device='cuda:0')



h[100].sum tensor(17.8724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.4659, device='cuda:0')



h[200].sum tensor(20.5058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.8285, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26453.8301, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0045, 0.0000,  ..., 0.0072, 0.0000, 0.0002],
        [0.0000, 0.0049, 0.0000,  ..., 0.0073, 0.0000, 0.0006],
        [0.0000, 0.0103, 0.0000,  ..., 0.0094, 0.0000, 0.0021],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(145503.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-43.0797, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-53.2338, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-135.5621, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9986],
        [0.9948],
        ...,
        [0.9878],
        [0.9872],
        [0.9866]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363960.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9986],
        [0.9948],
        ...,
        [0.9877],
        [0.9871],
        [0.9865]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363948.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0081,  0.0049,  ...,  0.0068,  0.0070,  0.0073],
        [ 0.0016,  0.0083,  0.0051,  ...,  0.0070,  0.0071,  0.0075],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(541.0769, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0460, device='cuda:0')



h[100].sum tensor(19.2724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0150, device='cuda:0')



h[200].sum tensor(23.4793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.8636, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0016, 0.0094, 0.0049,  ..., 0.0068, 0.0078, 0.0078],
        [0.0060, 0.0272, 0.0171,  ..., 0.0234, 0.0233, 0.0245],
        [0.0146, 0.0626, 0.0412,  ..., 0.0561, 0.0540, 0.0573],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35861.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0453, 0.0000,  ..., 0.0239, 0.0000, 0.0100],
        [0.0000, 0.0893, 0.0000,  ..., 0.0417, 0.0000, 0.0226],
        [0.0000, 0.1362, 0.0000,  ..., 0.0606, 0.0000, 0.0363],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(186590.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-88.6711, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.4588, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-203.9264, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9986],
        [0.9948],
        ...,
        [0.9877],
        [0.9871],
        [0.9865]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363948.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9986],
        [0.9947],
        ...,
        [0.9876],
        [0.9871],
        [0.9865]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363936.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(574.1169, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1136, device='cuda:0')



h[100].sum tensor(19.4273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3547, device='cuda:0')



h[200].sum tensor(23.8084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.0676, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37495.3164, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0018, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(202534.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-96.6214, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.8006, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-215.9875, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9986],
        [0.9947],
        ...,
        [0.9876],
        [0.9871],
        [0.9865]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363936.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9986],
        [0.9947],
        ...,
        [0.9876],
        [0.9870],
        [0.9864]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363923.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0037,  0.0157,  0.0103,  ...,  0.0140,  0.0135,  0.0143],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(494.1056, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5523, device='cuda:0')



h[100].sum tensor(19.0148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5397, device='cuda:0')



h[200].sum tensor(22.9323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.1789, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0037, 0.0169, 0.0103,  ..., 0.0140, 0.0144, 0.0148],
        [0.0062, 0.0260, 0.0168,  ..., 0.0228, 0.0223, 0.0233],
        [0.0296, 0.1174, 0.0800,  ..., 0.1085, 0.1014, 0.1083],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33157.7773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1240, 0.0000,  ..., 0.0546, 0.0000, 0.0354],
        [0.0000, 0.1871, 0.0000,  ..., 0.0790, 0.0000, 0.0557],
        [0.0000, 0.3005, 0.0000,  ..., 0.1232, 0.0000, 0.0915],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(173085.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-75.3999, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.1934, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-183.9785, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9986],
        [0.9947],
        ...,
        [0.9876],
        [0.9870],
        [0.9864]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363923.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9985],
        [0.9946],
        ...,
        [0.9875],
        [0.9869],
        [0.9863]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363911.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0194,  0.0732,  0.0511,  ...,  0.0691,  0.0634,  0.0679],
        [ 0.0074,  0.0293,  0.0200,  ...,  0.0271,  0.0253,  0.0270],
        [ 0.0036,  0.0156,  0.0102,  ...,  0.0139,  0.0134,  0.0143],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(559.9681, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1776, device='cuda:0')



h[100].sum tensor(19.3336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0569, device='cuda:0')



h[200].sum tensor(23.6094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.0120, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0315, 0.1243, 0.0849,  ..., 0.1151, 0.1074, 0.1147],
        [0.0407, 0.1578, 0.1086,  ..., 0.1471, 0.1365, 0.1459],
        [0.0227, 0.0922, 0.0621,  ..., 0.0844, 0.0796, 0.0848],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36944.2305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 4.0084e-01, 0.0000e+00,  ..., 1.6158e-01, 0.0000e+00,
         1.2383e-01],
        [0.0000e+00, 4.0739e-01, 0.0000e+00,  ..., 1.6413e-01, 0.0000e+00,
         1.2554e-01],
        [0.0000e+00, 3.0710e-01, 0.0000e+00,  ..., 1.2634e-01, 0.0000e+00,
         9.1938e-02],
        ...,
        [0.0000e+00, 3.4048e-04, 0.0000e+00,  ..., 5.1796e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.4072e-04, 0.0000e+00,  ..., 5.1791e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.4073e-04, 0.0000e+00,  ..., 5.1789e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(196050.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-93.7440, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.8652, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-211.1426, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9985],
        [0.9946],
        ...,
        [0.9875],
        [0.9869],
        [0.9863]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363911.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9985],
        [0.9946],
        ...,
        [0.9874],
        [0.9868],
        [0.9862]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363899.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0107,  0.0414,  0.0285,  ...,  0.0386,  0.0358,  0.0382],
        [ 0.0075,  0.0298,  0.0203,  ...,  0.0276,  0.0258,  0.0275],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0037,  0.0157,  0.0103,  ...,  0.0140,  0.0135,  0.0144]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(933.9852, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.9886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.0397, device='cuda:0')



h[100].sum tensor(21.1859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.5132, device='cuda:0')



h[200].sum tensor(27.5437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.2623, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0404, 0.1566, 0.1078,  ..., 0.1460, 0.1354, 0.1447],
        [0.0190, 0.0766, 0.0516,  ..., 0.0700, 0.0661, 0.0704],
        [0.0101, 0.0423, 0.0278,  ..., 0.0378, 0.0364, 0.0385],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0036, 0.0166, 0.0101,  ..., 0.0138, 0.0141, 0.0146],
        [0.0064, 0.0286, 0.0181,  ..., 0.0248, 0.0245, 0.0258]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50993.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.4064, 0.0000,  ..., 0.1630, 0.0000, 0.1266],
        [0.0000, 0.2495, 0.0000,  ..., 0.1028, 0.0000, 0.0759],
        [0.0000, 0.1313, 0.0000,  ..., 0.0570, 0.0000, 0.0384],
        ...,
        [0.0000, 0.0187, 0.0000,  ..., 0.0126, 0.0000, 0.0039],
        [0.0000, 0.0494, 0.0000,  ..., 0.0248, 0.0000, 0.0128],
        [0.0000, 0.0898, 0.0000,  ..., 0.0409, 0.0000, 0.0249]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(273243.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-160.9159, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-30.9480, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-313.0407, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9985],
        [0.9946],
        ...,
        [0.9874],
        [0.9868],
        [0.9862]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363899.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9985],
        [0.9945],
        ...,
        [0.9873],
        [0.9867],
        [0.9861]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363887.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(771.6108, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.6728, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.2698, device='cuda:0')



h[100].sum tensor(20.3629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9954, device='cuda:0')



h[200].sum tensor(25.7955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.8828, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40694.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0206, 0.0000,  ..., 0.0139, 0.0000, 0.0034],
        [0.0000, 0.0207, 0.0000,  ..., 0.0139, 0.0000, 0.0034],
        [0.0000, 0.0203, 0.0000,  ..., 0.0137, 0.0000, 0.0033],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(201182.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-111.6850, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.8292, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-239.2271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9985],
        [0.9945],
        ...,
        [0.9873],
        [0.9867],
        [0.9861]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363887.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9997],
        [0.9985],
        [0.9945],
        ...,
        [0.9872],
        [0.9866],
        [0.9860]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363874.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0050,  0.0207,  0.0138,  ...,  0.0188,  0.0178,  0.0190],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(881.6664, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.5857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.3815, device='cuda:0')



h[100].sum tensor(20.8941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6673, device='cuda:0')



h[200].sum tensor(26.9239, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.2643, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0181, 0.0752, 0.0501,  ..., 0.0682, 0.0649, 0.0691],
        [0.0040, 0.0182, 0.0112,  ..., 0.0153, 0.0155, 0.0160],
        [0.0050, 0.0219, 0.0138,  ..., 0.0188, 0.0187, 0.0195],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48405.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1050, 0.0000,  ..., 0.0472, 0.0000, 0.0296],
        [0.0000, 0.0614, 0.0000,  ..., 0.0298, 0.0000, 0.0165],
        [0.0000, 0.0479, 0.0000,  ..., 0.0244, 0.0000, 0.0125],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(254467.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-147.6669, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-33.0215, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-294.6819, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9997],
        [0.9985],
        [0.9945],
        ...,
        [0.9872],
        [0.9866],
        [0.9860]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363874.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9985],
        [0.9945],
        ...,
        [0.9872],
        [0.9866],
        [0.9859]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363862.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0019,  0.0091,  0.0057,  ...,  0.0078,  0.0078,  0.0083],
        [ 0.0042,  0.0176,  0.0117,  ...,  0.0159,  0.0152,  0.0162],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(368.3197, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.8066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6949, device='cuda:0')



h[100].sum tensor(18.3427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.3123, device='cuda:0')



h[200].sum tensor(21.5046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.8285, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0105, 0.0456, 0.0296,  ..., 0.0404, 0.0392, 0.0415],
        [0.0052, 0.0244, 0.0151,  ..., 0.0207, 0.0209, 0.0218],
        [0.0150, 0.0641, 0.0422,  ..., 0.0575, 0.0552, 0.0587],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29016.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1644, 0.0000,  ..., 0.0712, 0.0000, 0.0463],
        [0.0000, 0.1099, 0.0000,  ..., 0.0496, 0.0000, 0.0298],
        [0.0000, 0.1050, 0.0000,  ..., 0.0476, 0.0000, 0.0286],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(155565.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-55.8480, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.6917, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-154.4150, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9985],
        [0.9945],
        ...,
        [0.9872],
        [0.9866],
        [0.9859]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363862.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9985],
        [0.9944],
        ...,
        [0.9871],
        [0.9865],
        [0.9859]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363850.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(653.5721, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.9479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.2651, device='cuda:0')



h[100].sum tensor(19.7397, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7211, device='cuda:0')



h[200].sum tensor(24.4720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.3663, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40658.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(218726.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-110.6258, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.2318, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-236.9420, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9985],
        [0.9944],
        ...,
        [0.9871],
        [0.9865],
        [0.9859]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363850.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 220.0 event: 1100 loss: tensor(630.7459, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9984],
        [0.9944],
        ...,
        [0.9870],
        [0.9864],
        [0.9858]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363837.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0040,  0.0169,  0.0112,  ...,  0.0152,  0.0146,  0.0155],
        [ 0.0041,  0.0172,  0.0114,  ...,  0.0156,  0.0149,  0.0158],
        [ 0.0056,  0.0228,  0.0153,  ...,  0.0208,  0.0196,  0.0209],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(564.9244, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1853, device='cuda:0')



h[100].sum tensor(19.2922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0593, device='cuda:0')



h[200].sum tensor(23.5213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.0207, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0184, 0.0763, 0.0508,  ..., 0.0692, 0.0658, 0.0701],
        [0.0173, 0.0725, 0.0481,  ..., 0.0655, 0.0625, 0.0665],
        [0.0192, 0.0772, 0.0520,  ..., 0.0706, 0.0666, 0.0709],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34785.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1741, 0.0000,  ..., 0.0746, 0.0000, 0.0504],
        [0.0000, 0.2047, 0.0000,  ..., 0.0866, 0.0000, 0.0599],
        [0.0000, 0.2181, 0.0000,  ..., 0.0914, 0.0000, 0.0648],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(179583.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-84.0245, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.2335, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-196.4240, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9984],
        [0.9944],
        ...,
        [0.9870],
        [0.9864],
        [0.9858]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363837.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9984],
        [0.9943],
        ...,
        [0.9869],
        [0.9863],
        [0.9857]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363825.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0069,  0.0275,  0.0187,  ...,  0.0254,  0.0238,  0.0254],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [ 0.0026,  0.0119,  0.0077,  ...,  0.0105,  0.0103,  0.0109],
        [ 0.0065,  0.0259,  0.0175,  ...,  0.0238,  0.0224,  0.0238],
        [ 0.0044,  0.0185,  0.0123,  ...,  0.0167,  0.0159,  0.0170]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(655.3328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.9795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.4124, device='cuda:0')



h[100].sum tensor(19.7243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7680, device='cuda:0')



h[200].sum tensor(24.4392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.5324, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0259, 0.1020, 0.0695,  ..., 0.0943, 0.0881, 0.0939],
        [0.0162, 0.0665, 0.0444,  ..., 0.0603, 0.0573, 0.0609],
        [0.0070, 0.0308, 0.0197,  ..., 0.0268, 0.0264, 0.0278],
        ...,
        [0.0115, 0.0473, 0.0314,  ..., 0.0426, 0.0407, 0.0431],
        [0.0200, 0.0820, 0.0549,  ..., 0.0747, 0.0707, 0.0753],
        [0.0238, 0.0961, 0.0649,  ..., 0.0882, 0.0830, 0.0885]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37503.9414, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3607, 0.0000,  ..., 0.1460, 0.0000, 0.1114],
        [0.0000, 0.2754, 0.0000,  ..., 0.1136, 0.0000, 0.0834],
        [0.0000, 0.2227, 0.0000,  ..., 0.0932, 0.0000, 0.0664],
        ...,
        [0.0000, 0.1294, 0.0000,  ..., 0.0567, 0.0000, 0.0367],
        [0.0000, 0.1819, 0.0000,  ..., 0.0775, 0.0000, 0.0525],
        [0.0000, 0.1862, 0.0000,  ..., 0.0790, 0.0000, 0.0544]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(191326.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-96.6949, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.8361, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-216.0242, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9984],
        [0.9943],
        ...,
        [0.9869],
        [0.9863],
        [0.9857]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363825.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9984],
        [0.9943],
        ...,
        [0.9868],
        [0.9862],
        [0.9856]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363813.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0094,  0.0366,  0.0252,  ...,  0.0341,  0.0317,  0.0338],
        [ 0.0102,  0.0395,  0.0272,  ...,  0.0369,  0.0342,  0.0365],
        [ 0.0093,  0.0364,  0.0250,  ...,  0.0338,  0.0314,  0.0336],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(507.2116, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3827, device='cuda:0')



h[100].sum tensor(18.9887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4857, device='cuda:0')



h[200].sum tensor(22.8768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.9876, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0330, 0.1296, 0.0886,  ..., 0.1202, 0.1120, 0.1197],
        [0.0339, 0.1329, 0.0910,  ..., 0.1233, 0.1149, 0.1227],
        [0.0310, 0.1225, 0.0836,  ..., 0.1133, 0.1058, 0.1130],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33208.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3199, 0.0000,  ..., 0.1302, 0.0000, 0.0984],
        [0.0000, 0.3067, 0.0000,  ..., 0.1250, 0.0000, 0.0945],
        [0.0000, 0.2696, 0.0000,  ..., 0.1107, 0.0000, 0.0825],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(174230.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-75.7481, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.2314, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-184.1319, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9984],
        [0.9943],
        ...,
        [0.9868],
        [0.9862],
        [0.9856]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363813.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9984],
        [0.9942],
        ...,
        [0.9868],
        [0.9862],
        [0.9855]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363800.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0020,  0.0098,  0.0061,  ...,  0.0084,  0.0084,  0.0089],
        [ 0.0035,  0.0150,  0.0098,  ...,  0.0134,  0.0130,  0.0138],
        [ 0.0080,  0.0314,  0.0215,  ...,  0.0291,  0.0272,  0.0290],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(590.4137, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6291, device='cuda:0')



h[100].sum tensor(19.3842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2005, device='cuda:0')



h[200].sum tensor(23.7169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.5211, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0077, 0.0353, 0.0223,  ..., 0.0306, 0.0303, 0.0320],
        [0.0190, 0.0787, 0.0525,  ..., 0.0714, 0.0679, 0.0723],
        [0.0246, 0.0990, 0.0669,  ..., 0.0909, 0.0855, 0.0912],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36331.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1322, 0.0000,  ..., 0.0588, 0.0000, 0.0351],
        [0.0000, 0.2144, 0.0000,  ..., 0.0909, 0.0000, 0.0614],
        [0.0000, 0.2709, 0.0000,  ..., 0.1127, 0.0000, 0.0799],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(189349.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-90.5733, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.3716, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-206.8614, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9984],
        [0.9942],
        ...,
        [0.9868],
        [0.9862],
        [0.9855]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363800.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9984],
        [0.9942],
        ...,
        [0.9867],
        [0.9861],
        [0.9854]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363788.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0033,  0.0145,  0.0095,  ...,  0.0129,  0.0125,  0.0133],
        [ 0.0067,  0.0267,  0.0181,  ...,  0.0246,  0.0230,  0.0246],
        [ 0.0013,  0.0070,  0.0041,  ...,  0.0057,  0.0060,  0.0063],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(515.5475, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3681, device='cuda:0')



h[100].sum tensor(19.0096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4811, device='cuda:0')



h[200].sum tensor(22.9211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.9711, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0273, 0.1088, 0.0739,  ..., 0.1003, 0.0940, 0.1003],
        [0.0139, 0.0600, 0.0393,  ..., 0.0536, 0.0517, 0.0549],
        [0.0129, 0.0563, 0.0367,  ..., 0.0501, 0.0485, 0.0515],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33685.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2293, 0.0000,  ..., 0.0967, 0.0000, 0.0666],
        [0.0000, 0.1922, 0.0000,  ..., 0.0827, 0.0000, 0.0534],
        [0.0000, 0.1617, 0.0000,  ..., 0.0710, 0.0000, 0.0431],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(177963.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-78.0647, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.4754, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-188.0903, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9984],
        [0.9942],
        ...,
        [0.9867],
        [0.9861],
        [0.9854]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363788.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9983],
        [0.9942],
        ...,
        [0.9866],
        [0.9860],
        [0.9854]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363775.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0090,  0.0351,  0.0241,  ...,  0.0326,  0.0303,  0.0324],
        [ 0.0093,  0.0364,  0.0250,  ...,  0.0338,  0.0314,  0.0336],
        [ 0.0039,  0.0166,  0.0109,  ...,  0.0149,  0.0143,  0.0152],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(577.3170, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.2520, device='cuda:0')



h[100].sum tensor(19.2990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0806, device='cuda:0')



h[200].sum tensor(23.5359, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.0959, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0271, 0.1080, 0.0733,  ..., 0.0995, 0.0933, 0.0995],
        [0.0271, 0.1082, 0.0735,  ..., 0.0997, 0.0935, 0.0997],
        [0.0257, 0.1032, 0.0699,  ..., 0.0949, 0.0891, 0.0951],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35907.5977, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2357, 0.0000,  ..., 0.0981, 0.0000, 0.0708],
        [0.0000, 0.2367, 0.0000,  ..., 0.0985, 0.0000, 0.0710],
        [0.0000, 0.2060, 0.0000,  ..., 0.0865, 0.0000, 0.0610],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(189691.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-88.1393, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.8526, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-203.5934, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9983],
        [0.9942],
        ...,
        [0.9866],
        [0.9860],
        [0.9854]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363775.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9983],
        [0.9941],
        ...,
        [0.9865],
        [0.9859],
        [0.9853]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363763.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0017,  0.0084,  0.0051,  ...,  0.0071,  0.0072,  0.0076],
        [ 0.0015,  0.0078,  0.0048,  ...,  0.0066,  0.0067,  0.0071],
        [ 0.0037,  0.0158,  0.0104,  ...,  0.0142,  0.0136,  0.0145],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(353.2592, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0070, device='cuda:0')



h[100].sum tensor(18.2057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.0934, device='cuda:0')



h[200].sum tensor(21.2138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.0528, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0270, 0.0170,  ..., 0.0232, 0.0232, 0.0243],
        [0.0125, 0.0548, 0.0356,  ..., 0.0487, 0.0472, 0.0501],
        [0.0054, 0.0270, 0.0164,  ..., 0.0226, 0.0231, 0.0242],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28389.5273, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0841, 0.0000,  ..., 0.0397, 0.0000, 0.0209],
        [0.0000, 0.1171, 0.0000,  ..., 0.0531, 0.0000, 0.0303],
        [0.0000, 0.0913, 0.0000,  ..., 0.0428, 0.0000, 0.0227],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(152217.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.2267, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-51.0596, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-150.1167, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9983],
        [0.9941],
        ...,
        [0.9865],
        [0.9859],
        [0.9853]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363763.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9996],
        [0.9983],
        [0.9941],
        ...,
        [0.9864],
        [0.9858],
        [0.9852]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363751.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(688.4529, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.0580, device='cuda:0')



h[100].sum tensor(19.8135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9734, device='cuda:0')



h[200].sum tensor(24.6286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.2605, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39264.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0054, 0.0000,  ..., 0.0076, 0.0000, 0.0003],
        [0.0000, 0.0069, 0.0000,  ..., 0.0082, 0.0000, 0.0006],
        [0.0000, 0.0081, 0.0000,  ..., 0.0087, 0.0000, 0.0009],
        ...,
        [0.0000, 0.0102, 0.0000,  ..., 0.0092, 0.0000, 0.0019],
        [0.0000, 0.0026, 0.0000,  ..., 0.0061, 0.0000, 0.0002],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(204439.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-104.5471, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.6072, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-228.1418, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9996],
        [0.9983],
        [0.9941],
        ...,
        [0.9864],
        [0.9858],
        [0.9852]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363751.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9983],
        [0.9940],
        ...,
        [0.9864],
        [0.9857],
        [0.9851]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363738.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0039,  0.0164,  0.0108,  ...,  0.0147,  0.0141,  0.0150],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(434.1518, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2626, device='cuda:0')



h[100].sum tensor(18.5794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8111, device='cuda:0')



h[200].sum tensor(22.0075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.5967, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0058, 0.0265, 0.0166,  ..., 0.0226, 0.0226, 0.0237],
        [0.0039, 0.0176, 0.0108,  ..., 0.0147, 0.0150, 0.0155],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31880.6230, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1285, 0.0000,  ..., 0.0562, 0.0000, 0.0370],
        [0.0000, 0.0912, 0.0000,  ..., 0.0412, 0.0000, 0.0261],
        [0.0000, 0.0602, 0.0000,  ..., 0.0289, 0.0000, 0.0168],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(170995.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-70.0138, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.8215, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-175.4812, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9983],
        [0.9940],
        ...,
        [0.9864],
        [0.9857],
        [0.9851]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363738.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9983],
        [0.9940],
        ...,
        [0.9863],
        [0.9857],
        [0.9850]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363726.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0020,  0.0097,  0.0060,  ...,  0.0083,  0.0083,  0.0088],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(590.9295, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3576, device='cuda:0')



h[100].sum tensor(19.3223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.1141, device='cuda:0')



h[200].sum tensor(23.5854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.2149, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0030, 0.0165, 0.0095,  ..., 0.0131, 0.0140, 0.0144],
        [0.0020, 0.0109, 0.0060,  ..., 0.0083, 0.0092, 0.0093],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38883.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0466, 0.0000,  ..., 0.0247, 0.0000, 0.0101],
        [0.0000, 0.0303, 0.0000,  ..., 0.0178, 0.0000, 0.0061],
        [0.0000, 0.0153, 0.0000,  ..., 0.0116, 0.0000, 0.0020],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(218073.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-102.8444, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.9444, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-225.3933, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9983],
        [0.9940],
        ...,
        [0.9863],
        [0.9857],
        [0.9850]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363726.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 230.0 event: 1150 loss: tensor(602.6853, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9983],
        [0.9939],
        ...,
        [0.9862],
        [0.9856],
        [0.9849]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363713.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0036,  0.0156,  0.0103,  ...,  0.0140,  0.0135,  0.0143],
        [ 0.0067,  0.0268,  0.0182,  ...,  0.0247,  0.0232,  0.0247],
        [ 0.0018,  0.0089,  0.0055,  ...,  0.0076,  0.0077,  0.0081],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(593.2668, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1615, device='cuda:0')



h[100].sum tensor(19.3230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0517, device='cuda:0')



h[200].sum tensor(23.5869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.9938, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0308, 0.1217, 0.0830,  ..., 0.1126, 0.1052, 0.1123],
        [0.0207, 0.0847, 0.0568,  ..., 0.0772, 0.0731, 0.0779],
        [0.0113, 0.0465, 0.0308,  ..., 0.0418, 0.0400, 0.0424],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37821.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2791, 0.0000,  ..., 0.1153, 0.0000, 0.0838],
        [0.0000, 0.2326, 0.0000,  ..., 0.0973, 0.0000, 0.0686],
        [0.0000, 0.1545, 0.0000,  ..., 0.0666, 0.0000, 0.0445],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(206915.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-97.3754, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.4525, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-216.9730, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9983],
        [0.9939],
        ...,
        [0.9862],
        [0.9856],
        [0.9849]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363713.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9982],
        [0.9939],
        ...,
        [0.9861],
        [0.9855],
        [0.9848]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363701.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0034,  0.0148,  0.0097,  ...,  0.0132,  0.0128,  0.0136],
        [ 0.0072,  0.0285,  0.0194,  ...,  0.0263,  0.0246,  0.0263],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(713.1014, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.4829, device='cuda:0')



h[100].sum tensor(19.8839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.1086, device='cuda:0')



h[200].sum tensor(24.7781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.7397, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0061, 0.0278, 0.0175,  ..., 0.0239, 0.0238, 0.0250],
        [0.0124, 0.0526, 0.0346,  ..., 0.0471, 0.0453, 0.0480],
        [0.0188, 0.0779, 0.0520,  ..., 0.0708, 0.0672, 0.0716],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41031.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0890, 0.0000,  ..., 0.0410, 0.0000, 0.0241],
        [0.0000, 0.1614, 0.0000,  ..., 0.0695, 0.0000, 0.0465],
        [0.0000, 0.2281, 0.0000,  ..., 0.0957, 0.0000, 0.0673],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0086, 0.0000, 0.0013],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(218152.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-113.1668, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.1330, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-240.7067, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9982],
        [0.9939],
        ...,
        [0.9861],
        [0.9855],
        [0.9848]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363701.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9982],
        [0.9939],
        ...,
        [0.9860],
        [0.9854],
        [0.9848]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363688.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0017,  0.0086,  0.0053,  ...,  0.0073,  0.0074,  0.0078],
        [ 0.0053,  0.0216,  0.0145,  ...,  0.0197,  0.0186,  0.0198],
        [ 0.0066,  0.0263,  0.0178,  ...,  0.0242,  0.0227,  0.0242],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(784.6661, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.9810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.2653, device='cuda:0')



h[100].sum tensor(20.2122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.6758, device='cuda:0')



h[200].sum tensor(25.4756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.7499, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0102, 0.0446, 0.0289,  ..., 0.0395, 0.0384, 0.0406],
        [0.0196, 0.0808, 0.0540,  ..., 0.0735, 0.0697, 0.0742],
        [0.0295, 0.1170, 0.0797,  ..., 0.1081, 0.1011, 0.1079],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42563.0508, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1771, 0.0000,  ..., 0.0758, 0.0000, 0.0510],
        [0.0000, 0.2213, 0.0000,  ..., 0.0930, 0.0000, 0.0649],
        [0.0000, 0.2529, 0.0000,  ..., 0.1052, 0.0000, 0.0752],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(220001.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-120.9324, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.4403, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-252.2327, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9982],
        [0.9939],
        ...,
        [0.9860],
        [0.9854],
        [0.9848]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363688.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9982],
        [0.9938],
        ...,
        [0.9860],
        [0.9853],
        [0.9847]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363676.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(429.2057, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8334, device='cuda:0')



h[100].sum tensor(18.5138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.6745, device='cuda:0')



h[200].sum tensor(21.8682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.1125, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30656.6934, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(165449., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-63.1918, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.5101, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-165.7772, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9982],
        [0.9938],
        ...,
        [0.9860],
        [0.9853],
        [0.9847]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363676.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9982],
        [0.9938],
        ...,
        [0.9859],
        [0.9852],
        [0.9846]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363663.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(412.5833, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1002, device='cuda:0')



h[100].sum tensor(18.4270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4413, device='cuda:0')



h[200].sum tensor(21.6839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.2857, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29693.6602, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(158054.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-59.2105, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.0881, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-159.2441, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9982],
        [0.9938],
        ...,
        [0.9859],
        [0.9852],
        [0.9846]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363663.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9982],
        [0.9937],
        ...,
        [0.9858],
        [0.9852],
        [0.9845]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363651.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0019,  0.0091,  0.0057,  ...,  0.0078,  0.0078,  0.0083],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(532.8188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3056, device='cuda:0')



h[100].sum tensor(18.9863, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4612, device='cuda:0')



h[200].sum tensor(22.8717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.9008, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0025, 0.0146, 0.0081,  ..., 0.0113, 0.0124, 0.0127],
        [0.0019, 0.0104, 0.0057,  ..., 0.0078, 0.0087, 0.0088],
        [0.0014, 0.0088, 0.0045,  ..., 0.0063, 0.0073, 0.0073],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33462.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0520, 0.0000,  ..., 0.0271, 0.0000, 0.0108],
        [0.0000, 0.0369, 0.0000,  ..., 0.0208, 0.0000, 0.0072],
        [0.0000, 0.0311, 0.0000,  ..., 0.0184, 0.0000, 0.0058],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(175746.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-76.8681, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.7417, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.4009, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9982],
        [0.9937],
        ...,
        [0.9858],
        [0.9852],
        [0.9845]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363651.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9981],
        [0.9937],
        ...,
        [0.9857],
        [0.9851],
        [0.9844]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363638.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0013,  0.0071,  0.0042,  ...,  0.0058,  0.0061,  0.0064],
        [ 0.0013,  0.0071,  0.0042,  ...,  0.0058,  0.0061,  0.0064],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(769.0114, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.9602, device='cuda:0')



h[100].sum tensor(20.0882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5787, device='cuda:0')



h[200].sum tensor(25.2120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.4058, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0051, 0.0258, 0.0156,  ..., 0.0215, 0.0221, 0.0231],
        [0.0034, 0.0198, 0.0113,  ..., 0.0157, 0.0169, 0.0175],
        [0.0023, 0.0137, 0.0076,  ..., 0.0105, 0.0116, 0.0119],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43575.3086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0767, 0.0000,  ..., 0.0373, 0.0000, 0.0175],
        [0.0000, 0.0675, 0.0000,  ..., 0.0336, 0.0000, 0.0147],
        [0.0000, 0.0486, 0.0000,  ..., 0.0256, 0.0000, 0.0100],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(236961.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-125.5622, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.0123, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-260.2826, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9981],
        [0.9937],
        ...,
        [0.9857],
        [0.9851],
        [0.9844]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363638.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9981],
        [0.9937],
        ...,
        [0.9857],
        [0.9851],
        [0.9844]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363638.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(399.3074, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6116, device='cuda:0')



h[100].sum tensor(18.3488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.2858, device='cuda:0')



h[200].sum tensor(21.5176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.7347, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30659.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0018, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(167338.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-63.2796, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.4357, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-165.8690, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9981],
        [0.9937],
        ...,
        [0.9857],
        [0.9851],
        [0.9844]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363638.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9981],
        [0.9936],
        ...,
        [0.9856],
        [0.9850],
        [0.9843]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363626.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(501.3941, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4098, device='cuda:0')



h[100].sum tensor(18.8201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1762, device='cuda:0')



h[200].sum tensor(22.5188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.8905, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33129.8711, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0014, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0070, 0.0000, 0.0000],
        [0.0000, 0.0111, 0.0000,  ..., 0.0100, 0.0000, 0.0012],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(178339.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-74.8167, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.2672, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-183.6484, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9981],
        [0.9936],
        ...,
        [0.9856],
        [0.9850],
        [0.9843]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363626.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9995],
        [0.9981],
        [0.9936],
        ...,
        [0.9856],
        [0.9849],
        [0.9842]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363613.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1236.2888, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.8095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.2259, device='cuda:0')



h[100].sum tensor(22.2509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.1634, device='cuda:0')



h[200].sum tensor(29.8055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.1113, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(60468.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0086, 0.0000, 0.0014],
        [0.0000, 0.0018, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0076, 0.0000, 0.0005],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(324201.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-205.8741, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-21.6159, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-382.4517, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9995],
        [0.9981],
        [0.9936],
        ...,
        [0.9856],
        [0.9849],
        [0.9842]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363613.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 240.0 event: 1200 loss: tensor(582.2134, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9981],
        [0.9935],
        ...,
        [0.9855],
        [0.9848],
        [0.9842]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363600.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0025,  0.0115,  0.0073,  ...,  0.0101,  0.0099,  0.0105],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(763.5759, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3613, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0121, device='cuda:0')



h[100].sum tensor(20.0264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2770, device='cuda:0')



h[200].sum tensor(25.0808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.3365, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0066, 0.0297, 0.0188,  ..., 0.0257, 0.0254, 0.0267],
        [0.0025, 0.0127, 0.0073,  ..., 0.0100, 0.0107, 0.0110],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42790.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1103, 0.0000,  ..., 0.0494, 0.0000, 0.0306],
        [0.0000, 0.0494, 0.0000,  ..., 0.0252, 0.0000, 0.0124],
        [0.0000, 0.0139, 0.0000,  ..., 0.0109, 0.0000, 0.0030],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(228883.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-122.2725, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.5351, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-254.8452, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9981],
        [0.9935],
        ...,
        [0.9855],
        [0.9848],
        [0.9842]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363600.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9981],
        [0.9935],
        ...,
        [0.9854],
        [0.9847],
        [0.9841]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363588.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0022,  0.0103,  0.0065,  ...,  0.0089,  0.0089,  0.0094],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(972.1873, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.3981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.8500, device='cuda:0')



h[100].sum tensor(20.9858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.1346, device='cuda:0')



h[200].sum tensor(27.1186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.9205, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0022, 0.0115, 0.0065,  ..., 0.0089, 0.0097, 0.0098],
        [0.0017, 0.0097, 0.0052,  ..., 0.0072, 0.0081, 0.0081],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48046.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0085, 0.0000, 0.0011],
        [0.0000, 0.0246, 0.0000,  ..., 0.0155, 0.0000, 0.0045],
        [0.0000, 0.0372, 0.0000,  ..., 0.0208, 0.0000, 0.0073],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(255800.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-147.1349, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-33.5055, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-291.8484, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9981],
        [0.9935],
        ...,
        [0.9854],
        [0.9847],
        [0.9841]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363588.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9980],
        [0.9935],
        ...,
        [0.9853],
        [0.9847],
        [0.9840]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363575.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0033,  0.0144,  0.0094,  ...,  0.0128,  0.0124,  0.0131],
        [ 0.0062,  0.0251,  0.0170,  ...,  0.0231,  0.0217,  0.0231],
        [ 0.0117,  0.0450,  0.0311,  ...,  0.0421,  0.0389,  0.0416],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(594.9170, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.7366, device='cuda:0')



h[100].sum tensor(19.2194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9165, device='cuda:0')



h[200].sum tensor(23.3667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.5146, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0114, 0.0471, 0.0312,  ..., 0.0424, 0.0406, 0.0430],
        [0.0271, 0.1083, 0.0735,  ..., 0.0998, 0.0935, 0.0998],
        [0.0338, 0.1325, 0.0907,  ..., 0.1229, 0.1145, 0.1223],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35719.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.6012e-01, 0.0000e+00,  ..., 6.8894e-02, 0.0000e+00,
         4.5747e-02],
        [0.0000e+00, 2.6962e-01, 0.0000e+00,  ..., 1.1146e-01, 0.0000e+00,
         8.0505e-02],
        [0.0000e+00, 3.4971e-01, 0.0000e+00,  ..., 1.4228e-01, 0.0000e+00,
         1.0664e-01],
        ...,
        [0.0000e+00, 1.3307e-02, 0.0000e+00,  ..., 1.0425e-02, 0.0000e+00,
         2.7957e-03],
        [0.0000e+00, 3.4312e-04, 0.0000e+00,  ..., 5.1551e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.4313e-04, 0.0000e+00,  ..., 5.1549e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(188632.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-88.1424, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.6989, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-202.6979, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9980],
        [0.9935],
        ...,
        [0.9853],
        [0.9847],
        [0.9840]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363575.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9980],
        [0.9934],
        ...,
        [0.9852],
        [0.9846],
        [0.9839]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363563.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0029,  0.0128,  0.0082,  ...,  0.0113,  0.0110,  0.0116],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0029,  0.0128,  0.0082,  ...,  0.0113,  0.0110,  0.0116],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(718.0957, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.0852, device='cuda:0')



h[100].sum tensor(19.7802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9821, device='cuda:0')



h[200].sum tensor(24.5579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.2912, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0022, 0.0117, 0.0066,  ..., 0.0091, 0.0099, 0.0100],
        [0.0102, 0.0465, 0.0297,  ..., 0.0407, 0.0400, 0.0423],
        [0.0022, 0.0117, 0.0066,  ..., 0.0091, 0.0099, 0.0100],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40828.3398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0386, 0.0000,  ..., 0.0211, 0.0000, 0.0086],
        [0.0000, 0.0728, 0.0000,  ..., 0.0352, 0.0000, 0.0177],
        [0.0000, 0.0615, 0.0000,  ..., 0.0307, 0.0000, 0.0141],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(221871.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-111.6653, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.0672, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-239.6306, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9980],
        [0.9934],
        ...,
        [0.9852],
        [0.9846],
        [0.9839]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363563.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9980],
        [0.9934],
        ...,
        [0.9851],
        [0.9845],
        [0.9838]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363550.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0080,  0.0049,  ...,  0.0067,  0.0069,  0.0072],
        [ 0.0015,  0.0076,  0.0046,  ...,  0.0064,  0.0065,  0.0069],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(495.8781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9889, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9292, device='cuda:0')



h[100].sum tensor(18.7423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0232, device='cuda:0')



h[200].sum tensor(22.3534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.3484, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0016, 0.0093, 0.0049,  ..., 0.0067, 0.0077, 0.0077],
        [0.0057, 0.0264, 0.0165,  ..., 0.0226, 0.0226, 0.0237],
        [0.0120, 0.0531, 0.0344,  ..., 0.0471, 0.0458, 0.0485],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32703.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0432, 0.0000,  ..., 0.0232, 0.0000, 0.0093],
        [0.0000, 0.0810, 0.0000,  ..., 0.0386, 0.0000, 0.0199],
        [0.0000, 0.1132, 0.0000,  ..., 0.0517, 0.0000, 0.0289],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(175769.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-73.2294, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.5646, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-180.6589, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9980],
        [0.9934],
        ...,
        [0.9851],
        [0.9845],
        [0.9838]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363550.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9980],
        [0.9933],
        ...,
        [0.9851],
        [0.9844],
        [0.9837]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363537.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(754.6602, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.8021, device='cuda:0')



h[100].sum tensor(19.9265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2102, device='cuda:0')



h[200].sum tensor(24.8686, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.0997, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38802.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(197512.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-102.3737, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.8348, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-225.0996, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9980],
        [0.9933],
        ...,
        [0.9851],
        [0.9844],
        [0.9837]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363537.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9980],
        [0.9933],
        ...,
        [0.9850],
        [0.9843],
        [0.9836]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363525.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(971.2438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.5518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.0973, device='cuda:0')



h[100].sum tensor(20.9107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.8951, device='cuda:0')



h[200].sum tensor(26.9591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.0717, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48955.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0193, 0.0000,  ..., 0.0132, 0.0000, 0.0038],
        [0.0000, 0.0157, 0.0000,  ..., 0.0117, 0.0000, 0.0024],
        [0.0000, 0.0515, 0.0000,  ..., 0.0256, 0.0000, 0.0142],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(266335.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-150.7913, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-32.7188, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-298.3664, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9980],
        [0.9933],
        ...,
        [0.9850],
        [0.9843],
        [0.9836]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363525.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9979],
        [0.9932],
        ...,
        [0.9849],
        [0.9842],
        [0.9836]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363512.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0045,  0.0188,  0.0125,  ...,  0.0170,  0.0162,  0.0172],
        [ 0.0068,  0.0273,  0.0185,  ...,  0.0251,  0.0236,  0.0251],
        [ 0.0045,  0.0188,  0.0125,  ...,  0.0170,  0.0162,  0.0172],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(921.3027, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.0488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.9327, device='cuda:0')



h[100].sum tensor(20.6678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.5245, device='cuda:0')



h[200].sum tensor(26.4432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.7582, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0248, 0.0999, 0.0676,  ..., 0.0918, 0.0863, 0.0920],
        [0.0267, 0.1067, 0.0724,  ..., 0.0982, 0.0921, 0.0983],
        [0.0470, 0.1808, 0.1249,  ..., 0.1691, 0.1564, 0.1672],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45632.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.9090e-01, 0.0000e+00,  ..., 1.1986e-01, 0.0000e+00,
         8.7619e-02],
        [0.0000e+00, 3.3945e-01, 0.0000e+00,  ..., 1.3831e-01, 0.0000e+00,
         1.0364e-01],
        [0.0000e+00, 4.3813e-01, 0.0000e+00,  ..., 1.7573e-01, 0.0000e+00,
         1.3617e-01],
        ...,
        [0.0000e+00, 3.4332e-04, 0.0000e+00,  ..., 5.1512e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.4357e-04, 0.0000e+00,  ..., 5.1507e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 4.3783e-03, 0.0000e+00,  ..., 6.8344e-03, 0.0000e+00,
         3.0259e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(234282.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-135.3082, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.7034, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-274.3995, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9979],
        [0.9932],
        ...,
        [0.9849],
        [0.9842],
        [0.9836]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363512.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9979],
        [0.9932],
        ...,
        [0.9848],
        [0.9842],
        [0.9835]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363499.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(733.1298, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.7129, device='cuda:0')



h[100].sum tensor(19.7939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8636, device='cuda:0')



h[200].sum tensor(24.5870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.8712, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40081.7148, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0076, 0.0000,  ..., 0.0084, 0.0000, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(211212.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-108.4462, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.0627, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-233.7777, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9979],
        [0.9932],
        ...,
        [0.9848],
        [0.9842],
        [0.9835]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363499.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9994],
        [0.9979],
        [0.9931],
        ...,
        [0.9847],
        [0.9841],
        [0.9834]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363487.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0105,  0.0405,  0.0279,  ...,  0.0378,  0.0350,  0.0375],
        [ 0.0116,  0.0445,  0.0307,  ...,  0.0416,  0.0385,  0.0412],
        [ 0.0072,  0.0285,  0.0194,  ...,  0.0263,  0.0246,  0.0263],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(424.9747, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9607, device='cuda:0')



h[100].sum tensor(18.3770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.3969, device='cuda:0')



h[200].sum tensor(21.5775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.1283, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0489, 0.1876, 0.1297,  ..., 0.1756, 0.1622, 0.1735],
        [0.0404, 0.1569, 0.1079,  ..., 0.1462, 0.1356, 0.1450],
        [0.0302, 0.1194, 0.0814,  ..., 0.1104, 0.1031, 0.1101],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29423.2988, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 4.4724e-01, 0.0000e+00,  ..., 1.7905e-01, 0.0000e+00,
         1.3921e-01],
        [0.0000e+00, 4.0602e-01, 0.0000e+00,  ..., 1.6352e-01, 0.0000e+00,
         1.2547e-01],
        [0.0000e+00, 3.3680e-01, 0.0000e+00,  ..., 1.3728e-01, 0.0000e+00,
         1.0262e-01],
        ...,
        [0.0000e+00, 3.4350e-04, 0.0000e+00,  ..., 5.1494e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.4374e-04, 0.0000e+00,  ..., 5.1489e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.4375e-04, 0.0000e+00,  ..., 5.1487e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(157170.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-57.6901, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.4052, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-157.1232, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9994],
        [0.9979],
        [0.9931],
        ...,
        [0.9847],
        [0.9841],
        [0.9834]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363487.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 250.0 event: 1250 loss: tensor(622.2748, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9979],
        [0.9931],
        ...,
        [0.9847],
        [0.9840],
        [0.9833]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363474.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0011,  0.0064,  0.0038,  ...,  0.0052,  0.0055,  0.0058],
        [ 0.0011,  0.0064,  0.0038,  ...,  0.0052,  0.0055,  0.0058],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(539.1809, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9693, device='cuda:0')



h[100].sum tensor(18.8893, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.3542, device='cuda:0')



h[200].sum tensor(22.6657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.5215, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0019, 0.0126, 0.0067,  ..., 0.0094, 0.0106, 0.0108],
        [0.0019, 0.0126, 0.0067,  ..., 0.0094, 0.0106, 0.0108],
        [0.0019, 0.0126, 0.0067,  ..., 0.0093, 0.0106, 0.0108],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32208.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0365, 0.0000,  ..., 0.0209, 0.0000, 0.0056],
        [0.0000, 0.0398, 0.0000,  ..., 0.0224, 0.0000, 0.0062],
        [0.0000, 0.0367, 0.0000,  ..., 0.0210, 0.0000, 0.0055],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(170320.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-70.1666, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.2587, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-176.7780, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9979],
        [0.9931],
        ...,
        [0.9847],
        [0.9840],
        [0.9833]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363474.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9979],
        [0.9931],
        ...,
        [0.9846],
        [0.9839],
        [0.9832]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363461.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0034,  0.0146,  0.0096,  ...,  0.0130,  0.0126,  0.0134],
        [ 0.0069,  0.0276,  0.0187,  ...,  0.0254,  0.0238,  0.0254],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(396.9591, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2505, device='cuda:0')



h[100].sum tensor(18.2350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.1709, device='cuda:0')



h[200].sum tensor(21.2760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.3274, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0034, 0.0158, 0.0095,  ..., 0.0130, 0.0135, 0.0139],
        [0.0125, 0.0511, 0.0340,  ..., 0.0462, 0.0440, 0.0466],
        [0.0313, 0.1233, 0.0842,  ..., 0.1141, 0.1065, 0.1138],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28427.4023, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0704, 0.0000,  ..., 0.0331, 0.0000, 0.0196],
        [0.0000, 0.1632, 0.0000,  ..., 0.0694, 0.0000, 0.0487],
        [0.0000, 0.2940, 0.0000,  ..., 0.1201, 0.0000, 0.0901],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(153140.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-52.9644, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-51.1625, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-150.1473, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9979],
        [0.9931],
        ...,
        [0.9846],
        [0.9839],
        [0.9832]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363461.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9979],
        [0.9930],
        ...,
        [0.9845],
        [0.9838],
        [0.9831]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363448.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0021,  0.0101,  0.0063,  ...,  0.0087,  0.0087,  0.0091],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0038,  0.0162,  0.0107,  ...,  0.0145,  0.0140,  0.0148],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(657.2926, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.8945, device='cuda:0')



h[100].sum tensor(19.4070, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2850, device='cuda:0')



h[200].sum tensor(23.7652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.8205, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0121, 0.0514, 0.0337,  ..., 0.0459, 0.0442, 0.0469],
        [0.0106, 0.0478, 0.0307,  ..., 0.0420, 0.0411, 0.0436],
        [0.0077, 0.0337, 0.0217,  ..., 0.0296, 0.0289, 0.0305],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0049, 0.0214, 0.0136,  ..., 0.0184, 0.0183, 0.0191]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35351.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1843, 0.0000,  ..., 0.0790, 0.0000, 0.0527],
        [0.0000, 0.1447, 0.0000,  ..., 0.0638, 0.0000, 0.0394],
        [0.0000, 0.1310, 0.0000,  ..., 0.0580, 0.0000, 0.0360],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0065, 0.0000, 0.0006],
        [0.0000, 0.0179, 0.0000,  ..., 0.0121, 0.0000, 0.0044],
        [0.0000, 0.0601, 0.0000,  ..., 0.0288, 0.0000, 0.0165]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(184965.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-85.8691, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.0584, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-199.9691, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9979],
        [0.9930],
        ...,
        [0.9845],
        [0.9838],
        [0.9831]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363448.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9978],
        [0.9930],
        ...,
        [0.9844],
        [0.9837],
        [0.9830]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363436.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0041,  0.0174,  0.0116,  ...,  0.0157,  0.0150,  0.0160],
        [ 0.0035,  0.0152,  0.0100,  ...,  0.0136,  0.0131,  0.0139],
        [ 0.0035,  0.0149,  0.0098,  ...,  0.0133,  0.0129,  0.0137],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(645.5424, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1833, device='cuda:0')



h[100].sum tensor(19.3440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0587, device='cuda:0')



h[200].sum tensor(23.6315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.0184, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0147, 0.0631, 0.0415,  ..., 0.0565, 0.0543, 0.0577],
        [0.0201, 0.0825, 0.0552,  ..., 0.0751, 0.0712, 0.0758],
        [0.0190, 0.0786, 0.0525,  ..., 0.0714, 0.0678, 0.0722],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36079.7695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1975, 0.0000,  ..., 0.0845, 0.0000, 0.0561],
        [0.0000, 0.2458, 0.0000,  ..., 0.1034, 0.0000, 0.0716],
        [0.0000, 0.2515, 0.0000,  ..., 0.1056, 0.0000, 0.0735],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(190438.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-89.3813, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.5077, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-205.0559, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9978],
        [0.9930],
        ...,
        [0.9844],
        [0.9837],
        [0.9830]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363436.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9978],
        [0.9929],
        ...,
        [0.9843],
        [0.9836],
        [0.9830]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363423.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0015,  0.0078,  0.0047,  ...,  0.0065,  0.0067,  0.0070],
        [ 0.0015,  0.0079,  0.0048,  ...,  0.0067,  0.0068,  0.0072],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(501.9854, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7118, device='cuda:0')



h[100].sum tensor(18.6874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.9541, device='cuda:0')



h[200].sum tensor(22.2369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.1032, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0120, 0.0531, 0.0344,  ..., 0.0470, 0.0457, 0.0485],
        [0.0058, 0.0265, 0.0166,  ..., 0.0227, 0.0227, 0.0238],
        [0.0015, 0.0092, 0.0048,  ..., 0.0066, 0.0077, 0.0076],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30623.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1142, 0.0000,  ..., 0.0521, 0.0000, 0.0293],
        [0.0000, 0.0829, 0.0000,  ..., 0.0393, 0.0000, 0.0204],
        [0.0000, 0.0481, 0.0000,  ..., 0.0252, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(159052.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-64.0063, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.1300, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-166.0613, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9978],
        [0.9929],
        ...,
        [0.9843],
        [0.9836],
        [0.9830]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363423.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9978],
        [0.9929],
        ...,
        [0.9842],
        [0.9836],
        [0.9829]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363410.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(765.4051, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.2619, device='cuda:0')



h[100].sum tensor(19.8635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0383, device='cuda:0')



h[200].sum tensor(24.7349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.4905, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0017, 0.0117, 0.0061,  ..., 0.0085, 0.0099, 0.0100],
        [0.0009, 0.0067, 0.0031,  ..., 0.0043, 0.0055, 0.0053],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39374.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0282, 0.0000,  ..., 0.0174, 0.0000, 0.0038],
        [0.0000, 0.0200, 0.0000,  ..., 0.0138, 0.0000, 0.0022],
        [0.0000, 0.0103, 0.0000,  ..., 0.0097, 0.0000, 0.0008],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(206924.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-105.2481, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.2451, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-229.2999, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9978],
        [0.9929],
        ...,
        [0.9842],
        [0.9836],
        [0.9829]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363410.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9978],
        [0.9929],
        ...,
        [0.9842],
        [0.9836],
        [0.9829]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363410.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(520.2454, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9266, device='cuda:0')



h[100].sum tensor(18.7615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0224, device='cuda:0')



h[200].sum tensor(22.3942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.3454, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33498.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(181638.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-76.9241, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.9925, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.1773, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9978],
        [0.9929],
        ...,
        [0.9842],
        [0.9836],
        [0.9829]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363410.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9978],
        [0.9928],
        ...,
        [0.9842],
        [0.9835],
        [0.9828]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363397.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(466.0670, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6684, device='cuda:0')



h[100].sum tensor(18.5104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.6221, device='cuda:0')



h[200].sum tensor(21.8608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.9265, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0016, 0.0093, 0.0049,  ..., 0.0068, 0.0078, 0.0078],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30304.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0478, 0.0000,  ..., 0.0247, 0.0000, 0.0114],
        [0.0000, 0.0179, 0.0000,  ..., 0.0126, 0.0000, 0.0030],
        [0.0000, 0.0127, 0.0000,  ..., 0.0105, 0.0000, 0.0017],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(161002.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-62.3018, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.3438, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-163.8838, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9978],
        [0.9928],
        ...,
        [0.9842],
        [0.9835],
        [0.9828]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363397.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9978],
        [0.9928],
        ...,
        [0.9841],
        [0.9834],
        [0.9827]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363385.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(656.4280, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6495, device='cuda:0')



h[100].sum tensor(19.3544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2070, device='cuda:0')



h[200].sum tensor(23.6535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.5442, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34525.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(176266.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.8202, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.8738, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-193.8602, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9978],
        [0.9928],
        ...,
        [0.9841],
        [0.9834],
        [0.9827]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363385.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9993],
        [0.9977],
        [0.9927],
        ...,
        [0.9840],
        [0.9833],
        [0.9826]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363372.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0060,  0.0242,  0.0164,  ...,  0.0222,  0.0209,  0.0223],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(581.7900, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.4818, device='cuda:0')



h[100].sum tensor(19.0117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5173, device='cuda:0')



h[200].sum tensor(22.9257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.0994, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0108, 0.0449, 0.0296,  ..., 0.0402, 0.0386, 0.0409],
        [0.0221, 0.0880, 0.0596,  ..., 0.0809, 0.0759, 0.0809],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33498.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0575, 0.0000,  ..., 0.0282, 0.0000, 0.0158],
        [0.0000, 0.1425, 0.0000,  ..., 0.0617, 0.0000, 0.0416],
        [0.0000, 0.2299, 0.0000,  ..., 0.0958, 0.0000, 0.0688],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(174817.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-77.2960, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.6039, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.7574, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9993],
        [0.9977],
        [0.9927],
        ...,
        [0.9840],
        [0.9833],
        [0.9826]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363372.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 260.0 event: 1300 loss: tensor(552.5645, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9977],
        [0.9927],
        ...,
        [0.9839],
        [0.9832],
        [0.9825]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363359.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0013,  0.0072,  0.0043,  ...,  0.0059,  0.0061,  0.0064],
        [ 0.0017,  0.0086,  0.0053,  ...,  0.0073,  0.0074,  0.0078],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(833.7514, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.9456, device='cuda:0')



h[100].sum tensor(20.1247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5740, device='cuda:0')



h[200].sum tensor(25.2896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.3893, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0127, 0.0557, 0.0362,  ..., 0.0495, 0.0479, 0.0509],
        [0.0058, 0.0265, 0.0166,  ..., 0.0227, 0.0227, 0.0238],
        [0.0034, 0.0179, 0.0105,  ..., 0.0144, 0.0152, 0.0157],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44009.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1205, 0.0000,  ..., 0.0545, 0.0000, 0.0313],
        [0.0000, 0.0897, 0.0000,  ..., 0.0421, 0.0000, 0.0223],
        [0.0000, 0.0675, 0.0000,  ..., 0.0331, 0.0000, 0.0160],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(244265.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-127.9800, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-36.6051, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-263.3919, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9977],
        [0.9927],
        ...,
        [0.9839],
        [0.9832],
        [0.9825]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363359.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9977],
        [0.9926],
        ...,
        [0.9838],
        [0.9831],
        [0.9824]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363346.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(905.6806, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.2435, device='cuda:0')



h[100].sum tensor(20.4328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9870, device='cuda:0')



h[200].sum tensor(25.9440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.8531, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44621.9414, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0071, 0.0000, 0.0002],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0063, 0.0000, 0.0005],
        [0.0000, 0.0132, 0.0000,  ..., 0.0103, 0.0000, 0.0030]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(232735.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-130.7238, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.9622, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-267.9629, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9977],
        [0.9926],
        ...,
        [0.9838],
        [0.9831],
        [0.9824]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363346.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9977],
        [0.9926],
        ...,
        [0.9838],
        [0.9831],
        [0.9823]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363333.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0018,  0.0091,  0.0056,  ...,  0.0077,  0.0078,  0.0082],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(553.6478, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.6622, device='cuda:0')



h[100].sum tensor(18.8611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2565, device='cuda:0')



h[200].sum tensor(22.6058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.1750, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0018, 0.0103, 0.0056,  ..., 0.0077, 0.0086, 0.0087],
        [0.0014, 0.0087, 0.0045,  ..., 0.0062, 0.0073, 0.0072],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34426.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0087, 0.0000, 0.0008],
        [0.0000, 0.0212, 0.0000,  ..., 0.0142, 0.0000, 0.0035],
        [0.0000, 0.0270, 0.0000,  ..., 0.0166, 0.0000, 0.0048],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(187226.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.6054, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.9040, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-193.2309, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9977],
        [0.9926],
        ...,
        [0.9838],
        [0.9831],
        [0.9823]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363333.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9977],
        [0.9926],
        ...,
        [0.9837],
        [0.9830],
        [0.9823]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363321., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(941.7454, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.2527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.3831, device='cuda:0')



h[100].sum tensor(20.5682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.3496, device='cuda:0')



h[200].sum tensor(26.2315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.1383, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44619.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0266, 0.0000,  ..., 0.0158, 0.0000, 0.0058],
        [0.0000, 0.0253, 0.0000,  ..., 0.0152, 0.0000, 0.0054],
        [0.0000, 0.0190, 0.0000,  ..., 0.0127, 0.0000, 0.0041]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(237660.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-130.3079, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-36.3009, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-267.4804, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9977],
        [0.9926],
        ...,
        [0.9837],
        [0.9830],
        [0.9823]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363321., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9976],
        [0.9925],
        ...,
        [0.9836],
        [0.9829],
        [0.9822]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363308.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0025,  0.0113,  0.0072,  ...,  0.0098,  0.0097,  0.0103]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(647.5664, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9144, device='cuda:0')



h[100].sum tensor(19.2588, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9731, device='cuda:0')



h[200].sum tensor(23.4504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.7152, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0024, 0.0122, 0.0070,  ..., 0.0096, 0.0103, 0.0105],
        [0.0019, 0.0103, 0.0056,  ..., 0.0077, 0.0086, 0.0087]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36011.5508, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0110, 0.0000,  ..., 0.0098, 0.0000, 0.0018],
        [0.0000, 0.0055, 0.0000,  ..., 0.0076, 0.0000, 0.0004],
        [0.0000, 0.0107, 0.0000,  ..., 0.0098, 0.0000, 0.0015],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0083, 0.0000, 0.0012],
        [0.0000, 0.0252, 0.0000,  ..., 0.0154, 0.0000, 0.0050],
        [0.0000, 0.0324, 0.0000,  ..., 0.0184, 0.0000, 0.0068]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(188161.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-89.2391, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.3044, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-204.9599, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9976],
        [0.9925],
        ...,
        [0.9836],
        [0.9829],
        [0.9822]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363308.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9976],
        [0.9925],
        ...,
        [0.9835],
        [0.9828],
        [0.9821]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363295.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(561.8883, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.6425, device='cuda:0')



h[100].sum tensor(18.8728, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2502, device='cuda:0')



h[200].sum tensor(22.6306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.1529, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33256.8164, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0159, 0.0000,  ..., 0.0119, 0.0000, 0.0025],
        [0.0000, 0.0071, 0.0000,  ..., 0.0083, 0.0000, 0.0006],
        [0.0000, 0.0075, 0.0000,  ..., 0.0085, 0.0000, 0.0007],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(177309.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-75.6607, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.3122, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-184.2641, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9976],
        [0.9925],
        ...,
        [0.9835],
        [0.9828],
        [0.9821]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363295.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9976],
        [0.9924],
        ...,
        [0.9834],
        [0.9827],
        [0.9820]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363282.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(429.9126, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.9237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2838, device='cuda:0')



h[100].sum tensor(18.2854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.1815, device='cuda:0')



h[200].sum tensor(21.3831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.3649, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29325.9336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(160519.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-56.7683, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.6474, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-156.2095, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9976],
        [0.9924],
        ...,
        [0.9834],
        [0.9827],
        [0.9820]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363282.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9976],
        [0.9924],
        ...,
        [0.9833],
        [0.9826],
        [0.9819]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363269.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0115,  0.0443,  0.0306,  ...,  0.0415,  0.0383,  0.0410],
        [ 0.0090,  0.0350,  0.0240,  ...,  0.0325,  0.0302,  0.0323],
        [ 0.0075,  0.0298,  0.0203,  ...,  0.0276,  0.0258,  0.0275],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(779.8664, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.2305, device='cuda:0')



h[100].sum tensor(19.8108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0283, device='cuda:0')



h[200].sum tensor(24.6229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.4550, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0334, 0.1311, 0.0897,  ..., 0.1216, 0.1133, 0.1210],
        [0.0423, 0.1635, 0.1126,  ..., 0.1526, 0.1414, 0.1511],
        [0.0446, 0.1719, 0.1186,  ..., 0.1606, 0.1486, 0.1589],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37059.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 4.0283e-01, 0.0000e+00,  ..., 1.6247e-01, 0.0000e+00,
         1.2433e-01],
        [0.0000e+00, 4.7780e-01, 0.0000e+00,  ..., 1.9088e-01, 0.0000e+00,
         1.4922e-01],
        [0.0000e+00, 5.0710e-01, 0.0000e+00,  ..., 2.0193e-01, 0.0000e+00,
         1.5906e-01],
        ...,
        [0.0000e+00, 3.4499e-04, 0.0000e+00,  ..., 5.1344e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.4524e-04, 0.0000e+00,  ..., 5.1339e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.4525e-04, 0.0000e+00,  ..., 5.1336e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(184646.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-94.8049, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.0391, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-212.9201, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9976],
        [0.9924],
        ...,
        [0.9833],
        [0.9826],
        [0.9819]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363269.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9976],
        [0.9923],
        ...,
        [0.9833],
        [0.9825],
        [0.9818]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363256.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0092,  0.0359,  0.0247,  ...,  0.0334,  0.0311,  0.0332],
        [ 0.0060,  0.0243,  0.0164,  ...,  0.0223,  0.0210,  0.0224],
        [ 0.0127,  0.0488,  0.0338,  ...,  0.0457,  0.0422,  0.0451],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(938.2683, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.1299, device='cuda:0')



h[100].sum tensor(20.4923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.2691, device='cuda:0')



h[200].sum tensor(26.0704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.8528, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0259, 0.1037, 0.0703,  ..., 0.0954, 0.0896, 0.0956],
        [0.0393, 0.1527, 0.1050,  ..., 0.1422, 0.1320, 0.1411],
        [0.0205, 0.0841, 0.0563,  ..., 0.0766, 0.0725, 0.0773],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43616.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2816, 0.0000,  ..., 0.1163, 0.0000, 0.0843],
        [0.0000, 0.3369, 0.0000,  ..., 0.1375, 0.0000, 0.1022],
        [0.0000, 0.2669, 0.0000,  ..., 0.1107, 0.0000, 0.0794],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(225511.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-125.1016, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.8351, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-259.3290, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9976],
        [0.9923],
        ...,
        [0.9833],
        [0.9825],
        [0.9818]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363256.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9992],
        [0.9975],
        [0.9923],
        ...,
        [0.9832],
        [0.9825],
        [0.9817]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363243.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061,  0.0247,  0.0167,  ...,  0.0227,  0.0213,  0.0228],
        [ 0.0099,  0.0385,  0.0265,  ...,  0.0359,  0.0333,  0.0356],
        [ 0.0113,  0.0437,  0.0302,  ...,  0.0409,  0.0378,  0.0404],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(618.1675, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0854, device='cuda:0')



h[100].sum tensor(19.0856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7093, device='cuda:0')



h[200].sum tensor(23.0825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.7802, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0297, 0.1175, 0.0800,  ..., 0.1085, 0.1015, 0.1083],
        [0.0307, 0.1213, 0.0827,  ..., 0.1122, 0.1048, 0.1119],
        [0.0349, 0.1365, 0.0935,  ..., 0.1268, 0.1180, 0.1261],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32908.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.5738e-01, 0.0000e+00,  ..., 1.4514e-01, 0.0000e+00,
         1.0962e-01],
        [0.0000e+00, 3.4739e-01, 0.0000e+00,  ..., 1.4138e-01, 0.0000e+00,
         1.0630e-01],
        [0.0000e+00, 3.2855e-01, 0.0000e+00,  ..., 1.3388e-01, 0.0000e+00,
         1.0064e-01],
        ...,
        [0.0000e+00, 3.4517e-04, 0.0000e+00,  ..., 5.1326e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.4542e-04, 0.0000e+00,  ..., 5.1321e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.4543e-04, 0.0000e+00,  ..., 5.1319e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(170416.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-74.2822, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.1824, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-182.4220, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9992],
        [0.9975],
        [0.9923],
        ...,
        [0.9832],
        [0.9825],
        [0.9817]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363243.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 270.0 event: 1350 loss: tensor(642.3937, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9975],
        [0.9922],
        ...,
        [0.9831],
        [0.9824],
        [0.9817]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363230.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(564.3408, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4977, device='cuda:0')



h[100].sum tensor(18.8430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2041, device='cuda:0')



h[200].sum tensor(22.5673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.9896, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32477.2285, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0162, 0.0000,  ..., 0.0119, 0.0000, 0.0032],
        [0.0000, 0.0365, 0.0000,  ..., 0.0201, 0.0000, 0.0088],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(169194.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-72.5539, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.5208, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-179.3750, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9975],
        [0.9922],
        ...,
        [0.9831],
        [0.9824],
        [0.9817]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363230.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9975],
        [0.9922],
        ...,
        [0.9830],
        [0.9823],
        [0.9816]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363217.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(454.9733, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6908, device='cuda:0')



h[100].sum tensor(18.3607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.3110, device='cuda:0')



h[200].sum tensor(21.5429, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.8239, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0016, 0.0094, 0.0050,  ..., 0.0069, 0.0079, 0.0079],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0017, 0.0098, 0.0053,  ..., 0.0073, 0.0082, 0.0083],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29433.2793, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0207, 0.0000,  ..., 0.0140, 0.0000, 0.0032],
        [0.0000, 0.0124, 0.0000,  ..., 0.0105, 0.0000, 0.0012],
        [0.0000, 0.0217, 0.0000,  ..., 0.0144, 0.0000, 0.0035],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(159262.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-58.0386, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.1729, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-157.5230, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9975],
        [0.9922],
        ...,
        [0.9830],
        [0.9823],
        [0.9816]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363217.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9975],
        [0.9921],
        ...,
        [0.9829],
        [0.9822],
        [0.9815]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363204.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(781.6860, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8909, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.0734, device='cuda:0')



h[100].sum tensor(19.7676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9783, device='cuda:0')



h[200].sum tensor(24.5312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.2779, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38007.7891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0014, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0084, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(195435.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-98.6483, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.7770, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-219.0065, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9975],
        [0.9921],
        ...,
        [0.9829],
        [0.9822],
        [0.9815]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363204.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9975],
        [0.9921],
        ...,
        [0.9828],
        [0.9821],
        [0.9814]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363191.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0018,  0.0090,  0.0056,  ...,  0.0077,  0.0078,  0.0082],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0018,  0.0090,  0.0056,  ...,  0.0077,  0.0078,  0.0082],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1099.2268, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.1062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.8309, device='cuda:0')



h[100].sum tensor(21.1285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.4467, device='cuda:0')



h[200].sum tensor(27.4216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.0268, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0014, 0.0087, 0.0045,  ..., 0.0062, 0.0073, 0.0072],
        [0.0065, 0.0330, 0.0202,  ..., 0.0278, 0.0283, 0.0298],
        [0.0014, 0.0087, 0.0045,  ..., 0.0062, 0.0072, 0.0072],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48790.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0395, 0.0000,  ..., 0.0218, 0.0000, 0.0081],
        [0.0000, 0.0536, 0.0000,  ..., 0.0278, 0.0000, 0.0112],
        [0.0000, 0.0402, 0.0000,  ..., 0.0223, 0.0000, 0.0075],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(254268.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-150.5709, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-33.0691, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-296.8779, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9975],
        [0.9921],
        ...,
        [0.9828],
        [0.9821],
        [0.9814]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363191.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9974],
        [0.9920],
        ...,
        [0.9828],
        [0.9820],
        [0.9813]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363178.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0020,  0.0095,  0.0059,  ...,  0.0081,  0.0082,  0.0086],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(571.6341, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5476, device='cuda:0')



h[100].sum tensor(18.8428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2200, device='cuda:0')



h[200].sum tensor(22.5669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.0458, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0058, 0.0287, 0.0176,  ..., 0.0242, 0.0246, 0.0258],
        [0.0054, 0.0253, 0.0158,  ..., 0.0216, 0.0217, 0.0227],
        [0.0083, 0.0394, 0.0247,  ..., 0.0339, 0.0339, 0.0358],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32667.3242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0771, 0.0000,  ..., 0.0375, 0.0000, 0.0176],
        [0.0000, 0.0803, 0.0000,  ..., 0.0386, 0.0000, 0.0187],
        [0.0000, 0.0815, 0.0000,  ..., 0.0392, 0.0000, 0.0188],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(173439.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-73.3558, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.5463, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-180.3989, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9974],
        [0.9920],
        ...,
        [0.9828],
        [0.9820],
        [0.9813]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363178.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9974],
        [0.9920],
        ...,
        [0.9827],
        [0.9820],
        [0.9812]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363166., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0072,  0.0284,  0.0193,  ...,  0.0262,  0.0245,  0.0262],
        [ 0.0045,  0.0189,  0.0126,  ...,  0.0171,  0.0163,  0.0173],
        [ 0.0054,  0.0220,  0.0148,  ...,  0.0201,  0.0190,  0.0202],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(607.8839, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.4809, device='cuda:0')



h[100].sum tensor(18.9907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5170, device='cuda:0')



h[200].sum tensor(22.8811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.0984, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0193, 0.0795, 0.0531,  ..., 0.0723, 0.0686, 0.0731],
        [0.0224, 0.0910, 0.0612,  ..., 0.0832, 0.0785, 0.0837],
        [0.0200, 0.0824, 0.0551,  ..., 0.0750, 0.0711, 0.0757],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35607.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2323, 0.0000,  ..., 0.0978, 0.0000, 0.0676],
        [0.0000, 0.2309, 0.0000,  ..., 0.0972, 0.0000, 0.0674],
        [0.0000, 0.2113, 0.0000,  ..., 0.0896, 0.0000, 0.0611],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(195888.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-87.4260, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.7549, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-201.8602, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9974],
        [0.9920],
        ...,
        [0.9827],
        [0.9820],
        [0.9812]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363166., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9974],
        [0.9919],
        ...,
        [0.9826],
        [0.9819],
        [0.9811]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363153., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(516.0119, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0455, device='cuda:0')



h[100].sum tensor(18.5886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7421, device='cuda:0')



h[200].sum tensor(22.0270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.3518, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0012, 0.0079, 0.0039,  ..., 0.0054, 0.0065, 0.0064],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30885.0742, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0459, 0.0000,  ..., 0.0244, 0.0000, 0.0100],
        [0.0000, 0.0241, 0.0000,  ..., 0.0153, 0.0000, 0.0048],
        [0.0000, 0.0086, 0.0000,  ..., 0.0088, 0.0000, 0.0013],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(165059.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-64.6166, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.9742, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-167.8458, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9974],
        [0.9919],
        ...,
        [0.9826],
        [0.9819],
        [0.9811]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363153., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9974],
        [0.9919],
        ...,
        [0.9825],
        [0.9818],
        [0.9810]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363140., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1275.9028, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.6663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.2701, device='cuda:0')



h[100].sum tensor(21.8321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.8593, device='cuda:0')



h[200].sum tensor(28.9162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.0333, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0097, 0.0409, 0.0268,  ..., 0.0364, 0.0352, 0.0372],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51786.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1565, 0.0000,  ..., 0.0667, 0.0000, 0.0465],
        [0.0000, 0.0483, 0.0000,  ..., 0.0244, 0.0000, 0.0134],
        [0.0000, 0.0130, 0.0000,  ..., 0.0105, 0.0000, 0.0026],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(258503.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-164.7130, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-29.5586, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-319.5811, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9974],
        [0.9919],
        ...,
        [0.9825],
        [0.9818],
        [0.9810]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363140., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9991],
        [0.9974],
        [0.9919],
        ...,
        [0.9824],
        [0.9817],
        [0.9810]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363127., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0019,  0.0093,  0.0058,  ...,  0.0079,  0.0079,  0.0084],
        [ 0.0051,  0.0208,  0.0139,  ...,  0.0190,  0.0180,  0.0191],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(683.1116, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3738, device='cuda:0')



h[100].sum tensor(19.2874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.1193, device='cuda:0')



h[200].sum tensor(23.5113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.2332, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0243, 0.0978, 0.0661,  ..., 0.0897, 0.0844, 0.0900],
        [0.0120, 0.0513, 0.0336,  ..., 0.0458, 0.0441, 0.0468],
        [0.0126, 0.0533, 0.0350,  ..., 0.0477, 0.0459, 0.0487],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34245.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2502, 0.0000,  ..., 0.1047, 0.0000, 0.0732],
        [0.0000, 0.1819, 0.0000,  ..., 0.0782, 0.0000, 0.0514],
        [0.0000, 0.1424, 0.0000,  ..., 0.0626, 0.0000, 0.0394],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(175943.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.5097, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.0089, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-192.0237, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9991],
        [0.9974],
        [0.9919],
        ...,
        [0.9824],
        [0.9817],
        [0.9810]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363127., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9973],
        [0.9918],
        ...,
        [0.9823],
        [0.9816],
        [0.9809]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363113.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(477.9046, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0948, device='cuda:0')



h[100].sum tensor(18.4048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4395, device='cuda:0')



h[200].sum tensor(21.6366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.2795, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0052, 0.0246, 0.0152,  ..., 0.0208, 0.0210, 0.0220],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30443.6367, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0060, 0.0000,  ..., 0.0077, 0.0000, 0.0009],
        [0.0000, 0.0372, 0.0000,  ..., 0.0201, 0.0000, 0.0098],
        [0.0000, 0.1023, 0.0000,  ..., 0.0461, 0.0000, 0.0283],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(165554.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-62.5060, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.5832, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.3103, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9973],
        [0.9918],
        ...,
        [0.9823],
        [0.9816],
        [0.9809]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363113.9375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 280.0 event: 1400 loss: tensor(622.9807, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9973],
        [0.9918],
        ...,
        [0.9823],
        [0.9815],
        [0.9808]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363100.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(603.0270, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9755, device='cuda:0')



h[100].sum tensor(18.9297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.3562, device='cuda:0')



h[200].sum tensor(22.7514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.5284, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0052, 0.0243, 0.0151,  ..., 0.0206, 0.0208, 0.0218],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33757.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0142, 0.0000,  ..., 0.0110, 0.0000, 0.0031],
        [0.0000, 0.0492, 0.0000,  ..., 0.0250, 0.0000, 0.0129],
        [0.0000, 0.1048, 0.0000,  ..., 0.0475, 0.0000, 0.0283],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(178012.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-78.9980, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.0962, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-188.9630, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9973],
        [0.9918],
        ...,
        [0.9823],
        [0.9815],
        [0.9808]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363100.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9973],
        [0.9917],
        ...,
        [0.9822],
        [0.9814],
        [0.9807]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363087.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(666.5466, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.4549, device='cuda:0')



h[100].sum tensor(19.1911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.8269, device='cuda:0')



h[200].sum tensor(23.3066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.1969, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35032.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0365, 0.0000,  ..., 0.0200, 0.0000, 0.0087],
        [0.0000, 0.0418, 0.0000,  ..., 0.0222, 0.0000, 0.0101],
        [0.0000, 0.0414, 0.0000,  ..., 0.0220, 0.0000, 0.0100],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(181426.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-84.9160, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.9118, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-198.2104, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9973],
        [0.9917],
        ...,
        [0.9822],
        [0.9814],
        [0.9807]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363087.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9973],
        [0.9917],
        ...,
        [0.9821],
        [0.9814],
        [0.9806]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363074.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0018,  0.0091,  0.0056,  ...,  0.0077,  0.0078,  0.0082],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0018,  0.0091,  0.0056,  ...,  0.0077,  0.0078,  0.0082],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(642.3934, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.8816, device='cuda:0')



h[100].sum tensor(19.0804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6445, device='cuda:0')



h[200].sum tensor(23.0717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.5503, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0031, 0.0150, 0.0089,  ..., 0.0122, 0.0127, 0.0130],
        [0.0099, 0.0455, 0.0291,  ..., 0.0398, 0.0392, 0.0415],
        [0.0031, 0.0149, 0.0089,  ..., 0.0121, 0.0126, 0.0130],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34468.4883, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0558, 0.0000,  ..., 0.0283, 0.0000, 0.0128],
        [0.0000, 0.0857, 0.0000,  ..., 0.0406, 0.0000, 0.0208],
        [0.0000, 0.0579, 0.0000,  ..., 0.0291, 0.0000, 0.0132],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(181030.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.5408, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.8783, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-193.4774, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9973],
        [0.9917],
        ...,
        [0.9821],
        [0.9814],
        [0.9806]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363074.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9972],
        [0.9916],
        ...,
        [0.9820],
        [0.9813],
        [0.9805]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363061.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0048,  0.0197,  0.0132,  ...,  0.0179,  0.0170,  0.0181],
        [ 0.0019,  0.0093,  0.0058,  ...,  0.0079,  0.0080,  0.0084],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(505.5059, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5252, device='cuda:0')



h[100].sum tensor(18.4945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.5765, device='cuda:0')



h[200].sum tensor(21.8272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.7650, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0057, 0.0263, 0.0164,  ..., 0.0224, 0.0225, 0.0235],
        [0.0062, 0.0281, 0.0178,  ..., 0.0242, 0.0241, 0.0253],
        [0.0098, 0.0451, 0.0287,  ..., 0.0393, 0.0388, 0.0410],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29881.5605, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1068, 0.0000,  ..., 0.0482, 0.0000, 0.0288],
        [0.0000, 0.0998, 0.0000,  ..., 0.0459, 0.0000, 0.0257],
        [0.0000, 0.1128, 0.0000,  ..., 0.0516, 0.0000, 0.0285],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(158889.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-59.8016, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.9891, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-160.4116, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9972],
        [0.9916],
        ...,
        [0.9820],
        [0.9813],
        [0.9805]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363061.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9972],
        [0.9916],
        ...,
        [0.9819],
        [0.9812],
        [0.9804]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363048.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0029,  0.0129,  0.0083,  ...,  0.0114,  0.0111,  0.0118],
        [ 0.0041,  0.0173,  0.0114,  ...,  0.0156,  0.0149,  0.0159],
        [ 0.0040,  0.0170,  0.0112,  ...,  0.0153,  0.0147,  0.0156],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1109.6986, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.3035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.0664, device='cuda:0')



h[100].sum tensor(21.0320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.2035, device='cuda:0')



h[200].sum tensor(27.2168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.1645, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0064, 0.0287, 0.0182,  ..., 0.0248, 0.0246, 0.0258],
        [0.0102, 0.0445, 0.0288,  ..., 0.0393, 0.0382, 0.0405],
        [0.0235, 0.0949, 0.0640,  ..., 0.0870, 0.0819, 0.0873],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48221.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1212, 0.0000,  ..., 0.0544, 0.0000, 0.0322],
        [0.0000, 0.1679, 0.0000,  ..., 0.0725, 0.0000, 0.0473],
        [0.0000, 0.2402, 0.0000,  ..., 0.1000, 0.0000, 0.0715],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(246229.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-148.1771, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-32.5908, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-294.0750, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9972],
        [0.9916],
        ...,
        [0.9819],
        [0.9812],
        [0.9804]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363048.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9972],
        [0.9915],
        ...,
        [0.9818],
        [0.9811],
        [0.9803]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363035.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0035,  0.0152,  0.0099,  ...,  0.0136,  0.0131,  0.0139],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(528.2102, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1807, device='cuda:0')



h[100].sum tensor(18.5763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7851, device='cuda:0')



h[200].sum tensor(22.0008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.5042, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0035, 0.0164, 0.0099,  ..., 0.0135, 0.0139, 0.0143],
        [0.0028, 0.0136, 0.0080,  ..., 0.0109, 0.0115, 0.0118],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30382.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0141, 0.0000,  ..., 0.0109, 0.0000, 0.0026],
        [0.0000, 0.0424, 0.0000,  ..., 0.0222, 0.0000, 0.0107],
        [0.0000, 0.0769, 0.0000,  ..., 0.0360, 0.0000, 0.0211],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(161147.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-62.6357, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.1906, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.5262, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9972],
        [0.9915],
        ...,
        [0.9818],
        [0.9811],
        [0.9803]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363035.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9972],
        [0.9915],
        ...,
        [0.9818],
        [0.9810],
        [0.9803]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363022.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056,  0.0226,  0.0152,  ...,  0.0207,  0.0195,  0.0208],
        [ 0.0136,  0.0518,  0.0359,  ...,  0.0486,  0.0448,  0.0480],
        [ 0.0052,  0.0214,  0.0144,  ...,  0.0195,  0.0185,  0.0197],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(716.3088, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3814, device='cuda:0')



h[100].sum tensor(19.3579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.1217, device='cuda:0')



h[200].sum tensor(23.6610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.2418, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0276, 0.1099, 0.0746,  ..., 0.1013, 0.0949, 0.1013],
        [0.0212, 0.0868, 0.0583,  ..., 0.0792, 0.0749, 0.0798],
        [0.0221, 0.0881, 0.0597,  ..., 0.0810, 0.0761, 0.0810],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35649.8789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2493, 0.0000,  ..., 0.1035, 0.0000, 0.0746],
        [0.0000, 0.2433, 0.0000,  ..., 0.1012, 0.0000, 0.0728],
        [0.0000, 0.2161, 0.0000,  ..., 0.0902, 0.0000, 0.0649],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(184107.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-87.5272, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.8188, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-202.0070, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9972],
        [0.9915],
        ...,
        [0.9818],
        [0.9810],
        [0.9803]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363022.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9990],
        [0.9972],
        [0.9914],
        ...,
        [0.9817],
        [0.9809],
        [0.9802]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(363009.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0039,  0.0166,  0.0110,  ...,  0.0149,  0.0143,  0.0152],
        [ 0.0020,  0.0098,  0.0061,  ...,  0.0084,  0.0084,  0.0089],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(586.1301, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8613, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4078, device='cuda:0')



h[100].sum tensor(18.8046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1755, device='cuda:0')



h[200].sum tensor(22.4859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.8881, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0127, 0.0557, 0.0363,  ..., 0.0495, 0.0480, 0.0509],
        [0.0070, 0.0311, 0.0198,  ..., 0.0270, 0.0266, 0.0280],
        [0.0020, 0.0110, 0.0061,  ..., 0.0084, 0.0092, 0.0093],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32068.0430, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1232, 0.0000,  ..., 0.0554, 0.0000, 0.0324],
        [0.0000, 0.0872, 0.0000,  ..., 0.0407, 0.0000, 0.0224],
        [0.0000, 0.0447, 0.0000,  ..., 0.0235, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(168421.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-70.4273, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.7691, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-176.5671, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9990],
        [0.9972],
        [0.9914],
        ...,
        [0.9817],
        [0.9809],
        [0.9802]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(363009.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9971],
        [0.9914],
        ...,
        [0.9816],
        [0.9808],
        [0.9801]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362996.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(524.7068, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7392, device='cuda:0')



h[100].sum tensor(18.5408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.6446, device='cuda:0')



h[200].sum tensor(21.9255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.0063, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29775.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(156380.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-59.4233, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.9613, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-159.8422, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9971],
        [0.9914],
        ...,
        [0.9816],
        [0.9808],
        [0.9801]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362996.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9971],
        [0.9913],
        ...,
        [0.9815],
        [0.9808],
        [0.9800]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362983.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(680.7245, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.7452, device='cuda:0')



h[100].sum tensor(19.1838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9193, device='cuda:0')



h[200].sum tensor(23.2912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.5243, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35421.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.1266e-04, 0.0000e+00,  ..., 5.3851e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 9.4291e-04, 0.0000e+00,  ..., 5.6493e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.5065e-03, 0.0000e+00,  ..., 6.3235e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 3.1910e-03, 0.0000e+00,  ..., 6.3287e-03, 0.0000e+00,
         2.7396e-05],
        [0.0000e+00, 8.8794e-03, 0.0000e+00,  ..., 8.7557e-03, 0.0000e+00,
         6.4947e-04],
        [0.0000e+00, 1.1723e-02, 0.0000e+00,  ..., 9.9692e-03, 0.0000e+00,
         1.1041e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(187331.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-85.8339, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.0533, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-200.3233, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9971],
        [0.9913],
        ...,
        [0.9815],
        [0.9808],
        [0.9800]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362983.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 290.0 event: 1450 loss: tensor(615.7450, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9971],
        [0.9913],
        ...,
        [0.9814],
        [0.9807],
        [0.9799]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362970., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0027,  0.0120,  0.0077,  ...,  0.0105,  0.0103,  0.0109],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(396.2794, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.5212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.6686, device='cuda:0')



h[100].sum tensor(17.9934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.6675, device='cuda:0')



h[200].sum tensor(20.7629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.5432, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0026, 0.0132, 0.0077,  ..., 0.0105, 0.0112, 0.0114],
        [0.0095, 0.0401, 0.0262,  ..., 0.0357, 0.0344, 0.0364],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27453.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0283, 0.0000,  ..., 0.0169, 0.0000, 0.0056],
        [0.0000, 0.0599, 0.0000,  ..., 0.0293, 0.0000, 0.0154],
        [0.0000, 0.1365, 0.0000,  ..., 0.0593, 0.0000, 0.0395],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(151302.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-48.5277, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-51.9339, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-143.1897, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9971],
        [0.9913],
        ...,
        [0.9814],
        [0.9807],
        [0.9799]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362970., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9971],
        [0.9912],
        ...,
        [0.9813],
        [0.9806],
        [0.9798]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362956.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0091,  0.0355,  0.0244,  ...,  0.0330,  0.0307,  0.0328],
        [ 0.0083,  0.0325,  0.0222,  ...,  0.0302,  0.0281,  0.0300],
        [ 0.0077,  0.0305,  0.0208,  ...,  0.0282,  0.0264,  0.0282],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(760.0497, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.2998, device='cuda:0')



h[100].sum tensor(19.4964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.4140, device='cuda:0')



h[200].sum tensor(23.9551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.2776, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0185, 0.0748, 0.0503,  ..., 0.0683, 0.0646, 0.0687],
        [0.0263, 0.1051, 0.0713,  ..., 0.0967, 0.0908, 0.0969],
        [0.0285, 0.1133, 0.0771,  ..., 0.1046, 0.0979, 0.1045],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36829.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2696, 0.0000,  ..., 0.1116, 0.0000, 0.0808],
        [0.0000, 0.3008, 0.0000,  ..., 0.1239, 0.0000, 0.0905],
        [0.0000, 0.3094, 0.0000,  ..., 0.1275, 0.0000, 0.0930],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(194691.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-93.1958, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.5937, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-210.7826, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9971],
        [0.9912],
        ...,
        [0.9813],
        [0.9806],
        [0.9798]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362956.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9971],
        [0.9912],
        ...,
        [0.9813],
        [0.9806],
        [0.9798]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362956.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0043,  0.0180,  0.0120,  ...,  0.0163,  0.0156,  0.0165],
        [ 0.0016,  0.0081,  0.0049,  ...,  0.0068,  0.0069,  0.0073],
        [ 0.0015,  0.0079,  0.0048,  ...,  0.0066,  0.0067,  0.0071],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(563.1768, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6274, device='cuda:0')



h[100].sum tensor(18.6799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.9272, device='cuda:0')



h[200].sum tensor(22.2210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.0080, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0074, 0.0362, 0.0224,  ..., 0.0308, 0.0311, 0.0328],
        [0.0109, 0.0491, 0.0316,  ..., 0.0432, 0.0423, 0.0448],
        [0.0086, 0.0406, 0.0256,  ..., 0.0351, 0.0349, 0.0369],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31525.0586, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1351, 0.0000,  ..., 0.0609, 0.0000, 0.0342],
        [0.0000, 0.1419, 0.0000,  ..., 0.0636, 0.0000, 0.0365],
        [0.0000, 0.1256, 0.0000,  ..., 0.0570, 0.0000, 0.0317],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(167954.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-67.6506, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.3090, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-172.5441, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9971],
        [0.9912],
        ...,
        [0.9813],
        [0.9806],
        [0.9798]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362956.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9971],
        [0.9912],
        ...,
        [0.9813],
        [0.9805],
        [0.9797]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362943.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0063,  0.0253,  0.0171,  ...,  0.0233,  0.0219,  0.0233],
        [ 0.0026,  0.0117,  0.0075,  ...,  0.0102,  0.0100,  0.0106],
        [ 0.0029,  0.0128,  0.0083,  ...,  0.0113,  0.0110,  0.0117],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(610.5487, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.6955, device='cuda:0')



h[100].sum tensor(18.8689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2671, device='cuda:0')



h[200].sum tensor(22.6224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.2127, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0147, 0.0628, 0.0413,  ..., 0.0563, 0.0541, 0.0575],
        [0.0162, 0.0683, 0.0452,  ..., 0.0616, 0.0589, 0.0627],
        [0.0092, 0.0409, 0.0263,  ..., 0.0359, 0.0351, 0.0371],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32641.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1413, 0.0000,  ..., 0.0621, 0.0000, 0.0392],
        [0.0000, 0.1414, 0.0000,  ..., 0.0621, 0.0000, 0.0395],
        [0.0000, 0.1163, 0.0000,  ..., 0.0521, 0.0000, 0.0319],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(172787.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-72.6506, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.6528, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-180.1479, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9971],
        [0.9912],
        ...,
        [0.9813],
        [0.9805],
        [0.9797]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362943.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9971],
        [0.9912],
        ...,
        [0.9813],
        [0.9805],
        [0.9797]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362943.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0031,  0.0135,  0.0088,  ...,  0.0120,  0.0117,  0.0124],
        [ 0.0018,  0.0089,  0.0055,  ...,  0.0076,  0.0077,  0.0081],
        [ 0.0013,  0.0070,  0.0042,  ...,  0.0058,  0.0060,  0.0063],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(905.5833, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.9735, device='cuda:0')



h[100].sum tensor(20.0898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5829, device='cuda:0')



h[200].sum tensor(25.2154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.4208, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0107, 0.0464, 0.0302,  ..., 0.0412, 0.0399, 0.0423],
        [0.0089, 0.0398, 0.0255,  ..., 0.0348, 0.0342, 0.0361],
        [0.0163, 0.0686, 0.0454,  ..., 0.0618, 0.0591, 0.0629],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40502.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2115, 0.0000,  ..., 0.0894, 0.0000, 0.0613],
        [0.0000, 0.1717, 0.0000,  ..., 0.0743, 0.0000, 0.0480],
        [0.0000, 0.1866, 0.0000,  ..., 0.0803, 0.0000, 0.0524],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(205534.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-111.3164, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.9659, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-237.7103, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9971],
        [0.9912],
        ...,
        [0.9813],
        [0.9805],
        [0.9797]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362943.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9970],
        [0.9911],
        ...,
        [0.9812],
        [0.9804],
        [0.9796]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362930.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(456.7617, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9907, device='cuda:0')



h[100].sum tensor(18.2264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.0882, device='cuda:0')



h[200].sum tensor(21.2578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.0344, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0116, 0.0479, 0.0318,  ..., 0.0431, 0.0412, 0.0437],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28606.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1277, 0.0000,  ..., 0.0561, 0.0000, 0.0365],
        [0.0000, 0.0462, 0.0000,  ..., 0.0238, 0.0000, 0.0120],
        [0.0000, 0.0189, 0.0000,  ..., 0.0129, 0.0000, 0.0038],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(153361.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.0687, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-51.0184, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-151.3190, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9970],
        [0.9911],
        ...,
        [0.9812],
        [0.9804],
        [0.9796]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362930.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9970],
        [0.9911],
        ...,
        [0.9812],
        [0.9804],
        [0.9796]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362930.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(882.8710, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.4442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0886, device='cuda:0')



h[100].sum tensor(19.9859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3014, device='cuda:0')



h[200].sum tensor(24.9948, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.4229, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0053, 0.0249, 0.0154,  ..., 0.0211, 0.0213, 0.0222],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41481.4648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0142, 0.0000,  ..., 0.0111, 0.0000, 0.0024],
        [0.0000, 0.0273, 0.0000,  ..., 0.0163, 0.0000, 0.0065],
        [0.0000, 0.0781, 0.0000,  ..., 0.0369, 0.0000, 0.0201],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(220031.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-115.0914, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.7804, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-243.7891, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9970],
        [0.9911],
        ...,
        [0.9812],
        [0.9804],
        [0.9796]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362930.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9970],
        [0.9911],
        ...,
        [0.9811],
        [0.9803],
        [0.9796]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362917.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0015,  0.0079,  0.0048,  ...,  0.0066,  0.0068,  0.0071],
        [ 0.0016,  0.0083,  0.0051,  ...,  0.0070,  0.0071,  0.0075],
        [ 0.0037,  0.0158,  0.0104,  ...,  0.0141,  0.0136,  0.0144],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1002.0803, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.7885, device='cuda:0')



h[100].sum tensor(20.4672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1605, device='cuda:0')



h[200].sum tensor(26.0172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.4678, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0271, 0.0170,  ..., 0.0233, 0.0232, 0.0243],
        [0.0121, 0.0533, 0.0345,  ..., 0.0472, 0.0459, 0.0487],
        [0.0093, 0.0432, 0.0274,  ..., 0.0376, 0.0371, 0.0393],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45121.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1238, 0.0000,  ..., 0.0554, 0.0000, 0.0332],
        [0.0000, 0.1320, 0.0000,  ..., 0.0592, 0.0000, 0.0346],
        [0.0000, 0.1370, 0.0000,  ..., 0.0610, 0.0000, 0.0361],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(240768.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-133.1285, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.9314, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-270.8758, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9970],
        [0.9911],
        ...,
        [0.9811],
        [0.9803],
        [0.9796]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362917.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9970],
        [0.9910],
        ...,
        [0.9810],
        [0.9802],
        [0.9795]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362904.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0035,  0.0151,  0.0099,  ...,  0.0135,  0.0131,  0.0139],
        [ 0.0082,  0.0322,  0.0220,  ...,  0.0299,  0.0279,  0.0298],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(701.6440, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0089, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8026, device='cuda:0')



h[100].sum tensor(19.2212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9376, device='cuda:0')



h[200].sum tensor(23.3706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.5890, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0091, 0.0388, 0.0253,  ..., 0.0344, 0.0333, 0.0352],
        [0.0161, 0.0662, 0.0442,  ..., 0.0601, 0.0571, 0.0607],
        [0.0124, 0.0526, 0.0346,  ..., 0.0471, 0.0453, 0.0480],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34964.1289, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1456, 0.0000,  ..., 0.0635, 0.0000, 0.0413],
        [0.0000, 0.1849, 0.0000,  ..., 0.0789, 0.0000, 0.0536],
        [0.0000, 0.1721, 0.0000,  ..., 0.0740, 0.0000, 0.0492],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(184139.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-84.0782, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.4348, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-197.0839, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9970],
        [0.9910],
        ...,
        [0.9810],
        [0.9802],
        [0.9795]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362904.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9989],
        [0.9970],
        [0.9910],
        ...,
        [0.9809],
        [0.9802],
        [0.9794]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362890.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0034,  0.0149,  0.0097,  ...,  0.0133,  0.0128,  0.0136],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(970.6042, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.1001, device='cuda:0')



h[100].sum tensor(20.3165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9414, device='cuda:0')



h[200].sum tensor(25.6971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.6914, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0123, 0.0541, 0.0351,  ..., 0.0480, 0.0466, 0.0494],
        [0.0027, 0.0134, 0.0078,  ..., 0.0107, 0.0114, 0.0116],
        [0.0034, 0.0161, 0.0097,  ..., 0.0132, 0.0136, 0.0141],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41759.3867, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0963, 0.0000,  ..., 0.0444, 0.0000, 0.0251],
        [0.0000, 0.0591, 0.0000,  ..., 0.0294, 0.0000, 0.0145],
        [0.0000, 0.0759, 0.0000,  ..., 0.0360, 0.0000, 0.0197],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(213542.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-116.8795, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.0920, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-246.4596, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9989],
        [0.9970],
        [0.9910],
        ...,
        [0.9809],
        [0.9802],
        [0.9794]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362890.9062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 300.0 event: 1500 loss: tensor(434.1452, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9988],
        [0.9970],
        [0.9909],
        ...,
        [0.9808],
        [0.9801],
        [0.9793]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362877.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(766.3564, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1889, device='cuda:0')



h[100].sum tensor(19.4699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3787, device='cuda:0')



h[200].sum tensor(23.8989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.1525, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37303.5430, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0016, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0073, 0.0000, 0.0004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(195720.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-95.5405, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.2307, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-214.0814, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9988],
        [0.9970],
        [0.9909],
        ...,
        [0.9808],
        [0.9801],
        [0.9793]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362877.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9988],
        [0.9969],
        [0.9909],
        ...,
        [0.9807],
        [0.9800],
        [0.9792]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362864.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0066,  0.0264,  0.0179,  ...,  0.0243,  0.0228,  0.0244],
        [ 0.0037,  0.0160,  0.0105,  ...,  0.0143,  0.0138,  0.0146],
        [ 0.0016,  0.0081,  0.0049,  ...,  0.0068,  0.0069,  0.0073],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(514.3387, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1999, device='cuda:0')



h[100].sum tensor(18.4318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4730, device='cuda:0')



h[200].sum tensor(21.6939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.3981, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0184, 0.0766, 0.0511,  ..., 0.0695, 0.0661, 0.0703],
        [0.0178, 0.0744, 0.0495,  ..., 0.0674, 0.0642, 0.0683],
        [0.0154, 0.0655, 0.0432,  ..., 0.0589, 0.0565, 0.0601],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29530.7695, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2641, 0.0000,  ..., 0.1103, 0.0000, 0.0776],
        [0.0000, 0.2401, 0.0000,  ..., 0.1012, 0.0000, 0.0695],
        [0.0000, 0.2203, 0.0000,  ..., 0.0936, 0.0000, 0.0628],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(157189.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-57.9604, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.1611, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-158.0477, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9988],
        [0.9969],
        [0.9909],
        ...,
        [0.9807],
        [0.9800],
        [0.9792]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362864.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9988],
        [0.9969],
        [0.9908],
        ...,
        [0.9807],
        [0.9799],
        [0.9791]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362851.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0052,  0.0214,  0.0144,  ...,  0.0195,  0.0185,  0.0197],
        [ 0.0055,  0.0225,  0.0151,  ...,  0.0205,  0.0194,  0.0207],
        [ 0.0077,  0.0304,  0.0208,  ...,  0.0282,  0.0263,  0.0281],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(929.3433, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.6980, device='cuda:0')



h[100].sum tensor(20.1172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4953, device='cuda:0')



h[200].sum tensor(25.2737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.1101, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0296, 0.1173, 0.0799,  ..., 0.1084, 0.1014, 0.1082],
        [0.0220, 0.0877, 0.0594,  ..., 0.0806, 0.0757, 0.0807],
        [0.0117, 0.0481, 0.0319,  ..., 0.0433, 0.0414, 0.0438],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0020, 0.0107, 0.0060,  ..., 0.0082, 0.0090, 0.0091]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41323.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3582, 0.0000,  ..., 0.1452, 0.0000, 0.1102],
        [0.0000, 0.2721, 0.0000,  ..., 0.1121, 0.0000, 0.0825],
        [0.0000, 0.1808, 0.0000,  ..., 0.0768, 0.0000, 0.0534],
        ...,
        [0.0000, 0.0019, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0101, 0.0000,  ..., 0.0091, 0.0000, 0.0018],
        [0.0000, 0.0378, 0.0000,  ..., 0.0204, 0.0000, 0.0087]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(214640.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-113.9414, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.9040, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-242.6984, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9988],
        [0.9969],
        [0.9908],
        ...,
        [0.9807],
        [0.9799],
        [0.9791]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362851.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9988],
        [0.9969],
        [0.9908],
        ...,
        [0.9806],
        [0.9798],
        [0.9790]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362838., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056,  0.0226,  0.0152,  ...,  0.0206,  0.0195,  0.0208],
        [ 0.0030,  0.0133,  0.0086,  ...,  0.0118,  0.0114,  0.0121],
        [ 0.0016,  0.0081,  0.0049,  ...,  0.0068,  0.0069,  0.0073],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(620.2772, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5610, device='cuda:0')



h[100].sum tensor(18.8500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2243, device='cuda:0')



h[200].sum tensor(22.5822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.0609, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0157, 0.0665, 0.0439,  ..., 0.0598, 0.0573, 0.0609],
        [0.0124, 0.0544, 0.0353,  ..., 0.0483, 0.0469, 0.0497],
        [0.0063, 0.0321, 0.0195,  ..., 0.0269, 0.0275, 0.0290],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32493.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1464, 0.0000,  ..., 0.0643, 0.0000, 0.0401],
        [0.0000, 0.1324, 0.0000,  ..., 0.0590, 0.0000, 0.0350],
        [0.0000, 0.1099, 0.0000,  ..., 0.0506, 0.0000, 0.0270],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(170525.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-72.3937, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.5095, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-179.4286, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9988],
        [0.9969],
        [0.9908],
        ...,
        [0.9806],
        [0.9798],
        [0.9790]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362838., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9988],
        [0.9969],
        [0.9907],
        ...,
        [0.9805],
        [0.9797],
        [0.9789]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362824.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0052,  0.0214,  0.0143,  ...,  0.0195,  0.0184,  0.0197],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(702.7645, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5919, device='cuda:0')



h[100].sum tensor(19.1776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.8705, device='cuda:0')



h[200].sum tensor(23.2781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.3514, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0069, 0.0307, 0.0196,  ..., 0.0267, 0.0264, 0.0277],
        [0.0139, 0.0582, 0.0385,  ..., 0.0524, 0.0501, 0.0532],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34179.2461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0294, 0.0000,  ..., 0.0171, 0.0000, 0.0073],
        [0.0000, 0.0967, 0.0000,  ..., 0.0439, 0.0000, 0.0268],
        [0.0000, 0.1734, 0.0000,  ..., 0.0740, 0.0000, 0.0505],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(180792.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-79.9679, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.2804, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.2153, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9988],
        [0.9969],
        [0.9907],
        ...,
        [0.9805],
        [0.9797],
        [0.9789]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362824.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9988],
        [0.9968],
        [0.9907],
        ...,
        [0.9804],
        [0.9796],
        [0.9789]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362811.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0043,  0.0181,  0.0120,  ...,  0.0164,  0.0156,  0.0166],
        [ 0.0038,  0.0162,  0.0107,  ...,  0.0146,  0.0140,  0.0149],
        [ 0.0039,  0.0164,  0.0108,  ...,  0.0148,  0.0142,  0.0150],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(867.0622, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.2941, device='cuda:0')



h[100].sum tensor(19.8353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0485, device='cuda:0')



h[200].sum tensor(24.6750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.5267, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0169, 0.0711, 0.0471,  ..., 0.0642, 0.0613, 0.0652],
        [0.0107, 0.0482, 0.0309,  ..., 0.0423, 0.0415, 0.0439],
        [0.0111, 0.0496, 0.0319,  ..., 0.0437, 0.0427, 0.0452],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41061.7109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1931, 0.0000,  ..., 0.0819, 0.0000, 0.0564],
        [0.0000, 0.1432, 0.0000,  ..., 0.0629, 0.0000, 0.0396],
        [0.0000, 0.1222, 0.0000,  ..., 0.0547, 0.0000, 0.0329],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(218853.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-113.2644, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.6298, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-241.5283, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9988],
        [0.9968],
        [0.9907],
        ...,
        [0.9804],
        [0.9796],
        [0.9789]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362811.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9988],
        [0.9968],
        [0.9907],
        ...,
        [0.9803],
        [0.9795],
        [0.9788]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362798.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0020,  0.0097,  0.0060,  ...,  0.0083,  0.0083,  0.0088],
        [ 0.0012,  0.0065,  0.0038,  ...,  0.0053,  0.0056,  0.0058],
        [ 0.0037,  0.0158,  0.0104,  ...,  0.0141,  0.0136,  0.0144],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(521.1716, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2096, device='cuda:0')



h[100].sum tensor(18.4278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4761, device='cuda:0')



h[200].sum tensor(21.6855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.4090, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0271, 0.0170,  ..., 0.0232, 0.0232, 0.0243],
        [0.0156, 0.0660, 0.0436,  ..., 0.0594, 0.0569, 0.0605],
        [0.0088, 0.0394, 0.0252,  ..., 0.0345, 0.0338, 0.0357],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29775.7070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0984, 0.0000,  ..., 0.0451, 0.0000, 0.0259],
        [0.0000, 0.1476, 0.0000,  ..., 0.0649, 0.0000, 0.0404],
        [0.0000, 0.1282, 0.0000,  ..., 0.0572, 0.0000, 0.0344],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(160150.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-59.7726, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.8755, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-159.8842, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9988],
        [0.9968],
        [0.9907],
        ...,
        [0.9803],
        [0.9795],
        [0.9788]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362798.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9988],
        [0.9968],
        [0.9906],
        ...,
        [0.9802],
        [0.9795],
        [0.9787]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362784.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0045,  0.0187,  0.0125,  ...,  0.0169,  0.0161,  0.0172],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1008.9855, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.6184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.1883, device='cuda:0')



h[100].sum tensor(20.3895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9695, device='cuda:0')



h[200].sum tensor(25.8520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.7908, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0036, 0.0166, 0.0101,  ..., 0.0137, 0.0141, 0.0145],
        [0.0045, 0.0199, 0.0124,  ..., 0.0169, 0.0170, 0.0177],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43597.2773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0911, 0.0000,  ..., 0.0417, 0.0000, 0.0250],
        [0.0000, 0.0606, 0.0000,  ..., 0.0296, 0.0000, 0.0158],
        [0.0000, 0.0390, 0.0000,  ..., 0.0211, 0.0000, 0.0091],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(229500.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-125.6966, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.1758, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-260.0649, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9988],
        [0.9968],
        [0.9906],
        ...,
        [0.9802],
        [0.9795],
        [0.9787]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362784.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9988],
        [0.9968],
        [0.9906],
        ...,
        [0.9802],
        [0.9794],
        [0.9786]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362771.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(728.0894, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.6791, device='cuda:0')



h[100].sum tensor(19.2483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.8982, device='cuda:0')



h[200].sum tensor(23.4282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.4497, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0015, 0.0090, 0.0047,  ..., 0.0065, 0.0075, 0.0075],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35910.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0092, 0.0000, 0.0015],
        [0.0000, 0.0293, 0.0000,  ..., 0.0174, 0.0000, 0.0058],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(189233.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-88.8920, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.6199, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-203.8096, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9988],
        [0.9968],
        [0.9906],
        ...,
        [0.9802],
        [0.9794],
        [0.9786]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362771.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9987],
        [0.9968],
        [0.9905],
        ...,
        [0.9801],
        [0.9793],
        [0.9785]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362758.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0037,  0.0158,  0.0104,  ...,  0.0142,  0.0136,  0.0145],
        [ 0.0019,  0.0094,  0.0059,  ...,  0.0081,  0.0081,  0.0085],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(876.4993, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.4534, device='cuda:0')



h[100].sum tensor(19.8366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0992, device='cuda:0')



h[200].sum tensor(24.6778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.7065, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0078, 0.0379, 0.0236,  ..., 0.0324, 0.0325, 0.0343],
        [0.0043, 0.0232, 0.0137,  ..., 0.0190, 0.0198, 0.0207],
        [0.0134, 0.0580, 0.0379,  ..., 0.0517, 0.0500, 0.0531],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40712.3086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0847, 0.0000,  ..., 0.0403, 0.0000, 0.0201],
        [0.0000, 0.0930, 0.0000,  ..., 0.0436, 0.0000, 0.0226],
        [0.0000, 0.1546, 0.0000,  ..., 0.0678, 0.0000, 0.0418],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(215636.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-111.7347, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.2841, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-238.5601, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9987],
        [0.9968],
        [0.9905],
        ...,
        [0.9801],
        [0.9793],
        [0.9785]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362758.3125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 310.0 event: 1550 loss: tensor(641.1583, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9987],
        [0.9967],
        [0.9905],
        ...,
        [0.9800],
        [0.9792],
        [0.9784]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362745., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(521.3263, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0349, device='cuda:0')



h[100].sum tensor(18.4035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4205, device='cuda:0')



h[200].sum tensor(21.6339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.2121, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29439.1445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0028, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(157475.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-57.8106, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.3211, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-157.2763, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9987],
        [0.9967],
        [0.9905],
        ...,
        [0.9800],
        [0.9792],
        [0.9784]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362745., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9987],
        [0.9967],
        [0.9904],
        ...,
        [0.9799],
        [0.9791],
        [0.9783]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362731.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1437.7955, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.1915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.5570, device='cuda:0')



h[100].sum tensor(22.0642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.2688, device='cuda:0')



h[200].sum tensor(29.4090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.4847, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53374.3633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(267854.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-173.1480, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-27.5230, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-331.8079, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9987],
        [0.9967],
        [0.9904],
        ...,
        [0.9799],
        [0.9791],
        [0.9783]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362731.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9987],
        [0.9967],
        [0.9904],
        ...,
        [0.9798],
        [0.9790],
        [0.9782]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362718.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0083,  0.0050,  ...,  0.0070,  0.0071,  0.0075],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(420.6331, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.5298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.8338, device='cuda:0')



h[100].sum tensor(17.9892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.7201, device='cuda:0')



h[200].sum tensor(20.7540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.7296, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0016, 0.0095, 0.0050,  ..., 0.0069, 0.0079, 0.0079],
        [0.0056, 0.0258, 0.0161,  ..., 0.0220, 0.0221, 0.0231],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26888.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0172, 0.0000,  ..., 0.0123, 0.0000, 0.0038],
        [0.0000, 0.0512, 0.0000,  ..., 0.0262, 0.0000, 0.0123],
        [0.0000, 0.0963, 0.0000,  ..., 0.0445, 0.0000, 0.0249],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(145944.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-45.7820, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-52.3297, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-139.2529, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9987],
        [0.9967],
        [0.9904],
        ...,
        [0.9798],
        [0.9790],
        [0.9782]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362718.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9987],
        [0.9967],
        [0.9903],
        ...,
        [0.9797],
        [0.9789],
        [0.9781]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362705., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0087,  0.0340,  0.0233,  ...,  0.0316,  0.0294,  0.0314],
        [ 0.0073,  0.0290,  0.0197,  ...,  0.0268,  0.0250,  0.0267],
        [ 0.0070,  0.0278,  0.0189,  ...,  0.0256,  0.0240,  0.0256],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(646.1234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7756, device='cuda:0')



h[100].sum tensor(18.8825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2926, device='cuda:0')



h[200].sum tensor(22.6512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.3030, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0222, 0.0904, 0.0608,  ..., 0.0827, 0.0780, 0.0832],
        [0.0315, 0.1243, 0.0849,  ..., 0.1151, 0.1074, 0.1147],
        [0.0213, 0.0869, 0.0584,  ..., 0.0793, 0.0750, 0.0799],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32868.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2212, 0.0000,  ..., 0.0932, 0.0000, 0.0640],
        [0.0000, 0.2569, 0.0000,  ..., 0.1069, 0.0000, 0.0759],
        [0.0000, 0.2176, 0.0000,  ..., 0.0918, 0.0000, 0.0633],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(172267.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-74.4845, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.0932, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-182.2143, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9987],
        [0.9967],
        [0.9903],
        ...,
        [0.9797],
        [0.9789],
        [0.9781]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362705., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9987],
        [0.9966],
        [0.9903],
        ...,
        [0.9796],
        [0.9789],
        [0.9781]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362691.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(946.0607, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.3500, device='cuda:0')



h[100].sum tensor(20.0682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3845, device='cuda:0')



h[200].sum tensor(25.1695, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.7176, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0040, 0.0201, 0.0121,  ..., 0.0166, 0.0172, 0.0178],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42349.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0143, 0.0000,  ..., 0.0113, 0.0000, 0.0020],
        [0.0000, 0.0359, 0.0000,  ..., 0.0201, 0.0000, 0.0080],
        [0.0000, 0.0767, 0.0000,  ..., 0.0368, 0.0000, 0.0188],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(223951.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-119.0968, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.9805, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-250.0590, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9987],
        [0.9966],
        [0.9903],
        ...,
        [0.9796],
        [0.9789],
        [0.9781]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362691.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9987],
        [0.9966],
        [0.9902],
        ...,
        [0.9796],
        [0.9788],
        [0.9780]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362678.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0063,  0.0252,  0.0170,  ...,  0.0231,  0.0217,  0.0232],
        [ 0.0033,  0.0145,  0.0095,  ...,  0.0130,  0.0125,  0.0133],
        [ 0.0050,  0.0206,  0.0138,  ...,  0.0187,  0.0178,  0.0189],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0028,  0.0127,  0.0082,  ...,  0.0112,  0.0109,  0.0116],
        [ 0.0028,  0.0127,  0.0082,  ...,  0.0112,  0.0109,  0.0116]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(673.6514, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2209, device='cuda:0')



h[100].sum tensor(18.9777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4343, device='cuda:0')



h[200].sum tensor(22.8534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.8052, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0078, 0.0360, 0.0228,  ..., 0.0312, 0.0309, 0.0326],
        [0.0181, 0.0753, 0.0502,  ..., 0.0683, 0.0650, 0.0692],
        [0.0099, 0.0436, 0.0282,  ..., 0.0385, 0.0375, 0.0397],
        ...,
        [0.0049, 0.0232, 0.0143,  ..., 0.0196, 0.0198, 0.0207],
        [0.0049, 0.0232, 0.0143,  ..., 0.0196, 0.0198, 0.0207],
        [0.0049, 0.0232, 0.0143,  ..., 0.0196, 0.0198, 0.0207]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32632.2988, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1249, 0.0000,  ..., 0.0559, 0.0000, 0.0336],
        [0.0000, 0.1505, 0.0000,  ..., 0.0658, 0.0000, 0.0419],
        [0.0000, 0.1230, 0.0000,  ..., 0.0550, 0.0000, 0.0333],
        ...,
        [0.0000, 0.0500, 0.0000,  ..., 0.0255, 0.0000, 0.0118],
        [0.0000, 0.0605, 0.0000,  ..., 0.0298, 0.0000, 0.0147],
        [0.0000, 0.0605, 0.0000,  ..., 0.0298, 0.0000, 0.0147]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(167967.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-73.3646, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.0746, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-180.8028, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9987],
        [0.9966],
        [0.9902],
        ...,
        [0.9796],
        [0.9788],
        [0.9780]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362678.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9987],
        [0.9966],
        [0.9902],
        ...,
        [0.9795],
        [0.9787],
        [0.9779]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362664.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(535.2755, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1218, device='cuda:0')



h[100].sum tensor(18.4224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4481, device='cuda:0')



h[200].sum tensor(21.6741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.3100, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29301.4668, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0081, 0.0000, 0.0005],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(154623.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-57.4347, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.1177, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-156.7094, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9987],
        [0.9966],
        [0.9902],
        ...,
        [0.9795],
        [0.9787],
        [0.9779]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362664.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9987],
        [0.9966],
        [0.9901],
        ...,
        [0.9794],
        [0.9786],
        [0.9778]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362651.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [ 0.0012,  0.0066,  0.0039,  ...,  0.0054,  0.0057,  0.0059],
        [ 0.0023,  0.0107,  0.0068,  ...,  0.0093,  0.0092,  0.0097],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(675.7242, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3106, device='cuda:0')



h[100].sum tensor(18.9715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4628, device='cuda:0')



h[200].sum tensor(22.8403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.9064, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0128, 0.0558, 0.0364,  ..., 0.0497, 0.0481, 0.0510],
        [0.0042, 0.0207, 0.0125,  ..., 0.0172, 0.0176, 0.0184],
        [0.0022, 0.0115, 0.0066,  ..., 0.0090, 0.0097, 0.0099]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35163.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0222, 0.0000,  ..., 0.0144, 0.0000, 0.0049],
        [0.0000, 0.0046, 0.0000,  ..., 0.0072, 0.0000, 0.0004],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.1231, 0.0000,  ..., 0.0548, 0.0000, 0.0331],
        [0.0000, 0.0795, 0.0000,  ..., 0.0373, 0.0000, 0.0205],
        [0.0000, 0.0406, 0.0000,  ..., 0.0215, 0.0000, 0.0096]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(190455.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-85.4034, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.9416, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-198.8784, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9987],
        [0.9966],
        [0.9901],
        ...,
        [0.9794],
        [0.9786],
        [0.9778]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362651.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9986],
        [0.9966],
        [0.9901],
        ...,
        [0.9793],
        [0.9785],
        [0.9777]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362638.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(458.6456, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.2867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3888, device='cuda:0')



h[100].sum tensor(18.1081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.8967, device='cuda:0')



h[200].sum tensor(21.0063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.3555, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27706.6465, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0129, 0.0000,  ..., 0.0103, 0.0000, 0.0020],
        [0.0000, 0.0045, 0.0000,  ..., 0.0068, 0.0000, 0.0003],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(148658.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-49.4016, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-51.7712, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-144.8809, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9986],
        [0.9966],
        [0.9901],
        ...,
        [0.9793],
        [0.9785],
        [0.9777]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362638.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9986],
        [0.9965],
        [0.9900],
        ...,
        [0.9792],
        [0.9784],
        [0.9776]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362624.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0045,  0.0186,  0.0123,  ...,  0.0168,  0.0160,  0.0170]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1365.7434, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.9906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.7481, device='cuda:0')



h[100].sum tensor(21.6737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.3750, device='cuda:0')



h[200].sum tensor(28.5796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.3168, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0043, 0.0192, 0.0120,  ..., 0.0163, 0.0164, 0.0170],
        [0.0078, 0.0336, 0.0217,  ..., 0.0296, 0.0289, 0.0304]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53381.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0092, 0.0000, 0.0015],
        [0.0000, 0.0395, 0.0000,  ..., 0.0212, 0.0000, 0.0099],
        ...,
        [0.0000, 0.0159, 0.0000,  ..., 0.0113, 0.0000, 0.0037],
        [0.0000, 0.0560, 0.0000,  ..., 0.0272, 0.0000, 0.0152],
        [0.0000, 0.1049, 0.0000,  ..., 0.0465, 0.0000, 0.0300]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(277837.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-172.6291, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-28.4613, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-330.6013, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9986],
        [0.9965],
        [0.9900],
        ...,
        [0.9792],
        [0.9784],
        [0.9776]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362624.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 320.0 event: 1600 loss: tensor(630.2165, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9986],
        [0.9965],
        [0.9900],
        ...,
        [0.9791],
        [0.9783],
        [0.9775]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362611.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(435.1711, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.4974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.7598, device='cuda:0')



h[100].sum tensor(18.0051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.6965, device='cuda:0')



h[200].sum tensor(20.7876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.6461, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0015, 0.0108, 0.0054,  ..., 0.0076, 0.0091, 0.0091],
        [0.0007, 0.0062, 0.0027,  ..., 0.0038, 0.0051, 0.0049],
        [0.0099, 0.0417, 0.0274,  ..., 0.0373, 0.0359, 0.0379],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26423.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0360, 0.0000,  ..., 0.0204, 0.0000, 0.0061],
        [0.0000, 0.0526, 0.0000,  ..., 0.0269, 0.0000, 0.0118],
        [0.0000, 0.1175, 0.0000,  ..., 0.0523, 0.0000, 0.0324],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(140735.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-43.9022, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-52.5142, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-136.2211, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9986],
        [0.9965],
        [0.9900],
        ...,
        [0.9791],
        [0.9783],
        [0.9775]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362611.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9986],
        [0.9965],
        [0.9899],
        ...,
        [0.9791],
        [0.9782],
        [0.9774]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362597.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1247.7188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.9886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.9788, device='cuda:0')



h[100].sum tensor(21.1860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.4938, device='cuda:0')



h[200].sum tensor(27.5437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.1936, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0057, 0.0263, 0.0165,  ..., 0.0225, 0.0225, 0.0236],
        [0.0059, 0.0271, 0.0170,  ..., 0.0233, 0.0232, 0.0243],
        [0.0019, 0.0104, 0.0057,  ..., 0.0078, 0.0087, 0.0088],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47665.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0745, 0.0000,  ..., 0.0360, 0.0000, 0.0180],
        [0.0000, 0.0745, 0.0000,  ..., 0.0359, 0.0000, 0.0181],
        [0.0000, 0.0473, 0.0000,  ..., 0.0248, 0.0000, 0.0106],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(249961.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-144.4732, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-33.8691, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-288.8895, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9986],
        [0.9965],
        [0.9899],
        ...,
        [0.9791],
        [0.9782],
        [0.9774]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362597.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9986],
        [0.9965],
        [0.9899],
        ...,
        [0.9790],
        [0.9782],
        [0.9773]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362584.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0017,  0.0084,  0.0051,  ...,  0.0071,  0.0072,  0.0076],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1188.1235, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.4892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.2620, device='cuda:0')



h[100].sum tensor(20.9413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.9475, device='cuda:0')



h[200].sum tensor(27.0241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.2573, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0061, 0.0277, 0.0174,  ..., 0.0238, 0.0237, 0.0249],
        [0.0017, 0.0096, 0.0051,  ..., 0.0070, 0.0080, 0.0080],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49994.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0875, 0.0000,  ..., 0.0411, 0.0000, 0.0221],
        [0.0000, 0.0560, 0.0000,  ..., 0.0283, 0.0000, 0.0133],
        [0.0000, 0.0434, 0.0000,  ..., 0.0229, 0.0000, 0.0103],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(268307.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-156.7383, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-30.7364, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-307.1812, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9986],
        [0.9965],
        [0.9899],
        ...,
        [0.9790],
        [0.9782],
        [0.9773]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362584.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9986],
        [0.9964],
        [0.9898],
        ...,
        [0.9789],
        [0.9781],
        [0.9773]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362570.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0035,  0.0150,  0.0099,  ...,  0.0134,  0.0130,  0.0138],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(709.6393, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6378, device='cuda:0')



h[100].sum tensor(19.0616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5669, device='cuda:0')



h[200].sum tensor(23.0317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.2753, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0168, 0.0687, 0.0460,  ..., 0.0624, 0.0592, 0.0630],
        [0.0057, 0.0263, 0.0164,  ..., 0.0224, 0.0225, 0.0235],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35342.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2277, 0.0000,  ..., 0.0953, 0.0000, 0.0674],
        [0.0000, 0.1592, 0.0000,  ..., 0.0686, 0.0000, 0.0459],
        [0.0000, 0.1115, 0.0000,  ..., 0.0497, 0.0000, 0.0315],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(189492.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-85.6623, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.3295, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-199.3643, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9986],
        [0.9964],
        [0.9898],
        ...,
        [0.9789],
        [0.9781],
        [0.9773]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362570.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9986],
        [0.9964],
        [0.9898],
        ...,
        [0.9788],
        [0.9780],
        [0.9772]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362557.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1212.1206, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.3424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.7854, device='cuda:0')



h[100].sum tensor(21.0131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.1140, device='cuda:0')



h[200].sum tensor(27.1765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.8476, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51417.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0018, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(284856.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-163.7386, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-29.5687, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-317.2692, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9986],
        [0.9964],
        [0.9898],
        ...,
        [0.9788],
        [0.9780],
        [0.9772]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362557.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9986],
        [0.9964],
        [0.9897],
        ...,
        [0.9787],
        [0.9779],
        [0.9771]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362544.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0068,  0.0040,  ...,  0.0056,  0.0059,  0.0061],
        [ 0.0044,  0.0183,  0.0121,  ...,  0.0165,  0.0158,  0.0168],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(718.9127, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0686, device='cuda:0')



h[100].sum tensor(19.0834, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7040, device='cuda:0')



h[200].sum tensor(23.0779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.7612, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0240, 0.0969, 0.0654,  ..., 0.0889, 0.0837, 0.0892],
        [0.0061, 0.0279, 0.0176,  ..., 0.0240, 0.0239, 0.0251],
        [0.0044, 0.0194, 0.0121,  ..., 0.0165, 0.0166, 0.0172],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33779.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1892, 0.0000,  ..., 0.0806, 0.0000, 0.0545],
        [0.0000, 0.1097, 0.0000,  ..., 0.0492, 0.0000, 0.0301],
        [0.0000, 0.0622, 0.0000,  ..., 0.0302, 0.0000, 0.0161],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(178178.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-78.3241, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.5005, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-188.4447, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9986],
        [0.9964],
        [0.9897],
        ...,
        [0.9787],
        [0.9779],
        [0.9771]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362544.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9985],
        [0.9964],
        [0.9897],
        ...,
        [0.9786],
        [0.9778],
        [0.9770]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362530.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0022,  0.0103,  0.0065,  ...,  0.0089,  0.0089,  0.0094],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(981.4005, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.0748, device='cuda:0')



h[100].sum tensor(20.0953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.6152, device='cuda:0')



h[200].sum tensor(25.2272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.5351, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0077, 0.0376, 0.0234,  ..., 0.0322, 0.0323, 0.0340],
        [0.0017, 0.0097, 0.0052,  ..., 0.0072, 0.0081, 0.0082],
        [0.0080, 0.0347, 0.0224,  ..., 0.0305, 0.0298, 0.0314],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38701.7148, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0567, 0.0000,  ..., 0.0287, 0.0000, 0.0130],
        [0.0000, 0.0614, 0.0000,  ..., 0.0300, 0.0000, 0.0156],
        [0.0000, 0.1147, 0.0000,  ..., 0.0508, 0.0000, 0.0324],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(193644.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-102.3315, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.7191, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-224.5215, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9985],
        [0.9964],
        [0.9897],
        ...,
        [0.9786],
        [0.9778],
        [0.9770]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362530.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9985],
        [0.9964],
        [0.9896],
        ...,
        [0.9785],
        [0.9777],
        [0.9769]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362517.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0019,  0.0092,  0.0057,  ...,  0.0079,  0.0079,  0.0083],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(648.4327, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3045, device='cuda:0')



h[100].sum tensor(18.7959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1426, device='cuda:0')



h[200].sum tensor(22.4674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.7716, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0063, 0.0286, 0.0181,  ..., 0.0247, 0.0245, 0.0257],
        [0.0019, 0.0104, 0.0057,  ..., 0.0078, 0.0087, 0.0088],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33098.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1393, 0.0000,  ..., 0.0610, 0.0000, 0.0394],
        [0.0000, 0.0778, 0.0000,  ..., 0.0366, 0.0000, 0.0209],
        [0.0000, 0.0440, 0.0000,  ..., 0.0228, 0.0000, 0.0117],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(176853.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-75.1744, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.9818, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-183.7328, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9985],
        [0.9964],
        [0.9896],
        ...,
        [0.9785],
        [0.9777],
        [0.9769]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362517.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9985],
        [0.9963],
        [0.9896],
        ...,
        [0.9785],
        [0.9776],
        [0.9768]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362503.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(833.5995, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.2347, device='cuda:0')



h[100].sum tensor(19.5055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3932, device='cuda:0')



h[200].sum tensor(23.9744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.2041, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0031, 0.0170, 0.0098,  ..., 0.0135, 0.0144, 0.0149],
        [0.0007, 0.0062, 0.0027,  ..., 0.0038, 0.0051, 0.0049],
        [0.0017, 0.0099, 0.0054,  ..., 0.0074, 0.0083, 0.0084],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38703.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0577, 0.0000,  ..., 0.0294, 0.0000, 0.0123],
        [0.0000, 0.0488, 0.0000,  ..., 0.0257, 0.0000, 0.0100],
        [0.0000, 0.0531, 0.0000,  ..., 0.0275, 0.0000, 0.0114],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(207739.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-102.2937, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.9941, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-224.0943, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9985],
        [0.9963],
        [0.9896],
        ...,
        [0.9785],
        [0.9776],
        [0.9768]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362503.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9985],
        [0.9963],
        [0.9895],
        ...,
        [0.9784],
        [0.9776],
        [0.9767]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362490.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0013,  0.0071,  0.0043,  ...,  0.0059,  0.0061,  0.0064],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(931.5500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.2560, device='cuda:0')



h[100].sum tensor(19.8756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0364, device='cuda:0')



h[200].sum tensor(24.7606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.4838, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0039, 0.0198, 0.0118,  ..., 0.0163, 0.0169, 0.0175],
        [0.0050, 0.0257, 0.0155,  ..., 0.0214, 0.0220, 0.0230],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39634.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0626, 0.0000,  ..., 0.0309, 0.0000, 0.0151],
        [0.0000, 0.0726, 0.0000,  ..., 0.0355, 0.0000, 0.0167],
        [0.0000, 0.0913, 0.0000,  ..., 0.0432, 0.0000, 0.0216],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(206936.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-106.9784, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.9181, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-231.1605, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9985],
        [0.9963],
        [0.9895],
        ...,
        [0.9784],
        [0.9776],
        [0.9767]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362490.0625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 330.0 event: 1650 loss: tensor(634.4520, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9985],
        [0.9963],
        [0.9895],
        ...,
        [0.9783],
        [0.9775],
        [0.9766]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362476.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0089,  0.0348,  0.0239,  ...,  0.0324,  0.0301,  0.0322],
        [ 0.0058,  0.0236,  0.0159,  ...,  0.0216,  0.0204,  0.0217],
        [ 0.0028,  0.0127,  0.0082,  ...,  0.0112,  0.0109,  0.0116],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(801.5806, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.4399, device='cuda:0')



h[100].sum tensor(19.3663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.1403, device='cuda:0')



h[200].sum tensor(23.6788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.3078, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0265, 0.1059, 0.0718,  ..., 0.0975, 0.0915, 0.0976],
        [0.0260, 0.1040, 0.0705,  ..., 0.0957, 0.0898, 0.0958],
        [0.0127, 0.0519, 0.0346,  ..., 0.0470, 0.0447, 0.0474],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36661.3828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2736, 0.0000,  ..., 0.1134, 0.0000, 0.0820],
        [0.0000, 0.2560, 0.0000,  ..., 0.1065, 0.0000, 0.0762],
        [0.0000, 0.1826, 0.0000,  ..., 0.0778, 0.0000, 0.0532],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(193784.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-92.3002, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.8460, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-209.3372, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9985],
        [0.9963],
        [0.9895],
        ...,
        [0.9783],
        [0.9775],
        [0.9766]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362476.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9985],
        [0.9963],
        [0.9894],
        ...,
        [0.9782],
        [0.9774],
        [0.9765]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362463.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(913.9361, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.1747, device='cuda:0')



h[100].sum tensor(19.7907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0106, device='cuda:0')



h[200].sum tensor(24.5802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.3921, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38788.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(202948.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-102.7814, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.5837, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-225.1539, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9985],
        [0.9963],
        [0.9894],
        ...,
        [0.9782],
        [0.9774],
        [0.9765]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362463.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9985],
        [0.9962],
        [0.9894],
        ...,
        [0.9781],
        [0.9773],
        [0.9765]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362449.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(646.5304, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8662, device='cuda:0')



h[100].sum tensor(18.7560, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0032, device='cuda:0')



h[200].sum tensor(22.3826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.2774, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33071.6133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0146, 0.0000,  ..., 0.0112, 0.0000, 0.0028],
        [0.0000, 0.0017, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(177320.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-75.4276, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.8257, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-183.7410, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9985],
        [0.9962],
        [0.9894],
        ...,
        [0.9781],
        [0.9773],
        [0.9765]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362449.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9985],
        [0.9962],
        [0.9893],
        ...,
        [0.9780],
        [0.9772],
        [0.9764]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362435.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0027,  0.0123,  0.0079,  ...,  0.0108,  0.0106,  0.0112],
        [ 0.0033,  0.0143,  0.0093,  ...,  0.0127,  0.0123,  0.0131],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(986.5569, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.2896, device='cuda:0')



h[100].sum tensor(20.0521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3653, device='cuda:0')



h[200].sum tensor(25.1354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.6495, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0305, 0.1206, 0.0822,  ..., 0.1115, 0.1042, 0.1112],
        [0.0103, 0.0431, 0.0283,  ..., 0.0385, 0.0371, 0.0392],
        [0.0033, 0.0155, 0.0093,  ..., 0.0127, 0.0131, 0.0135],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40810.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.8355e-01, 0.0000e+00,  ..., 1.5448e-01, 0.0000e+00,
         1.1888e-01],
        [0.0000e+00, 2.2221e-01, 0.0000e+00,  ..., 9.2271e-02, 0.0000e+00,
         6.7210e-02],
        [0.0000e+00, 1.1276e-01, 0.0000e+00,  ..., 4.9859e-02, 0.0000e+00,
         3.2397e-02],
        ...,
        [0.0000e+00, 3.5048e-04, 0.0000e+00,  ..., 5.0788e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5073e-04, 0.0000e+00,  ..., 5.0783e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5074e-04, 0.0000e+00,  ..., 5.0781e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(211581.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-112.2779, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.9047, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-239.6010, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9985],
        [0.9962],
        [0.9893],
        ...,
        [0.9780],
        [0.9772],
        [0.9764]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362435.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9985],
        [0.9962],
        [0.9892],
        ...,
        [0.9779],
        [0.9771],
        [0.9763]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362422.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(770.0625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5574, device='cuda:0')



h[100].sum tensor(19.2155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.8595, device='cuda:0')



h[200].sum tensor(23.3586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.3125, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0022, 0.0136, 0.0074,  ..., 0.0103, 0.0115, 0.0118],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37054.0664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0622, 0.0000,  ..., 0.0303, 0.0000, 0.0162],
        [0.0000, 0.0864, 0.0000,  ..., 0.0399, 0.0000, 0.0234],
        [0.0000, 0.1160, 0.0000,  ..., 0.0520, 0.0000, 0.0314],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(198082.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-94.2365, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.5134, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-212.1366, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9985],
        [0.9962],
        [0.9892],
        ...,
        [0.9779],
        [0.9771],
        [0.9763]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362422.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9984],
        [0.9962],
        [0.9892],
        ...,
        [0.9779],
        [0.9770],
        [0.9762]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362408.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0060,  0.0242,  0.0163,  ...,  0.0222,  0.0209,  0.0222],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(643.0654, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9438, device='cuda:0')



h[100].sum tensor(18.7237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0279, device='cuda:0')



h[200].sum tensor(22.3140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.3648, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0060, 0.0253, 0.0163,  ..., 0.0221, 0.0217, 0.0227],
        [0.0048, 0.0209, 0.0132,  ..., 0.0179, 0.0179, 0.0186],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30982.7207, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0176, 0.0000,  ..., 0.0123, 0.0000, 0.0044],
        [0.0000, 0.0591, 0.0000,  ..., 0.0286, 0.0000, 0.0164],
        [0.0000, 0.0829, 0.0000,  ..., 0.0380, 0.0000, 0.0237],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(161166.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-64.9405, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.8485, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-168.4832, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9984],
        [0.9962],
        [0.9892],
        ...,
        [0.9779],
        [0.9770],
        [0.9762]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362408.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9984],
        [0.9962],
        [0.9891],
        ...,
        [0.9778],
        [0.9769],
        [0.9761]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362395.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049,  0.0202,  0.0135,  ...,  0.0183,  0.0174,  0.0185],
        [ 0.0037,  0.0159,  0.0105,  ...,  0.0143,  0.0137,  0.0146],
        [ 0.0112,  0.0433,  0.0298,  ...,  0.0404,  0.0374,  0.0400],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(880.0580, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7495, device='cuda:0')



h[100].sum tensor(19.6200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5571, device='cuda:0')



h[200].sum tensor(24.2177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7848, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0111, 0.0459, 0.0303,  ..., 0.0412, 0.0395, 0.0418],
        [0.0306, 0.1208, 0.0824,  ..., 0.1117, 0.1044, 0.1114],
        [0.0284, 0.1130, 0.0769,  ..., 0.1043, 0.0976, 0.1042],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37536.4023, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1643, 0.0000,  ..., 0.0699, 0.0000, 0.0490],
        [0.0000, 0.2796, 0.0000,  ..., 0.1149, 0.0000, 0.0851],
        [0.0000, 0.3227, 0.0000,  ..., 0.1321, 0.0000, 0.0981],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(193769.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-97.0639, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.4449, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-216.5210, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9984],
        [0.9962],
        [0.9891],
        ...,
        [0.9778],
        [0.9769],
        [0.9761]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362395.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9984],
        [0.9961],
        [0.9891],
        ...,
        [0.9777],
        [0.9769],
        [0.9760]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362381.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0027,  0.0121,  0.0078,  ...,  0.0106,  0.0104,  0.0110],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0012,  0.0065,  0.0038,  ...,  0.0053,  0.0056,  0.0059],
        ...,
        [ 0.0050,  0.0207,  0.0139,  ...,  0.0189,  0.0179,  0.0190],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1203.2928, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.6949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.5184, device='cuda:0')



h[100].sum tensor(20.8408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.7109, device='cuda:0')



h[200].sum tensor(26.8106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.4187, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0101, 0.0441, 0.0285,  ..., 0.0389, 0.0379, 0.0401],
        [0.0068, 0.0339, 0.0208,  ..., 0.0287, 0.0291, 0.0307],
        [0.0031, 0.0187, 0.0105,  ..., 0.0146, 0.0159, 0.0165],
        ...,
        [0.0039, 0.0177, 0.0109,  ..., 0.0148, 0.0150, 0.0156],
        [0.0049, 0.0213, 0.0134,  ..., 0.0183, 0.0182, 0.0189],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44753.9727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1677, 0.0000,  ..., 0.0728, 0.0000, 0.0465],
        [0.0000, 0.1280, 0.0000,  ..., 0.0577, 0.0000, 0.0328],
        [0.0000, 0.0925, 0.0000,  ..., 0.0438, 0.0000, 0.0216],
        ...,
        [0.0000, 0.0581, 0.0000,  ..., 0.0281, 0.0000, 0.0156],
        [0.0000, 0.0452, 0.0000,  ..., 0.0230, 0.0000, 0.0118],
        [0.0000, 0.0144, 0.0000,  ..., 0.0107, 0.0000, 0.0034]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(226482.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-131.4002, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.8799, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-268.7324, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9984],
        [0.9961],
        [0.9891],
        ...,
        [0.9777],
        [0.9769],
        [0.9760]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362381.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9984],
        [0.9961],
        [0.9890],
        ...,
        [0.9776],
        [0.9768],
        [0.9759]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362368.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(904.1123, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.0386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.4291, device='cuda:0')



h[100].sum tensor(19.6954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7733, device='cuda:0')



h[200].sum tensor(24.3778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.5512, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40195.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(215880.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-109.5900, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.3785, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-235.2789, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9984],
        [0.9961],
        [0.9890],
        ...,
        [0.9776],
        [0.9768],
        [0.9759]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362368.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9984],
        [0.9961],
        [0.9890],
        ...,
        [0.9775],
        [0.9767],
        [0.9758]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362354.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1206.7719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.7097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.6333, device='cuda:0')



h[100].sum tensor(20.8335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.7475, device='cuda:0')



h[200].sum tensor(26.7952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.5483, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46269.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(238252.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-138.3012, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-34.6248, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-279.4860, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9984],
        [0.9961],
        [0.9890],
        ...,
        [0.9775],
        [0.9767],
        [0.9758]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362354.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9984],
        [0.9961],
        [0.9890],
        ...,
        [0.9775],
        [0.9767],
        [0.9758]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362354.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [ 0.0024,  0.0110,  0.0070,  ...,  0.0095,  0.0094,  0.0100],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1056.6603, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.8729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.6231, device='cuda:0')



h[100].sum tensor(20.2651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7896, device='cuda:0')



h[200].sum tensor(25.5878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.1534, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0075, 0.0326, 0.0210,  ..., 0.0286, 0.0280, 0.0295],
        [0.0023, 0.0118, 0.0067,  ..., 0.0092, 0.0100, 0.0101],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43585.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0076, 0.0000,  ..., 0.0083, 0.0000, 0.0012],
        [0.0000, 0.0268, 0.0000,  ..., 0.0161, 0.0000, 0.0061],
        [0.0000, 0.0509, 0.0000,  ..., 0.0260, 0.0000, 0.0122],
        ...,
        [0.0000, 0.1227, 0.0000,  ..., 0.0539, 0.0000, 0.0347],
        [0.0000, 0.0629, 0.0000,  ..., 0.0302, 0.0000, 0.0167],
        [0.0000, 0.0196, 0.0000,  ..., 0.0128, 0.0000, 0.0048]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(237100.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-125.4470, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.4129, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-259.6360, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9984],
        [0.9961],
        [0.9890],
        ...,
        [0.9775],
        [0.9767],
        [0.9758]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362354.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9984],
        [0.9961],
        [0.9889],
        ...,
        [0.9774],
        [0.9766],
        [0.9757]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362340.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(643.9140, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9063, device='cuda:0')



h[100].sum tensor(18.6958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0160, device='cuda:0')



h[200].sum tensor(22.2547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.3226, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0017, 0.0099, 0.0053,  ..., 0.0073, 0.0083, 0.0083],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30699.0820, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.4867e-02, 0.0000e+00,  ..., 1.9696e-02, 0.0000e+00,
         7.2162e-03],
        [0.0000e+00, 1.0370e-02, 0.0000e+00,  ..., 9.5450e-03, 0.0000e+00,
         1.6917e-03],
        [0.0000e+00, 3.3793e-03, 0.0000e+00,  ..., 6.6402e-03, 0.0000e+00,
         5.5470e-06],
        ...,
        [0.0000e+00, 3.5108e-04, 0.0000e+00,  ..., 5.0727e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5134e-04, 0.0000e+00,  ..., 5.0722e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5134e-04, 0.0000e+00,  ..., 5.0720e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(161587.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-64.0839, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.9362, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-166.6710, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9984],
        [0.9961],
        [0.9889],
        ...,
        [0.9774],
        [0.9766],
        [0.9757]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362340.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9984],
        [0.9960],
        [0.9889],
        ...,
        [0.9773],
        [0.9765],
        [0.9757]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362327.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0025,  0.0115,  0.0073,  ...,  0.0100,  0.0099,  0.0105],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0081,  0.0320,  0.0218,  ...,  0.0296,  0.0276,  0.0295],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(909.1333, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.0490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.4134, device='cuda:0')



h[100].sum tensor(19.6903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7683, device='cuda:0')



h[200].sum tensor(24.3670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.5335, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0020, 0.0107, 0.0059,  ..., 0.0081, 0.0090, 0.0091],
        [0.0145, 0.0622, 0.0409,  ..., 0.0557, 0.0536, 0.0570],
        [0.0072, 0.0317, 0.0203,  ..., 0.0276, 0.0272, 0.0286],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39399.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1156, 0.0000,  ..., 0.0517, 0.0000, 0.0318],
        [0.0000, 0.1558, 0.0000,  ..., 0.0678, 0.0000, 0.0436],
        [0.0000, 0.1517, 0.0000,  ..., 0.0660, 0.0000, 0.0428],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(206964.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-104.8413, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.4834, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-228.9665, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9984],
        [0.9960],
        [0.9889],
        ...,
        [0.9773],
        [0.9765],
        [0.9757]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362327.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9984],
        [0.9960],
        [0.9888],
        ...,
        [0.9773],
        [0.9764],
        [0.9756]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362313.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0036,  0.0153,  0.0100,  ...,  0.0137,  0.0132,  0.0140],
        [ 0.0021,  0.0098,  0.0061,  ...,  0.0084,  0.0084,  0.0089],
        [ 0.0015,  0.0077,  0.0047,  ...,  0.0065,  0.0066,  0.0070],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(608.6793, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8593, device='cuda:0')



h[100].sum tensor(18.5509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.6828, device='cuda:0')



h[200].sum tensor(21.9468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.1418, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0106, 0.0479, 0.0307,  ..., 0.0421, 0.0412, 0.0437],
        [0.0094, 0.0435, 0.0276,  ..., 0.0378, 0.0374, 0.0395],
        [0.0032, 0.0170, 0.0098,  ..., 0.0136, 0.0144, 0.0149],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30363.3887, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1106, 0.0000,  ..., 0.0506, 0.0000, 0.0280],
        [0.0000, 0.0914, 0.0000,  ..., 0.0428, 0.0000, 0.0226],
        [0.0000, 0.0528, 0.0000,  ..., 0.0271, 0.0000, 0.0118],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(162355.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-62.4140, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.3441, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.0803, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9984],
        [0.9960],
        [0.9888],
        ...,
        [0.9773],
        [0.9764],
        [0.9756]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362313.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9983],
        [0.9960],
        [0.9888],
        ...,
        [0.9772],
        [0.9763],
        [0.9755]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362300., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(876.7487, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.3307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.4586, device='cuda:0')



h[100].sum tensor(19.5527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.4645, device='cuda:0')



h[200].sum tensor(24.0747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.4566, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0077, 0.0334, 0.0215,  ..., 0.0293, 0.0287, 0.0302],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38117.0977, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0094, 0.0000,  ..., 0.0090, 0.0000, 0.0018],
        [0.0000, 0.0345, 0.0000,  ..., 0.0191, 0.0000, 0.0087],
        [0.0000, 0.1115, 0.0000,  ..., 0.0497, 0.0000, 0.0313],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(198194.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-99.3148, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.2934, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-220.1519, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9983],
        [0.9960],
        [0.9888],
        ...,
        [0.9772],
        [0.9763],
        [0.9755]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362300., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9983],
        [0.9960],
        [0.9887],
        ...,
        [0.9771],
        [0.9762],
        [0.9754]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362286.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0016,  0.0082,  0.0050,  ...,  0.0069,  0.0071,  0.0074],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(629.4752, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.3026, device='cuda:0')



h[100].sum tensor(18.6172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8239, device='cuda:0')



h[200].sum tensor(22.0878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.6418, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0054, 0.0272, 0.0165,  ..., 0.0228, 0.0233, 0.0244],
        [0.0044, 0.0215, 0.0130,  ..., 0.0179, 0.0183, 0.0191],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32762.8457, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0799, 0.0000,  ..., 0.0385, 0.0000, 0.0186],
        [0.0000, 0.0621, 0.0000,  ..., 0.0310, 0.0000, 0.0140],
        [0.0000, 0.0259, 0.0000,  ..., 0.0161, 0.0000, 0.0049],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(180262.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-74.5790, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.8153, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-181.9306, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9983],
        [0.9960],
        [0.9887],
        ...,
        [0.9771],
        [0.9762],
        [0.9754]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362286.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9983],
        [0.9959],
        [0.9887],
        ...,
        [0.9770],
        [0.9762],
        [0.9753]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362272.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(907.6109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.1988, device='cuda:0')



h[100].sum tensor(19.6529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7000, device='cuda:0')



h[200].sum tensor(24.2876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.2915, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0016, 0.0112, 0.0057,  ..., 0.0080, 0.0094, 0.0095],
        [0.0031, 0.0168, 0.0097,  ..., 0.0134, 0.0143, 0.0148],
        [0.0046, 0.0224, 0.0137,  ..., 0.0188, 0.0191, 0.0200],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38657.4180, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0540, 0.0000,  ..., 0.0279, 0.0000, 0.0112],
        [0.0000, 0.0584, 0.0000,  ..., 0.0295, 0.0000, 0.0129],
        [0.0000, 0.0638, 0.0000,  ..., 0.0316, 0.0000, 0.0149],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(202733.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-102.5853, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.4175, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-224.5985, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9983],
        [0.9959],
        [0.9887],
        ...,
        [0.9770],
        [0.9762],
        [0.9753]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362272.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9983],
        [0.9959],
        [0.9886],
        ...,
        [0.9769],
        [0.9761],
        [0.9752]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362259.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0020,  0.0095,  0.0060,  ...,  0.0082,  0.0082,  0.0087],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0072,  0.0286,  0.0194,  ...,  0.0264,  0.0247,  0.0263],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(708.6656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7885, device='cuda:0')



h[100].sum tensor(18.9015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2967, device='cuda:0')



h[200].sum tensor(22.6915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.3175, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0015, 0.0091, 0.0048,  ..., 0.0066, 0.0076, 0.0076],
        [0.0091, 0.0388, 0.0253,  ..., 0.0345, 0.0334, 0.0352],
        [0.0057, 0.0245, 0.0157,  ..., 0.0213, 0.0210, 0.0219],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32554.2559, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0647, 0.0000,  ..., 0.0319, 0.0000, 0.0153],
        [0.0000, 0.0946, 0.0000,  ..., 0.0432, 0.0000, 0.0257],
        [0.0000, 0.0920, 0.0000,  ..., 0.0418, 0.0000, 0.0258],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(169049.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-73.0524, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.2545, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-180.0249, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9983],
        [0.9959],
        [0.9886],
        ...,
        [0.9769],
        [0.9761],
        [0.9752]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362259.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9983],
        [0.9959],
        [0.9886],
        ...,
        [0.9768],
        [0.9760],
        [0.9751]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362245.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0019,  0.0092,  0.0057,  ...,  0.0078,  0.0079,  0.0083],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(711.0929, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8563, device='cuda:0')



h[100].sum tensor(18.9041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.3183, device='cuda:0')



h[200].sum tensor(22.6971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.3941, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0040, 0.0202, 0.0121,  ..., 0.0166, 0.0172, 0.0179],
        [0.0019, 0.0104, 0.0057,  ..., 0.0078, 0.0087, 0.0088],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33584.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0926, 0.0000,  ..., 0.0427, 0.0000, 0.0241],
        [0.0000, 0.0424, 0.0000,  ..., 0.0226, 0.0000, 0.0094],
        [0.0000, 0.0135, 0.0000,  ..., 0.0109, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(177190.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-77.6099, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.4295, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-187.3713, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9983],
        [0.9959],
        [0.9886],
        ...,
        [0.9768],
        [0.9760],
        [0.9751]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362245.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9983],
        [0.9959],
        [0.9885],
        ...,
        [0.9767],
        [0.9759],
        [0.9750]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362231.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0017,  0.0085,  0.0052,  ...,  0.0072,  0.0073,  0.0077],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(627.4414, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0136, device='cuda:0')



h[100].sum tensor(18.5862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7319, device='cuda:0')



h[200].sum tensor(22.0218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.3157, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0043, 0.0212, 0.0128,  ..., 0.0176, 0.0181, 0.0188],
        [0.0017, 0.0097, 0.0052,  ..., 0.0072, 0.0081, 0.0082],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31850.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0730, 0.0000,  ..., 0.0353, 0.0000, 0.0176],
        [0.0000, 0.0439, 0.0000,  ..., 0.0233, 0.0000, 0.0098],
        [0.0000, 0.0264, 0.0000,  ..., 0.0160, 0.0000, 0.0052],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(171889.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-69.2344, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.3777, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-174.2375, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9983],
        [0.9959],
        [0.9885],
        ...,
        [0.9767],
        [0.9759],
        [0.9750]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362231.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 350.0 event: 1750 loss: tensor(1201.8411, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9983],
        [0.9959],
        [0.9885],
        ...,
        [0.9767],
        [0.9758],
        [0.9749]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362218., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0084,  0.0330,  0.0225,  ...,  0.0306,  0.0285,  0.0304],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0050,  0.0206,  0.0138,  ...,  0.0188,  0.0178,  0.0190],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1032.0833, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.6952, device='cuda:0')



h[100].sum tensor(20.0847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4944, device='cuda:0')



h[200].sum tensor(25.2046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.1069, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0228, 0.0906, 0.0615,  ..., 0.0834, 0.0782, 0.0833],
        [0.0268, 0.1069, 0.0725,  ..., 0.0985, 0.0923, 0.0985],
        [0.0056, 0.0260, 0.0162,  ..., 0.0222, 0.0222, 0.0233],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40855.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3379, 0.0000,  ..., 0.1374, 0.0000, 0.1038],
        [0.0000, 0.2695, 0.0000,  ..., 0.1113, 0.0000, 0.0811],
        [0.0000, 0.1537, 0.0000,  ..., 0.0667, 0.0000, 0.0433],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(209372.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-112.9265, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.5935, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-240.2976, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9983],
        [0.9959],
        [0.9885],
        ...,
        [0.9767],
        [0.9758],
        [0.9749]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362218., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9983],
        [0.9958],
        [0.9884],
        ...,
        [0.9766],
        [0.9757],
        [0.9749]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362204.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0046,  0.0191,  0.0127,  ...,  0.0173,  0.0165,  0.0175],
        [ 0.0046,  0.0190,  0.0127,  ...,  0.0173,  0.0164,  0.0175],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(809.1526, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9912, device='cuda:0')



h[100].sum tensor(19.2489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9976, device='cuda:0')



h[200].sum tensor(23.4295, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.8018, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0046, 0.0203, 0.0127,  ..., 0.0173, 0.0173, 0.0180],
        [0.0082, 0.0354, 0.0229,  ..., 0.0312, 0.0304, 0.0321],
        [0.0264, 0.1054, 0.0715,  ..., 0.0971, 0.0911, 0.0972],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34123.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0630, 0.0000,  ..., 0.0302, 0.0000, 0.0172],
        [0.0000, 0.1165, 0.0000,  ..., 0.0513, 0.0000, 0.0337],
        [0.0000, 0.2080, 0.0000,  ..., 0.0869, 0.0000, 0.0626],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(179007.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.0128, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.0234, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.0861, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9983],
        [0.9958],
        [0.9884],
        ...,
        [0.9766],
        [0.9757],
        [0.9749]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362204.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9982],
        [0.9958],
        [0.9884],
        ...,
        [0.9765],
        [0.9756],
        [0.9748]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362190.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0028,  0.0124,  0.0080,  ...,  0.0109,  0.0107,  0.0113],
        [ 0.0104,  0.0401,  0.0276,  ...,  0.0374,  0.0347,  0.0371],
        [ 0.0046,  0.0189,  0.0126,  ...,  0.0172,  0.0163,  0.0174],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(586.5233, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2185, device='cuda:0')



h[100].sum tensor(18.4173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4789, device='cuda:0')



h[200].sum tensor(21.6632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.4191, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0303, 0.1198, 0.0817,  ..., 0.1108, 0.1035, 0.1105],
        [0.0220, 0.0895, 0.0602,  ..., 0.0818, 0.0772, 0.0823],
        [0.0224, 0.0908, 0.0612,  ..., 0.0831, 0.0784, 0.0836],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30347.1133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2337, 0.0000,  ..., 0.0974, 0.0000, 0.0697],
        [0.0000, 0.2425, 0.0000,  ..., 0.1012, 0.0000, 0.0718],
        [0.0000, 0.2392, 0.0000,  ..., 0.0999, 0.0000, 0.0708],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(163813.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-62.4218, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.1941, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.2076, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9982],
        [0.9958],
        [0.9884],
        ...,
        [0.9765],
        [0.9756],
        [0.9748]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362190.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9982],
        [0.9958],
        [0.9883],
        ...,
        [0.9764],
        [0.9755],
        [0.9747]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362176.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1168.9642, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.2584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.4827, device='cuda:0')



h[100].sum tensor(20.5654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.3814, device='cuda:0')



h[200].sum tensor(26.2257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.2507, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0018, 0.0101, 0.0055,  ..., 0.0075, 0.0084, 0.0085],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0078, 0.0340, 0.0219,  ..., 0.0298, 0.0291, 0.0307],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44690.9805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0655, 0.0000,  ..., 0.0320, 0.0000, 0.0160],
        [0.0000, 0.0808, 0.0000,  ..., 0.0376, 0.0000, 0.0219],
        [0.0000, 0.1758, 0.0000,  ..., 0.0744, 0.0000, 0.0521],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(236156.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-131.3553, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.8921, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-268.2870, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9982],
        [0.9958],
        [0.9883],
        ...,
        [0.9764],
        [0.9755],
        [0.9747]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362176.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9982],
        [0.9958],
        [0.9883],
        ...,
        [0.9763],
        [0.9754],
        [0.9746]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362163.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0033,  0.0145,  0.0095,  ...,  0.0130,  0.0125,  0.0133],
        [ 0.0014,  0.0073,  0.0044,  ...,  0.0060,  0.0062,  0.0065],
        [ 0.0056,  0.0228,  0.0153,  ...,  0.0208,  0.0196,  0.0209],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(884.0834, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.4922, device='cuda:0')



h[100].sum tensor(19.5047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.4752, device='cuda:0')



h[200].sum tensor(23.9727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.4946, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0105, 0.0477, 0.0306,  ..., 0.0418, 0.0410, 0.0434],
        [0.0142, 0.0609, 0.0400,  ..., 0.0545, 0.0525, 0.0558],
        [0.0096, 0.0445, 0.0283,  ..., 0.0388, 0.0382, 0.0405],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36082.6992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1570, 0.0000,  ..., 0.0692, 0.0000, 0.0417],
        [0.0000, 0.1649, 0.0000,  ..., 0.0721, 0.0000, 0.0446],
        [0.0000, 0.1571, 0.0000,  ..., 0.0692, 0.0000, 0.0419],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(182323.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-89.9974, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.8976, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-205.7443, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9982],
        [0.9958],
        [0.9883],
        ...,
        [0.9763],
        [0.9754],
        [0.9746]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362163.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9982],
        [0.9957],
        [0.9882],
        ...,
        [0.9762],
        [0.9754],
        [0.9745]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362149.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049,  0.0203,  0.0136,  ...,  0.0185,  0.0175,  0.0187],
        [ 0.0073,  0.0288,  0.0196,  ...,  0.0266,  0.0249,  0.0265],
        [ 0.0063,  0.0253,  0.0171,  ...,  0.0233,  0.0219,  0.0233],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(977.1899, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.4153, device='cuda:0')



h[100].sum tensor(19.8403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0871, device='cuda:0')



h[200].sum tensor(24.6857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.6635, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0195, 0.0804, 0.0538,  ..., 0.0731, 0.0694, 0.0739],
        [0.0239, 0.0964, 0.0651,  ..., 0.0884, 0.0832, 0.0887],
        [0.0249, 0.1002, 0.0678,  ..., 0.0921, 0.0865, 0.0923],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37120.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.0172e-01, 0.0000e+00,  ..., 8.5818e-02, 0.0000e+00,
         5.7999e-02],
        [0.0000e+00, 2.4025e-01, 0.0000e+00,  ..., 1.0083e-01, 0.0000e+00,
         7.0347e-02],
        [0.0000e+00, 2.5062e-01, 0.0000e+00,  ..., 1.0477e-01, 0.0000e+00,
         7.3938e-02],
        ...,
        [0.0000e+00, 3.4951e-03, 0.0000e+00,  ..., 6.3920e-03, 0.0000e+00,
         1.4362e-04],
        [0.0000e+00, 5.0670e-03, 0.0000e+00,  ..., 7.0574e-03, 0.0000e+00,
         2.8726e-04],
        [0.0000e+00, 3.4955e-03, 0.0000e+00,  ..., 6.3914e-03, 0.0000e+00,
         1.4363e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(185309.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-95.0993, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.9395, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-213.3862, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9982],
        [0.9957],
        [0.9882],
        ...,
        [0.9762],
        [0.9754],
        [0.9745]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362149.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9982],
        [0.9957],
        [0.9882],
        ...,
        [0.9761],
        [0.9753],
        [0.9744]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362135.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0107,  0.0412,  0.0284,  ...,  0.0385,  0.0356,  0.0381],
        [ 0.0038,  0.0162,  0.0107,  ...,  0.0146,  0.0140,  0.0149],
        [ 0.0055,  0.0223,  0.0150,  ...,  0.0203,  0.0192,  0.0205],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(807.0668, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5996, device='cuda:0')



h[100].sum tensor(19.2068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.8730, device='cuda:0')



h[200].sum tensor(23.3400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.3601, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0193, 0.0799, 0.0534,  ..., 0.0726, 0.0689, 0.0734],
        [0.0242, 0.0976, 0.0659,  ..., 0.0895, 0.0842, 0.0898],
        [0.0217, 0.0883, 0.0594,  ..., 0.0807, 0.0762, 0.0812],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34401.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2397, 0.0000,  ..., 0.0996, 0.0000, 0.0721],
        [0.0000, 0.2354, 0.0000,  ..., 0.0981, 0.0000, 0.0703],
        [0.0000, 0.2137, 0.0000,  ..., 0.0903, 0.0000, 0.0624],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(176904.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.9566, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.3953, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-193.6473, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9982],
        [0.9957],
        [0.9882],
        ...,
        [0.9761],
        [0.9753],
        [0.9744]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362135.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9982],
        [0.9957],
        [0.9881],
        ...,
        [0.9761],
        [0.9752],
        [0.9743]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362121.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056,  0.0227,  0.0153,  ...,  0.0208,  0.0196,  0.0209],
        [ 0.0045,  0.0188,  0.0125,  ...,  0.0170,  0.0162,  0.0173],
        [ 0.0055,  0.0224,  0.0151,  ...,  0.0205,  0.0194,  0.0206],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(669.5414, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8033, device='cuda:0')



h[100].sum tensor(18.6951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.9832, device='cuda:0')



h[200].sum tensor(22.2533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.2064, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0160, 0.0678, 0.0449,  ..., 0.0611, 0.0585, 0.0622],
        [0.0214, 0.0875, 0.0588,  ..., 0.0799, 0.0756, 0.0805],
        [0.0242, 0.0976, 0.0660,  ..., 0.0896, 0.0843, 0.0899],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30785.0332, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2661, 0.0000,  ..., 0.1109, 0.0000, 0.0787],
        [0.0000, 0.3138, 0.0000,  ..., 0.1292, 0.0000, 0.0944],
        [0.0000, 0.3214, 0.0000,  ..., 0.1318, 0.0000, 0.0973],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(160585.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-64.1597, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.9719, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-167.0896, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9982],
        [0.9957],
        [0.9881],
        ...,
        [0.9761],
        [0.9752],
        [0.9743]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362121.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9982],
        [0.9957],
        [0.9881],
        ...,
        [0.9760],
        [0.9751],
        [0.9742]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362108.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0017,  0.0086,  0.0053,  ...,  0.0072,  0.0073,  0.0077],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1388.6165, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.7055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.1354, device='cuda:0')



h[100].sum tensor(21.3243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.8618, device='cuda:0')



h[200].sum tensor(27.8375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.4980, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0039, 0.0196, 0.0117,  ..., 0.0161, 0.0167, 0.0174],
        [0.0017, 0.0098, 0.0052,  ..., 0.0072, 0.0082, 0.0082],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(53108.8984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0665, 0.0000,  ..., 0.0328, 0.0000, 0.0155],
        [0.0000, 0.0359, 0.0000,  ..., 0.0202, 0.0000, 0.0073],
        [0.0000, 0.0141, 0.0000,  ..., 0.0112, 0.0000, 0.0020],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0065, 0.0000, 0.0007],
        [0.0000, 0.0157, 0.0000,  ..., 0.0112, 0.0000, 0.0038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(284052.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-171.3157, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-27.8870, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-329.5840, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9982],
        [0.9957],
        [0.9881],
        ...,
        [0.9760],
        [0.9751],
        [0.9742]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362108.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9982],
        [0.9956],
        [0.9880],
        ...,
        [0.9759],
        [0.9750],
        [0.9741]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362094.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(687.3672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0719, device='cuda:0')



h[100].sum tensor(18.7487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0687, device='cuda:0')



h[200].sum tensor(22.3670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.5094, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31301.1289, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(163632.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-66.8044, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.3895, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-170.9575, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9982],
        [0.9956],
        [0.9880],
        ...,
        [0.9759],
        [0.9750],
        [0.9741]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362094.3438, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 360.0 event: 1800 loss: tensor(612.2153, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9981],
        [0.9956],
        [0.9880],
        ...,
        [0.9758],
        [0.9749],
        [0.9740]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362080.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0012,  0.0068,  0.0040,  ...,  0.0056,  0.0058,  0.0061],
        [ 0.0012,  0.0068,  0.0040,  ...,  0.0056,  0.0058,  0.0061],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(676.4318, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8842, device='cuda:0')



h[100].sum tensor(18.7028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0089, device='cuda:0')



h[200].sum tensor(22.2696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.2976, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0133, 0.0072,  ..., 0.0100, 0.0112, 0.0115],
        [0.0021, 0.0132, 0.0072,  ..., 0.0100, 0.0112, 0.0114],
        [0.0021, 0.0132, 0.0072,  ..., 0.0100, 0.0112, 0.0114],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31731.3477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0420, 0.0000,  ..., 0.0229, 0.0000, 0.0083],
        [0.0000, 0.0360, 0.0000,  ..., 0.0206, 0.0000, 0.0061],
        [0.0000, 0.0339, 0.0000,  ..., 0.0197, 0.0000, 0.0056],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(169218.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-69.2593, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.8142, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-174.3589, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9981],
        [0.9956],
        [0.9880],
        ...,
        [0.9758],
        [0.9749],
        [0.9740]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362080.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9981],
        [0.9956],
        [0.9879],
        ...,
        [0.9757],
        [0.9748],
        [0.9740]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362066.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(947.8141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.0574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.2747, device='cuda:0')



h[100].sum tensor(19.6862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7242, device='cuda:0')



h[200].sum tensor(24.3583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.3771, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39353.3398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0400, 0.0000,  ..., 0.0213, 0.0000, 0.0100],
        [0.0000, 0.0361, 0.0000,  ..., 0.0197, 0.0000, 0.0092],
        [0.0000, 0.0293, 0.0000,  ..., 0.0169, 0.0000, 0.0076],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(208728.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-106.1396, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.7645, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-229.6273, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9981],
        [0.9956],
        [0.9879],
        ...,
        [0.9757],
        [0.9748],
        [0.9740]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362066.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9981],
        [0.9956],
        [0.9878],
        ...,
        [0.9756],
        [0.9747],
        [0.9739]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362052.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0011,  0.0065,  0.0038,  ...,  0.0053,  0.0056,  0.0058],
        [ 0.0019,  0.0094,  0.0059,  ...,  0.0081,  0.0081,  0.0086],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1168.6375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.5424, device='cuda:0')



h[100].sum tensor(20.4822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0821, device='cuda:0')



h[200].sum tensor(26.0489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.1902, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0116, 0.0515, 0.0333,  ..., 0.0455, 0.0443, 0.0470],
        [0.0040, 0.0200, 0.0120,  ..., 0.0165, 0.0171, 0.0178],
        [0.0019, 0.0106, 0.0059,  ..., 0.0080, 0.0089, 0.0090],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45023.4141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1223, 0.0000,  ..., 0.0551, 0.0000, 0.0323],
        [0.0000, 0.0788, 0.0000,  ..., 0.0374, 0.0000, 0.0200],
        [0.0000, 0.0391, 0.0000,  ..., 0.0212, 0.0000, 0.0089],
        ...,
        [0.0000, 0.0035, 0.0000,  ..., 0.0064, 0.0000, 0.0001],
        [0.0000, 0.0051, 0.0000,  ..., 0.0071, 0.0000, 0.0003],
        [0.0000, 0.0035, 0.0000,  ..., 0.0064, 0.0000, 0.0001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(234149.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-132.8069, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.7670, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-270.4422, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9981],
        [0.9956],
        [0.9878],
        ...,
        [0.9756],
        [0.9747],
        [0.9739]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362052.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9981],
        [0.9955],
        [0.9878],
        ...,
        [0.9755],
        [0.9747],
        [0.9738]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362039.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0025,  0.0116,  0.0074,  ...,  0.0101,  0.0100,  0.0105],
        [ 0.0049,  0.0201,  0.0134,  ...,  0.0183,  0.0174,  0.0185],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(806.6940, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3903, device='cuda:0')



h[100].sum tensor(19.1586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.8063, device='cuda:0')



h[200].sum tensor(23.2377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.1240, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0045, 0.0219, 0.0133,  ..., 0.0183, 0.0187, 0.0195],
        [0.0082, 0.0372, 0.0237,  ..., 0.0324, 0.0320, 0.0337],
        [0.0147, 0.0629, 0.0414,  ..., 0.0564, 0.0542, 0.0576],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33717.4883, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0771, 0.0000,  ..., 0.0364, 0.0000, 0.0201],
        [0.0000, 0.1347, 0.0000,  ..., 0.0595, 0.0000, 0.0371],
        [0.0000, 0.1944, 0.0000,  ..., 0.0830, 0.0000, 0.0552],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(170762.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-79.1478, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.6471, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-189.2093, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9981],
        [0.9955],
        [0.9878],
        ...,
        [0.9755],
        [0.9747],
        [0.9738]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362039.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9981],
        [0.9955],
        [0.9877],
        ...,
        [0.9754],
        [0.9746],
        [0.9737]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362025.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0039,  0.0164,  0.0108,  ...,  0.0147,  0.0141,  0.0150],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0029,  0.0129,  0.0083,  ...,  0.0114,  0.0111,  0.0118],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(535.9534, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.1592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.6072, device='cuda:0')



h[100].sum tensor(18.1704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.9662, device='cuda:0')



h[200].sum tensor(21.1387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.6018, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0053, 0.0268, 0.0163,  ..., 0.0224, 0.0229, 0.0240],
        [0.0128, 0.0561, 0.0366,  ..., 0.0499, 0.0484, 0.0513],
        [0.0193, 0.0778, 0.0525,  ..., 0.0712, 0.0672, 0.0715],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28250.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1481, 0.0000,  ..., 0.0654, 0.0000, 0.0397],
        [0.0000, 0.2254, 0.0000,  ..., 0.0950, 0.0000, 0.0650],
        [0.0000, 0.3126, 0.0000,  ..., 0.1279, 0.0000, 0.0947],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(150615.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.0733, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.8202, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-149.3641, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9981],
        [0.9955],
        [0.9877],
        ...,
        [0.9754],
        [0.9746],
        [0.9737]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362025.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9981],
        [0.9955],
        [0.9877],
        ...,
        [0.9754],
        [0.9745],
        [0.9736]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(362011.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0026,  0.0119,  0.0076,  ...,  0.0105,  0.0103,  0.0109],
        [ 0.0016,  0.0082,  0.0050,  ...,  0.0069,  0.0070,  0.0074],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(655.0020, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2056, device='cuda:0')



h[100].sum tensor(18.5965, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7930, device='cuda:0')



h[200].sum tensor(22.0437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.5323, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0026, 0.0131, 0.0076,  ..., 0.0104, 0.0111, 0.0113],
        [0.0036, 0.0188, 0.0111,  ..., 0.0153, 0.0160, 0.0166],
        [0.0163, 0.0686, 0.0454,  ..., 0.0619, 0.0592, 0.0629],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29106.7539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0520, 0.0000,  ..., 0.0265, 0.0000, 0.0126],
        [0.0000, 0.0931, 0.0000,  ..., 0.0431, 0.0000, 0.0243],
        [0.0000, 0.1638, 0.0000,  ..., 0.0711, 0.0000, 0.0457],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0051, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(149756.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-56.6300, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.0638, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-155.5425, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9981],
        [0.9955],
        [0.9877],
        ...,
        [0.9754],
        [0.9745],
        [0.9736]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(362011.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9981],
        [0.9955],
        [0.9876],
        ...,
        [0.9753],
        [0.9744],
        [0.9735]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361997.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(638.6919, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4192, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5777, device='cuda:0')



h[100].sum tensor(18.5320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.5932, device='cuda:0')



h[200].sum tensor(21.9068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.8242, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32568.8242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.1332e-04, 0.0000e+00,  ..., 5.3792e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.2047e-03, 0.0000e+00,  ..., 6.1424e-03, 0.0000e+00,
         8.9715e-05],
        [0.0000e+00, 1.5682e-02, 0.0000e+00,  ..., 1.1510e-02, 0.0000e+00,
         3.6209e-03],
        ...,
        [0.0000e+00, 3.5322e-04, 0.0000e+00,  ..., 5.0508e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5348e-04, 0.0000e+00,  ..., 5.0503e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5348e-04, 0.0000e+00,  ..., 5.0501e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(179106.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-72.9501, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.3935, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-179.8961, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9981],
        [0.9955],
        [0.9876],
        ...,
        [0.9753],
        [0.9744],
        [0.9735]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361997.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9981],
        [0.9955],
        [0.9876],
        ...,
        [0.9752],
        [0.9743],
        [0.9734]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361983.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(798.3478, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3090, device='cuda:0')



h[100].sum tensor(19.1024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7805, device='cuda:0')



h[200].sum tensor(23.1184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.0323, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34797.3477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(183963., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-84.1134, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.1553, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-196.2997, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9981],
        [0.9955],
        [0.9876],
        ...,
        [0.9752],
        [0.9743],
        [0.9734]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361983.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9980],
        [0.9954],
        [0.9875],
        ...,
        [0.9751],
        [0.9742],
        [0.9733]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361969.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(668.7582, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2587, device='cuda:0')



h[100].sum tensor(18.6294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8099, device='cuda:0')



h[200].sum tensor(22.1137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.5922, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31601.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(169694.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-68.1216, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.4265, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-172.6916, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9980],
        [0.9954],
        [0.9875],
        ...,
        [0.9751],
        [0.9742],
        [0.9733]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361969.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9980],
        [0.9954],
        [0.9875],
        ...,
        [0.9750],
        [0.9741],
        [0.9732]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361956.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(730.8549, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5544, device='cuda:0')



h[100].sum tensor(18.8471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2222, device='cuda:0')



h[200].sum tensor(22.5760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.0535, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33696.3789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(182914.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-78.0332, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.1072, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-188.4639, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9980],
        [0.9954],
        [0.9875],
        ...,
        [0.9750],
        [0.9741],
        [0.9732]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361956.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 370.0 event: 1850 loss: tensor(592.2729, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9980],
        [0.9954],
        [0.9874],
        ...,
        [0.9749],
        [0.9740],
        [0.9732]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361942.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0018,  0.0090,  0.0056,  ...,  0.0076,  0.0077,  0.0081],
        [ 0.0012,  0.0068,  0.0040,  ...,  0.0056,  0.0058,  0.0061],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(566.4916, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.9934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9575, device='cuda:0')



h[100].sum tensor(18.2514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.0776, device='cuda:0')



h[200].sum tensor(21.3108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.9969, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0018, 0.0102, 0.0055,  ..., 0.0076, 0.0086, 0.0086],
        [0.0040, 0.0202, 0.0121,  ..., 0.0166, 0.0172, 0.0179],
        [0.0153, 0.0650, 0.0429,  ..., 0.0584, 0.0560, 0.0596],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30142.0020, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0442, 0.0000,  ..., 0.0234, 0.0000, 0.0100],
        [0.0000, 0.0893, 0.0000,  ..., 0.0415, 0.0000, 0.0231],
        [0.0000, 0.1647, 0.0000,  ..., 0.0714, 0.0000, 0.0461],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(165022.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-62.1195, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.1073, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-162.9973, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9980],
        [0.9954],
        [0.9874],
        ...,
        [0.9749],
        [0.9740],
        [0.9732]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361942.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9980],
        [0.9954],
        [0.9874],
        ...,
        [0.9748],
        [0.9740],
        [0.9731]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361928.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0018,  0.0087,  0.0054,  ...,  0.0074,  0.0075,  0.0079],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1032.6047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.8437, device='cuda:0')



h[100].sum tensor(19.9162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2234, device='cuda:0')



h[200].sum tensor(24.8468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.1466, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0058, 0.0266, 0.0166,  ..., 0.0227, 0.0227, 0.0238],
        [0.0106, 0.0462, 0.0300,  ..., 0.0410, 0.0398, 0.0421],
        [0.0090, 0.0384, 0.0250,  ..., 0.0340, 0.0330, 0.0348],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39284.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1253, 0.0000,  ..., 0.0560, 0.0000, 0.0336],
        [0.0000, 0.1577, 0.0000,  ..., 0.0685, 0.0000, 0.0445],
        [0.0000, 0.1620, 0.0000,  ..., 0.0698, 0.0000, 0.0466],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(207647.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-105.1490, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.9406, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-228.9346, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9980],
        [0.9954],
        [0.9874],
        ...,
        [0.9748],
        [0.9740],
        [0.9731]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361928.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9980],
        [0.9953],
        [0.9873],
        ...,
        [0.9748],
        [0.9739],
        [0.9730]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361914.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061,  0.0245,  0.0166,  ...,  0.0225,  0.0212,  0.0226],
        [ 0.0089,  0.0348,  0.0239,  ...,  0.0324,  0.0301,  0.0322],
        [ 0.0063,  0.0252,  0.0170,  ...,  0.0231,  0.0217,  0.0232],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1064.1259, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0991, device='cuda:0')



h[100].sum tensor(20.0212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3047, device='cuda:0')



h[200].sum tensor(25.0698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.4346, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0329, 0.1293, 0.0884,  ..., 0.1199, 0.1118, 0.1194],
        [0.0283, 0.1127, 0.0766,  ..., 0.1040, 0.0973, 0.1039],
        [0.0231, 0.0934, 0.0630,  ..., 0.0856, 0.0807, 0.0860],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41475.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.6947e-01, 0.0000e+00,  ..., 1.5026e-01, 0.0000e+00,
         1.1305e-01],
        [0.0000e+00, 3.4449e-01, 0.0000e+00,  ..., 1.4082e-01, 0.0000e+00,
         1.0474e-01],
        [0.0000e+00, 3.0749e-01, 0.0000e+00,  ..., 1.2673e-01, 0.0000e+00,
         9.2265e-02],
        ...,
        [0.0000e+00, 3.5373e-04, 0.0000e+00,  ..., 5.0456e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5399e-04, 0.0000e+00,  ..., 5.0451e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5400e-04, 0.0000e+00,  ..., 5.0448e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(214526.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-115.6627, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.1366, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-244.6195, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9980],
        [0.9953],
        [0.9873],
        ...,
        [0.9748],
        [0.9739],
        [0.9730]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361914.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9980],
        [0.9953],
        [0.9873],
        ...,
        [0.9747],
        [0.9738],
        [0.9729]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361900.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1004.8994, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.7936, device='cuda:0')



h[100].sum tensor(19.8019, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8893, device='cuda:0')



h[200].sum tensor(24.6040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.9623, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39020.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0500, 0.0000,  ..., 0.0255, 0.0000, 0.0122],
        [0.0000, 0.0529, 0.0000,  ..., 0.0267, 0.0000, 0.0132],
        [0.0000, 0.0584, 0.0000,  ..., 0.0288, 0.0000, 0.0149],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(201020.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-103.1281, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.8703, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-226.0659, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9980],
        [0.9953],
        [0.9873],
        ...,
        [0.9747],
        [0.9738],
        [0.9729]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361900.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9980],
        [0.9953],
        [0.9872],
        ...,
        [0.9746],
        [0.9737],
        [0.9728]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361886.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(531.1459, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.2908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3123, device='cuda:0')



h[100].sum tensor(18.1061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.8723, device='cuda:0')



h[200].sum tensor(21.0021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.2693, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0021, 0.0113, 0.0063,  ..., 0.0087, 0.0095, 0.0096],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26521.1836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.9812e-03, 0.0000e+00,  ..., 6.0680e-03, 0.0000e+00,
         1.2729e-05],
        [0.0000e+00, 1.1765e-02, 0.0000e+00,  ..., 1.0058e-02, 0.0000e+00,
         2.2361e-03],
        [0.0000e+00, 4.0193e-02, 0.0000e+00,  ..., 2.1667e-02, 0.0000e+00,
         9.1431e-03],
        ...,
        [0.0000e+00, 3.5390e-04, 0.0000e+00,  ..., 5.0438e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5416e-04, 0.0000e+00,  ..., 5.0433e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5417e-04, 0.0000e+00,  ..., 5.0431e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(138704.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-44.4802, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-52.1529, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-137.1889, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9980],
        [0.9953],
        [0.9872],
        ...,
        [0.9746],
        [0.9737],
        [0.9728]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361886.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9980],
        [0.9953],
        [0.9872],
        ...,
        [0.9745],
        [0.9736],
        [0.9727]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361872.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0058,  0.0237,  0.0160,  ...,  0.0217,  0.0204,  0.0218],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(783.3932, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5595, device='cuda:0')



h[100].sum tensor(18.9989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5420, device='cuda:0')



h[200].sum tensor(22.8985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.1871, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0058, 0.0248, 0.0159,  ..., 0.0216, 0.0212, 0.0222],
        [0.0047, 0.0205, 0.0129,  ..., 0.0175, 0.0175, 0.0182],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32901.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0173, 0.0000,  ..., 0.0121, 0.0000, 0.0043],
        [0.0000, 0.0547, 0.0000,  ..., 0.0270, 0.0000, 0.0148],
        [0.0000, 0.0707, 0.0000,  ..., 0.0334, 0.0000, 0.0193],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(170631.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-74.7404, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.8721, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-182.5935, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9980],
        [0.9953],
        [0.9872],
        ...,
        [0.9745],
        [0.9736],
        [0.9727]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361872.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9979],
        [0.9952],
        [0.9871],
        ...,
        [0.9744],
        [0.9735],
        [0.9726]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361858.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0017,  0.0084,  0.0051,  ...,  0.0071,  0.0072,  0.0076],
        [ 0.0014,  0.0076,  0.0046,  ...,  0.0063,  0.0065,  0.0068],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(667.3158, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1572, device='cuda:0')



h[100].sum tensor(18.5806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7776, device='cuda:0')



h[200].sum tensor(22.0100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.4777, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0017, 0.0096, 0.0051,  ..., 0.0071, 0.0080, 0.0080],
        [0.0027, 0.0153, 0.0086,  ..., 0.0119, 0.0130, 0.0133],
        [0.0074, 0.0362, 0.0225,  ..., 0.0309, 0.0311, 0.0328],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31163.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0548, 0.0000,  ..., 0.0279, 0.0000, 0.0125],
        [0.0000, 0.0806, 0.0000,  ..., 0.0387, 0.0000, 0.0190],
        [0.0000, 0.1066, 0.0000,  ..., 0.0494, 0.0000, 0.0258],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(165434.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-66.4150, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.3133, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-170.2373, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9979],
        [0.9952],
        [0.9871],
        ...,
        [0.9744],
        [0.9735],
        [0.9726]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361858.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9979],
        [0.9952],
        [0.9870],
        ...,
        [0.9743],
        [0.9734],
        [0.9725]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361844.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0083,  0.0328,  0.0224,  ...,  0.0304,  0.0283,  0.0303],
        [ 0.0115,  0.0441,  0.0304,  ...,  0.0412,  0.0381,  0.0408],
        [ 0.0087,  0.0341,  0.0233,  ...,  0.0316,  0.0294,  0.0314],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(951.9673, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.2655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7341, device='cuda:0')



h[100].sum tensor(19.5845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5521, device='cuda:0')



h[200].sum tensor(24.1424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7673, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0380, 0.1481, 0.1017,  ..., 0.1378, 0.1280, 0.1368],
        [0.0429, 0.1660, 0.1144,  ..., 0.1549, 0.1435, 0.1534],
        [0.0420, 0.1623, 0.1118,  ..., 0.1515, 0.1404, 0.1501],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38437.2148, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 4.0508e-01, 0.0000e+00,  ..., 1.6346e-01, 0.0000e+00,
         1.2489e-01],
        [0.0000e+00, 4.4777e-01, 0.0000e+00,  ..., 1.7948e-01, 0.0000e+00,
         1.3919e-01],
        [0.0000e+00, 4.3765e-01, 0.0000e+00,  ..., 1.7556e-01, 0.0000e+00,
         1.3576e-01],
        ...,
        [0.0000e+00, 3.5415e-04, 0.0000e+00,  ..., 5.0412e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5442e-04, 0.0000e+00,  ..., 5.0407e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5442e-04, 0.0000e+00,  ..., 5.0405e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(202885.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-101.1092, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.6744, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-222.8999, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9979],
        [0.9952],
        [0.9870],
        ...,
        [0.9743],
        [0.9734],
        [0.9725]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361844.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9979],
        [0.9952],
        [0.9870],
        ...,
        [0.9743],
        [0.9734],
        [0.9725]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361844.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(716.2629, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1872, device='cuda:0')



h[100].sum tensor(18.7488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1053, device='cuda:0')



h[200].sum tensor(22.3672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.6393, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0020, 0.0128, 0.0069,  ..., 0.0096, 0.0108, 0.0110],
        [0.0010, 0.0072, 0.0034,  ..., 0.0048, 0.0060, 0.0058],
        [0.0017, 0.0096, 0.0052,  ..., 0.0071, 0.0081, 0.0081],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31949.1602, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0391, 0.0000,  ..., 0.0219, 0.0000, 0.0067],
        [0.0000, 0.0331, 0.0000,  ..., 0.0194, 0.0000, 0.0054],
        [0.0000, 0.0389, 0.0000,  ..., 0.0218, 0.0000, 0.0068],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(168260.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-69.9780, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.9127, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-175.5036, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9979],
        [0.9952],
        [0.9870],
        ...,
        [0.9743],
        [0.9734],
        [0.9725]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361844.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9979],
        [0.9952],
        [0.9870],
        ...,
        [0.9742],
        [0.9733],
        [0.9724]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361830.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0033,  0.0144,  0.0094,  ...,  0.0129,  0.0124,  0.0132],
        [ 0.0022,  0.0102,  0.0065,  ...,  0.0089,  0.0088,  0.0093],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(813.7176, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9065, device='cuda:0')



h[100].sum tensor(19.0881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6524, device='cuda:0')



h[200].sum tensor(23.0880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.5785, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0130, 0.0568, 0.0370,  ..., 0.0505, 0.0489, 0.0519],
        [0.0050, 0.0237, 0.0146,  ..., 0.0200, 0.0202, 0.0211],
        [0.0022, 0.0114, 0.0064,  ..., 0.0088, 0.0096, 0.0098],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33342.6367, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1322, 0.0000,  ..., 0.0587, 0.0000, 0.0359],
        [0.0000, 0.0908, 0.0000,  ..., 0.0423, 0.0000, 0.0232],
        [0.0000, 0.0584, 0.0000,  ..., 0.0293, 0.0000, 0.0136],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(171865.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-76.5458, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.5122, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-185.7103, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9979],
        [0.9952],
        [0.9870],
        ...,
        [0.9742],
        [0.9733],
        [0.9724]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361830.9375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 380.0 event: 1900 loss: tensor(555.5647, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9979],
        [0.9952],
        [0.9869],
        ...,
        [0.9741],
        [0.9732],
        [0.9723]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361816.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(689.5645, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2438, device='cuda:0')



h[100].sum tensor(18.6432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8051, device='cuda:0')



h[200].sum tensor(22.1430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.5754, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30697.0508, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.1345e-04, 0.0000e+00,  ..., 5.3781e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.1513e-04, 0.0000e+00,  ..., 5.3668e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.1900e-03, 0.0000e+00,  ..., 6.1387e-03, 0.0000e+00,
         3.6350e-09],
        ...,
        [0.0000e+00, 3.5432e-04, 0.0000e+00,  ..., 5.0395e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5459e-04, 0.0000e+00,  ..., 5.0390e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5459e-04, 0.0000e+00,  ..., 5.0387e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(160277.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-64.2925, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.7518, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-166.8216, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9979],
        [0.9952],
        [0.9869],
        ...,
        [0.9741],
        [0.9732],
        [0.9723]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361816.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9979],
        [0.9951],
        [0.9869],
        ...,
        [0.9741],
        [0.9732],
        [0.9723]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361803.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1080.2427, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.3770, device='cuda:0')



h[100].sum tensor(20.0161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3931, device='cuda:0')



h[200].sum tensor(25.0590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.7481, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40501.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0313, 0.0000,  ..., 0.0183, 0.0000, 0.0060],
        [0.0000, 0.0170, 0.0000,  ..., 0.0125, 0.0000, 0.0021],
        [0.0000, 0.0118, 0.0000,  ..., 0.0102, 0.0000, 0.0011],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(209537.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-110.7855, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.1888, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-237.2154, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9979],
        [0.9951],
        [0.9869],
        ...,
        [0.9741],
        [0.9732],
        [0.9723]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361803.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9979],
        [0.9951],
        [0.9868],
        ...,
        [0.9740],
        [0.9731],
        [0.9722]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361789.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0039,  0.0166,  0.0109,  ...,  0.0149,  0.0143,  0.0152],
        [ 0.0021,  0.0101,  0.0063,  ...,  0.0087,  0.0087,  0.0092],
        [ 0.0054,  0.0220,  0.0148,  ...,  0.0201,  0.0190,  0.0202],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1210.3527, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4603, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.4286, device='cuda:0')



h[100].sum tensor(20.4667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0459, device='cuda:0')



h[200].sum tensor(26.0160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.0619, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0086, 0.0406, 0.0256,  ..., 0.0351, 0.0349, 0.0369],
        [0.0127, 0.0539, 0.0355,  ..., 0.0483, 0.0464, 0.0492],
        [0.0064, 0.0288, 0.0182,  ..., 0.0248, 0.0246, 0.0259],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42399.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1250, 0.0000,  ..., 0.0565, 0.0000, 0.0319],
        [0.0000, 0.1368, 0.0000,  ..., 0.0607, 0.0000, 0.0368],
        [0.0000, 0.1143, 0.0000,  ..., 0.0516, 0.0000, 0.0306],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(213316.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-120.1360, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.2440, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-251.3176, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9979],
        [0.9951],
        [0.9868],
        ...,
        [0.9740],
        [0.9731],
        [0.9722]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361789.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9979],
        [0.9951],
        [0.9868],
        ...,
        [0.9739],
        [0.9730],
        [0.9721]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361775.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0016,  0.0081,  0.0049,  ...,  0.0068,  0.0069,  0.0073],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(637.9470, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.5960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2416, device='cuda:0')



h[100].sum tensor(18.4456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4863, device='cuda:0')



h[200].sum tensor(21.7232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.4452, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0060, 0.0274, 0.0172,  ..., 0.0236, 0.0235, 0.0246],
        [0.0016, 0.0093, 0.0049,  ..., 0.0068, 0.0078, 0.0078],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30261.2305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1095, 0.0000,  ..., 0.0496, 0.0000, 0.0294],
        [0.0000, 0.0588, 0.0000,  ..., 0.0293, 0.0000, 0.0145],
        [0.0000, 0.0235, 0.0000,  ..., 0.0149, 0.0000, 0.0047],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(160994.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-61.1679, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.6642, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-162.9368, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9979],
        [0.9951],
        [0.9868],
        ...,
        [0.9739],
        [0.9730],
        [0.9721]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361775.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9979],
        [0.9951],
        [0.9867],
        ...,
        [0.9738],
        [0.9729],
        [0.9720]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361761.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0036,  0.0153,  0.0100,  ...,  0.0137,  0.0132,  0.0140],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(571.2010, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9165, device='cuda:0')



h[100].sum tensor(18.2062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.0646, device='cuda:0')



h[200].sum tensor(21.2149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.9507, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0047, 0.0226, 0.0138,  ..., 0.0189, 0.0193, 0.0201],
        [0.0038, 0.0212, 0.0123,  ..., 0.0170, 0.0181, 0.0188],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28249.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0358, 0.0000,  ..., 0.0198, 0.0000, 0.0081],
        [0.0000, 0.0589, 0.0000,  ..., 0.0295, 0.0000, 0.0139],
        [0.0000, 0.0769, 0.0000,  ..., 0.0371, 0.0000, 0.0182],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(150664.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-52.7927, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.7617, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-149.3906, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9979],
        [0.9951],
        [0.9867],
        ...,
        [0.9738],
        [0.9729],
        [0.9720]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361761.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9978],
        [0.9950],
        [0.9867],
        ...,
        [0.9737],
        [0.9728],
        [0.9719]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361747.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(770.5242, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.0378, device='cuda:0')



h[100].sum tensor(18.9002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.3760, device='cuda:0')



h[200].sum tensor(22.6888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.5987, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32007.8574, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0072, 0.0000, 0.0003],
        [0.0000, 0.0050, 0.0000,  ..., 0.0074, 0.0000, 0.0003],
        [0.0000, 0.0075, 0.0000,  ..., 0.0084, 0.0000, 0.0007],
        ...,
        [0.0000, 0.0028, 0.0000,  ..., 0.0060, 0.0000, 0.0003],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(166334.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-70.9646, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.1916, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-176.8173, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9978],
        [0.9950],
        [0.9867],
        ...,
        [0.9737],
        [0.9728],
        [0.9719]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361747.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9978],
        [0.9950],
        [0.9866],
        ...,
        [0.9736],
        [0.9727],
        [0.9718]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361733.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(889.4380, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1887, device='cuda:0')



h[100].sum tensor(19.3105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0604, device='cuda:0')



h[200].sum tensor(23.5602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.0245, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35438.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0029, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(180960.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-86.9340, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.3463, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-201.2308, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9978],
        [0.9950],
        [0.9866],
        ...,
        [0.9736],
        [0.9727],
        [0.9718]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361733.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9978],
        [0.9950],
        [0.9866],
        ...,
        [0.9735],
        [0.9726],
        [0.9717]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361719.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(719.0200, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8527, device='cuda:0')



h[100].sum tensor(18.7088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.9989, device='cuda:0')



h[200].sum tensor(22.2823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.2622, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30958.0273, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(163368.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-65.5117, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.6632, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-168.4769, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9978],
        [0.9950],
        [0.9866],
        ...,
        [0.9735],
        [0.9726],
        [0.9717]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361719.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9978],
        [0.9950],
        [0.9865],
        ...,
        [0.9735],
        [0.9725],
        [0.9716]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361705.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0038,  0.0161,  0.0106,  ...,  0.0145,  0.0139,  0.0147],
        [ 0.0040,  0.0171,  0.0113,  ...,  0.0154,  0.0147,  0.0157],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(909.5719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6343, device='cuda:0')



h[100].sum tensor(19.3679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2022, device='cuda:0')



h[200].sum tensor(23.6822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.5270, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0038, 0.0173, 0.0106,  ..., 0.0144, 0.0147, 0.0152],
        [0.0070, 0.0310, 0.0198,  ..., 0.0270, 0.0266, 0.0280],
        [0.0218, 0.0886, 0.0596,  ..., 0.0810, 0.0765, 0.0815],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35742.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0522, 0.0000,  ..., 0.0261, 0.0000, 0.0137],
        [0.0000, 0.0984, 0.0000,  ..., 0.0444, 0.0000, 0.0277],
        [0.0000, 0.1772, 0.0000,  ..., 0.0753, 0.0000, 0.0523],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(190795.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-88.5064, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.2427, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-203.1699, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9978],
        [0.9950],
        [0.9865],
        ...,
        [0.9735],
        [0.9725],
        [0.9716]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361705.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9978],
        [0.9949],
        [0.9865],
        ...,
        [0.9734],
        [0.9724],
        [0.9715]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361691.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1186.5864, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.0879, device='cuda:0')



h[100].sum tensor(20.3259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9375, device='cuda:0')



h[200].sum tensor(25.7169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.6776, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43974.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0089, 0.0000, 0.0010],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(228235.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-127.0679, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.1435, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-262.2196, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9978],
        [0.9949],
        [0.9865],
        ...,
        [0.9734],
        [0.9724],
        [0.9715]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361691.0312, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 390.0 event: 1950 loss: tensor(629.1576, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9978],
        [0.9949],
        [0.9864],
        ...,
        [0.9733],
        [0.9724],
        [0.9714]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361677., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(854.4506, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.4515, device='cuda:0')



h[100].sum tensor(19.1634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.8258, device='cuda:0')



h[200].sum tensor(23.2478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.1930, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36844.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(202010.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-94.3953, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.6302, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-212.0107, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9978],
        [0.9949],
        [0.9864],
        ...,
        [0.9733],
        [0.9724],
        [0.9714]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361677., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9978],
        [0.9949],
        [0.9864],
        ...,
        [0.9733],
        [0.9724],
        [0.9714]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361677., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(885.4822, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8260, device='cuda:0')



h[100].sum tensor(19.2712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9450, device='cuda:0')



h[200].sum tensor(23.4769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.6155, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35045.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0015, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0082, 0.0000, 0.0004],
        [0.0000, 0.0204, 0.0000,  ..., 0.0138, 0.0000, 0.0036],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(183099.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-84.3453, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.5385, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-197.1668, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9978],
        [0.9949],
        [0.9864],
        ...,
        [0.9733],
        [0.9724],
        [0.9714]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361677., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9978],
        [0.9949],
        [0.9863],
        ...,
        [0.9732],
        [0.9723],
        [0.9714]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361662.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0041,  0.0172,  0.0114,  ...,  0.0155,  0.0148,  0.0158],
        [ 0.0079,  0.0313,  0.0213,  ...,  0.0289,  0.0270,  0.0288],
        [ 0.0097,  0.0377,  0.0259,  ...,  0.0351,  0.0326,  0.0348],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(752.9564, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3208, device='cuda:0')



h[100].sum tensor(18.8050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1478, device='cuda:0')



h[200].sum tensor(22.4867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.7900, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0275, 0.1095, 0.0744,  ..., 0.1009, 0.0946, 0.1009],
        [0.0370, 0.1442, 0.0990,  ..., 0.1341, 0.1247, 0.1332],
        [0.0367, 0.1430, 0.0981,  ..., 0.1330, 0.1236, 0.1321],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32873.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.3974e-01, 0.0000e+00,  ..., 1.3879e-01, 0.0000e+00,
         1.0291e-01],
        [0.0000e+00, 3.8799e-01, 0.0000e+00,  ..., 1.5697e-01, 0.0000e+00,
         1.1912e-01],
        [0.0000e+00, 4.0174e-01, 0.0000e+00,  ..., 1.6223e-01, 0.0000e+00,
         1.2382e-01],
        ...,
        [0.0000e+00, 3.5525e-04, 0.0000e+00,  ..., 5.0299e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5552e-04, 0.0000e+00,  ..., 5.0294e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5552e-04, 0.0000e+00,  ..., 5.0291e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(177599.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-74.3557, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.1658, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-181.9862, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9978],
        [0.9949],
        [0.9863],
        ...,
        [0.9732],
        [0.9723],
        [0.9714]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361662.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9977],
        [0.9949],
        [0.9863],
        ...,
        [0.9731],
        [0.9722],
        [0.9713]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361648.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0035,  0.0150,  0.0098,  ...,  0.0134,  0.0129,  0.0137],
        [ 0.0027,  0.0122,  0.0079,  ...,  0.0108,  0.0105,  0.0112],
        [ 0.0068,  0.0273,  0.0185,  ...,  0.0251,  0.0236,  0.0251],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(414.0486, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.2744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.6991, device='cuda:0')



h[100].sum tensor(17.6253, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.0408, device='cuda:0')



h[200].sum tensor(19.9811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.3221, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0054, 0.0253, 0.0158,  ..., 0.0216, 0.0217, 0.0227],
        [0.0184, 0.0763, 0.0509,  ..., 0.0692, 0.0659, 0.0701],
        [0.0191, 0.0788, 0.0526,  ..., 0.0716, 0.0680, 0.0724],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(23909.1133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0834, 0.0000,  ..., 0.0387, 0.0000, 0.0225],
        [0.0000, 0.1512, 0.0000,  ..., 0.0655, 0.0000, 0.0435],
        [0.0000, 0.1759, 0.0000,  ..., 0.0752, 0.0000, 0.0511],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(129886.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-32.1452, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-54.5860, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-118.1563, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9977],
        [0.9949],
        [0.9863],
        ...,
        [0.9731],
        [0.9722],
        [0.9713]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361648.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9977],
        [0.9948],
        [0.9862],
        ...,
        [0.9730],
        [0.9721],
        [0.9712]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361634.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0016,  0.0080,  0.0049,  ...,  0.0067,  0.0069,  0.0072],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0080,  0.0049,  ...,  0.0067,  0.0069,  0.0072],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(686.9869, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0273, device='cuda:0')



h[100].sum tensor(18.5659, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7363, device='cuda:0')



h[200].sum tensor(21.9787, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.3313, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0012, 0.0079, 0.0039,  ..., 0.0054, 0.0065, 0.0064],
        [0.0055, 0.0292, 0.0175,  ..., 0.0242, 0.0250, 0.0263],
        [0.0012, 0.0078, 0.0039,  ..., 0.0054, 0.0065, 0.0064],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29769.6836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0241, 0.0000,  ..., 0.0155, 0.0000, 0.0039],
        [0.0000, 0.0411, 0.0000,  ..., 0.0227, 0.0000, 0.0079],
        [0.0000, 0.0240, 0.0000,  ..., 0.0154, 0.0000, 0.0038],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(153964.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-59.9394, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.4928, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-160.2181, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9977],
        [0.9948],
        [0.9862],
        ...,
        [0.9730],
        [0.9721],
        [0.9712]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361634.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9977],
        [0.9948],
        [0.9862],
        ...,
        [0.9729],
        [0.9720],
        [0.9711]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361620.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0023,  0.0109,  0.0069,  ...,  0.0095,  0.0094,  0.0099],
        [ 0.0102,  0.0394,  0.0271,  ...,  0.0367,  0.0341,  0.0364],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2019.2480, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.9457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(44.6864, device='cuda:0')



h[100].sum tensor(23.1617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.2191, device='cuda:0')



h[200].sum tensor(31.7401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(50.3975, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0065, 0.0293, 0.0186,  ..., 0.0253, 0.0251, 0.0264],
        [0.0206, 0.0826, 0.0558,  ..., 0.0758, 0.0713, 0.0759],
        [0.0275, 0.1094, 0.0743,  ..., 0.1009, 0.0945, 0.1009],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(63323.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.6594e-01, 0.0000e+00,  ..., 7.0729e-02, 0.0000e+00,
         4.8856e-02],
        [0.0000e+00, 3.0109e-01, 0.0000e+00,  ..., 1.2292e-01, 0.0000e+00,
         9.2184e-02],
        [0.0000e+00, 4.0862e-01, 0.0000e+00,  ..., 1.6434e-01, 0.0000e+00,
         1.2673e-01],
        ...,
        [0.0000e+00, 3.5551e-04, 0.0000e+00,  ..., 5.0273e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5577e-04, 0.0000e+00,  ..., 5.0268e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5578e-04, 0.0000e+00,  ..., 5.0265e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(322876.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-220.8781, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-18.3291, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-403.9002, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9977],
        [0.9948],
        [0.9862],
        ...,
        [0.9729],
        [0.9720],
        [0.9711]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361620.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9977],
        [0.9948],
        [0.9861],
        ...,
        [0.9728],
        [0.9719],
        [0.9710]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361606.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(505.8552, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.6486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4186, device='cuda:0')



h[100].sum tensor(17.9312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.5880, device='cuda:0')



h[200].sum tensor(20.6307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.2613, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(25905.7461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0064, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(137087.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-42.3517, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-52.4101, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-133.1357, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9977],
        [0.9948],
        [0.9861],
        ...,
        [0.9728],
        [0.9719],
        [0.9710]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361606.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9977],
        [0.9948],
        [0.9861],
        ...,
        [0.9728],
        [0.9718],
        [0.9709]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361592.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(793.8325, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1726, device='cuda:0')



h[100].sum tensor(18.9185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4189, device='cuda:0')



h[200].sum tensor(22.7276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.7507, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32077.6680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(165881.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-70.8365, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.4983, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-176.7740, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9977],
        [0.9948],
        [0.9861],
        ...,
        [0.9728],
        [0.9718],
        [0.9709]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361592.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9977],
        [0.9947],
        [0.9860],
        ...,
        [0.9727],
        [0.9717],
        [0.9708]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361578.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0106,  0.0411,  0.0283,  ...,  0.0384,  0.0356,  0.0380],
        [ 0.0068,  0.0273,  0.0185,  ...,  0.0252,  0.0236,  0.0252],
        [ 0.0033,  0.0144,  0.0094,  ...,  0.0128,  0.0124,  0.0132],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1321.0767, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.9320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.9695, device='cuda:0')



h[100].sum tensor(20.7249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.5363, device='cuda:0')



h[200].sum tensor(26.5645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.7997, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0309, 0.1221, 0.0833,  ..., 0.1130, 0.1055, 0.1126],
        [0.0220, 0.0895, 0.0602,  ..., 0.0818, 0.0773, 0.0823],
        [0.0118, 0.0506, 0.0331,  ..., 0.0452, 0.0435, 0.0461],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45668.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3202, 0.0000,  ..., 0.1311, 0.0000, 0.0969],
        [0.0000, 0.2732, 0.0000,  ..., 0.1131, 0.0000, 0.0813],
        [0.0000, 0.2128, 0.0000,  ..., 0.0899, 0.0000, 0.0617],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(238241.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-135.7746, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.3720, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-274.8510, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9977],
        [0.9947],
        [0.9860],
        ...,
        [0.9727],
        [0.9717],
        [0.9708]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361578.5000, device='cuda:0', grad_fn=<SumBackward0>)
time passed so far:
 0:00:23.951744
evaluation loss: 602.9794311523438
epoch: 0 mean loss: 590.0025024414062
=> saveing checkpoint at epoch 0
checkpoint is saved at: /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9977],
        [0.9947],
        [0.9860],
        ...,
        [0.9726],
        [0.9716],
        [0.9707]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361564.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0043,  0.0179,  0.0119,  ...,  0.0162,  0.0155,  0.0165],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1139.7568, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.7751, device='cuda:0')



h[100].sum tensor(20.0943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5198, device='cuda:0')



h[200].sum tensor(25.2250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.1971, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0166, 0.0679, 0.0454,  ..., 0.0617, 0.0585, 0.0622],
        [0.0133, 0.0561, 0.0370,  ..., 0.0504, 0.0483, 0.0513],
        [0.0030, 0.0143, 0.0085,  ..., 0.0116, 0.0122, 0.0125],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40388.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2420, 0.0000,  ..., 0.1010, 0.0000, 0.0720],
        [0.0000, 0.2073, 0.0000,  ..., 0.0873, 0.0000, 0.0611],
        [0.0000, 0.1655, 0.0000,  ..., 0.0706, 0.0000, 0.0487],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(203292.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-110.2878, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.9531, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-236.8729, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9977],
        [0.9947],
        [0.9860],
        ...,
        [0.9726],
        [0.9716],
        [0.9707]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361564.4062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 0.0 event: 0 loss: tensor(68.2985, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9977],
        [0.9947],
        [0.9859],
        ...,
        [0.9725],
        [0.9716],
        [0.9706]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361550.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.3519, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1035.8268, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.9663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.6311, device='cuda:0')



h[100].sum tensor(19.7308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8376, device='cuda:0')



h[200].sum tensor(24.4529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.7790, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38947.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(203725.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-104.0510, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.2877, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-226.3516, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9977],
        [0.9947],
        [0.9859],
        ...,
        [0.9725],
        [0.9716],
        [0.9706]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361550.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9976],
        [0.9947],
        [0.9859],
        ...,
        [0.9724],
        [0.9715],
        [0.9705]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361536.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(672.7091, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.5218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5039, device='cuda:0')



h[100].sum tensor(18.4819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.5697, device='cuda:0')



h[200].sum tensor(21.8003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.7409, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0146, 0.0605, 0.0402,  ..., 0.0547, 0.0522, 0.0554],
        [0.0097, 0.0407, 0.0266,  ..., 0.0362, 0.0350, 0.0370],
        [0.0078, 0.0339, 0.0218,  ..., 0.0297, 0.0291, 0.0306],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31563.9336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2033, 0.0000,  ..., 0.0864, 0.0000, 0.0590],
        [0.0000, 0.1631, 0.0000,  ..., 0.0704, 0.0000, 0.0466],
        [0.0000, 0.1237, 0.0000,  ..., 0.0550, 0.0000, 0.0343],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(173624.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-68.6484, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.0610, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-172.9478, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9976],
        [0.9947],
        [0.9859],
        ...,
        [0.9724],
        [0.9715],
        [0.9705]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361536.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9976],
        [0.9946],
        [0.9858],
        ...,
        [0.9723],
        [0.9714],
        [0.9704]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361522.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8990, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0013,  0.0069,  0.0041,  ...,  0.0057,  0.0059,  0.0062],
        [ 0.0038,  0.0162,  0.0107,  ...,  0.0145,  0.0140,  0.0148],
        [ 0.0026,  0.0120,  0.0077,  ...,  0.0105,  0.0103,  0.0109],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(762.2484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2430, device='cuda:0')



h[100].sum tensor(18.7828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1231, device='cuda:0')



h[200].sum tensor(22.4394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.7023, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0133, 0.0579, 0.0378,  ..., 0.0516, 0.0499, 0.0530],
        [0.0078, 0.0379, 0.0237,  ..., 0.0325, 0.0326, 0.0344],
        [0.0068, 0.0320, 0.0200,  ..., 0.0274, 0.0274, 0.0289],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31987.8828, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1396, 0.0000,  ..., 0.0621, 0.0000, 0.0370],
        [0.0000, 0.1250, 0.0000,  ..., 0.0565, 0.0000, 0.0321],
        [0.0000, 0.1070, 0.0000,  ..., 0.0491, 0.0000, 0.0271],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(168956.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-70.4336, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.7216, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-175.9050, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9976],
        [0.9946],
        [0.9858],
        ...,
        [0.9723],
        [0.9714],
        [0.9704]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361522.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9976],
        [0.9946],
        [0.9857],
        ...,
        [0.9722],
        [0.9713],
        [0.9704]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361507.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.0282, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(692.4146, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9175, device='cuda:0')



h[100].sum tensor(18.5393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7013, device='cuda:0')



h[200].sum tensor(21.9224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.2075, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29896.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0087, 0.0000, 0.0010],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(155844.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-60.7316, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.3075, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-161.1866, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9976],
        [0.9946],
        [0.9857],
        ...,
        [0.9722],
        [0.9713],
        [0.9704]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361507.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9976],
        [0.9946],
        [0.9857],
        ...,
        [0.9721],
        [0.9712],
        [0.9703]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361493.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(288.0699, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0025,  0.0115,  0.0073,  ...,  0.0101,  0.0099,  0.0105],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1110.6146, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5010, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.6755, device='cuda:0')



h[100].sum tensor(19.9581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.1699, device='cuda:0')



h[200].sum tensor(24.9358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.9569, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0103, 0.0430, 0.0282,  ..., 0.0384, 0.0369, 0.0391],
        [0.0025, 0.0127, 0.0073,  ..., 0.0100, 0.0107, 0.0109],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41334.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1243, 0.0000,  ..., 0.0548, 0.0000, 0.0350],
        [0.0000, 0.0562, 0.0000,  ..., 0.0278, 0.0000, 0.0145],
        [0.0000, 0.0158, 0.0000,  ..., 0.0116, 0.0000, 0.0036],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(220453.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-115.3748, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.2075, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-243.5401, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9976],
        [0.9946],
        [0.9857],
        ...,
        [0.9721],
        [0.9712],
        [0.9703]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361493.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9976],
        [0.9946],
        [0.9857],
        ...,
        [0.9721],
        [0.9712],
        [0.9703]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361493.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.2333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(963.4479, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5262, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6597, device='cuda:0')



h[100].sum tensor(19.4571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2103, device='cuda:0')



h[200].sum tensor(23.8717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.5557, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37476.8320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0219, 0.0000,  ..., 0.0143, 0.0000, 0.0048],
        [0.0000, 0.0049, 0.0000,  ..., 0.0073, 0.0000, 0.0005],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(195122.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-97.1688, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.2205, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-216.2509, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9976],
        [0.9946],
        [0.9857],
        ...,
        [0.9721],
        [0.9712],
        [0.9703]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361493.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9976],
        [0.9946],
        [0.9856],
        ...,
        [0.9721],
        [0.9711],
        [0.9702]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361479.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.8365, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0051,  0.0208,  0.0139,  ...,  0.0189,  0.0179,  0.0191],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(706.3862, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9896, device='cuda:0')



h[100].sum tensor(18.5770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7243, device='cuda:0')



h[200].sum tensor(22.0024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.2887, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0090, 0.0382, 0.0249,  ..., 0.0339, 0.0328, 0.0346],
        [0.0051, 0.0220, 0.0139,  ..., 0.0189, 0.0188, 0.0196],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31044.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1729, 0.0000,  ..., 0.0730, 0.0000, 0.0517],
        [0.0000, 0.0865, 0.0000,  ..., 0.0393, 0.0000, 0.0248],
        [0.0000, 0.0289, 0.0000,  ..., 0.0169, 0.0000, 0.0065],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(164883.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-65.6937, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.7485, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-168.8432, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9976],
        [0.9946],
        [0.9856],
        ...,
        [0.9721],
        [0.9711],
        [0.9702]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361479.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9976],
        [0.9945],
        [0.9856],
        ...,
        [0.9720],
        [0.9710],
        [0.9701]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361465.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.5609, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(535.8459, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.5215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.8455, device='cuda:0')



h[100].sum tensor(17.9933, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.7238, device='cuda:0')



h[200].sum tensor(20.7626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.7428, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0305, 0.0194,  ..., 0.0265, 0.0262, 0.0275],
        [0.0034, 0.0161, 0.0097,  ..., 0.0133, 0.0137, 0.0141],
        [0.0063, 0.0266, 0.0172,  ..., 0.0233, 0.0227, 0.0238],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27077.3184, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0803, 0.0000,  ..., 0.0377, 0.0000, 0.0212],
        [0.0000, 0.0719, 0.0000,  ..., 0.0342, 0.0000, 0.0191],
        [0.0000, 0.0861, 0.0000,  ..., 0.0395, 0.0000, 0.0241],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(146740.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-47.1487, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-51.9765, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-140.6332, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9976],
        [0.9945],
        [0.9856],
        ...,
        [0.9720],
        [0.9710],
        [0.9701]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361465.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9976],
        [0.9945],
        [0.9855],
        ...,
        [0.9719],
        [0.9709],
        [0.9700]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361451.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(149.5398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0017,  0.0085,  0.0052,  ...,  0.0072,  0.0073,  0.0077],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(465.8071, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.0154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(13.3284, device='cuda:0')



h[100].sum tensor(17.7519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.2411, device='cuda:0')



h[200].sum tensor(20.2500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.0318, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0017, 0.0098, 0.0052,  ..., 0.0072, 0.0082, 0.0082],
        [0.0026, 0.0131, 0.0076,  ..., 0.0104, 0.0111, 0.0113],
        [0.0086, 0.0406, 0.0256,  ..., 0.0351, 0.0349, 0.0369],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(25813.1621, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0342, 0.0000,  ..., 0.0195, 0.0000, 0.0067],
        [0.0000, 0.0504, 0.0000,  ..., 0.0262, 0.0000, 0.0109],
        [0.0000, 0.0775, 0.0000,  ..., 0.0374, 0.0000, 0.0180],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(142682.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-40.6564, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-53.3317, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-131.2000, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9976],
        [0.9945],
        [0.9855],
        ...,
        [0.9719],
        [0.9709],
        [0.9700]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361451.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9976],
        [0.9945],
        [0.9855],
        ...,
        [0.9718],
        [0.9708],
        [0.9699]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361437.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5518],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4784, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0068,  0.0272,  0.0185,  ...,  0.0251,  0.0235,  0.0251],
        [ 0.0069,  0.0276,  0.0187,  ...,  0.0254,  0.0238,  0.0254],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(774.7939, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2055, device='cuda:0')



h[100].sum tensor(18.7936, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1112, device='cuda:0')



h[200].sum tensor(22.4625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.6600, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0086, 0.0369, 0.0239,  ..., 0.0326, 0.0317, 0.0334],
        [0.0118, 0.0506, 0.0331,  ..., 0.0452, 0.0436, 0.0462],
        [0.0259, 0.1038, 0.0703,  ..., 0.0955, 0.0896, 0.0956],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32119.8789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1198, 0.0000,  ..., 0.0531, 0.0000, 0.0332],
        [0.0000, 0.1858, 0.0000,  ..., 0.0790, 0.0000, 0.0540],
        [0.0000, 0.2792, 0.0000,  ..., 0.1153, 0.0000, 0.0836],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(168820.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-70.7652, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.6743, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-176.7750, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9976],
        [0.9945],
        [0.9855],
        ...,
        [0.9718],
        [0.9708],
        [0.9699]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361437.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 10.0 event: 50 loss: tensor(536.3282, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9975],
        [0.9945],
        [0.9854],
        ...,
        [0.9717],
        [0.9708],
        [0.9698]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361422.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3843],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.8324, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0021,  0.0099,  0.0062,  ...,  0.0085,  0.0085,  0.0090],
        [ 0.0026,  0.0119,  0.0076,  ...,  0.0104,  0.0102,  0.0108],
        [ 0.0047,  0.0193,  0.0129,  ...,  0.0175,  0.0167,  0.0177],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1189.0822, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.0311, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.9913, device='cuda:0')



h[100].sum tensor(20.1878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5886, device='cuda:0')



h[200].sum tensor(25.4236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.4409, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0114, 0.0510, 0.0330,  ..., 0.0450, 0.0439, 0.0466],
        [0.0125, 0.0547, 0.0356,  ..., 0.0486, 0.0471, 0.0500],
        [0.0080, 0.0382, 0.0239,  ..., 0.0328, 0.0329, 0.0347],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40142.6992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1044, 0.0000,  ..., 0.0478, 0.0000, 0.0272],
        [0.0000, 0.1330, 0.0000,  ..., 0.0593, 0.0000, 0.0356],
        [0.0000, 0.1619, 0.0000,  ..., 0.0708, 0.0000, 0.0441],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(198400.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-109.5110, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.1674, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-235.1113, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9975],
        [0.9945],
        [0.9854],
        ...,
        [0.9717],
        [0.9708],
        [0.9698]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361422.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9975],
        [0.9944],
        [0.9854],
        ...,
        [0.9716],
        [0.9707],
        [0.9697]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361408.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(421.8401, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1725.8478, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.3420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.5984, device='cuda:0')



h[100].sum tensor(21.9907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.9637, device='cuda:0')



h[200].sum tensor(29.2528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.4036, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0031, 0.0147, 0.0087,  ..., 0.0119, 0.0124, 0.0128],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51462.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0025, 0.0000,  ..., 0.0063, 0.0000, 0.0002],
        [0.0000, 0.0115, 0.0000,  ..., 0.0099, 0.0000, 0.0022],
        [0.0000, 0.0366, 0.0000,  ..., 0.0202, 0.0000, 0.0082],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(253267.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-163.7312, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-29.7189, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-317.1924, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9975],
        [0.9944],
        [0.9854],
        ...,
        [0.9716],
        [0.9707],
        [0.9697]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361408.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9975],
        [0.9944],
        [0.9853],
        ...,
        [0.9715],
        [0.9706],
        [0.9696]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361394.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4951],
        [0.5117],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5466, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0069,  0.0274,  0.0186,  ...,  0.0252,  0.0236,  0.0252],
        [ 0.0102,  0.0396,  0.0273,  ...,  0.0369,  0.0342,  0.0366],
        [ 0.0072,  0.0284,  0.0193,  ...,  0.0262,  0.0246,  0.0262],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1202.3604, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.9699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5006, device='cuda:0')



h[100].sum tensor(20.2177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7506, device='cuda:0')



h[200].sum tensor(25.4872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0153, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0310, 0.1223, 0.0834,  ..., 0.1132, 0.1057, 0.1128],
        [0.0365, 0.1424, 0.0977,  ..., 0.1324, 0.1231, 0.1315],
        [0.0298, 0.1179, 0.0803,  ..., 0.1090, 0.1019, 0.1087],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41580.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.0148e-01, 0.0000e+00,  ..., 1.2327e-01, 0.0000e+00,
         9.1961e-02],
        [0.0000e+00, 3.6576e-01, 0.0000e+00,  ..., 1.4823e-01, 0.0000e+00,
         1.1252e-01],
        [0.0000e+00, 3.5171e-01, 0.0000e+00,  ..., 1.4306e-01, 0.0000e+00,
         1.0757e-01],
        ...,
        [0.0000e+00, 3.5685e-04, 0.0000e+00,  ..., 5.0133e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5712e-04, 0.0000e+00,  ..., 5.0128e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5713e-04, 0.0000e+00,  ..., 5.0126e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(210497.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-116.6139, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.8352, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-245.5195, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9975],
        [0.9944],
        [0.9853],
        ...,
        [0.9715],
        [0.9706],
        [0.9696]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361394.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9975],
        [0.9944],
        [0.9852],
        ...,
        [0.9714],
        [0.9705],
        [0.9695]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361380.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.6185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1153.4528, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0809, device='cuda:0')



h[100].sum tensor(20.0459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2989, device='cuda:0')



h[200].sum tensor(25.1222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.4142, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41337.4805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(216570.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-115.4210, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.9562, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-243.8390, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9975],
        [0.9944],
        [0.9852],
        ...,
        [0.9714],
        [0.9705],
        [0.9695]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361380.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9975],
        [0.9944],
        [0.9852],
        ...,
        [0.9714],
        [0.9704],
        [0.9695]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361366.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.5134, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0027,  0.0123,  0.0079,  ...,  0.0108,  0.0106,  0.0112],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0050,  0.0206,  0.0138,  ...,  0.0187,  0.0177,  0.0189],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1013.1687, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.2997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.6651, device='cuda:0')



h[100].sum tensor(19.5678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5302, device='cuda:0')



h[200].sum tensor(24.1068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.6895, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0113, 0.0063,  ..., 0.0087, 0.0095, 0.0096],
        [0.0119, 0.0528, 0.0342,  ..., 0.0468, 0.0455, 0.0483],
        [0.0063, 0.0301, 0.0187,  ..., 0.0256, 0.0258, 0.0272],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38773.1016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0479, 0.0000,  ..., 0.0249, 0.0000, 0.0112],
        [0.0000, 0.0972, 0.0000,  ..., 0.0450, 0.0000, 0.0246],
        [0.0000, 0.0994, 0.0000,  ..., 0.0462, 0.0000, 0.0243],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(203555.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-102.2040, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.0404, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-224.2829, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9975],
        [0.9944],
        [0.9852],
        ...,
        [0.9714],
        [0.9704],
        [0.9695]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361366.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9975],
        [0.9943],
        [0.9851],
        ...,
        [0.9713],
        [0.9703],
        [0.9694]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361351.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3511],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.7086, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0024,  0.0109,  0.0069,  ...,  0.0095,  0.0094,  0.0099],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0024,  0.0109,  0.0069,  ...,  0.0095,  0.0094,  0.0099],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(729.7258, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2456, device='cuda:0')



h[100].sum tensor(18.6114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8057, device='cuda:0')



h[200].sum tensor(22.0754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.5774, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0018, 0.0102, 0.0056,  ..., 0.0076, 0.0086, 0.0086],
        [0.0083, 0.0397, 0.0249,  ..., 0.0342, 0.0341, 0.0360],
        [0.0018, 0.0102, 0.0055,  ..., 0.0076, 0.0085, 0.0086],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31305.5488, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0326, 0.0000,  ..., 0.0188, 0.0000, 0.0067],
        [0.0000, 0.0557, 0.0000,  ..., 0.0283, 0.0000, 0.0128],
        [0.0000, 0.0324, 0.0000,  ..., 0.0187, 0.0000, 0.0066],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(166408.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-67.5652, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.0943, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-171.2844, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9975],
        [0.9943],
        [0.9851],
        ...,
        [0.9713],
        [0.9703],
        [0.9694]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361351.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9975],
        [0.9943],
        [0.9851],
        ...,
        [0.9712],
        [0.9702],
        [0.9693]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361337.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4473],
        [0.3840],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.0198, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0057,  0.0232,  0.0156,  ...,  0.0212,  0.0200,  0.0213],
        [ 0.0052,  0.0214,  0.0144,  ...,  0.0196,  0.0185,  0.0197],
        [ 0.0084,  0.0328,  0.0224,  ...,  0.0304,  0.0284,  0.0303],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(919.7527, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9276, device='cuda:0')



h[100].sum tensor(19.2424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9773, device='cuda:0')



h[200].sum tensor(23.4157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.7300, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0227, 0.0921, 0.0621,  ..., 0.0843, 0.0795, 0.0848],
        [0.0300, 0.1188, 0.0810,  ..., 0.1098, 0.1027, 0.1096],
        [0.0365, 0.1423, 0.0976,  ..., 0.1323, 0.1230, 0.1314],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34417.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.8418e-01, 0.0000e+00,  ..., 1.1747e-01, 0.0000e+00,
         8.4957e-02],
        [0.0000e+00, 3.3881e-01, 0.0000e+00,  ..., 1.3825e-01, 0.0000e+00,
         1.0306e-01],
        [0.0000e+00, 3.9988e-01, 0.0000e+00,  ..., 1.6133e-01, 0.0000e+00,
         1.2321e-01],
        ...,
        [0.0000e+00, 3.5719e-04, 0.0000e+00,  ..., 5.0099e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5746e-04, 0.0000e+00,  ..., 5.0094e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5746e-04, 0.0000e+00,  ..., 5.0091e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(176289.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-82.1858, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.3328, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-193.6777, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9975],
        [0.9943],
        [0.9851],
        ...,
        [0.9712],
        [0.9702],
        [0.9693]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361337.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9974],
        [0.9943],
        [0.9850],
        ...,
        [0.9711],
        [0.9701],
        [0.9692]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361323.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(384.1675, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0017,  0.0084,  0.0052,  ...,  0.0071,  0.0072,  0.0076],
        [ 0.0015,  0.0079,  0.0048,  ...,  0.0066,  0.0068,  0.0071],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1583.4546, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.4392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.2407, device='cuda:0')



h[100].sum tensor(21.4545, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.8953, device='cuda:0')



h[200].sum tensor(28.1140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.6167, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0149, 0.0635, 0.0418,  ..., 0.0570, 0.0548, 0.0582],
        [0.0028, 0.0157, 0.0090,  ..., 0.0124, 0.0134, 0.0138],
        [0.0068, 0.0320, 0.0200,  ..., 0.0274, 0.0274, 0.0289],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51953.2539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1612, 0.0000,  ..., 0.0699, 0.0000, 0.0450],
        [0.0000, 0.0958, 0.0000,  ..., 0.0443, 0.0000, 0.0244],
        [0.0000, 0.1046, 0.0000,  ..., 0.0480, 0.0000, 0.0268],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(271685.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-166.0284, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-29.8502, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-319.9640, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9974],
        [0.9943],
        [0.9850],
        ...,
        [0.9711],
        [0.9701],
        [0.9692]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361323.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9974],
        [0.9943],
        [0.9850],
        ...,
        [0.9711],
        [0.9701],
        [0.9692]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361323.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.2115, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1136.4653, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.4956, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.7773, device='cuda:0')



h[100].sum tensor(19.9608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2023, device='cuda:0')



h[200].sum tensor(24.9415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.0717, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40046.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(209871.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-108.2389, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.2332, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-232.9865, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9974],
        [0.9943],
        [0.9850],
        ...,
        [0.9711],
        [0.9701],
        [0.9692]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361323.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9974],
        [0.9943],
        [0.9850],
        ...,
        [0.9710],
        [0.9700],
        [0.9691]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361309.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3511],
        [0.3350],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(341.3232, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0022,  0.0104,  0.0066,  ...,  0.0090,  0.0090,  0.0095],
        [ 0.0024,  0.0109,  0.0069,  ...,  0.0095,  0.0094,  0.0099],
        [ 0.0022,  0.0104,  0.0066,  ...,  0.0090,  0.0090,  0.0095],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1372.3484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.8993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.4220, device='cuda:0')



h[100].sum tensor(20.7409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6802, device='cuda:0')



h[200].sum tensor(26.5984, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.3100, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0106, 0.0479, 0.0307,  ..., 0.0420, 0.0412, 0.0436],
        [0.0102, 0.0465, 0.0297,  ..., 0.0407, 0.0400, 0.0423],
        [0.0040, 0.0202, 0.0121,  ..., 0.0167, 0.0172, 0.0179],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46758.4961, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0927, 0.0000,  ..., 0.0434, 0.0000, 0.0227],
        [0.0000, 0.0936, 0.0000,  ..., 0.0438, 0.0000, 0.0227],
        [0.0000, 0.0726, 0.0000,  ..., 0.0353, 0.0000, 0.0166],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(244685.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-140.7136, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-34.4771, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-282.4439, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9974],
        [0.9943],
        [0.9850],
        ...,
        [0.9710],
        [0.9700],
        [0.9691]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361309.1250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 20.0 event: 100 loss: tensor(555.5646, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9974],
        [0.9942],
        [0.9849],
        ...,
        [0.9709],
        [0.9700],
        [0.9690]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361294.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.2831, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0016,  0.0080,  0.0049,  ...,  0.0067,  0.0068,  0.0072],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(879.3444, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2766, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1489, device='cuda:0')



h[100].sum tensor(19.0904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7296, device='cuda:0')



h[200].sum tensor(23.0927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.8518, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0308, 0.0196,  ..., 0.0268, 0.0264, 0.0278],
        [0.0015, 0.0092, 0.0048,  ..., 0.0067, 0.0077, 0.0077],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33965.8672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1102, 0.0000,  ..., 0.0497, 0.0000, 0.0298],
        [0.0000, 0.0557, 0.0000,  ..., 0.0279, 0.0000, 0.0139],
        [0.0000, 0.0179, 0.0000,  ..., 0.0125, 0.0000, 0.0041],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(177359.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-79.6422, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.9147, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-190.1652, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9974],
        [0.9942],
        [0.9849],
        ...,
        [0.9709],
        [0.9700],
        [0.9690]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361294.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9974],
        [0.9942],
        [0.9849],
        ...,
        [0.9709],
        [0.9700],
        [0.9690]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361294.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.5594, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(724.5688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0540, device='cuda:0')



h[100].sum tensor(18.5747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7448, device='cuda:0')



h[200].sum tensor(21.9976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.3614, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0044, 0.0197, 0.0123,  ..., 0.0167, 0.0168, 0.0174],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29806.8789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0057, 0.0000,  ..., 0.0076, 0.0000, 0.0005],
        [0.0000, 0.0173, 0.0000,  ..., 0.0122, 0.0000, 0.0039],
        [0.0000, 0.0527, 0.0000,  ..., 0.0262, 0.0000, 0.0139],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(154081.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-60.5370, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.1963, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-160.8457, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9974],
        [0.9942],
        [0.9849],
        ...,
        [0.9709],
        [0.9700],
        [0.9690]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361294.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9974],
        [0.9942],
        [0.9849],
        ...,
        [0.9708],
        [0.9699],
        [0.9689]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361280.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.4166, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1166.7312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.2412, device='cuda:0')



h[100].sum tensor(20.0408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3499, device='cuda:0')



h[200].sum tensor(25.1114, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.5949, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41449.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0268, 0.0000,  ..., 0.0161, 0.0000, 0.0063],
        [0.0000, 0.0074, 0.0000,  ..., 0.0083, 0.0000, 0.0012],
        [0.0000, 0.0018, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(216299.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-115.2476, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.4158, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-243.8703, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9974],
        [0.9942],
        [0.9849],
        ...,
        [0.9708],
        [0.9699],
        [0.9689]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361280.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9974],
        [0.9942],
        [0.9848],
        ...,
        [0.9707],
        [0.9698],
        [0.9688]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361266.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.4037, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0013,  0.0069,  0.0041,  ...,  0.0057,  0.0059,  0.0062],
        [ 0.0017,  0.0087,  0.0054,  ...,  0.0074,  0.0075,  0.0079],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [ 0.0024,  0.0111,  0.0071,  ...,  0.0097,  0.0096,  0.0101],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1017.7315, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.3588, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.5662, device='cuda:0')



h[100].sum tensor(19.5389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.4987, device='cuda:0')



h[200].sum tensor(24.0454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.5780, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0148, 0.0633, 0.0416,  ..., 0.0567, 0.0545, 0.0579],
        [0.0057, 0.0263, 0.0165,  ..., 0.0225, 0.0225, 0.0236],
        [0.0017, 0.0099, 0.0053,  ..., 0.0073, 0.0083, 0.0083],
        ...,
        [0.0189, 0.0759, 0.0512,  ..., 0.0695, 0.0655, 0.0698],
        [0.0165, 0.0674, 0.0452,  ..., 0.0614, 0.0582, 0.0619],
        [0.0119, 0.0489, 0.0325,  ..., 0.0441, 0.0421, 0.0446]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36189.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1463, 0.0000,  ..., 0.0644, 0.0000, 0.0399],
        [0.0000, 0.0973, 0.0000,  ..., 0.0447, 0.0000, 0.0255],
        [0.0000, 0.0493, 0.0000,  ..., 0.0253, 0.0000, 0.0117],
        ...,
        [0.0000, 0.3133, 0.0000,  ..., 0.1273, 0.0000, 0.0960],
        [0.0000, 0.2515, 0.0000,  ..., 0.1036, 0.0000, 0.0760],
        [0.0000, 0.1750, 0.0000,  ..., 0.0739, 0.0000, 0.0519]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(186438.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-90.7295, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.5521, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-206.7569, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9974],
        [0.9942],
        [0.9848],
        ...,
        [0.9707],
        [0.9698],
        [0.9688]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361266.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9974],
        [0.9942],
        [0.9847],
        ...,
        [0.9706],
        [0.9697],
        [0.9687]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361252.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.6886, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0029,  0.0131,  0.0084,  ...,  0.0115,  0.0112,  0.0119],
        [ 0.0029,  0.0131,  0.0084,  ...,  0.0115,  0.0112,  0.0119],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(849.3998, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2938, device='cuda:0')



h[100].sum tensor(18.9744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4574, device='cuda:0')



h[200].sum tensor(22.8465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.8874, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0052, 0.0246, 0.0152,  ..., 0.0208, 0.0210, 0.0220],
        [0.0075, 0.0348, 0.0219,  ..., 0.0300, 0.0298, 0.0314],
        [0.0116, 0.0496, 0.0325,  ..., 0.0443, 0.0427, 0.0453],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33146.5664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1140, 0.0000,  ..., 0.0511, 0.0000, 0.0313],
        [0.0000, 0.1622, 0.0000,  ..., 0.0700, 0.0000, 0.0463],
        [0.0000, 0.2208, 0.0000,  ..., 0.0927, 0.0000, 0.0652],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(172657.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-76.6749, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.0402, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-185.1921, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9974],
        [0.9942],
        [0.9847],
        ...,
        [0.9706],
        [0.9697],
        [0.9687]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361252.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9974],
        [0.9941],
        [0.9847],
        ...,
        [0.9706],
        [0.9696],
        [0.9686]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361237.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0083,  0.0051,  ...,  0.0070,  0.0071,  0.0075],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(891.9484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3108, device='cuda:0')



h[100].sum tensor(19.1100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7811, device='cuda:0')



h[200].sum tensor(23.1344, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.0344, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0016, 0.0095, 0.0051,  ..., 0.0070, 0.0080, 0.0080],
        [0.0039, 0.0196, 0.0117,  ..., 0.0161, 0.0167, 0.0173],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35567.8164, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0141, 0.0000,  ..., 0.0112, 0.0000, 0.0020],
        [0.0000, 0.0444, 0.0000,  ..., 0.0235, 0.0000, 0.0097],
        [0.0000, 0.0850, 0.0000,  ..., 0.0400, 0.0000, 0.0213],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(189523.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-87.5545, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.5943, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-201.6042, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9974],
        [0.9941],
        [0.9847],
        ...,
        [0.9706],
        [0.9696],
        [0.9686]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361237.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9973],
        [0.9941],
        [0.9846],
        ...,
        [0.9705],
        [0.9695],
        [0.9685]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361223.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.6964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0055,  0.0225,  0.0151,  ...,  0.0205,  0.0194,  0.0207],
        [ 0.0057,  0.0232,  0.0157,  ...,  0.0213,  0.0201,  0.0214],
        [ 0.0048,  0.0197,  0.0131,  ...,  0.0179,  0.0170,  0.0181],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(738.9407, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1554, device='cuda:0')



h[100].sum tensor(18.5985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7770, device='cuda:0')



h[200].sum tensor(22.0481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.4757, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0206, 0.0846, 0.0567,  ..., 0.0771, 0.0730, 0.0777],
        [0.0223, 0.0908, 0.0611,  ..., 0.0831, 0.0784, 0.0835],
        [0.0279, 0.1111, 0.0755,  ..., 0.1025, 0.0960, 0.1024],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30483.0293, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2286, 0.0000,  ..., 0.0961, 0.0000, 0.0670],
        [0.0000, 0.2485, 0.0000,  ..., 0.1039, 0.0000, 0.0732],
        [0.0000, 0.2687, 0.0000,  ..., 0.1117, 0.0000, 0.0797],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(160620., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-63.6761, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.7784, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-165.3682, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9973],
        [0.9941],
        [0.9846],
        ...,
        [0.9705],
        [0.9695],
        [0.9685]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361223.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9973],
        [0.9941],
        [0.9846],
        ...,
        [0.9704],
        [0.9694],
        [0.9685]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361209.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2839],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.0151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0018,  0.0089,  0.0055,  ...,  0.0076,  0.0076,  0.0081],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(889.3457, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9468, device='cuda:0')



h[100].sum tensor(19.0903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6652, device='cuda:0')



h[200].sum tensor(23.0927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.6238, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0080, 0.0384, 0.0240,  ..., 0.0330, 0.0330, 0.0348],
        [0.0014, 0.0086, 0.0044,  ..., 0.0061, 0.0071, 0.0071],
        [0.0018, 0.0101, 0.0055,  ..., 0.0075, 0.0085, 0.0085],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34175.6055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0939, 0.0000,  ..., 0.0440, 0.0000, 0.0232],
        [0.0000, 0.0567, 0.0000,  ..., 0.0287, 0.0000, 0.0131],
        [0.0000, 0.0334, 0.0000,  ..., 0.0191, 0.0000, 0.0067],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(178670.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.8946, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.7577, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.6384, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9973],
        [0.9941],
        [0.9846],
        ...,
        [0.9704],
        [0.9694],
        [0.9685]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361209.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9973],
        [0.9940],
        [0.9845],
        ...,
        [0.9703],
        [0.9693],
        [0.9684]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361194.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.6747, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1098.0249, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.8381, device='cuda:0')



h[100].sum tensor(19.7728, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9035, device='cuda:0')



h[200].sum tensor(24.5422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.0125, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0030, 0.0163, 0.0094,  ..., 0.0130, 0.0139, 0.0143],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0023, 0.0118, 0.0067,  ..., 0.0092, 0.0100, 0.0101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38863.9648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 5.1840e-03, 0.0000e+00,  ..., 7.4391e-03, 0.0000e+00,
         4.2606e-04],
        [0.0000e+00, 2.7822e-02, 0.0000e+00,  ..., 1.6617e-02, 0.0000e+00,
         6.3416e-03],
        [0.0000e+00, 9.5270e-02, 0.0000e+00,  ..., 4.3529e-02, 0.0000e+00,
         2.5649e-02],
        ...,
        [0.0000e+00, 2.0550e-03, 0.0000e+00,  ..., 5.6982e-03, 0.0000e+00,
         4.9198e-05],
        [0.0000e+00, 1.8879e-02, 0.0000e+00,  ..., 1.2492e-02, 0.0000e+00,
         4.4623e-03],
        [0.0000e+00, 5.4911e-02, 0.0000e+00,  ..., 2.7123e-02, 0.0000e+00,
         1.3796e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(201664.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-102.7803, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.6551, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-225.2919, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9973],
        [0.9940],
        [0.9845],
        ...,
        [0.9703],
        [0.9693],
        [0.9684]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361194.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9973],
        [0.9940],
        [0.9845],
        ...,
        [0.9702],
        [0.9692],
        [0.9683]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361180.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.6604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(679.4529, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7129, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9043, device='cuda:0')



h[100].sum tensor(18.3885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.3789, device='cuda:0')



h[200].sum tensor(21.6020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.0648, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28861.6465, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0018, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(151917.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-55.9852, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.1784, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-153.8217, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9973],
        [0.9940],
        [0.9845],
        ...,
        [0.9702],
        [0.9692],
        [0.9683]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361180.5625, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 30.0 event: 150 loss: tensor(540.0342, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9973],
        [0.9940],
        [0.9844],
        ...,
        [0.9701],
        [0.9691],
        [0.9682]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361166.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(238.5350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(908.0134, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.2605, device='cuda:0')



h[100].sum tensor(19.1353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7651, device='cuda:0')



h[200].sum tensor(23.1882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.9777, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34481.5273, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0018, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(180285.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-82.3476, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.6058, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-193.7049, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9973],
        [0.9940],
        [0.9844],
        ...,
        [0.9701],
        [0.9691],
        [0.9682]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361166.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9973],
        [0.9940],
        [0.9843],
        ...,
        [0.9700],
        [0.9691],
        [0.9681]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361151.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(265.2435, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0020,  0.0097,  0.0061,  ...,  0.0083,  0.0083,  0.0088],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1069.3357, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.6410, device='cuda:0')



h[100].sum tensor(19.6593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5225, device='cuda:0')



h[200].sum tensor(24.3011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.6624, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0034, 0.0180, 0.0106,  ..., 0.0145, 0.0153, 0.0159],
        [0.0096, 0.0424, 0.0273,  ..., 0.0373, 0.0364, 0.0385],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38966.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0328, 0.0000,  ..., 0.0188, 0.0000, 0.0070],
        [0.0000, 0.0849, 0.0000,  ..., 0.0398, 0.0000, 0.0216],
        [0.0000, 0.1577, 0.0000,  ..., 0.0686, 0.0000, 0.0440],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(206031.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-103.8604, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.1067, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-226.7012, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9973],
        [0.9940],
        [0.9843],
        ...,
        [0.9700],
        [0.9691],
        [0.9681]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361151.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9973],
        [0.9939],
        [0.9843],
        ...,
        [0.9699],
        [0.9690],
        [0.9680]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361137.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.6642, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(684.0956, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8155, device='cuda:0')



h[100].sum tensor(18.3905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.3507, device='cuda:0')



h[200].sum tensor(21.6063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.9646, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29276.2070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(155330.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-57.3117, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.2780, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-156.0775, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9973],
        [0.9939],
        [0.9843],
        ...,
        [0.9699],
        [0.9690],
        [0.9680]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361137.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9972],
        [0.9939],
        [0.9842],
        ...,
        [0.9699],
        [0.9689],
        [0.9679]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361123.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2788],
        [0.3132],
        [0.2900],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.2019, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0089,  0.0347,  0.0238,  ...,  0.0322,  0.0300,  0.0320],
        [ 0.0095,  0.0370,  0.0254,  ...,  0.0344,  0.0320,  0.0341],
        [ 0.0075,  0.0296,  0.0202,  ...,  0.0274,  0.0256,  0.0273],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1406.7019, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.8790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.8764, device='cuda:0')



h[100].sum tensor(20.7508, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.5066, device='cuda:0')



h[200].sum tensor(26.6194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.6947, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0370, 0.1441, 0.0989,  ..., 0.1340, 0.1246, 0.1331],
        [0.0381, 0.1483, 0.1018,  ..., 0.1380, 0.1282, 0.1370],
        [0.0417, 0.1615, 0.1112,  ..., 0.1507, 0.1396, 0.1493],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47954.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 4.2222e-01, 0.0000e+00,  ..., 1.6992e-01, 0.0000e+00,
         1.3077e-01],
        [0.0000e+00, 4.5826e-01, 0.0000e+00,  ..., 1.8351e-01, 0.0000e+00,
         1.4274e-01],
        [0.0000e+00, 4.7861e-01, 0.0000e+00,  ..., 1.9099e-01, 0.0000e+00,
         1.4965e-01],
        ...,
        [0.0000e+00, 3.5845e-04, 0.0000e+00,  ..., 4.9968e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5871e-04, 0.0000e+00,  ..., 4.9963e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5872e-04, 0.0000e+00,  ..., 4.9961e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(259131.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-147.3605, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-32.7230, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-292.0439, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9972],
        [0.9939],
        [0.9842],
        ...,
        [0.9699],
        [0.9689],
        [0.9679]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361123.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9972],
        [0.9939],
        [0.9842],
        ...,
        [0.9698],
        [0.9688],
        [0.9678]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361108.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.3772, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0020,  0.0095,  0.0059,  ...,  0.0081,  0.0081,  0.0086],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(705.6484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.5825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3247, device='cuda:0')



h[100].sum tensor(18.4522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.5127, device='cuda:0')



h[200].sum tensor(21.7373, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.5389, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0015, 0.0090, 0.0047,  ..., 0.0065, 0.0075, 0.0075],
        [0.0020, 0.0107, 0.0059,  ..., 0.0081, 0.0090, 0.0090],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29925.5195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0284, 0.0000,  ..., 0.0172, 0.0000, 0.0053],
        [0.0000, 0.0221, 0.0000,  ..., 0.0145, 0.0000, 0.0038],
        [0.0000, 0.0071, 0.0000,  ..., 0.0082, 0.0000, 0.0009],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(159679.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-60.2285, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.6181, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-160.9192, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9972],
        [0.9939],
        [0.9842],
        ...,
        [0.9698],
        [0.9688],
        [0.9678]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361108.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9972],
        [0.9939],
        [0.9841],
        ...,
        [0.9697],
        [0.9687],
        [0.9677]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361094.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.8457, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0040,  0.0170,  0.0112,  ...,  0.0153,  0.0146,  0.0155],
        [ 0.0047,  0.0193,  0.0129,  ...,  0.0175,  0.0167,  0.0178],
        [ 0.0140,  0.0534,  0.0370,  ...,  0.0501,  0.0462,  0.0494],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1191.3011, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.8338, device='cuda:0')



h[100].sum tensor(20.0323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2203, device='cuda:0')



h[200].sum tensor(25.0935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.1354, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0333, 0.1306, 0.0893,  ..., 0.1211, 0.1129, 0.1206],
        [0.0379, 0.1476, 0.1014,  ..., 0.1374, 0.1276, 0.1364],
        [0.0222, 0.0901, 0.0606,  ..., 0.0824, 0.0778, 0.0829],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42943.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.8726e-01, 0.0000e+00,  ..., 1.5670e-01, 0.0000e+00,
         1.1905e-01],
        [0.0000e+00, 4.1346e-01, 0.0000e+00,  ..., 1.6658e-01, 0.0000e+00,
         1.2774e-01],
        [0.0000e+00, 3.4145e-01, 0.0000e+00,  ..., 1.3914e-01, 0.0000e+00,
         1.0414e-01],
        ...,
        [0.0000e+00, 3.5861e-04, 0.0000e+00,  ..., 4.9951e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5888e-04, 0.0000e+00,  ..., 4.9946e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5888e-04, 0.0000e+00,  ..., 4.9944e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(229073.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-122.8633, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.4306, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-255.5128, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9972],
        [0.9939],
        [0.9841],
        ...,
        [0.9697],
        [0.9687],
        [0.9677]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361094.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9972],
        [0.9938],
        [0.9841],
        ...,
        [0.9696],
        [0.9686],
        [0.9676]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361080.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.1343, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1123.6036, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.1465, device='cuda:0')



h[100].sum tensor(19.8050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0016, device='cuda:0')



h[200].sum tensor(24.6107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.3603, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0051, 0.0239, 0.0148,  ..., 0.0202, 0.0204, 0.0214],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40354.2539, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0095, 0.0000,  ..., 0.0091, 0.0000, 0.0016],
        [0.0000, 0.0192, 0.0000,  ..., 0.0132, 0.0000, 0.0038],
        [0.0000, 0.0587, 0.0000,  ..., 0.0294, 0.0000, 0.0136],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(214730.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-110.2716, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.4224, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-235.9489, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9972],
        [0.9938],
        [0.9841],
        ...,
        [0.9696],
        [0.9686],
        [0.9676]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361080.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9972],
        [0.9938],
        [0.9840],
        ...,
        [0.9695],
        [0.9685],
        [0.9675]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361065.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3115],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4121, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0020,  0.0097,  0.0061,  ...,  0.0084,  0.0084,  0.0088],
        [ 0.0011,  0.0064,  0.0037,  ...,  0.0052,  0.0055,  0.0057],
        [ 0.0037,  0.0157,  0.0103,  ...,  0.0141,  0.0135,  0.0144],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(707.4810, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.5975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2387, device='cuda:0')



h[100].sum tensor(18.4449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4853, device='cuda:0')



h[200].sum tensor(21.7217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.4419, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0040, 0.0201, 0.0121,  ..., 0.0166, 0.0171, 0.0178],
        [0.0118, 0.0523, 0.0338,  ..., 0.0462, 0.0450, 0.0477],
        [0.0068, 0.0321, 0.0200,  ..., 0.0275, 0.0275, 0.0290],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30568.1445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0781, 0.0000,  ..., 0.0373, 0.0000, 0.0193],
        [0.0000, 0.1156, 0.0000,  ..., 0.0525, 0.0000, 0.0300],
        [0.0000, 0.1040, 0.0000,  ..., 0.0478, 0.0000, 0.0265],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(163680.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-64.1062, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.6680, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-166.0363, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9972],
        [0.9938],
        [0.9840],
        ...,
        [0.9695],
        [0.9685],
        [0.9675]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361065.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9972],
        [0.9938],
        [0.9839],
        ...,
        [0.9694],
        [0.9684],
        [0.9675]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361051.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(377.8278, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1618.7336, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.5490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.6756, device='cuda:0')



h[100].sum tensor(21.4008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.7155, device='cuda:0')



h[200].sum tensor(27.9999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.9794, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0008, 0.0063, 0.0028,  ..., 0.0040, 0.0052, 0.0050]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49099.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.1400e-04, 0.0000e+00,  ..., 5.3731e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.1603e-04, 0.0000e+00,  ..., 5.3594e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.5893e-03, 0.0000e+00,  ..., 6.2617e-03, 0.0000e+00,
         2.2633e-04],
        ...,
        [0.0000e+00, 4.1155e-03, 0.0000e+00,  ..., 6.6086e-03, 0.0000e+00,
         2.4533e-05],
        [0.0000e+00, 9.6158e-03, 0.0000e+00,  ..., 8.9741e-03, 0.0000e+00,
         7.2432e-04],
        [0.0000e+00, 1.8602e-02, 0.0000e+00,  ..., 1.2840e-02, 0.0000e+00,
         2.0542e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(247000.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-152.3691, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-31.4565, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-300.5484, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9972],
        [0.9938],
        [0.9839],
        ...,
        [0.9694],
        [0.9684],
        [0.9675]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361051.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9972],
        [0.9938],
        [0.9839],
        ...,
        [0.9693],
        [0.9683],
        [0.9674]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361036.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.1949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1009.0882, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1019, device='cuda:0')



h[100].sum tensor(19.4145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3510, device='cuda:0')



h[200].sum tensor(23.7811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.0544, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0044, 0.0198, 0.0123,  ..., 0.0168, 0.0169, 0.0175],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36663.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0675, 0.0000,  ..., 0.0323, 0.0000, 0.0181],
        [0.0000, 0.0361, 0.0000,  ..., 0.0197, 0.0000, 0.0089],
        [0.0000, 0.0206, 0.0000,  ..., 0.0136, 0.0000, 0.0040],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(191843.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-92.8362, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.4603, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-209.6930, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9972],
        [0.9938],
        [0.9839],
        ...,
        [0.9693],
        [0.9683],
        [0.9674]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361036.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 40.0 event: 200 loss: tensor(630.5695, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9972],
        [0.9937],
        [0.9838],
        ...,
        [0.9692],
        [0.9683],
        [0.9673]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361022.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.3210, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0067,  0.0039,  ...,  0.0055,  0.0057,  0.0060],
        [ 0.0012,  0.0067,  0.0039,  ...,  0.0055,  0.0057,  0.0060],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(891.6278, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6175, device='cuda:0')



h[100].sum tensor(19.0282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5605, device='cuda:0')



h[200].sum tensor(22.9607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.2525, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0131, 0.0071,  ..., 0.0098, 0.0110, 0.0113],
        [0.0021, 0.0130, 0.0071,  ..., 0.0098, 0.0110, 0.0113],
        [0.0021, 0.0130, 0.0070,  ..., 0.0098, 0.0110, 0.0112],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33925.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0489, 0.0000,  ..., 0.0261, 0.0000, 0.0090],
        [0.0000, 0.0374, 0.0000,  ..., 0.0212, 0.0000, 0.0063],
        [0.0000, 0.0277, 0.0000,  ..., 0.0171, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(176977.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-79.6329, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.9591, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-189.8379, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9972],
        [0.9937],
        [0.9838],
        ...,
        [0.9692],
        [0.9683],
        [0.9673]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361022.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9971],
        [0.9937],
        [0.9838],
        ...,
        [0.9692],
        [0.9682],
        [0.9672]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(361008.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.7308, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0073,  0.0289,  0.0197,  ...,  0.0267,  0.0249,  0.0266],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1329.9780, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.1409, device='cuda:0')



h[100].sum tensor(20.4409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9544, device='cuda:0')



h[200].sum tensor(25.9612, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.7374, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0112, 0.0462, 0.0306,  ..., 0.0415, 0.0398, 0.0421],
        [0.0142, 0.0572, 0.0383,  ..., 0.0520, 0.0493, 0.0523],
        [0.0305, 0.1204, 0.0821,  ..., 0.1114, 0.1040, 0.1110],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44611.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1241, 0.0000,  ..., 0.0543, 0.0000, 0.0360],
        [0.0000, 0.1650, 0.0000,  ..., 0.0703, 0.0000, 0.0488],
        [0.0000, 0.2009, 0.0000,  ..., 0.0841, 0.0000, 0.0604],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(232821.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-130.7761, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-36.1884, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-267.1945, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9971],
        [0.9937],
        [0.9838],
        ...,
        [0.9692],
        [0.9682],
        [0.9672]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(361008.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9971],
        [0.9937],
        [0.9837],
        ...,
        [0.9691],
        [0.9681],
        [0.9671]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360993.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4209],
        [0.3872],
        [0.4094],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.7292, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0122,  0.0469,  0.0324,  ...,  0.0439,  0.0405,  0.0433],
        [ 0.0115,  0.0443,  0.0306,  ...,  0.0414,  0.0383,  0.0410],
        [ 0.0117,  0.0448,  0.0310,  ...,  0.0419,  0.0388,  0.0415],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1090.3376, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.1300, device='cuda:0')



h[100].sum tensor(19.6596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.6781, device='cuda:0')



h[200].sum tensor(24.3018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.2138, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0459, 0.1768, 0.1220,  ..., 0.1653, 0.1529, 0.1635],
        [0.0501, 0.1921, 0.1329,  ..., 0.1799, 0.1662, 0.1777],
        [0.0489, 0.1875, 0.1297,  ..., 0.1756, 0.1622, 0.1735],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38738.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 5.3285e-01, 0.0000e+00,  ..., 2.1145e-01, 0.0000e+00,
         1.6763e-01],
        [0.0000e+00, 5.4862e-01, 0.0000e+00,  ..., 2.1734e-01, 0.0000e+00,
         1.7294e-01],
        [0.0000e+00, 5.1629e-01, 0.0000e+00,  ..., 2.0518e-01, 0.0000e+00,
         1.6218e-01],
        ...,
        [0.0000e+00, 3.5920e-04, 0.0000e+00,  ..., 4.9890e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5946e-04, 0.0000e+00,  ..., 4.9885e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.5947e-04, 0.0000e+00,  ..., 4.9883e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(203654.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-102.4264, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.9752, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-224.1722, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9971],
        [0.9937],
        [0.9837],
        ...,
        [0.9691],
        [0.9681],
        [0.9671]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360993.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9971],
        [0.9937],
        [0.9837],
        ...,
        [0.9690],
        [0.9680],
        [0.9670]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360979.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(311.7811, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1273.7256, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.9140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.7889, device='cuda:0')



h[100].sum tensor(20.2450, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.8424, device='cuda:0')



h[200].sum tensor(25.5452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.3404, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42263.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0185, 0.0000,  ..., 0.0129, 0.0000, 0.0035],
        [0.0000, 0.0043, 0.0000,  ..., 0.0071, 0.0000, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(219598.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-120.1449, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.7787, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-251.0200, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9971],
        [0.9937],
        [0.9837],
        ...,
        [0.9690],
        [0.9680],
        [0.9670]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360979.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9971],
        [0.9937],
        [0.9837],
        ...,
        [0.9690],
        [0.9680],
        [0.9670]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360979.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.8812, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0029,  0.0129,  0.0083,  ...,  0.0114,  0.0111,  0.0117],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(798.1910, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7957, device='cuda:0')



h[100].sum tensor(18.7113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.9808, device='cuda:0')



h[200].sum tensor(22.2876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.1979, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0054, 0.0233, 0.0148,  ..., 0.0202, 0.0199, 0.0208],
        [0.0029, 0.0141, 0.0083,  ..., 0.0113, 0.0119, 0.0122],
        [0.0089, 0.0379, 0.0247,  ..., 0.0336, 0.0325, 0.0343],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32045.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0997, 0.0000,  ..., 0.0451, 0.0000, 0.0277],
        [0.0000, 0.0932, 0.0000,  ..., 0.0426, 0.0000, 0.0255],
        [0.0000, 0.1285, 0.0000,  ..., 0.0563, 0.0000, 0.0367],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(172323.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-70.8527, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.3894, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-176.6064, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9971],
        [0.9937],
        [0.9837],
        ...,
        [0.9690],
        [0.9680],
        [0.9670]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360979.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9971],
        [0.9936],
        [0.9836],
        ...,
        [0.9689],
        [0.9679],
        [0.9669]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360964.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2500],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.7354, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0037,  0.0158,  0.0104,  ...,  0.0142,  0.0136,  0.0145],
        [ 0.0015,  0.0079,  0.0048,  ...,  0.0066,  0.0068,  0.0071],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1402.5830, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.0789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.5674, device='cuda:0')



h[100].sum tensor(20.6531, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4083, device='cuda:0')



h[200].sum tensor(26.4120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.3462, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0160, 0.0675, 0.0446,  ..., 0.0608, 0.0582, 0.0619],
        [0.0098, 0.0411, 0.0269,  ..., 0.0366, 0.0353, 0.0373],
        [0.0015, 0.0091, 0.0048,  ..., 0.0066, 0.0076, 0.0076],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44882.5352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1940, 0.0000,  ..., 0.0827, 0.0000, 0.0557],
        [0.0000, 0.1431, 0.0000,  ..., 0.0625, 0.0000, 0.0404],
        [0.0000, 0.0893, 0.0000,  ..., 0.0413, 0.0000, 0.0237],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(230042.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-132.0716, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.9950, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-269.1230, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9971],
        [0.9936],
        [0.9836],
        ...,
        [0.9689],
        [0.9679],
        [0.9669]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360964.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9971],
        [0.9936],
        [0.9835],
        ...,
        [0.9688],
        [0.9678],
        [0.9668]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360950.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.0159, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(819.9509, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0751, device='cuda:0')



h[100].sum tensor(18.7719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0697, device='cuda:0')



h[200].sum tensor(22.4162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.5130, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31413.8086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0685, 0.0000,  ..., 0.0323, 0.0000, 0.0195],
        [0.0000, 0.0189, 0.0000,  ..., 0.0128, 0.0000, 0.0045],
        [0.0000, 0.0044, 0.0000,  ..., 0.0071, 0.0000, 0.0002],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(161620.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-67.3986, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.1010, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-171.8770, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9971],
        [0.9936],
        [0.9835],
        ...,
        [0.9688],
        [0.9678],
        [0.9668]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360950.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9971],
        [0.9936],
        [0.9835],
        ...,
        [0.9687],
        [0.9677],
        [0.9667]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360935.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.3290, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0019,  0.0091,  0.0057,  ...,  0.0078,  0.0079,  0.0083],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1007.0962, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6682, device='cuda:0')



h[100].sum tensor(19.3681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2130, device='cuda:0')



h[200].sum tensor(23.6826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.5653, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0041, 0.0185, 0.0114,  ..., 0.0156, 0.0157, 0.0163],
        [0.0019, 0.0103, 0.0057,  ..., 0.0078, 0.0087, 0.0087],
        [0.0042, 0.0207, 0.0125,  ..., 0.0172, 0.0177, 0.0184],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35217.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0543, 0.0000,  ..., 0.0273, 0.0000, 0.0133],
        [0.0000, 0.0564, 0.0000,  ..., 0.0284, 0.0000, 0.0132],
        [0.0000, 0.0815, 0.0000,  ..., 0.0387, 0.0000, 0.0199],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(179877.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-85.9878, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.6486, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-199.4043, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9971],
        [0.9936],
        [0.9835],
        ...,
        [0.9687],
        [0.9677],
        [0.9667]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360935.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9971],
        [0.9935],
        [0.9834],
        ...,
        [0.9686],
        [0.9676],
        [0.9666]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360921.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3989, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(878.2172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1788, device='cuda:0')



h[100].sum tensor(18.9492, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4209, device='cuda:0')



h[200].sum tensor(22.7929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.7577, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0031, 0.0147, 0.0087,  ..., 0.0119, 0.0124, 0.0128],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32400.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0058, 0.0000,  ..., 0.0077, 0.0000, 0.0002],
        [0.0000, 0.0135, 0.0000,  ..., 0.0107, 0.0000, 0.0027],
        [0.0000, 0.0413, 0.0000,  ..., 0.0219, 0.0000, 0.0101],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(167766.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-72.0595, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.4082, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-178.7426, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9971],
        [0.9935],
        [0.9834],
        ...,
        [0.9686],
        [0.9676],
        [0.9666]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360921.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9970],
        [0.9935],
        [0.9834],
        ...,
        [0.9685],
        [0.9675],
        [0.9665]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360906.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5879],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.6108, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0043,  0.0180,  0.0119,  ...,  0.0162,  0.0155,  0.0165],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(852.0232, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7521, device='cuda:0')



h[100].sum tensor(18.8603, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2851, device='cuda:0')



h[200].sum tensor(22.6041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.2764, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0058, 0.0266, 0.0167,  ..., 0.0228, 0.0228, 0.0239],
        [0.0034, 0.0160, 0.0096,  ..., 0.0131, 0.0135, 0.0140],
        [0.0203, 0.0832, 0.0558,  ..., 0.0758, 0.0718, 0.0765],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33334.7383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0794, 0.0000,  ..., 0.0375, 0.0000, 0.0202],
        [0.0000, 0.0798, 0.0000,  ..., 0.0373, 0.0000, 0.0212],
        [0.0000, 0.1408, 0.0000,  ..., 0.0612, 0.0000, 0.0403],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(179582.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-76.5355, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.6040, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-185.3891, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9970],
        [0.9935],
        [0.9834],
        ...,
        [0.9685],
        [0.9675],
        [0.9665]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360906.9062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 50.0 event: 250 loss: tensor(580.2721, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9970],
        [0.9935],
        [0.9833],
        ...,
        [0.9684],
        [0.9674],
        [0.9665]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360892.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(527.6202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0024,  0.0112,  0.0071,  ...,  0.0098,  0.0096,  0.0102],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2396.2979, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-0.6523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(47.0265, device='cuda:0')



h[100].sum tensor(23.7938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.9638, device='cuda:0')



h[200].sum tensor(33.0828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(53.0367, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0035, 0.0182, 0.0107,  ..., 0.0148, 0.0155, 0.0161],
        [0.0090, 0.0401, 0.0257,  ..., 0.0351, 0.0344, 0.0364],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68298.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0288, 0.0000,  ..., 0.0172, 0.0000, 0.0057],
        [0.0000, 0.0736, 0.0000,  ..., 0.0353, 0.0000, 0.0181],
        [0.0000, 0.1328, 0.0000,  ..., 0.0590, 0.0000, 0.0357],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(355400.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-243.7078, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-14.4457, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-438.7748, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9970],
        [0.9935],
        [0.9833],
        ...,
        [0.9684],
        [0.9674],
        [0.9665]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360892.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9970],
        [0.9935],
        [0.9833],
        ...,
        [0.9684],
        [0.9674],
        [0.9664]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360877.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.2900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1141.4758, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.6256, device='cuda:0')



h[100].sum tensor(19.7749, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8358, device='cuda:0')



h[200].sum tensor(24.5467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.7728, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41937.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.3344e-02, 0.0000e+00,  ..., 1.0929e-02, 0.0000e+00,
         1.4103e-03],
        [0.0000e+00, 9.8238e-03, 0.0000e+00,  ..., 9.4128e-03, 0.0000e+00,
         8.3378e-04],
        [0.0000e+00, 3.2850e-03, 0.0000e+00,  ..., 6.6057e-03, 0.0000e+00,
         1.6769e-05],
        ...,
        [0.0000e+00, 3.5986e-04, 0.0000e+00,  ..., 4.9821e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6013e-04, 0.0000e+00,  ..., 4.9816e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6013e-04, 0.0000e+00,  ..., 4.9814e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(231945.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-118.8850, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.1552, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-248.4814, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9970],
        [0.9935],
        [0.9833],
        ...,
        [0.9684],
        [0.9674],
        [0.9664]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360877.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9970],
        [0.9934],
        [0.9832],
        ...,
        [0.9683],
        [0.9673],
        [0.9663]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360863.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3650],
        [0.3645],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.5333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0054,  0.0222,  0.0149,  ...,  0.0202,  0.0191,  0.0204],
        [ 0.0025,  0.0113,  0.0072,  ...,  0.0099,  0.0097,  0.0103],
        [ 0.0025,  0.0113,  0.0072,  ...,  0.0099,  0.0097,  0.0103],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1038.0770, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.9538, device='cuda:0')



h[100].sum tensor(19.4390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3039, device='cuda:0')



h[200].sum tensor(23.8333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.8874, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0136, 0.0589, 0.0385,  ..., 0.0526, 0.0508, 0.0539],
        [0.0141, 0.0608, 0.0399,  ..., 0.0544, 0.0524, 0.0556],
        [0.0044, 0.0213, 0.0129,  ..., 0.0177, 0.0182, 0.0190],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37602.0586, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1326, 0.0000,  ..., 0.0588, 0.0000, 0.0362],
        [0.0000, 0.1188, 0.0000,  ..., 0.0533, 0.0000, 0.0321],
        [0.0000, 0.0729, 0.0000,  ..., 0.0350, 0.0000, 0.0181],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(200394.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-96.9358, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.6928, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-216.3299, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9970],
        [0.9934],
        [0.9832],
        ...,
        [0.9683],
        [0.9673],
        [0.9663]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360863.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9970],
        [0.9934],
        [0.9831],
        ...,
        [0.9682],
        [0.9672],
        [0.9662]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360848.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0093, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056,  0.0227,  0.0153,  ...,  0.0208,  0.0196,  0.0209],
        [ 0.0037,  0.0159,  0.0104,  ...,  0.0142,  0.0137,  0.0145],
        [ 0.0055,  0.0223,  0.0150,  ...,  0.0204,  0.0192,  0.0205],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(998.9442, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3723, device='cuda:0')



h[100].sum tensor(19.3088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.1188, device='cuda:0')



h[200].sum tensor(23.5567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.2316, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0131, 0.0569, 0.0371,  ..., 0.0507, 0.0490, 0.0521],
        [0.0149, 0.0635, 0.0418,  ..., 0.0570, 0.0547, 0.0582],
        [0.0090, 0.0401, 0.0257,  ..., 0.0351, 0.0345, 0.0364],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35837.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1803, 0.0000,  ..., 0.0781, 0.0000, 0.0498],
        [0.0000, 0.1748, 0.0000,  ..., 0.0757, 0.0000, 0.0485],
        [0.0000, 0.1443, 0.0000,  ..., 0.0635, 0.0000, 0.0392],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(187622.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-88.5007, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.4129, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-203.4092, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9970],
        [0.9934],
        [0.9831],
        ...,
        [0.9682],
        [0.9672],
        [0.9662]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360848.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9970],
        [0.9934],
        [0.9831],
        ...,
        [0.9681],
        [0.9671],
        [0.9661]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360834.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.4366, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(844.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2018, device='cuda:0')



h[100].sum tensor(18.8128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1100, device='cuda:0')



h[200].sum tensor(22.5032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.6558, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32293.5664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(171208.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-71.9437, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.1870, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-178.3935, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9970],
        [0.9934],
        [0.9831],
        ...,
        [0.9681],
        [0.9671],
        [0.9661]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360834.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9970],
        [0.9934],
        [0.9830],
        ...,
        [0.9680],
        [0.9670],
        [0.9660]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360819.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(264.7843, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1074.2458, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.3626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.6001, device='cuda:0')



h[100].sum tensor(19.5370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5095, device='cuda:0')



h[200].sum tensor(24.0415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.6163, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35422.8164, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0104, 0.0000,  ..., 0.0094, 0.0000, 0.0022],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(177042.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-86.9466, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.0401, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-201.4725, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9970],
        [0.9934],
        [0.9830],
        ...,
        [0.9680],
        [0.9670],
        [0.9660]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360819.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9969],
        [0.9933],
        [0.9830],
        ...,
        [0.9679],
        [0.9669],
        [0.9659]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360805.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(358.1553, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1550.7476, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.2824, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.9222, device='cuda:0')



h[100].sum tensor(21.0424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.1576, device='cuda:0')



h[200].sum tensor(27.2387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.0020, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47962.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0251, 0.0000,  ..., 0.0158, 0.0000, 0.0042],
        [0.0000, 0.0167, 0.0000,  ..., 0.0123, 0.0000, 0.0022],
        [0.0000, 0.0109, 0.0000,  ..., 0.0098, 0.0000, 0.0012],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(246330.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-146.7965, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-33.4758, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-291.0273, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9969],
        [0.9933],
        [0.9830],
        ...,
        [0.9679],
        [0.9669],
        [0.9659]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360805.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9969],
        [0.9933],
        [0.9829],
        ...,
        [0.9678],
        [0.9668],
        [0.9658]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360790.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6011],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(454.4949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0077,  0.0305,  0.0208,  ...,  0.0283,  0.0264,  0.0282],
        [ 0.0018,  0.0088,  0.0055,  ...,  0.0075,  0.0076,  0.0080],
        [ 0.0067,  0.0268,  0.0182,  ...,  0.0247,  0.0231,  0.0247],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2043.1477, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.1077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.5089, device='cuda:0')



h[100].sum tensor(22.5939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.8899, device='cuda:0')



h[200].sum tensor(30.5341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(45.6861, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0107, 0.0465, 0.0302,  ..., 0.0412, 0.0400, 0.0423],
        [0.0281, 0.1117, 0.0760,  ..., 0.1031, 0.0965, 0.1030],
        [0.0170, 0.0713, 0.0474,  ..., 0.0645, 0.0615, 0.0655],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(61920.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1867, 0.0000,  ..., 0.0798, 0.0000, 0.0536],
        [0.0000, 0.2809, 0.0000,  ..., 0.1164, 0.0000, 0.0839],
        [0.0000, 0.2876, 0.0000,  ..., 0.1192, 0.0000, 0.0858],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0076, 0.0000, 0.0011],
        [0.0000, 0.0265, 0.0000,  ..., 0.0154, 0.0000, 0.0066]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(334623.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-213.6409, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-19.9530, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-393.1437, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9969],
        [0.9933],
        [0.9829],
        ...,
        [0.9678],
        [0.9668],
        [0.9658]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360790.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9969],
        [0.9933],
        [0.9829],
        ...,
        [0.9677],
        [0.9667],
        [0.9657]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360776.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(331.5157, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1428.1003, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.1079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.5478, device='cuda:0')



h[100].sum tensor(20.6389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.4021, device='cuda:0')



h[200].sum tensor(26.3818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.3241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44689.6133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(234963.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-131.1759, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-36.1721, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-267.7144, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9969],
        [0.9933],
        [0.9829],
        ...,
        [0.9677],
        [0.9667],
        [0.9657]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360776.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9969],
        [0.9933],
        [0.9828],
        ...,
        [0.9676],
        [0.9666],
        [0.9656]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360761.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.7849, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(946.0856, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1045, device='cuda:0')



h[100].sum tensor(19.1095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7154, device='cuda:0')



h[200].sum tensor(23.1334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.8017, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35284.1133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(186861.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-85.8053, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.7851, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-199.5569, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9969],
        [0.9933],
        [0.9828],
        ...,
        [0.9676],
        [0.9666],
        [0.9656]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360761.8125, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 60.0 event: 300 loss: tensor(636.7463, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9969],
        [0.9932],
        [0.9827],
        ...,
        [0.9676],
        [0.9665],
        [0.9655]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360747.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.6670, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1189.7979, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.3722, device='cuda:0')



h[100].sum tensor(19.8730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0734, device='cuda:0')



h[200].sum tensor(24.7550, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.6149, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0018, 0.0100, 0.0054,  ..., 0.0075, 0.0084, 0.0084],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38750.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0021, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        [0.0000, 0.0138, 0.0000,  ..., 0.0109, 0.0000, 0.0025],
        [0.0000, 0.0383, 0.0000,  ..., 0.0211, 0.0000, 0.0079],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(196929.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-102.8592, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.3605, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-225.0640, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9969],
        [0.9932],
        [0.9827],
        ...,
        [0.9676],
        [0.9665],
        [0.9655]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360747.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9969],
        [0.9932],
        [0.9827],
        ...,
        [0.9675],
        [0.9665],
        [0.9655]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360732.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0050,  0.0206,  0.0138,  ...,  0.0187,  0.0177,  0.0189],
        [ 0.0068,  0.0270,  0.0184,  ...,  0.0249,  0.0234,  0.0249],
        [ 0.0031,  0.0136,  0.0088,  ...,  0.0120,  0.0117,  0.0124],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(679.9950, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.9730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1690, device='cuda:0')



h[100].sum tensor(18.2613, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.1449, device='cuda:0')



h[200].sum tensor(21.3319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.2354, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0309, 0.1219, 0.0831,  ..., 0.1128, 0.1053, 0.1124],
        [0.0254, 0.1019, 0.0690,  ..., 0.0937, 0.0880, 0.0939],
        [0.0213, 0.0849, 0.0575,  ..., 0.0780, 0.0733, 0.0781],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29496.7461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 4.1647e-01, 0.0000e+00,  ..., 1.6768e-01, 0.0000e+00,
         1.2899e-01],
        [0.0000e+00, 3.7609e-01, 0.0000e+00,  ..., 1.5214e-01, 0.0000e+00,
         1.1585e-01],
        [0.0000e+00, 2.9429e-01, 0.0000e+00,  ..., 1.2034e-01, 0.0000e+00,
         9.0046e-02],
        ...,
        [0.0000e+00, 3.6069e-04, 0.0000e+00,  ..., 4.9735e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6096e-04, 0.0000e+00,  ..., 4.9730e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6096e-04, 0.0000e+00,  ..., 4.9727e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(160448.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-58.0570, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.1256, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-157.5355, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9969],
        [0.9932],
        [0.9827],
        ...,
        [0.9675],
        [0.9665],
        [0.9655]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360732.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9969],
        [0.9932],
        [0.9827],
        ...,
        [0.9675],
        [0.9665],
        [0.9655]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360732.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(389.1227, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0069,  0.0041,  ...,  0.0056,  0.0059,  0.0062],
        [ 0.0038,  0.0162,  0.0107,  ...,  0.0146,  0.0140,  0.0149],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1687.7087, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.4791, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.6823, device='cuda:0')



h[100].sum tensor(21.4350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.0359, device='cuda:0')



h[200].sum tensor(28.0726, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.1148, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0134, 0.0581, 0.0380,  ..., 0.0518, 0.0501, 0.0532],
        [0.0052, 0.0264, 0.0160,  ..., 0.0220, 0.0226, 0.0236],
        [0.0047, 0.0226, 0.0138,  ..., 0.0189, 0.0193, 0.0201],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47011.5430, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1197, 0.0000,  ..., 0.0541, 0.0000, 0.0312],
        [0.0000, 0.0913, 0.0000,  ..., 0.0428, 0.0000, 0.0225],
        [0.0000, 0.0676, 0.0000,  ..., 0.0329, 0.0000, 0.0163],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(228827.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-143.1166, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-32.8261, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-286.1933, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9969],
        [0.9932],
        [0.9827],
        ...,
        [0.9675],
        [0.9665],
        [0.9655]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360732.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9969],
        [0.9932],
        [0.9826],
        ...,
        [0.9674],
        [0.9664],
        [0.9654]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360718.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.7820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(822.1003, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6978, device='cuda:0')



h[100].sum tensor(18.7044, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.9496, device='cuda:0')



h[200].sum tensor(22.2729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.0874, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31099.1172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 4.1022e-03, 0.0000e+00,  ..., 6.9917e-03, 0.0000e+00,
         7.2874e-05],
        [0.0000e+00, 7.5626e-04, 0.0000e+00,  ..., 5.5460e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.1304e-04, 0.0000e+00,  ..., 5.3348e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 3.6077e-04, 0.0000e+00,  ..., 4.9726e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6104e-04, 0.0000e+00,  ..., 4.9721e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6105e-04, 0.0000e+00,  ..., 4.9719e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(162940.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-65.8925, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.4499, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-169.4835, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9969],
        [0.9932],
        [0.9826],
        ...,
        [0.9674],
        [0.9664],
        [0.9654]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360718.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9968],
        [0.9931],
        [0.9826],
        ...,
        [0.9673],
        [0.9663],
        [0.9653]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360703.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6270],
        [0.0000],
        [0.6748],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.2502, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0101,  0.0393,  0.0270,  ...,  0.0366,  0.0340,  0.0363],
        [ 0.0045,  0.0187,  0.0125,  ...,  0.0170,  0.0162,  0.0172],
        ...,
        [ 0.0033,  0.0144,  0.0094,  ...,  0.0128,  0.0124,  0.0132],
        [ 0.0022,  0.0102,  0.0065,  ...,  0.0089,  0.0088,  0.0093],
        [ 0.0033,  0.0144,  0.0094,  ...,  0.0128,  0.0124,  0.0132]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1061.6158, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1960, device='cuda:0')



h[100].sum tensor(19.4521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3809, device='cuda:0')



h[200].sum tensor(23.8611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.1605, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0336, 0.1318, 0.0901,  ..., 0.1222, 0.1139, 0.1216],
        [0.0121, 0.0516, 0.0338,  ..., 0.0461, 0.0444, 0.0471],
        [0.0337, 0.1320, 0.0903,  ..., 0.1225, 0.1141, 0.1218],
        ...,
        [0.0046, 0.0219, 0.0134,  ..., 0.0184, 0.0187, 0.0195],
        [0.0134, 0.0579, 0.0380,  ..., 0.0518, 0.0499, 0.0530],
        [0.0104, 0.0467, 0.0300,  ..., 0.0410, 0.0402, 0.0426]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34451.4453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3251, 0.0000,  ..., 0.1322, 0.0000, 0.0997],
        [0.0000, 0.2470, 0.0000,  ..., 0.1024, 0.0000, 0.0741],
        [0.0000, 0.3302, 0.0000,  ..., 0.1342, 0.0000, 0.1012],
        ...,
        [0.0000, 0.0611, 0.0000,  ..., 0.0297, 0.0000, 0.0154],
        [0.0000, 0.0993, 0.0000,  ..., 0.0450, 0.0000, 0.0267],
        [0.0000, 0.1091, 0.0000,  ..., 0.0489, 0.0000, 0.0294]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(172268.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-82.6242, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.9443, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-194.4095, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9968],
        [0.9931],
        [0.9826],
        ...,
        [0.9673],
        [0.9663],
        [0.9653]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360703.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9968],
        [0.9931],
        [0.9825],
        ...,
        [0.9672],
        [0.9662],
        [0.9652]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360688.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(834.6165, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8790, device='cuda:0')



h[100].sum tensor(18.7346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0073, device='cuda:0')



h[200].sum tensor(22.3372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.2917, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31793.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0361, 0.0000,  ..., 0.0198, 0.0000, 0.0089],
        [0.0000, 0.0129, 0.0000,  ..., 0.0105, 0.0000, 0.0023],
        [0.0000, 0.0056, 0.0000,  ..., 0.0075, 0.0000, 0.0003],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(167541.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-69.7846, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.6177, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-174.7981, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9968],
        [0.9931],
        [0.9825],
        ...,
        [0.9672],
        [0.9662],
        [0.9652]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360688.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9968],
        [0.9931],
        [0.9824],
        ...,
        [0.9671],
        [0.9661],
        [0.9651]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360674.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.0449, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1001.9778, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8407, device='cuda:0')



h[100].sum tensor(19.2543, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9497, device='cuda:0')



h[200].sum tensor(23.4410, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.6320, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35802.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0199, 0.0000,  ..., 0.0131, 0.0000, 0.0052],
        [0.0000, 0.0057, 0.0000,  ..., 0.0075, 0.0000, 0.0010],
        [0.0000, 0.0071, 0.0000,  ..., 0.0081, 0.0000, 0.0008],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(187375.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-88.3714, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.4205, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-203.1787, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9968],
        [0.9931],
        [0.9824],
        ...,
        [0.9671],
        [0.9661],
        [0.9651]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360674.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9968],
        [0.9931],
        [0.9824],
        ...,
        [0.9670],
        [0.9660],
        [0.9650]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360659.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.9961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(624.9633, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.3649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1517, device='cuda:0')



h[100].sum tensor(18.0698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.8212, device='cuda:0')



h[200].sum tensor(20.9252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.0881, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0010, 0.0072, 0.0034,  ..., 0.0048, 0.0060, 0.0058],
        [0.0020, 0.0127, 0.0068,  ..., 0.0095, 0.0107, 0.0109],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26190.3555, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0226, 0.0000,  ..., 0.0148, 0.0000, 0.0039],
        [0.0000, 0.0379, 0.0000,  ..., 0.0211, 0.0000, 0.0073],
        [0.0000, 0.0513, 0.0000,  ..., 0.0266, 0.0000, 0.0107],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(137250.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-43.3307, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-52.3451, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-134.7578, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9968],
        [0.9931],
        [0.9824],
        ...,
        [0.9670],
        [0.9660],
        [0.9650]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360659.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9968],
        [0.9930],
        [0.9823],
        ...,
        [0.9669],
        [0.9659],
        [0.9649]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360645.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.6708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0037,  0.0157,  0.0103,  ...,  0.0141,  0.0135,  0.0144],
        [ 0.0012,  0.0067,  0.0040,  ...,  0.0055,  0.0058,  0.0060],
        [ 0.0019,  0.0094,  0.0058,  ...,  0.0080,  0.0081,  0.0085],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1017.5421, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0748, device='cuda:0')



h[100].sum tensor(19.2925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0242, device='cuda:0')



h[200].sum tensor(23.5221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.8960, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0053, 0.0269, 0.0163,  ..., 0.0225, 0.0230, 0.0241],
        [0.0135, 0.0585, 0.0382,  ..., 0.0522, 0.0504, 0.0535],
        [0.0058, 0.0268, 0.0168,  ..., 0.0229, 0.0229, 0.0240],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34410.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0922, 0.0000,  ..., 0.0431, 0.0000, 0.0228],
        [0.0000, 0.1201, 0.0000,  ..., 0.0542, 0.0000, 0.0314],
        [0.0000, 0.0964, 0.0000,  ..., 0.0445, 0.0000, 0.0249],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(175973.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-82.9706, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.7244, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-194.4492, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9968],
        [0.9930],
        [0.9823],
        ...,
        [0.9669],
        [0.9659],
        [0.9649]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360645.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9968],
        [0.9930],
        [0.9823],
        ...,
        [0.9668],
        [0.9658],
        [0.9648]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360630.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.1464, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(952.8325, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9585, device='cuda:0')



h[100].sum tensor(19.0854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6690, device='cuda:0')



h[200].sum tensor(23.0823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.6370, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35239.5430, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0170, 0.0000,  ..., 0.0124, 0.0000, 0.0029],
        [0.0000, 0.0048, 0.0000,  ..., 0.0072, 0.0000, 0.0004],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(188774.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-85.6812, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.9258, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-199.0702, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9968],
        [0.9930],
        [0.9823],
        ...,
        [0.9668],
        [0.9658],
        [0.9648]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360630.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9968],
        [0.9930],
        [0.9823],
        ...,
        [0.9668],
        [0.9658],
        [0.9648]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360630.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3591],
        [0.3643],
        [0.3508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.4241, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0057,  0.0230,  0.0155,  ...,  0.0210,  0.0199,  0.0212],
        [ 0.0077,  0.0305,  0.0208,  ...,  0.0283,  0.0264,  0.0282],
        [ 0.0101,  0.0393,  0.0270,  ...,  0.0366,  0.0339,  0.0363],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1421.4435, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.2955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.0940, device='cuda:0')



h[100].sum tensor(20.5473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.2577, device='cuda:0')



h[200].sum tensor(26.1871, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.8123, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0256, 0.1028, 0.0696,  ..., 0.0945, 0.0888, 0.0947],
        [0.0282, 0.1122, 0.0763,  ..., 0.1035, 0.0969, 0.1034],
        [0.0337, 0.1323, 0.0906,  ..., 0.1228, 0.1144, 0.1222],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43350.6523, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2557, 0.0000,  ..., 0.1061, 0.0000, 0.0768],
        [0.0000, 0.3041, 0.0000,  ..., 0.1246, 0.0000, 0.0924],
        [0.0000, 0.3450, 0.0000,  ..., 0.1404, 0.0000, 0.1054],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(219560.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-124.9383, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.3363, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-257.9961, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9968],
        [0.9930],
        [0.9823],
        ...,
        [0.9668],
        [0.9658],
        [0.9648]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360630.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9967],
        [0.9930],
        [0.9822],
        ...,
        [0.9668],
        [0.9657],
        [0.9647]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360615.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(255.7729, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1058.3357, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.7969, device='cuda:0')



h[100].sum tensor(19.4092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2539, device='cuda:0')



h[200].sum tensor(23.7699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.7104, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0043, 0.0212, 0.0128,  ..., 0.0176, 0.0180, 0.0188],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36842.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0652, 0.0000,  ..., 0.0323, 0.0000, 0.0150],
        [0.0000, 0.0296, 0.0000,  ..., 0.0175, 0.0000, 0.0060],
        [0.0000, 0.0124, 0.0000,  ..., 0.0104, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(194428.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-93.5098, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.3495, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-210.8058, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9967],
        [0.9930],
        [0.9822],
        ...,
        [0.9668],
        [0.9657],
        [0.9647]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360615.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9967],
        [0.9930],
        [0.9822],
        ...,
        [0.9667],
        [0.9656],
        [0.9646]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360601.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4207],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.4604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0029,  0.0130,  0.0084,  ...,  0.0115,  0.0112,  0.0119],
        [ 0.0020,  0.0095,  0.0059,  ...,  0.0081,  0.0082,  0.0086],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1062.6431, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.8582, device='cuda:0')



h[100].sum tensor(19.4172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2734, device='cuda:0')



h[200].sum tensor(23.7869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.7795, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0071, 0.0313, 0.0200,  ..., 0.0273, 0.0269, 0.0283],
        [0.0063, 0.0283, 0.0179,  ..., 0.0244, 0.0243, 0.0255],
        [0.0148, 0.0633, 0.0416,  ..., 0.0568, 0.0545, 0.0579],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38166.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0877, 0.0000,  ..., 0.0405, 0.0000, 0.0236],
        [0.0000, 0.0941, 0.0000,  ..., 0.0433, 0.0000, 0.0250],
        [0.0000, 0.1341, 0.0000,  ..., 0.0594, 0.0000, 0.0367],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(203409.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-99.7255, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.0368, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-220.5943, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9967],
        [0.9930],
        [0.9822],
        ...,
        [0.9667],
        [0.9656],
        [0.9646]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360601.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9967],
        [0.9929],
        [0.9821],
        ...,
        [0.9666],
        [0.9656],
        [0.9645]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360586.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(270.9592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0011,  0.0064,  0.0037,  ...,  0.0051,  0.0054,  0.0057],
        [ 0.0011,  0.0064,  0.0037,  ...,  0.0051,  0.0054,  0.0057],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1126.4017, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.2135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.1505, device='cuda:0')



h[100].sum tensor(19.6099, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.6846, device='cuda:0')



h[200].sum tensor(24.1963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.2370, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0019, 0.0124, 0.0066,  ..., 0.0092, 0.0105, 0.0107],
        [0.0019, 0.0124, 0.0066,  ..., 0.0092, 0.0105, 0.0107],
        [0.0019, 0.0124, 0.0066,  ..., 0.0092, 0.0104, 0.0106],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37211.9766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0264, 0.0000,  ..., 0.0166, 0.0000, 0.0038],
        [0.0000, 0.0410, 0.0000,  ..., 0.0225, 0.0000, 0.0076],
        [0.0000, 0.0728, 0.0000,  ..., 0.0351, 0.0000, 0.0174],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(193899.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-95.1250, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.8816, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-213.7027, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9967],
        [0.9929],
        [0.9821],
        ...,
        [0.9666],
        [0.9656],
        [0.9645]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360586.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9967],
        [0.9929],
        [0.9820],
        ...,
        [0.9665],
        [0.9655],
        [0.9644]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360572.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.7188, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(825.8246, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5139, device='cuda:0')



h[100].sum tensor(18.6717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8911, device='cuda:0')



h[200].sum tensor(22.2035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.8800, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30349.3633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(156524.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-63.5712, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.4356, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.9929, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9967],
        [0.9929],
        [0.9820],
        ...,
        [0.9665],
        [0.9655],
        [0.9644]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360572.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9967],
        [0.9929],
        [0.9820],
        ...,
        [0.9664],
        [0.9654],
        [0.9644]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360557.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8715, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1022.3922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0927, device='cuda:0')



h[100].sum tensor(19.2764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0298, device='cuda:0')



h[200].sum tensor(23.4879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.9162, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0056, 0.0259, 0.0162,  ..., 0.0221, 0.0222, 0.0232],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38545.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0757, 0.0000,  ..., 0.0361, 0.0000, 0.0186],
        [0.0000, 0.0293, 0.0000,  ..., 0.0173, 0.0000, 0.0058],
        [0.0000, 0.0105, 0.0000,  ..., 0.0096, 0.0000, 0.0011],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(216934.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-102.1394, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.0227, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-224.2618, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9967],
        [0.9929],
        [0.9820],
        ...,
        [0.9664],
        [0.9654],
        [0.9644]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360557.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9967],
        [0.9928],
        [0.9819],
        ...,
        [0.9663],
        [0.9653],
        [0.9643]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360542.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.8999, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(888.7990, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7524, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.6887, device='cuda:0')



h[100].sum tensor(18.8579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2649, device='cuda:0')



h[200].sum tensor(22.5990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.2050, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0027, 0.0133, 0.0078,  ..., 0.0106, 0.0113, 0.0115],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33773.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0417, 0.0000,  ..., 0.0221, 0.0000, 0.0101],
        [0.0000, 0.0110, 0.0000,  ..., 0.0097, 0.0000, 0.0020],
        [0.0000, 0.0023, 0.0000,  ..., 0.0061, 0.0000, 0.0001],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(183991.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-78.9323, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.9357, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-188.9159, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9967],
        [0.9928],
        [0.9819],
        ...,
        [0.9663],
        [0.9653],
        [0.9643]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360542.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9967],
        [0.9928],
        [0.9819],
        ...,
        [0.9662],
        [0.9652],
        [0.9642]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360528.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(195.7550, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(761.5815, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.5662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4475, device='cuda:0')



h[100].sum tensor(18.4602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.5518, device='cuda:0')



h[200].sum tensor(21.7542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.6774, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0062, 0.0279, 0.0176,  ..., 0.0240, 0.0239, 0.0250],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30252.9805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0081, 0.0000, 0.0011],
        [0.0000, 0.0299, 0.0000,  ..., 0.0173, 0.0000, 0.0073],
        [0.0000, 0.0864, 0.0000,  ..., 0.0401, 0.0000, 0.0230],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(162915.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-62.3076, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.0117, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-163.6034, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9967],
        [0.9928],
        [0.9819],
        ...,
        [0.9662],
        [0.9652],
        [0.9642]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360528.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9966],
        [0.9928],
        [0.9818],
        ...,
        [0.9661],
        [0.9651],
        [0.9641]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360513.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(194.5568, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0016,  0.0081,  0.0049,  ...,  0.0068,  0.0070,  0.0073],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(763.5621, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.5620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.3407, device='cuda:0')



h[100].sum tensor(18.4622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.5178, device='cuda:0')



h[200].sum tensor(21.7586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.5569, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0270, 0.0169,  ..., 0.0231, 0.0231, 0.0242],
        [0.0016, 0.0093, 0.0049,  ..., 0.0068, 0.0078, 0.0078],
        [0.0103, 0.0431, 0.0284,  ..., 0.0386, 0.0371, 0.0392],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30212.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1163, 0.0000,  ..., 0.0522, 0.0000, 0.0315],
        [0.0000, 0.0952, 0.0000,  ..., 0.0435, 0.0000, 0.0259],
        [0.0000, 0.1386, 0.0000,  ..., 0.0601, 0.0000, 0.0403],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(160774.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-61.9815, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.2512, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-162.9972, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9966],
        [0.9928],
        [0.9818],
        ...,
        [0.9661],
        [0.9651],
        [0.9641]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360513.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9966],
        [0.9928],
        [0.9817],
        ...,
        [0.9660],
        [0.9650],
        [0.9640]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360498.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.2098, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(813.9706, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2903, device='cuda:0')



h[100].sum tensor(18.6136, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8199, device='cuda:0')



h[200].sum tensor(22.0801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.6278, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31791.0664, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0143, 0.0000,  ..., 0.0111, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(168916.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-70.2887, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.1342, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-175.4408, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9966],
        [0.9928],
        [0.9817],
        ...,
        [0.9660],
        [0.9650],
        [0.9640]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360498.6875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 80.0 event: 400 loss: tensor(1115.5414, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9966],
        [0.9927],
        [0.9817],
        ...,
        [0.9660],
        [0.9649],
        [0.9639]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360484.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.8358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0039,  0.0166,  0.0109,  ...,  0.0149,  0.0143,  0.0152],
        [ 0.0017,  0.0086,  0.0053,  ...,  0.0073,  0.0074,  0.0078],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1053.7015, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.2677, device='cuda:0')



h[100].sum tensor(19.3473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0856, device='cuda:0')



h[200].sum tensor(23.6385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.1136, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0052, 0.0244, 0.0151,  ..., 0.0207, 0.0209, 0.0218],
        [0.0055, 0.0276, 0.0168,  ..., 0.0232, 0.0236, 0.0248],
        [0.0234, 0.0945, 0.0638,  ..., 0.0866, 0.0816, 0.0870],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33893.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0782, 0.0000,  ..., 0.0370, 0.0000, 0.0199],
        [0.0000, 0.1338, 0.0000,  ..., 0.0591, 0.0000, 0.0364],
        [0.0000, 0.2424, 0.0000,  ..., 0.1013, 0.0000, 0.0711],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(171491.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-79.3814, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.8047, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-189.7962, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9966],
        [0.9927],
        [0.9817],
        ...,
        [0.9660],
        [0.9649],
        [0.9639]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360484.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9966],
        [0.9927],
        [0.9816],
        ...,
        [0.9659],
        [0.9648],
        [0.9638]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360469.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.2504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1277.3611, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.3155, device='cuda:0')



h[100].sum tensor(20.0297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3736, device='cuda:0')



h[200].sum tensor(25.0878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.6787, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42752.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(225196.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-121.9427, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.8283, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-253.7036, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9966],
        [0.9927],
        [0.9816],
        ...,
        [0.9659],
        [0.9648],
        [0.9638]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360469.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9966],
        [0.9927],
        [0.9816],
        ...,
        [0.9658],
        [0.9647],
        [0.9637]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360454.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(345.0648, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1542.3573, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.7022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.7555, device='cuda:0')



h[100].sum tensor(20.8372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.7863, device='cuda:0')



h[200].sum tensor(26.8030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.6861, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0011, 0.0075, 0.0036,  ..., 0.0050, 0.0062, 0.0061],
        [0.0021, 0.0133, 0.0072,  ..., 0.0100, 0.0112, 0.0115],
        [0.0011, 0.0074, 0.0036,  ..., 0.0050, 0.0062, 0.0060],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45420.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0355, 0.0000,  ..., 0.0203, 0.0000, 0.0062],
        [0.0000, 0.0406, 0.0000,  ..., 0.0225, 0.0000, 0.0073],
        [0.0000, 0.0386, 0.0000,  ..., 0.0215, 0.0000, 0.0070],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(231492.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-135.1211, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-34.9599, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-273.7231, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9966],
        [0.9927],
        [0.9816],
        ...,
        [0.9658],
        [0.9647],
        [0.9637]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360454.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9966],
        [0.9926],
        [0.9815],
        ...,
        [0.9657],
        [0.9647],
        [0.9636]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360439.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2078, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(953.9256, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6966, device='cuda:0')



h[100].sum tensor(19.0259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5856, device='cuda:0')



h[200].sum tensor(22.9559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.3416, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34163.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0579, 0.0000,  ..., 0.0286, 0.0000, 0.0149],
        [0.0000, 0.0351, 0.0000,  ..., 0.0195, 0.0000, 0.0085],
        [0.0000, 0.0250, 0.0000,  ..., 0.0153, 0.0000, 0.0056],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(180589.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.1456, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.5273, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.8120, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9966],
        [0.9926],
        [0.9815],
        ...,
        [0.9657],
        [0.9647],
        [0.9636]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360439.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9966],
        [0.9926],
        [0.9815],
        ...,
        [0.9656],
        [0.9646],
        [0.9635]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360425.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4585],
        [0.5786],
        [0.6382],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.6981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0120,  0.0460,  0.0318,  ...,  0.0431,  0.0398,  0.0426],
        [ 0.0085,  0.0332,  0.0227,  ...,  0.0308,  0.0287,  0.0306],
        [ 0.0042,  0.0177,  0.0117,  ...,  0.0160,  0.0153,  0.0162],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(935.3339, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1164, device='cuda:0')



h[100].sum tensor(18.9643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4010, device='cuda:0')



h[200].sum tensor(22.8249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.6873, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0361, 0.1411, 0.0968,  ..., 0.1312, 0.1220, 0.1303],
        [0.0330, 0.1296, 0.0886,  ..., 0.1202, 0.1120, 0.1196],
        [0.0268, 0.1069, 0.0726,  ..., 0.0985, 0.0924, 0.0986],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32204.6621, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3570, 0.0000,  ..., 0.1445, 0.0000, 0.1102],
        [0.0000, 0.3113, 0.0000,  ..., 0.1269, 0.0000, 0.0953],
        [0.0000, 0.2511, 0.0000,  ..., 0.1041, 0.0000, 0.0752],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(166099.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-72.2625, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.0235, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-178.0362, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9966],
        [0.9926],
        [0.9815],
        ...,
        [0.9656],
        [0.9646],
        [0.9635]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360425.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9965],
        [0.9926],
        [0.9814],
        ...,
        [0.9655],
        [0.9645],
        [0.9634]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360410.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.1169, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0135,  0.0517,  0.0358,  ...,  0.0485,  0.0447,  0.0478],
        [ 0.0038,  0.0160,  0.0106,  ...,  0.0144,  0.0138,  0.0147],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(926.6273, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5986, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1537, device='cuda:0')



h[100].sum tensor(18.9330, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4129, device='cuda:0')



h[200].sum tensor(22.7586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.7294, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0214, 0.0875, 0.0588,  ..., 0.0799, 0.0756, 0.0805],
        [0.0263, 0.1034, 0.0706,  ..., 0.0957, 0.0893, 0.0953],
        [0.0101, 0.0422, 0.0277,  ..., 0.0377, 0.0363, 0.0383],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33180.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2617, 0.0000,  ..., 0.1084, 0.0000, 0.0785],
        [0.0000, 0.2607, 0.0000,  ..., 0.1078, 0.0000, 0.0785],
        [0.0000, 0.1903, 0.0000,  ..., 0.0805, 0.0000, 0.0560],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(174231.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-76.3261, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.3726, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-184.7516, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9965],
        [0.9926],
        [0.9814],
        ...,
        [0.9655],
        [0.9645],
        [0.9634]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360410.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9965],
        [0.9926],
        [0.9813],
        ...,
        [0.9654],
        [0.9644],
        [0.9634]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360395.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.4592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1544.4065, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.7471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.2363, device='cuda:0')



h[100].sum tensor(20.8153, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.9393, device='cuda:0')



h[200].sum tensor(26.7564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.2283, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45559.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(238061.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-135.1444, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.0201, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-274.4651, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9965],
        [0.9926],
        [0.9813],
        ...,
        [0.9654],
        [0.9644],
        [0.9634]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360395.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9965],
        [0.9925],
        [0.9813],
        ...,
        [0.9653],
        [0.9643],
        [0.9633]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360381., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4517],
        [0.2886],
        [0.2786],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.3673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0046,  0.0190,  0.0126,  ...,  0.0172,  0.0164,  0.0174],
        [ 0.0101,  0.0391,  0.0269,  ...,  0.0364,  0.0338,  0.0361],
        [ 0.0079,  0.0313,  0.0214,  ...,  0.0290,  0.0270,  0.0289],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1374.4700, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.8221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5738, device='cuda:0')



h[100].sum tensor(20.2899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7739, device='cuda:0')



h[200].sum tensor(25.6405, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0978, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0239, 0.0964, 0.0651,  ..., 0.0884, 0.0832, 0.0888],
        [0.0255, 0.1021, 0.0692,  ..., 0.0939, 0.0882, 0.0941],
        [0.0318, 0.1252, 0.0855,  ..., 0.1160, 0.1082, 0.1156],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42847.0039, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2146, 0.0000,  ..., 0.0901, 0.0000, 0.0635],
        [0.0000, 0.2609, 0.0000,  ..., 0.1083, 0.0000, 0.0780],
        [0.0000, 0.2925, 0.0000,  ..., 0.1205, 0.0000, 0.0880],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(220778.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-122.1215, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.7994, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-254.3418, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9965],
        [0.9925],
        [0.9813],
        ...,
        [0.9653],
        [0.9643],
        [0.9633]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360381., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9965],
        [0.9925],
        [0.9812],
        ...,
        [0.9652],
        [0.9642],
        [0.9632]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360366.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.2778],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(397.7091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0015,  0.0077,  0.0046,  ...,  0.0064,  0.0066,  0.0069],
        [ 0.0061,  0.0244,  0.0165,  ...,  0.0224,  0.0211,  0.0225],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1824.8521, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.0276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.4476, device='cuda:0')



h[100].sum tensor(21.6556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.2794, device='cuda:0')



h[200].sum tensor(28.5412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.9779, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0190, 0.0785, 0.0524,  ..., 0.0713, 0.0677, 0.0721],
        [0.0071, 0.0331, 0.0208,  ..., 0.0285, 0.0284, 0.0299],
        [0.0120, 0.0530, 0.0344,  ..., 0.0469, 0.0456, 0.0484],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50419.2109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2092, 0.0000,  ..., 0.0888, 0.0000, 0.0602],
        [0.0000, 0.1615, 0.0000,  ..., 0.0702, 0.0000, 0.0451],
        [0.0000, 0.1405, 0.0000,  ..., 0.0618, 0.0000, 0.0389],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(254059.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-158.6206, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-30.8364, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-309.2050, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9965],
        [0.9925],
        [0.9812],
        ...,
        [0.9652],
        [0.9642],
        [0.9632]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360366.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9965],
        [0.9925],
        [0.9812],
        ...,
        [0.9652],
        [0.9641],
        [0.9631]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360351.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(372.3856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0081,  0.0050,  ...,  0.0068,  0.0070,  0.0073],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1709.2463, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.7637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.1905, device='cuda:0')



h[100].sum tensor(21.2958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.5612, device='cuda:0')



h[200].sum tensor(27.7771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.4324, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0016, 0.0093, 0.0049,  ..., 0.0068, 0.0078, 0.0078],
        [0.0060, 0.0274, 0.0173,  ..., 0.0236, 0.0235, 0.0246],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49840.8398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0167, 0.0000,  ..., 0.0121, 0.0000, 0.0034],
        [0.0000, 0.0459, 0.0000,  ..., 0.0242, 0.0000, 0.0103],
        [0.0000, 0.0850, 0.0000,  ..., 0.0401, 0.0000, 0.0212],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(257946.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-155.9865, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-31.2273, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-305.2604, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9965],
        [0.9925],
        [0.9812],
        ...,
        [0.9652],
        [0.9641],
        [0.9631]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360351.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9965],
        [0.9925],
        [0.9812],
        ...,
        [0.9652],
        [0.9641],
        [0.9631]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360351.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.1106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1071.4911, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.2031, device='cuda:0')



h[100].sum tensor(19.3556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0650, device='cuda:0')



h[200].sum tensor(23.6560, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.0407, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38905.6953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 7.2984e-03, 0.0000e+00,  ..., 8.3417e-03, 0.0000e+00,
         2.4225e-04],
        [0.0000e+00, 4.3985e-03, 0.0000e+00,  ..., 7.1096e-03, 0.0000e+00,
         8.3312e-06],
        [0.0000e+00, 1.5057e-02, 0.0000e+00,  ..., 1.1430e-02, 0.0000e+00,
         2.4880e-03],
        ...,
        [0.0000e+00, 3.6283e-04, 0.0000e+00,  ..., 4.9510e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6310e-04, 0.0000e+00,  ..., 4.9505e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6311e-04, 0.0000e+00,  ..., 4.9503e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(217529.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-103.3460, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.4262, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-225.8596, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9965],
        [0.9925],
        [0.9812],
        ...,
        [0.9652],
        [0.9641],
        [0.9631]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360351.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9965],
        [0.9924],
        [0.9811],
        ...,
        [0.9651],
        [0.9640],
        [0.9630]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360336.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.4202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(905.9404, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7351, device='cuda:0')



h[100].sum tensor(18.8475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2797, device='cuda:0')



h[200].sum tensor(22.5769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.2573, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31912.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0197, 0.0000,  ..., 0.0131, 0.0000, 0.0050],
        [0.0000, 0.0120, 0.0000,  ..., 0.0101, 0.0000, 0.0018],
        [0.0000, 0.0278, 0.0000,  ..., 0.0167, 0.0000, 0.0059],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0050, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(164812.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-70.5623, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.3505, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-175.8378, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9965],
        [0.9924],
        [0.9811],
        ...,
        [0.9651],
        [0.9640],
        [0.9630]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360336.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9965],
        [0.9924],
        [0.9810],
        ...,
        [0.9650],
        [0.9639],
        [0.9629]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360322., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(185.8114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0018,  0.0088,  0.0055,  ...,  0.0075,  0.0076,  0.0080],
        [ 0.0013,  0.0071,  0.0042,  ...,  0.0059,  0.0061,  0.0064],
        [ 0.0036,  0.0155,  0.0102,  ...,  0.0139,  0.0134,  0.0142],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(730.5796, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.8716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.5613, device='cuda:0')



h[100].sum tensor(18.3109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.2698, device='cuda:0')



h[200].sum tensor(21.4372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.6779, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0058, 0.0267, 0.0167,  ..., 0.0228, 0.0228, 0.0239],
        [0.0129, 0.0563, 0.0367,  ..., 0.0501, 0.0485, 0.0514],
        [0.0053, 0.0265, 0.0161,  ..., 0.0222, 0.0227, 0.0238],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28752.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0834, 0.0000,  ..., 0.0394, 0.0000, 0.0207],
        [0.0000, 0.1173, 0.0000,  ..., 0.0531, 0.0000, 0.0304],
        [0.0000, 0.0900, 0.0000,  ..., 0.0422, 0.0000, 0.0222],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(153670.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-54.9566, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.3937, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-152.7332, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9965],
        [0.9924],
        [0.9810],
        ...,
        [0.9650],
        [0.9639],
        [0.9629]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360322., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9964],
        [0.9924],
        [0.9810],
        ...,
        [0.9649],
        [0.9638],
        [0.9628]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360307.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(297.2622, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0020,  0.0097,  0.0060,  ...,  0.0083,  0.0083,  0.0088],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1290.2009, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.4087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.4948, device='cuda:0')



h[100].sum tensor(20.0032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4306, device='cuda:0')



h[200].sum tensor(25.0317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.8810, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0069, 0.0287, 0.0187,  ..., 0.0253, 0.0246, 0.0258],
        [0.0020, 0.0108, 0.0060,  ..., 0.0082, 0.0091, 0.0092],
        [0.0033, 0.0175, 0.0102,  ..., 0.0141, 0.0148, 0.0154],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39844.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0909, 0.0000,  ..., 0.0411, 0.0000, 0.0261],
        [0.0000, 0.0577, 0.0000,  ..., 0.0285, 0.0000, 0.0148],
        [0.0000, 0.0699, 0.0000,  ..., 0.0337, 0.0000, 0.0174],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(206585.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-108.8304, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.9547, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-233.4616, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9964],
        [0.9924],
        [0.9810],
        ...,
        [0.9649],
        [0.9638],
        [0.9628]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360307.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9964],
        [0.9924],
        [0.9809],
        ...,
        [0.9648],
        [0.9637],
        [0.9627]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360292.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.3600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0021,  0.0101,  0.0063,  ...,  0.0087,  0.0086,  0.0091],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1411.2432, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.6710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.1970, device='cuda:0')



h[100].sum tensor(20.3638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9722, device='cuda:0')



h[200].sum tensor(25.7974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.8007, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0021, 0.0112, 0.0063,  ..., 0.0086, 0.0095, 0.0096],
        [0.0016, 0.0094, 0.0050,  ..., 0.0069, 0.0079, 0.0079],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44079., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0094, 0.0000,  ..., 0.0092, 0.0000, 0.0011],
        [0.0000, 0.0257, 0.0000,  ..., 0.0160, 0.0000, 0.0044],
        [0.0000, 0.0384, 0.0000,  ..., 0.0213, 0.0000, 0.0076],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(233542.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-128.9482, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-36.3244, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-263.7398, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9964],
        [0.9924],
        [0.9809],
        ...,
        [0.9648],
        [0.9637],
        [0.9627]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360292.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9964],
        [0.9923],
        [0.9809],
        ...,
        [0.9647],
        [0.9637],
        [0.9626]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360277.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(376.0889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1747.7952, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.6021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.5206, device='cuda:0')



h[100].sum tensor(21.3748, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.6662, device='cuda:0')



h[200].sum tensor(27.9448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.8046, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48841.4336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(246614.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-151.2039, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-31.9308, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-298.2462, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9964],
        [0.9923],
        [0.9809],
        ...,
        [0.9647],
        [0.9637],
        [0.9626]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360277.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9964],
        [0.9923],
        [0.9808],
        ...,
        [0.9646],
        [0.9636],
        [0.9625]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360262.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.9006, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0017,  0.0083,  0.0051,  ...,  0.0070,  0.0072,  0.0075],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1045.6240, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9555, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9170, device='cuda:0')



h[100].sum tensor(19.2473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9740, device='cuda:0')



h[200].sum tensor(23.4261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.7181, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0062, 0.0281, 0.0177,  ..., 0.0242, 0.0241, 0.0253],
        [0.0016, 0.0096, 0.0051,  ..., 0.0070, 0.0080, 0.0080],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34811.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0885, 0.0000,  ..., 0.0415, 0.0000, 0.0224],
        [0.0000, 0.0465, 0.0000,  ..., 0.0243, 0.0000, 0.0107],
        [0.0000, 0.0182, 0.0000,  ..., 0.0126, 0.0000, 0.0036],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(178562.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-83.9875, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.6793, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-196.8462, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9964],
        [0.9923],
        [0.9808],
        ...,
        [0.9646],
        [0.9636],
        [0.9625]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360262.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9964],
        [0.9923],
        [0.9807],
        ...,
        [0.9645],
        [0.9635],
        [0.9624]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360248.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(276.5858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0121,  0.0465,  0.0322,  ...,  0.0436,  0.0403,  0.0431],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0070,  0.0278,  0.0189,  ...,  0.0256,  0.0240,  0.0256],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1225.5759, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.6520, device='cuda:0')



h[100].sum tensor(19.7851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8442, device='cuda:0')



h[200].sum tensor(24.5682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.8026, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0206, 0.0826, 0.0558,  ..., 0.0758, 0.0713, 0.0760],
        [0.0302, 0.1193, 0.0813,  ..., 0.1103, 0.1031, 0.1101],
        [0.0056, 0.0238, 0.0152,  ..., 0.0207, 0.0204, 0.0213],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38819.2734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2810, 0.0000,  ..., 0.1149, 0.0000, 0.0861],
        [0.0000, 0.2262, 0.0000,  ..., 0.0937, 0.0000, 0.0689],
        [0.0000, 0.1047, 0.0000,  ..., 0.0463, 0.0000, 0.0307],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(202984., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-103.7470, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.0900, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-225.7876, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9964],
        [0.9923],
        [0.9807],
        ...,
        [0.9645],
        [0.9635],
        [0.9624]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360248.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9964],
        [0.9923],
        [0.9807],
        ...,
        [0.9644],
        [0.9634],
        [0.9623]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360233.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(321.4385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0030,  0.0133,  0.0087,  ...,  0.0118,  0.0115,  0.0122],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1478.1116, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.3103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.6497, device='cuda:0')



h[100].sum tensor(20.5400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1163, device='cuda:0')



h[200].sum tensor(26.1718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.3112, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0030, 0.0145, 0.0086,  ..., 0.0118, 0.0123, 0.0126],
        [0.0111, 0.0461, 0.0305,  ..., 0.0414, 0.0397, 0.0420],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(46037.7578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0334, 0.0000,  ..., 0.0188, 0.0000, 0.0081],
        [0.0000, 0.0970, 0.0000,  ..., 0.0438, 0.0000, 0.0273],
        [0.0000, 0.2041, 0.0000,  ..., 0.0854, 0.0000, 0.0613],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(239457.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-138.1201, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-34.2733, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-278.2693, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9964],
        [0.9923],
        [0.9807],
        ...,
        [0.9644],
        [0.9634],
        [0.9623]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360233.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9963],
        [0.9922],
        [0.9806],
        ...,
        [0.9644],
        [0.9633],
        [0.9623]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360218.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(181.1398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(712.8642, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0332, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.1449, device='cuda:0')



h[100].sum tensor(18.2320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.1373, device='cuda:0')



h[200].sum tensor(21.2695, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.2083, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0071, 0.0293, 0.0191,  ..., 0.0259, 0.0251, 0.0263],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28214.6973, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0072, 0.0000, 0.0010],
        [0.0000, 0.0204, 0.0000,  ..., 0.0133, 0.0000, 0.0054],
        [0.0000, 0.0689, 0.0000,  ..., 0.0323, 0.0000, 0.0196],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(149602.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.0157, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.5407, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-149.2924, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9963],
        [0.9922],
        [0.9806],
        ...,
        [0.9644],
        [0.9633],
        [0.9623]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360218.4375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 100.0 event: 500 loss: tensor(1186.8400, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9963],
        [0.9922],
        [0.9806],
        ...,
        [0.9643],
        [0.9632],
        [0.9622]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360203.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.6600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0083,  0.0051,  ...,  0.0070,  0.0071,  0.0075],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(782.1880, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1717, device='cuda:0')



h[100].sum tensor(18.4366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4640, device='cuda:0')



h[200].sum tensor(21.7043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.3663, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0016, 0.0095, 0.0050,  ..., 0.0069, 0.0079, 0.0079],
        [0.0012, 0.0080, 0.0040,  ..., 0.0055, 0.0066, 0.0066],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29456.7266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0079, 0.0000, 0.0006],
        [0.0000, 0.0192, 0.0000,  ..., 0.0133, 0.0000, 0.0029],
        [0.0000, 0.0257, 0.0000,  ..., 0.0161, 0.0000, 0.0043],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0052, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(155961.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-58.8319, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.6203, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-157.9838, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9963],
        [0.9922],
        [0.9806],
        ...,
        [0.9643],
        [0.9632],
        [0.9622]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360203.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9963],
        [0.9922],
        [0.9805],
        ...,
        [0.9642],
        [0.9631],
        [0.9621]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360188.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(363.3357, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0018,  0.0089,  0.0055,  ...,  0.0076,  0.0077,  0.0081],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0018,  0.0089,  0.0055,  ...,  0.0076,  0.0077,  0.0081],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1656.0276, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.2577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.3839, device='cuda:0')



h[100].sum tensor(21.0545, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.3045, device='cuda:0')



h[200].sum tensor(27.2644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.5227, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0061, 0.0278, 0.0175,  ..., 0.0239, 0.0238, 0.0250],
        [0.0098, 0.0448, 0.0286,  ..., 0.0391, 0.0386, 0.0408],
        [0.0031, 0.0147, 0.0088,  ..., 0.0120, 0.0125, 0.0128],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49316.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0967, 0.0000,  ..., 0.0449, 0.0000, 0.0243],
        [0.0000, 0.0963, 0.0000,  ..., 0.0449, 0.0000, 0.0236],
        [0.0000, 0.0579, 0.0000,  ..., 0.0291, 0.0000, 0.0133],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(255857.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-153.2340, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-32.1252, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-300.8919, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9963],
        [0.9922],
        [0.9805],
        ...,
        [0.9642],
        [0.9631],
        [0.9621]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360188.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9963],
        [0.9921],
        [0.9804],
        ...,
        [0.9641],
        [0.9630],
        [0.9620]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360173.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.3185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1315.4580, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.2325, device='cuda:0')



h[100].sum tensor(20.0269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3471, device='cuda:0')



h[200].sum tensor(25.0819, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.5850, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0014, 0.0087, 0.0045,  ..., 0.0062, 0.0073, 0.0072],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40308.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.1164e-02, 0.0000e+00,  ..., 1.0042e-02, 0.0000e+00,
         3.6893e-04],
        [0.0000e+00, 1.4229e-02, 0.0000e+00,  ..., 1.1191e-02, 0.0000e+00,
         1.6454e-03],
        [0.0000e+00, 3.0065e-02, 0.0000e+00,  ..., 1.7610e-02, 0.0000e+00,
         6.1146e-03],
        ...,
        [0.0000e+00, 1.9061e-03, 0.0000e+00,  ..., 5.5780e-03, 0.0000e+00,
         1.4323e-06],
        [0.0000e+00, 3.6409e-04, 0.0000e+00,  ..., 4.9402e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6409e-04, 0.0000e+00,  ..., 4.9400e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(206838.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-110.3817, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.7321, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-236.5392, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9963],
        [0.9921],
        [0.9804],
        ...,
        [0.9641],
        [0.9630],
        [0.9620]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360173.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9963],
        [0.9921],
        [0.9804],
        ...,
        [0.9640],
        [0.9629],
        [0.9619]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360159.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3997],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(169.7075, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0027,  0.0124,  0.0080,  ...,  0.0109,  0.0106,  0.0113],
        [ 0.0023,  0.0107,  0.0068,  ...,  0.0093,  0.0092,  0.0097],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(665.8480, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.3500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.1259, device='cuda:0')



h[100].sum tensor(18.0771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.8131, device='cuda:0')



h[200].sum tensor(20.9407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.0591, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0027, 0.0136, 0.0079,  ..., 0.0108, 0.0115, 0.0117],
        [0.0067, 0.0299, 0.0190,  ..., 0.0259, 0.0256, 0.0269],
        [0.0170, 0.0711, 0.0472,  ..., 0.0642, 0.0613, 0.0652],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27341.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0733, 0.0000,  ..., 0.0348, 0.0000, 0.0194],
        [0.0000, 0.1068, 0.0000,  ..., 0.0482, 0.0000, 0.0292],
        [0.0000, 0.1650, 0.0000,  ..., 0.0713, 0.0000, 0.0466],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(145461.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-48.6890, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-51.3951, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-142.9017, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9963],
        [0.9921],
        [0.9804],
        ...,
        [0.9640],
        [0.9629],
        [0.9619]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360159.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9963],
        [0.9921],
        [0.9803],
        ...,
        [0.9639],
        [0.9628],
        [0.9618]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360144.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.9486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0037,  0.0157,  0.0103,  ...,  0.0141,  0.0135,  0.0144],
        [ 0.0066,  0.0264,  0.0179,  ...,  0.0243,  0.0228,  0.0243],
        [ 0.0069,  0.0275,  0.0186,  ...,  0.0253,  0.0237,  0.0253],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(929.8817, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7822, device='cuda:0')



h[100].sum tensor(18.8630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2947, device='cuda:0')



h[200].sum tensor(22.6098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.3104, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0200, 0.0822, 0.0550,  ..., 0.0748, 0.0709, 0.0756],
        [0.0273, 0.1088, 0.0739,  ..., 0.1003, 0.0940, 0.1003],
        [0.0238, 0.0959, 0.0647,  ..., 0.0879, 0.0828, 0.0883],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33595.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2589, 0.0000,  ..., 0.1082, 0.0000, 0.0758],
        [0.0000, 0.2733, 0.0000,  ..., 0.1133, 0.0000, 0.0814],
        [0.0000, 0.2286, 0.0000,  ..., 0.0957, 0.0000, 0.0677],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(182245.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-78.7252, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.7002, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-188.1441, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9963],
        [0.9921],
        [0.9803],
        ...,
        [0.9639],
        [0.9628],
        [0.9618]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360144.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9963],
        [0.9921],
        [0.9803],
        ...,
        [0.9638],
        [0.9628],
        [0.9617]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360129.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3884],
        [0.6616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049,  0.0202,  0.0135,  ...,  0.0184,  0.0174,  0.0185],
        [ 0.0027,  0.0120,  0.0077,  ...,  0.0106,  0.0103,  0.0110],
        [ 0.0107,  0.0415,  0.0286,  ...,  0.0387,  0.0359,  0.0383],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(996.2856, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7831, device='cuda:0')



h[100].sum tensor(19.0569, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6132, device='cuda:0')



h[200].sum tensor(23.0217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.4393, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0139, 0.0598, 0.0392,  ..., 0.0534, 0.0515, 0.0547],
        [0.0259, 0.1038, 0.0704,  ..., 0.0955, 0.0897, 0.0956],
        [0.0107, 0.0464, 0.0302,  ..., 0.0412, 0.0399, 0.0423],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33890.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1352, 0.0000,  ..., 0.0593, 0.0000, 0.0381],
        [0.0000, 0.1855, 0.0000,  ..., 0.0787, 0.0000, 0.0545],
        [0.0000, 0.1486, 0.0000,  ..., 0.0643, 0.0000, 0.0428],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(177457.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-79.7300, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.6201, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-189.9740, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9963],
        [0.9921],
        [0.9803],
        ...,
        [0.9638],
        [0.9628],
        [0.9617]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360129.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9962],
        [0.9920],
        [0.9802],
        ...,
        [0.9637],
        [0.9627],
        [0.9616]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360114.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.7400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(894.7717, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9614, device='cuda:0')



h[100].sum tensor(18.7497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0335, device='cuda:0')



h[200].sum tensor(22.3692, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.3847, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0020, 0.0109, 0.0060,  ..., 0.0083, 0.0091, 0.0092],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31532.7461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0357, 0.0000,  ..., 0.0200, 0.0000, 0.0072],
        [0.0000, 0.0134, 0.0000,  ..., 0.0108, 0.0000, 0.0022],
        [0.0000, 0.0032, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(164298.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-68.4407, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.8117, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-172.8518, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9962],
        [0.9920],
        [0.9802],
        ...,
        [0.9637],
        [0.9627],
        [0.9616]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360114.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9962],
        [0.9920],
        [0.9801],
        ...,
        [0.9636],
        [0.9626],
        [0.9615]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360099.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(419.5275, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0069,  0.0041,  ...,  0.0056,  0.0059,  0.0062],
        [ 0.0012,  0.0069,  0.0041,  ...,  0.0056,  0.0059,  0.0062],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1980.5605, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.3663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.3923, device='cuda:0')



h[100].sum tensor(21.9788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.8982, device='cuda:0')



h[200].sum tensor(29.2277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.1711, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0091, 0.0425, 0.0269,  ..., 0.0369, 0.0366, 0.0387],
        [0.0051, 0.0258, 0.0156,  ..., 0.0215, 0.0221, 0.0231],
        [0.0111, 0.0496, 0.0319,  ..., 0.0437, 0.0427, 0.0452],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(55817.5859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1269, 0.0000,  ..., 0.0574, 0.0000, 0.0321],
        [0.0000, 0.1139, 0.0000,  ..., 0.0522, 0.0000, 0.0285],
        [0.0000, 0.1316, 0.0000,  ..., 0.0590, 0.0000, 0.0343],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(289385.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-185.3262, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-24.8186, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-349.7920, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9962],
        [0.9920],
        [0.9801],
        ...,
        [0.9636],
        [0.9626],
        [0.9615]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360099.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9962],
        [0.9920],
        [0.9801],
        ...,
        [0.9636],
        [0.9625],
        [0.9614]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360084.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(284.3948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1273.5314, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.3480, device='cuda:0')



h[100].sum tensor(19.8679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0657, device='cuda:0')



h[200].sum tensor(24.7442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.5875, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0072, 0.0298, 0.0194,  ..., 0.0264, 0.0255, 0.0268],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39502.1445, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0171, 0.0000,  ..., 0.0122, 0.0000, 0.0030],
        [0.0000, 0.0354, 0.0000,  ..., 0.0194, 0.0000, 0.0091],
        [0.0000, 0.0867, 0.0000,  ..., 0.0396, 0.0000, 0.0246],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(202629.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-106.6583, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.5914, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-230.4700, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9962],
        [0.9920],
        [0.9801],
        ...,
        [0.9636],
        [0.9625],
        [0.9614]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360084.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9962],
        [0.9919],
        [0.9800],
        ...,
        [0.9635],
        [0.9624],
        [0.9613]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360069.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.5537, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [ 0.0042,  0.0178,  0.0118,  ...,  0.0161,  0.0154,  0.0164],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1593.0676, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.7546, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.3534, device='cuda:0')



h[100].sum tensor(20.8116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6584, device='cuda:0')



h[200].sum tensor(26.7486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.2326, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0058, 0.0283, 0.0175,  ..., 0.0240, 0.0243, 0.0255],
        [0.0054, 0.0249, 0.0155,  ..., 0.0213, 0.0213, 0.0223],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48278.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0061, 0.0000,  ..., 0.0078, 0.0000, 0.0009],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0965, 0.0000,  ..., 0.0442, 0.0000, 0.0250],
        [0.0000, 0.0703, 0.0000,  ..., 0.0334, 0.0000, 0.0180],
        [0.0000, 0.0238, 0.0000,  ..., 0.0145, 0.0000, 0.0055]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(254483.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-148.5077, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-32.3655, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-294.2758, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9962],
        [0.9919],
        [0.9800],
        ...,
        [0.9635],
        [0.9624],
        [0.9613]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360069.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 110.0 event: 550 loss: tensor(606.0385, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9962],
        [0.9919],
        [0.9800],
        ...,
        [0.9634],
        [0.9623],
        [0.9612]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360055., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9872, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0069,  0.0041,  ...,  0.0056,  0.0059,  0.0062],
        [ 0.0017,  0.0087,  0.0053,  ...,  0.0074,  0.0074,  0.0078],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1273.2877, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.4899, device='cuda:0')



h[100].sum tensor(19.8562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.1108, device='cuda:0')



h[200].sum tensor(24.7194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.7476, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0154, 0.0654, 0.0431,  ..., 0.0588, 0.0564, 0.0599],
        [0.0061, 0.0278, 0.0175,  ..., 0.0239, 0.0238, 0.0250],
        [0.0017, 0.0098, 0.0053,  ..., 0.0073, 0.0082, 0.0083],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39069.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1475, 0.0000,  ..., 0.0649, 0.0000, 0.0404],
        [0.0000, 0.0982, 0.0000,  ..., 0.0450, 0.0000, 0.0260],
        [0.0000, 0.0499, 0.0000,  ..., 0.0255, 0.0000, 0.0122],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(202158.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-104.3587, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.1733, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-227.0764, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9962],
        [0.9919],
        [0.9800],
        ...,
        [0.9634],
        [0.9623],
        [0.9612]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360055., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9962],
        [0.9919],
        [0.9799],
        ...,
        [0.9633],
        [0.9622],
        [0.9612]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360040.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.2189, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0012,  0.0069,  0.0041,  ...,  0.0056,  0.0059,  0.0062],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1684.0979, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.2298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.0170, device='cuda:0')



h[100].sum tensor(21.0681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.1878, device='cuda:0')



h[200].sum tensor(27.2933, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.1089, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0009, 0.0069, 0.0032,  ..., 0.0045, 0.0057, 0.0055],
        [0.0058, 0.0286, 0.0176,  ..., 0.0242, 0.0245, 0.0258],
        [0.0112, 0.0480, 0.0314,  ..., 0.0427, 0.0413, 0.0438],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49577.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0468, 0.0000,  ..., 0.0246, 0.0000, 0.0101],
        [0.0000, 0.0943, 0.0000,  ..., 0.0439, 0.0000, 0.0233],
        [0.0000, 0.1309, 0.0000,  ..., 0.0585, 0.0000, 0.0344],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(258159.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-154.5363, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-31.7627, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-302.8633, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9962],
        [0.9919],
        [0.9799],
        ...,
        [0.9633],
        [0.9622],
        [0.9612]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360040.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9961],
        [0.9919],
        [0.9798],
        ...,
        [0.9632],
        [0.9621],
        [0.9611]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360025.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.4399, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(973.3386, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1825, device='cuda:0')



h[100].sum tensor(18.9576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4220, device='cuda:0')



h[200].sum tensor(22.8109, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.7619, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33860.8359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(177366.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-79.9874, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.4635, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-190.0061, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9961],
        [0.9919],
        [0.9798],
        ...,
        [0.9632],
        [0.9621],
        [0.9611]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360025.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9961],
        [0.9919],
        [0.9798],
        ...,
        [0.9632],
        [0.9621],
        [0.9611]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360025.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.1526, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0083,  0.0050,  ...,  0.0070,  0.0071,  0.0075],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1242.7141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.9169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.8807, device='cuda:0')



h[100].sum tensor(19.7549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9170, device='cuda:0')



h[200].sum tensor(24.5041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.0606, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0016, 0.0094, 0.0050,  ..., 0.0069, 0.0079, 0.0079],
        [0.0027, 0.0132, 0.0077,  ..., 0.0105, 0.0112, 0.0114],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39670.7617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0103, 0.0000,  ..., 0.0095, 0.0000, 0.0017],
        [0.0000, 0.0338, 0.0000,  ..., 0.0193, 0.0000, 0.0070],
        [0.0000, 0.0554, 0.0000,  ..., 0.0281, 0.0000, 0.0125],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(207521.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-106.9546, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.7605, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-231.2505, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9961],
        [0.9919],
        [0.9798],
        ...,
        [0.9632],
        [0.9621],
        [0.9611]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360025.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9961],
        [0.9918],
        [0.9798],
        ...,
        [0.9631],
        [0.9620],
        [0.9610]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(360010.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(257.6786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1133.4893, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.9668, device='cuda:0')



h[100].sum tensor(19.4267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3080, device='cuda:0')



h[200].sum tensor(23.8071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.9020, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0029, 0.0161, 0.0093,  ..., 0.0128, 0.0137, 0.0141],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35678.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0072, 0.0000, 0.0004],
        [0.0000, 0.0175, 0.0000,  ..., 0.0125, 0.0000, 0.0033],
        [0.0000, 0.0496, 0.0000,  ..., 0.0258, 0.0000, 0.0109],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(184327.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-88.3161, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.1500, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-202.7207, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9961],
        [0.9918],
        [0.9798],
        ...,
        [0.9631],
        [0.9620],
        [0.9610]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(360010.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9961],
        [0.9918],
        [0.9797],
        ...,
        [0.9630],
        [0.9619],
        [0.9609]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359995.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.8618],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(411.6315, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0110,  0.0423,  0.0292,  ...,  0.0395,  0.0366,  0.0392],
        [ 0.0024,  0.0109,  0.0069,  ...,  0.0095,  0.0094,  0.0099],
        [ 0.0088,  0.0345,  0.0236,  ...,  0.0320,  0.0298,  0.0319],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1959.8274, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.6065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.6885, device='cuda:0')



h[100].sum tensor(21.8614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.6742, device='cuda:0')



h[200].sum tensor(28.9783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.3774, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0185, 0.0767, 0.0511,  ..., 0.0696, 0.0661, 0.0704],
        [0.0440, 0.1699, 0.1172,  ..., 0.1587, 0.1469, 0.1571],
        [0.0270, 0.1077, 0.0732,  ..., 0.0993, 0.0931, 0.0993],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(54608.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.7886e-01, 0.0000e+00,  ..., 1.1508e-01, 0.0000e+00,
         8.3211e-02],
        [0.0000e+00, 3.7347e-01, 0.0000e+00,  ..., 1.5095e-01, 0.0000e+00,
         1.1494e-01],
        [0.0000e+00, 3.3893e-01, 0.0000e+00,  ..., 1.3784e-01, 0.0000e+00,
         1.0371e-01],
        ...,
        [0.0000e+00, 3.6479e-04, 0.0000e+00,  ..., 4.9304e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6506e-04, 0.0000e+00,  ..., 4.9299e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6507e-04, 0.0000e+00,  ..., 4.9296e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(280472.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-178.8939, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-26.9964, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-339.5414, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9961],
        [0.9918],
        [0.9797],
        ...,
        [0.9630],
        [0.9619],
        [0.9609]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359995.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9961],
        [0.9918],
        [0.9797],
        ...,
        [0.9629],
        [0.9618],
        [0.9608]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359980.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.4360, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0061,  0.0246,  0.0167,  ...,  0.0226,  0.0213,  0.0227],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1012.8102, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9843, device='cuda:0')



h[100].sum tensor(19.0611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6772, device='cuda:0')



h[200].sum tensor(23.0306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.6662, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0061, 0.0258, 0.0166,  ..., 0.0225, 0.0221, 0.0231],
        [0.0049, 0.0214, 0.0135,  ..., 0.0183, 0.0182, 0.0190],
        [0.0218, 0.0888, 0.0597,  ..., 0.0812, 0.0767, 0.0817],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34164.3555, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0567, 0.0000,  ..., 0.0277, 0.0000, 0.0155],
        [0.0000, 0.0723, 0.0000,  ..., 0.0339, 0.0000, 0.0202],
        [0.0000, 0.1225, 0.0000,  ..., 0.0537, 0.0000, 0.0356],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(178313.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.0693, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.3823, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.9209, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9961],
        [0.9918],
        [0.9797],
        ...,
        [0.9629],
        [0.9618],
        [0.9608]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359980.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9961],
        [0.9917],
        [0.9796],
        ...,
        [0.9628],
        [0.9618],
        [0.9607]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359965.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.4908, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1055.2124, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0901, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3457, device='cuda:0')



h[100].sum tensor(19.1815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7922, device='cuda:0')



h[200].sum tensor(23.2864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.0737, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34161.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(176707.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.6678, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.6572, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.5736, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9961],
        [0.9917],
        [0.9796],
        ...,
        [0.9628],
        [0.9618],
        [0.9607]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359965.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9961],
        [0.9917],
        [0.9795],
        ...,
        [0.9627],
        [0.9617],
        [0.9606]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359950.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.1547, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0012,  0.0069,  0.0041,  ...,  0.0056,  0.0059,  0.0062],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(936.8387, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8119, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7114, device='cuda:0')



h[100].sum tensor(18.8288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2721, device='cuda:0')



h[200].sum tensor(22.5371, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.2306, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0022, 0.0133, 0.0073,  ..., 0.0101, 0.0113, 0.0115],
        [0.0030, 0.0181, 0.0102,  ..., 0.0142, 0.0154, 0.0160],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31078.5098, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0150, 0.0000,  ..., 0.0115, 0.0000, 0.0026],
        [0.0000, 0.0429, 0.0000,  ..., 0.0232, 0.0000, 0.0087],
        [0.0000, 0.0611, 0.0000,  ..., 0.0309, 0.0000, 0.0130],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(158346.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-66.9794, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.6913, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-170.2985, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9961],
        [0.9917],
        [0.9795],
        ...,
        [0.9627],
        [0.9617],
        [0.9606]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359950.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9961],
        [0.9917],
        [0.9795],
        ...,
        [0.9627],
        [0.9616],
        [0.9605]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359935.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.6978]], device='cuda:0') 
g.ndata[nfet].sum tensor(301.0491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0082,  0.0050,  ...,  0.0069,  0.0070,  0.0074],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0052,  0.0213,  0.0143,  ...,  0.0194,  0.0183,  0.0195],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1370.2173, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.8324, device='cuda:0')



h[100].sum tensor(20.0979, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5380, device='cuda:0')



h[200].sum tensor(25.2328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.2616, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0016, 0.0094, 0.0050,  ..., 0.0069, 0.0079, 0.0079],
        [0.0060, 0.0273, 0.0171,  ..., 0.0234, 0.0233, 0.0245],
        ...,
        [0.0049, 0.0214, 0.0136,  ..., 0.0185, 0.0183, 0.0191],
        [0.0039, 0.0178, 0.0110,  ..., 0.0150, 0.0151, 0.0157],
        [0.0178, 0.0737, 0.0491,  ..., 0.0668, 0.0635, 0.0676]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41980.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0300, 0.0000,  ..., 0.0175, 0.0000, 0.0064],
        [0.0000, 0.0505, 0.0000,  ..., 0.0260, 0.0000, 0.0117],
        [0.0000, 0.0851, 0.0000,  ..., 0.0401, 0.0000, 0.0213],
        ...,
        [0.0000, 0.0448, 0.0000,  ..., 0.0226, 0.0000, 0.0117],
        [0.0000, 0.0576, 0.0000,  ..., 0.0277, 0.0000, 0.0155],
        [0.0000, 0.0984, 0.0000,  ..., 0.0440, 0.0000, 0.0278]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(219673.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-117.6701, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.8523, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-247.6607, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9961],
        [0.9917],
        [0.9795],
        ...,
        [0.9627],
        [0.9616],
        [0.9605]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359935.6250, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 120.0 event: 600 loss: tensor(581.3310, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9960],
        [0.9916],
        [0.9794],
        ...,
        [0.9626],
        [0.9615],
        [0.9604]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359920.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.8708, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1092.9459, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8914, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0926, device='cuda:0')



h[100].sum tensor(19.2786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0298, device='cuda:0')



h[200].sum tensor(23.4926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.9161, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37326.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0042, 0.0000,  ..., 0.0070, 0.0000, 0.0005],
        [0.0000, 0.0006, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(198701.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-96.6321, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.3879, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-214.9679, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9960],
        [0.9916],
        [0.9794],
        ...,
        [0.9626],
        [0.9615],
        [0.9604]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359920.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9960],
        [0.9916],
        [0.9794],
        ...,
        [0.9625],
        [0.9614],
        [0.9603]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359905.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(905.1440, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7852, device='cuda:0')



h[100].sum tensor(18.7236, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.9774, device='cuda:0')



h[200].sum tensor(22.3136, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.1860, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0016, 0.0095, 0.0050,  ..., 0.0069, 0.0079, 0.0079],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0037, 0.0170, 0.0104,  ..., 0.0142, 0.0145, 0.0150],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32852.5195, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0220, 0.0000,  ..., 0.0145, 0.0000, 0.0036],
        [0.0000, 0.0176, 0.0000,  ..., 0.0124, 0.0000, 0.0030],
        [0.0000, 0.0381, 0.0000,  ..., 0.0206, 0.0000, 0.0092],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(175763.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-74.8156, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.5707, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-182.4600, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9960],
        [0.9916],
        [0.9794],
        ...,
        [0.9625],
        [0.9614],
        [0.9603]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359905.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9960],
        [0.9916],
        [0.9793],
        ...,
        [0.9624],
        [0.9613],
        [0.9602]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359890.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(287.5363, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1319.8938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.6280, device='cuda:0')



h[100].sum tensor(19.9337, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.1548, device='cuda:0')



h[200].sum tensor(24.8840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.9033, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39268.7383, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0019, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0094, 0.0000, 0.0010],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(202041.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-104.7728, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.5041, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-227.7822, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9960],
        [0.9916],
        [0.9793],
        ...,
        [0.9624],
        [0.9613],
        [0.9602]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359890.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9960],
        [0.9916],
        [0.9793],
        ...,
        [0.9624],
        [0.9613],
        [0.9602]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359890.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.1803, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1019.8495, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7832, device='cuda:0')



h[100].sum tensor(19.0554, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6132, device='cuda:0')



h[200].sum tensor(23.0184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.4394, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0170, 0.0693, 0.0464,  ..., 0.0631, 0.0598, 0.0636],
        [0.0025, 0.0126, 0.0073,  ..., 0.0099, 0.0106, 0.0108],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33629.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1836, 0.0000,  ..., 0.0782, 0.0000, 0.0535],
        [0.0000, 0.1126, 0.0000,  ..., 0.0502, 0.0000, 0.0316],
        [0.0000, 0.0805, 0.0000,  ..., 0.0373, 0.0000, 0.0222],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(174760.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-78.1250, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.0659, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-187.8154, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9960],
        [0.9916],
        [0.9793],
        ...,
        [0.9624],
        [0.9613],
        [0.9602]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359890.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9960],
        [0.9916],
        [0.9792],
        ...,
        [0.9623],
        [0.9612],
        [0.9601]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359875.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.0637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1034.5916, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0402, device='cuda:0')



h[100].sum tensor(19.0941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6950, device='cuda:0')



h[200].sum tensor(23.1007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.7292, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35193.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 9.0938e-03, 0.0000e+00,  ..., 9.0185e-03, 0.0000e+00,
         1.3011e-03],
        [0.0000e+00, 2.4028e-03, 0.0000e+00,  ..., 6.2121e-03, 0.0000e+00,
         7.3265e-05],
        [0.0000e+00, 3.1435e-04, 0.0000e+00,  ..., 5.3191e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 3.6544e-04, 0.0000e+00,  ..., 4.9235e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6571e-04, 0.0000e+00,  ..., 4.9230e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6572e-04, 0.0000e+00,  ..., 4.9228e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(187849.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-86.0037, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.7487, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-198.9408, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9960],
        [0.9916],
        [0.9792],
        ...,
        [0.9623],
        [0.9612],
        [0.9601]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359875.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9960],
        [0.9915],
        [0.9792],
        ...,
        [0.9622],
        [0.9611],
        [0.9600]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359860.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.7358, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1083.2957, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.7241, device='cuda:0')



h[100].sum tensor(19.2320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9126, device='cuda:0')



h[200].sum tensor(23.3935, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.5005, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34775.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0275, 0.0000,  ..., 0.0166, 0.0000, 0.0054],
        [0.0000, 0.0199, 0.0000,  ..., 0.0135, 0.0000, 0.0037],
        [0.0000, 0.0121, 0.0000,  ..., 0.0103, 0.0000, 0.0017],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(179728.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-83.2628, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.3311, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-195.6076, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9960],
        [0.9915],
        [0.9792],
        ...,
        [0.9622],
        [0.9611],
        [0.9600]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359860.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9960],
        [0.9915],
        [0.9791],
        ...,
        [0.9621],
        [0.9610],
        [0.9600]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359845.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.5437, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1053.8136, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1722, device='cuda:0')



h[100].sum tensor(19.1414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7369, device='cuda:0')



h[200].sum tensor(23.2012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.8780, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33674.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(173713.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-79.0640, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.6401, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-188.6957, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9960],
        [0.9915],
        [0.9791],
        ...,
        [0.9621],
        [0.9610],
        [0.9600]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359845.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9959],
        [0.9915],
        [0.9790],
        ...,
        [0.9620],
        [0.9609],
        [0.9599]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359830.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(237.0911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1054.4797, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.1318, device='cuda:0')



h[100].sum tensor(19.1389, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7241, device='cuda:0')



h[200].sum tensor(23.1959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.8325, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34694.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0045, 0.0000,  ..., 0.0071, 0.0000, 0.0003],
        [0.0000, 0.0181, 0.0000,  ..., 0.0125, 0.0000, 0.0038],
        [0.0000, 0.0342, 0.0000,  ..., 0.0190, 0.0000, 0.0079],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(183025.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-83.3157, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.2520, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-195.2603, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9959],
        [0.9915],
        [0.9790],
        ...,
        [0.9620],
        [0.9609],
        [0.9599]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359830.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9959],
        [0.9914],
        [0.9790],
        ...,
        [0.9619],
        [0.9608],
        [0.9598]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359815.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.5122, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1052.2959, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3476, device='cuda:0')



h[100].sum tensor(19.1282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7928, device='cuda:0')



h[200].sum tensor(23.1730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.0759, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33949.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0030, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0072, 0.0000, 0.0003],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0071, 0.0000, 0.0007],
        [0.0000, 0.0167, 0.0000,  ..., 0.0116, 0.0000, 0.0033],
        [0.0000, 0.0221, 0.0000,  ..., 0.0138, 0.0000, 0.0045]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(174560.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.2676, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.5444, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-190.3880, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9959],
        [0.9914],
        [0.9790],
        ...,
        [0.9619],
        [0.9608],
        [0.9598]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359815.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9959],
        [0.9914],
        [0.9789],
        ...,
        [0.9618],
        [0.9608],
        [0.9597]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359800.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(416.8326, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0048,  0.0200,  0.0134,  ...,  0.0182,  0.0173,  0.0184],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2015.7378, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.4782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.1521, device='cuda:0')



h[100].sum tensor(21.9241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.8217, device='cuda:0')



h[200].sum tensor(29.1115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.9002, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0087, 0.0370, 0.0241,  ..., 0.0328, 0.0318, 0.0336],
        [0.0086, 0.0368, 0.0239,  ..., 0.0325, 0.0316, 0.0333],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52117.7227, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0512, 0.0000,  ..., 0.0260, 0.0000, 0.0125],
        [0.0000, 0.0973, 0.0000,  ..., 0.0441, 0.0000, 0.0270],
        [0.0000, 0.1215, 0.0000,  ..., 0.0536, 0.0000, 0.0346],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(265198.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-166.9585, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-28.8842, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-322.0292, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9959],
        [0.9914],
        [0.9789],
        ...,
        [0.9618],
        [0.9608],
        [0.9597]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359800.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 130.0 event: 650 loss: tensor(576.2130, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9959],
        [0.9914],
        [0.9789],
        ...,
        [0.9618],
        [0.9607],
        [0.9596]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359785.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2490, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0013,  0.0069,  0.0041,  ...,  0.0057,  0.0059,  0.0062],
        [ 0.0073,  0.0291,  0.0198,  ...,  0.0269,  0.0251,  0.0268],
        [ 0.0117,  0.0449,  0.0310,  ...,  0.0420,  0.0388,  0.0415],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1494.9846, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.5436, device='cuda:0')



h[100].sum tensor(20.4046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0825, device='cuda:0')



h[200].sum tensor(25.8841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.1916, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0236, 0.0953, 0.0643,  ..., 0.0874, 0.0823, 0.0877],
        [0.0266, 0.1063, 0.0721,  ..., 0.0979, 0.0918, 0.0980],
        [0.0357, 0.1396, 0.0957,  ..., 0.1297, 0.1207, 0.1289],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44460.3477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.1194e-01, 0.0000e+00,  ..., 1.2800e-01, 0.0000e+00,
         9.4257e-02],
        [0.0000e+00, 3.3585e-01, 0.0000e+00,  ..., 1.3706e-01, 0.0000e+00,
         1.0201e-01],
        [0.0000e+00, 3.7363e-01, 0.0000e+00,  ..., 1.5131e-01, 0.0000e+00,
         1.1463e-01],
        ...,
        [0.0000e+00, 3.6593e-04, 0.0000e+00,  ..., 4.9184e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6620e-04, 0.0000e+00,  ..., 4.9179e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6620e-04, 0.0000e+00,  ..., 4.9176e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(232750.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-130.2716, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-36.0341, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-266.3425, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9959],
        [0.9914],
        [0.9789],
        ...,
        [0.9618],
        [0.9607],
        [0.9596]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359785.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9959],
        [0.9914],
        [0.9788],
        ...,
        [0.9617],
        [0.9606],
        [0.9595]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359770.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(399.5388, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0040,  0.0169,  0.0111,  ...,  0.0152,  0.0145,  0.0155],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0118,  0.0455,  0.0314,  ...,  0.0425,  0.0393,  0.0420],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1942.3800, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.9448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.6107, device='cuda:0')



h[100].sum tensor(21.6960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.3313, device='cuda:0')



h[200].sum tensor(28.6271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.1618, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0031, 0.0150, 0.0090,  ..., 0.0123, 0.0128, 0.0131],
        [0.0249, 0.1000, 0.0677,  ..., 0.0919, 0.0864, 0.0921],
        [0.0234, 0.0927, 0.0630,  ..., 0.0854, 0.0800, 0.0853],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48743.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0917, 0.0000,  ..., 0.0417, 0.0000, 0.0253],
        [0.0000, 0.2070, 0.0000,  ..., 0.0868, 0.0000, 0.0613],
        [0.0000, 0.2585, 0.0000,  ..., 0.1067, 0.0000, 0.0779],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0074, 0.0000, 0.0007],
        [0.0000, 0.0150, 0.0000,  ..., 0.0111, 0.0000, 0.0024]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(234191.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-150.9543, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-31.7645, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-297.9087, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9959],
        [0.9914],
        [0.9788],
        ...,
        [0.9617],
        [0.9606],
        [0.9595]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359770.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9959],
        [0.9914],
        [0.9788],
        ...,
        [0.9617],
        [0.9606],
        [0.9595]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359770.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.1643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0039,  0.0164,  0.0108,  ...,  0.0147,  0.0141,  0.0150],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(786.4432, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.8037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6819, device='cuda:0')



h[100].sum tensor(18.3441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.3081, device='cuda:0')



h[200].sum tensor(21.5077, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.8139, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0049, 0.0233, 0.0144,  ..., 0.0197, 0.0199, 0.0208],
        [0.0039, 0.0216, 0.0126,  ..., 0.0175, 0.0185, 0.0192],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29059.5840, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0185, 0.0000,  ..., 0.0129, 0.0000, 0.0036],
        [0.0000, 0.0563, 0.0000,  ..., 0.0285, 0.0000, 0.0129],
        [0.0000, 0.0713, 0.0000,  ..., 0.0349, 0.0000, 0.0161],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(153359.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-57.0479, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.9285, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-155.0805, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9959],
        [0.9914],
        [0.9788],
        ...,
        [0.9617],
        [0.9606],
        [0.9595]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359770.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9959],
        [0.9914],
        [0.9788],
        ...,
        [0.9617],
        [0.9606],
        [0.9595]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359770.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5137],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0074,  0.0292,  0.0199,  ...,  0.0270,  0.0253,  0.0270],
        [ 0.0074,  0.0294,  0.0201,  ...,  0.0272,  0.0254,  0.0271],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1428.6121, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.9933, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.3865, device='cuda:0')



h[100].sum tensor(20.2062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7143, device='cuda:0')



h[200].sum tensor(25.4628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.8866, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0302, 0.1195, 0.0815,  ..., 0.1105, 0.1033, 0.1103],
        [0.0217, 0.0864, 0.0585,  ..., 0.0794, 0.0746, 0.0795],
        [0.0191, 0.0791, 0.0528,  ..., 0.0719, 0.0682, 0.0726],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42125.6328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2686, 0.0000,  ..., 0.1111, 0.0000, 0.0807],
        [0.0000, 0.2690, 0.0000,  ..., 0.1114, 0.0000, 0.0804],
        [0.0000, 0.2537, 0.0000,  ..., 0.1059, 0.0000, 0.0747],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(217907.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-119.1227, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.1301, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-249.5191, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9959],
        [0.9914],
        [0.9788],
        ...,
        [0.9617],
        [0.9606],
        [0.9595]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359770.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9959],
        [0.9913],
        [0.9787],
        ...,
        [0.9616],
        [0.9605],
        [0.9594]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359755.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.7283, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0066,  0.0039,  ...,  0.0054,  0.0057,  0.0059],
        [ 0.0018,  0.0089,  0.0055,  ...,  0.0076,  0.0077,  0.0081],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(925.6083, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7821, device='cuda:0')



h[100].sum tensor(18.7437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.9764, device='cuda:0')



h[200].sum tensor(22.3565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.1825, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0131, 0.0572, 0.0373,  ..., 0.0509, 0.0493, 0.0523],
        [0.0039, 0.0198, 0.0119,  ..., 0.0163, 0.0169, 0.0176],
        [0.0018, 0.0101, 0.0055,  ..., 0.0075, 0.0084, 0.0085],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31828.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1493, 0.0000,  ..., 0.0655, 0.0000, 0.0410],
        [0.0000, 0.0819, 0.0000,  ..., 0.0386, 0.0000, 0.0208],
        [0.0000, 0.0377, 0.0000,  ..., 0.0207, 0.0000, 0.0083],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(167116.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-69.8562, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.6534, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-174.7843, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9959],
        [0.9913],
        [0.9787],
        ...,
        [0.9616],
        [0.9605],
        [0.9594]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359755.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9959],
        [0.9913],
        [0.9787],
        ...,
        [0.9615],
        [0.9604],
        [0.9593]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359740.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.2783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1019.1399, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7029, device='cuda:0')



h[100].sum tensor(19.0104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5876, device='cuda:0')



h[200].sum tensor(22.9229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.3487, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0024, 0.0121, 0.0069,  ..., 0.0095, 0.0102, 0.0104],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34143.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0205, 0.0000,  ..., 0.0137, 0.0000, 0.0042],
        [0.0000, 0.0338, 0.0000,  ..., 0.0190, 0.0000, 0.0080],
        [0.0000, 0.0591, 0.0000,  ..., 0.0293, 0.0000, 0.0146],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(183454.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.2814, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.4023, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.7444, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9959],
        [0.9913],
        [0.9787],
        ...,
        [0.9615],
        [0.9604],
        [0.9593]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359740.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9958],
        [0.9913],
        [0.9786],
        ...,
        [0.9614],
        [0.9603],
        [0.9592]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359725.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(225.5844, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1008.2707, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1062, device='cuda:0')



h[100].sum tensor(18.9748, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.3978, device='cuda:0')



h[200].sum tensor(22.8473, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.6759, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33190.4727, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 5.6009e-03, 0.0000e+00,  ..., 7.6293e-03, 0.0000e+00,
         2.3277e-05],
        [0.0000e+00, 2.1631e-03, 0.0000e+00,  ..., 6.1464e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.9140e-04, 0.0000e+00,  ..., 5.4383e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 3.6625e-04, 0.0000e+00,  ..., 4.9150e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6653e-04, 0.0000e+00,  ..., 4.9145e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6653e-04, 0.0000e+00,  ..., 4.9142e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(172661.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-76.7512, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.3425, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-184.7103, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9958],
        [0.9913],
        [0.9786],
        ...,
        [0.9614],
        [0.9603],
        [0.9592]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359725.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9958],
        [0.9912],
        [0.9786],
        ...,
        [0.9613],
        [0.9602],
        [0.9591]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359710.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.5266, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0034,  0.0146,  0.0096,  ...,  0.0131,  0.0126,  0.0134],
        [ 0.0039,  0.0165,  0.0109,  ...,  0.0148,  0.0142,  0.0151],
        [ 0.0040,  0.0169,  0.0111,  ...,  0.0152,  0.0145,  0.0155],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1052.4038, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9924, device='cuda:0')



h[100].sum tensor(19.0980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6797, device='cuda:0')



h[200].sum tensor(23.1090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.6753, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0121, 0.0535, 0.0347,  ..., 0.0474, 0.0461, 0.0489],
        [0.0144, 0.0619, 0.0407,  ..., 0.0555, 0.0534, 0.0567],
        [0.0187, 0.0773, 0.0516,  ..., 0.0702, 0.0667, 0.0710],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34392.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2375, 0.0000,  ..., 0.1000, 0.0000, 0.0686],
        [0.0000, 0.2157, 0.0000,  ..., 0.0917, 0.0000, 0.0613],
        [0.0000, 0.1896, 0.0000,  ..., 0.0814, 0.0000, 0.0533],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(180709.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-82.2161, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.2798, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-193.3886, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9958],
        [0.9912],
        [0.9786],
        ...,
        [0.9613],
        [0.9602],
        [0.9591]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359710.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9958],
        [0.9912],
        [0.9785],
        ...,
        [0.9612],
        [0.9601],
        [0.9590]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359695.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2920],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.0199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0036,  0.0155,  0.0102,  ...,  0.0139,  0.0134,  0.0142],
        [ 0.0012,  0.0068,  0.0040,  ...,  0.0055,  0.0058,  0.0061],
        [ 0.0019,  0.0091,  0.0057,  ...,  0.0078,  0.0078,  0.0083],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(932.4409, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9774, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8972, device='cuda:0')



h[100].sum tensor(18.7479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0131, device='cuda:0')



h[200].sum tensor(22.3653, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.3124, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0053, 0.0265, 0.0161,  ..., 0.0222, 0.0227, 0.0238],
        [0.0131, 0.0572, 0.0373,  ..., 0.0510, 0.0493, 0.0523],
        [0.0057, 0.0264, 0.0165,  ..., 0.0226, 0.0226, 0.0237],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30898.1152, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0908, 0.0000,  ..., 0.0426, 0.0000, 0.0224],
        [0.0000, 0.1178, 0.0000,  ..., 0.0533, 0.0000, 0.0306],
        [0.0000, 0.0819, 0.0000,  ..., 0.0387, 0.0000, 0.0204],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(159585.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-66.1676, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.8470, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-168.9963, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9958],
        [0.9912],
        [0.9785],
        ...,
        [0.9612],
        [0.9601],
        [0.9590]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359695.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9958],
        [0.9912],
        [0.9784],
        ...,
        [0.9611],
        [0.9600],
        [0.9589]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359680.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2932],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(354.2050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0014,  0.0074,  0.0044,  ...,  0.0061,  0.0063,  0.0066],
        [ 0.0038,  0.0161,  0.0106,  ...,  0.0145,  0.0139,  0.0148],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1694.5490, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.4949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.5701, device='cuda:0')



h[100].sum tensor(20.9385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.0456, device='cuda:0')



h[200].sum tensor(27.0181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.6049, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0128, 0.0560, 0.0365,  ..., 0.0498, 0.0483, 0.0512],
        [0.0052, 0.0264, 0.0160,  ..., 0.0220, 0.0226, 0.0236],
        [0.0048, 0.0228, 0.0140,  ..., 0.0192, 0.0195, 0.0204],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51163.4805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1184, 0.0000,  ..., 0.0536, 0.0000, 0.0307],
        [0.0000, 0.0893, 0.0000,  ..., 0.0420, 0.0000, 0.0220],
        [0.0000, 0.0627, 0.0000,  ..., 0.0309, 0.0000, 0.0151],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(276663.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-162.8812, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-29.7170, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-315.1456, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9958],
        [0.9912],
        [0.9784],
        ...,
        [0.9611],
        [0.9600],
        [0.9589]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359680.4688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 140.0 event: 700 loss: tensor(484.4425, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9958],
        [0.9911],
        [0.9784],
        ...,
        [0.9610],
        [0.9599],
        [0.9589]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359665.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(206.7020, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0083,  0.0325,  0.0222,  ...,  0.0301,  0.0281,  0.0300],
        [ 0.0170,  0.0645,  0.0449,  ...,  0.0607,  0.0558,  0.0597],
        [ 0.0083,  0.0327,  0.0223,  ...,  0.0303,  0.0282,  0.0302],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(900.1200, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4233, device='cuda:0')



h[100].sum tensor(18.6472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8622, device='cuda:0')



h[200].sum tensor(22.1514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.7778, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0407, 0.1577, 0.1085,  ..., 0.1470, 0.1363, 0.1457],
        [0.0376, 0.1466, 0.1007,  ..., 0.1364, 0.1267, 0.1354],
        [0.0409, 0.1584, 0.1091,  ..., 0.1477, 0.1370, 0.1464],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30423.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3425, 0.0000,  ..., 0.1391, 0.0000, 0.1048],
        [0.0000, 0.3624, 0.0000,  ..., 0.1468, 0.0000, 0.1111],
        [0.0000, 0.3585, 0.0000,  ..., 0.1454, 0.0000, 0.1098],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(158571.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-63.5458, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.5622, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-165.1449, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9958],
        [0.9911],
        [0.9784],
        ...,
        [0.9610],
        [0.9599],
        [0.9589]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359665.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9958],
        [0.9911],
        [0.9783],
        ...,
        [0.9610],
        [0.9598],
        [0.9588]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359650.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2639],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0094, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0083,  0.0051,  ...,  0.0070,  0.0071,  0.0075],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(997.1111, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9658, device='cuda:0')



h[100].sum tensor(18.9221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.3531, device='cuda:0')



h[200].sum tensor(22.7353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.5175, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0091, 0.0426, 0.0269,  ..., 0.0369, 0.0366, 0.0387],
        [0.0012, 0.0081, 0.0040,  ..., 0.0056, 0.0067, 0.0066],
        [0.0016, 0.0095, 0.0050,  ..., 0.0069, 0.0079, 0.0079],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32120.2617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0806, 0.0000,  ..., 0.0385, 0.0000, 0.0193],
        [0.0000, 0.0397, 0.0000,  ..., 0.0217, 0.0000, 0.0083],
        [0.0000, 0.0344, 0.0000,  ..., 0.0194, 0.0000, 0.0073],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(164275.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-72.0938, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.7358, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-177.8377, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9958],
        [0.9911],
        [0.9783],
        ...,
        [0.9610],
        [0.9598],
        [0.9588]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359650.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9957],
        [0.9911],
        [0.9783],
        ...,
        [0.9609],
        [0.9598],
        [0.9587]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359635.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3689],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0520, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0050,  0.0207,  0.0139,  ...,  0.0189,  0.0179,  0.0190],
        [ 0.0078,  0.0308,  0.0210,  ...,  0.0285,  0.0266,  0.0284],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1228.1394, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.2727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7131, device='cuda:0')



h[100].sum tensor(19.5810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5455, device='cuda:0')



h[200].sum tensor(24.1348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7437, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0089, 0.0399, 0.0256,  ..., 0.0350, 0.0343, 0.0363],
        [0.0134, 0.0562, 0.0372,  ..., 0.0506, 0.0484, 0.0514],
        [0.0207, 0.0846, 0.0567,  ..., 0.0771, 0.0730, 0.0778],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36903.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2022, 0.0000,  ..., 0.0857, 0.0000, 0.0584],
        [0.0000, 0.2376, 0.0000,  ..., 0.0995, 0.0000, 0.0700],
        [0.0000, 0.2799, 0.0000,  ..., 0.1160, 0.0000, 0.0835],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(188499.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-94.2623, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.8786, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-211.7577, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9957],
        [0.9911],
        [0.9783],
        ...,
        [0.9609],
        [0.9598],
        [0.9587]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359635.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9957],
        [0.9911],
        [0.9782],
        ...,
        [0.9608],
        [0.9597],
        [0.9586]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359620.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.3961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0018,  0.0089,  0.0055,  ...,  0.0075,  0.0076,  0.0080],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1013.4914, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2677, device='cuda:0')



h[100].sum tensor(18.9609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4492, device='cuda:0')



h[200].sum tensor(22.8179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.8580, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0018, 0.0100, 0.0054,  ..., 0.0075, 0.0084, 0.0085],
        [0.0114, 0.0471, 0.0312,  ..., 0.0424, 0.0405, 0.0429],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33095.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0305, 0.0000,  ..., 0.0176, 0.0000, 0.0072],
        [0.0000, 0.0702, 0.0000,  ..., 0.0336, 0.0000, 0.0181],
        [0.0000, 0.1431, 0.0000,  ..., 0.0622, 0.0000, 0.0404],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(173251.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-76.2608, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.1263, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-184.4670, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9957],
        [0.9911],
        [0.9782],
        ...,
        [0.9608],
        [0.9597],
        [0.9586]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359620.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9957],
        [0.9910],
        [0.9781],
        ...,
        [0.9607],
        [0.9596],
        [0.9585]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359605.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(160.5194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(658.7041, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.6282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.3070, device='cuda:0')



h[100].sum tensor(17.9411, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.5525, device='cuda:0')



h[200].sum tensor(20.6518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.1355, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0094, 0.0399, 0.0261,  ..., 0.0355, 0.0343, 0.0362],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26997.6758, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0886, 0.0000,  ..., 0.0405, 0.0000, 0.0247],
        [0.0000, 0.0296, 0.0000,  ..., 0.0170, 0.0000, 0.0075],
        [0.0000, 0.0111, 0.0000,  ..., 0.0096, 0.0000, 0.0024],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(147168.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-46.8049, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-51.9076, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-140.0029, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9957],
        [0.9910],
        [0.9781],
        ...,
        [0.9607],
        [0.9596],
        [0.9585]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359605.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9957],
        [0.9910],
        [0.9781],
        ...,
        [0.9606],
        [0.9595],
        [0.9584]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359589.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.6792, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0040,  0.0170,  0.0112,  ...,  0.0153,  0.0146,  0.0156],
        [ 0.0086,  0.0338,  0.0232,  ...,  0.0314,  0.0292,  0.0312],
        [ 0.0128,  0.0489,  0.0338,  ...,  0.0458,  0.0423,  0.0452],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(755.8436, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0147, device='cuda:0')



h[100].sum tensor(18.2160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.0959, device='cuda:0')



h[200].sum tensor(21.2357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.0614, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0275, 0.1097, 0.0745,  ..., 0.1011, 0.0948, 0.1011],
        [0.0399, 0.1548, 0.1065,  ..., 0.1443, 0.1339, 0.1431],
        [0.0595, 0.2263, 0.1571,  ..., 0.2126, 0.1958, 0.2095],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27482.0820, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.7056e-01, 0.0000e+00,  ..., 1.5012e-01, 0.0000e+00,
         1.1335e-01],
        [0.0000e+00, 5.1995e-01, 0.0000e+00,  ..., 2.0629e-01, 0.0000e+00,
         1.6327e-01],
        [0.0000e+00, 6.7134e-01, 0.0000e+00,  ..., 2.6299e-01, 0.0000e+00,
         2.1397e-01],
        ...,
        [0.0000e+00, 3.6698e-04, 0.0000e+00,  ..., 4.9073e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6725e-04, 0.0000e+00,  ..., 4.9068e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6725e-04, 0.0000e+00,  ..., 4.9065e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(143740.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-49.8234, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-51.0024, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-144.1622, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9957],
        [0.9910],
        [0.9781],
        ...,
        [0.9606],
        [0.9595],
        [0.9584]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359589.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9957],
        [0.9910],
        [0.9780],
        ...,
        [0.9605],
        [0.9594],
        [0.9583]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359574.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(232.9824, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1038.3855, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7656, device='cuda:0')



h[100].sum tensor(19.0199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6076, device='cuda:0')



h[200].sum tensor(22.9430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.4195, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0110, 0.0061,  ..., 0.0084, 0.0093, 0.0094],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0026, 0.0131, 0.0076,  ..., 0.0104, 0.0111, 0.0113],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34144.7734, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0250, 0.0000,  ..., 0.0156, 0.0000, 0.0046],
        [0.0000, 0.0175, 0.0000,  ..., 0.0125, 0.0000, 0.0028],
        [0.0000, 0.0372, 0.0000,  ..., 0.0203, 0.0000, 0.0085],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(181743.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.5723, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.2561, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.9467, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9957],
        [0.9910],
        [0.9780],
        ...,
        [0.9605],
        [0.9594],
        [0.9583]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359574.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9957],
        [0.9909],
        [0.9779],
        ...,
        [0.9604],
        [0.9593],
        [0.9582]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359559.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(308.5957, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0066,  0.0039,  ...,  0.0054,  0.0057,  0.0059],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1490.7725, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5050, device='cuda:0')



h[100].sum tensor(20.3065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7520, device='cuda:0')



h[200].sum tensor(25.6757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0202, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0049, 0.0232, 0.0143,  ..., 0.0195, 0.0198, 0.0207],
        [0.0092, 0.0409, 0.0263,  ..., 0.0359, 0.0352, 0.0372],
        [0.0162, 0.0665, 0.0444,  ..., 0.0604, 0.0573, 0.0609],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42789.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1478, 0.0000,  ..., 0.0645, 0.0000, 0.0412],
        [0.0000, 0.1949, 0.0000,  ..., 0.0829, 0.0000, 0.0563],
        [0.0000, 0.2452, 0.0000,  ..., 0.1021, 0.0000, 0.0731],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(220491.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-122.6444, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.2642, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-254.6731, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9957],
        [0.9909],
        [0.9779],
        ...,
        [0.9604],
        [0.9593],
        [0.9582]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359559.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9956],
        [0.9909],
        [0.9779],
        ...,
        [0.9603],
        [0.9592],
        [0.9581]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359544.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(188.7726, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(814.3732, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8252, device='cuda:0')



h[100].sum tensor(18.3732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.3538, device='cuda:0')



h[200].sum tensor(21.5696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.9755, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0017, 0.0115, 0.0060,  ..., 0.0083, 0.0097, 0.0098],
        [0.0008, 0.0066, 0.0030,  ..., 0.0042, 0.0054, 0.0052],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30311.7930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0275, 0.0000,  ..., 0.0171, 0.0000, 0.0036],
        [0.0000, 0.0195, 0.0000,  ..., 0.0136, 0.0000, 0.0021],
        [0.0000, 0.0101, 0.0000,  ..., 0.0095, 0.0000, 0.0008],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(164676.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-62.5581, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.0412, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-163.7939, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9956],
        [0.9909],
        [0.9779],
        ...,
        [0.9603],
        [0.9592],
        [0.9581]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359544.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9956],
        [0.9909],
        [0.9778],
        ...,
        [0.9602],
        [0.9591],
        [0.9580]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359529.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.5701, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0016,  0.0082,  0.0050,  ...,  0.0069,  0.0070,  0.0074],
        [ 0.0016,  0.0080,  0.0049,  ...,  0.0067,  0.0069,  0.0072],
        [ 0.0037,  0.0158,  0.0104,  ...,  0.0142,  0.0136,  0.0145],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1150.7302, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.2441, device='cuda:0')



h[100].sum tensor(19.3274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0780, device='cuda:0')



h[200].sum tensor(23.5961, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.0869, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0270, 0.0169,  ..., 0.0231, 0.0231, 0.0242],
        [0.0123, 0.0540, 0.0351,  ..., 0.0479, 0.0465, 0.0494],
        [0.0053, 0.0268, 0.0163,  ..., 0.0225, 0.0230, 0.0241],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35345.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0936, 0.0000,  ..., 0.0434, 0.0000, 0.0241],
        [0.0000, 0.1336, 0.0000,  ..., 0.0596, 0.0000, 0.0355],
        [0.0000, 0.1188, 0.0000,  ..., 0.0537, 0.0000, 0.0309],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(182231.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-86.5537, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.4333, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-200.2348, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9956],
        [0.9909],
        [0.9778],
        ...,
        [0.9602],
        [0.9591],
        [0.9580]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359529.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9956],
        [0.9909],
        [0.9778],
        ...,
        [0.9602],
        [0.9591],
        [0.9580]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359529.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3905, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1057.7028, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0694, device='cuda:0')



h[100].sum tensor(19.0625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7042, device='cuda:0')



h[200].sum tensor(23.0337, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.7621, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33979.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0023, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(177793.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.5711, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.8513, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.5260, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9956],
        [0.9909],
        [0.9778],
        ...,
        [0.9602],
        [0.9591],
        [0.9580]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359529.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9956],
        [0.9909],
        [0.9778],
        ...,
        [0.9601],
        [0.9590],
        [0.9579]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359514.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.5585, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(800.6880, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.8374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6279, device='cuda:0')



h[100].sum tensor(18.3276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.2910, device='cuda:0')



h[200].sum tensor(21.4727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.7530, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0008, 0.0066, 0.0030,  ..., 0.0042, 0.0054, 0.0052],
        [0.0016, 0.0115, 0.0059,  ..., 0.0083, 0.0097, 0.0098],
        [0.0008, 0.0065, 0.0030,  ..., 0.0041, 0.0054, 0.0052],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30364.0645, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0271, 0.0000,  ..., 0.0168, 0.0000, 0.0040],
        [0.0000, 0.0344, 0.0000,  ..., 0.0199, 0.0000, 0.0055],
        [0.0000, 0.0251, 0.0000,  ..., 0.0159, 0.0000, 0.0035],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(167850.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-62.6858, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.8463, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.3622, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9956],
        [0.9909],
        [0.9778],
        ...,
        [0.9601],
        [0.9590],
        [0.9579]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359514.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9956],
        [0.9909],
        [0.9778],
        ...,
        [0.9601],
        [0.9590],
        [0.9579]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359514.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(296.6881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0039,  0.0167,  0.0110,  ...,  0.0150,  0.0144,  0.0153],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1415.9846, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.4437, device='cuda:0')



h[100].sum tensor(20.0772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4143, device='cuda:0')



h[200].sum tensor(25.1886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.8232, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0019, 0.0105, 0.0057,  ..., 0.0079, 0.0088, 0.0089],
        [0.0115, 0.0495, 0.0324,  ..., 0.0441, 0.0426, 0.0451],
        [0.0181, 0.0733, 0.0492,  ..., 0.0669, 0.0632, 0.0672],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40226.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0835, 0.0000,  ..., 0.0389, 0.0000, 0.0221],
        [0.0000, 0.1818, 0.0000,  ..., 0.0775, 0.0000, 0.0527],
        [0.0000, 0.2634, 0.0000,  ..., 0.1091, 0.0000, 0.0790],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(203450.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-110.0336, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.7285, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-235.9453, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9956],
        [0.9909],
        [0.9778],
        ...,
        [0.9601],
        [0.9590],
        [0.9579]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359514.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9956],
        [0.9908],
        [0.9777],
        ...,
        [0.9601],
        [0.9589],
        [0.9578]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359499.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(189.5996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0024,  0.0113,  0.0072,  ...,  0.0098,  0.0097,  0.0102],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(815.2264, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.8989, device='cuda:0')



h[100].sum tensor(18.3656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.3772, device='cuda:0')



h[200].sum tensor(21.5534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.0587, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0084, 0.0361, 0.0234,  ..., 0.0318, 0.0310, 0.0327],
        [0.0024, 0.0124, 0.0071,  ..., 0.0098, 0.0105, 0.0107],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29298.9492, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1328, 0.0000,  ..., 0.0583, 0.0000, 0.0378],
        [0.0000, 0.0629, 0.0000,  ..., 0.0306, 0.0000, 0.0163],
        [0.0000, 0.0209, 0.0000,  ..., 0.0137, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(155260.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-57.8388, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.6610, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-156.8691, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9956],
        [0.9908],
        [0.9777],
        ...,
        [0.9601],
        [0.9589],
        [0.9578]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359499.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9956],
        [0.9908],
        [0.9776],
        ...,
        [0.9600],
        [0.9588],
        [0.9577]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359484.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9702],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.3213, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0074,  0.0294,  0.0200,  ...,  0.0272,  0.0254,  0.0271],
        [ 0.0088,  0.0344,  0.0236,  ...,  0.0319,  0.0297,  0.0317],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(861.9943, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.4980, device='cuda:0')



h[100].sum tensor(18.4949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.5678, device='cuda:0')



h[200].sum tensor(21.8281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.7343, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0074, 0.0305, 0.0199,  ..., 0.0271, 0.0262, 0.0275],
        [0.0164, 0.0651, 0.0440,  ..., 0.0596, 0.0562, 0.0597],
        [0.0389, 0.1512, 0.1039,  ..., 0.1408, 0.1307, 0.1397],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30726.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1063, 0.0000,  ..., 0.0470, 0.0000, 0.0309],
        [0.0000, 0.1946, 0.0000,  ..., 0.0814, 0.0000, 0.0585],
        [0.0000, 0.2992, 0.0000,  ..., 0.1219, 0.0000, 0.0915],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(163796.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-65.2290, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.9631, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-167.8219, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9956],
        [0.9908],
        [0.9776],
        ...,
        [0.9600],
        [0.9588],
        [0.9577]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359484.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9956],
        [0.9908],
        [0.9776],
        ...,
        [0.9599],
        [0.9588],
        [0.9577]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359468.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.4885, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1417.9424, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.6041, device='cuda:0')



h[100].sum tensor(20.0669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4654, device='cuda:0')



h[200].sum tensor(25.1669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.0042, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38670.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0255, 0.0000,  ..., 0.0158, 0.0000, 0.0052],
        [0.0000, 0.0083, 0.0000,  ..., 0.0087, 0.0000, 0.0009],
        [0.0000, 0.0015, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(192428.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-102.5159, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.1996, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-224.5997, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9956],
        [0.9908],
        [0.9776],
        ...,
        [0.9599],
        [0.9588],
        [0.9577]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359468.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9955],
        [0.9907],
        [0.9775],
        ...,
        [0.9598],
        [0.9587],
        [0.9576]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359453.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(214.1548, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(955.9752, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0875, device='cuda:0')



h[100].sum tensor(18.7540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0736, device='cuda:0')



h[200].sum tensor(22.3784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.5270, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0012, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31762.5586, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0068, 0.0000, 0.0007],
        [0.0000, 0.0090, 0.0000,  ..., 0.0087, 0.0000, 0.0018],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(166014.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-69.5888, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.6025, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-174.4279, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9955],
        [0.9907],
        [0.9775],
        ...,
        [0.9598],
        [0.9587],
        [0.9576]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359453.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9955],
        [0.9907],
        [0.9774],
        ...,
        [0.9597],
        [0.9586],
        [0.9575]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359438.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.4917],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(436.8009, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0019,  0.0094,  0.0058,  ...,  0.0080,  0.0081,  0.0085],
        [ 0.0053,  0.0216,  0.0145,  ...,  0.0197,  0.0186,  0.0198],
        [ 0.0068,  0.0272,  0.0184,  ...,  0.0250,  0.0235,  0.0250],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2261.1284, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.4214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.9319, device='cuda:0')



h[100].sum tensor(22.4406, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.3881, device='cuda:0')



h[200].sum tensor(30.2084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.9075, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0099, 0.0416, 0.0273,  ..., 0.0371, 0.0358, 0.0378],
        [0.0223, 0.0907, 0.0610,  ..., 0.0829, 0.0783, 0.0834],
        [0.0272, 0.1084, 0.0737,  ..., 0.1000, 0.0937, 0.1000],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(57331.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1582, 0.0000,  ..., 0.0683, 0.0000, 0.0452],
        [0.0000, 0.2552, 0.0000,  ..., 0.1061, 0.0000, 0.0759],
        [0.0000, 0.3211, 0.0000,  ..., 0.1316, 0.0000, 0.0974],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(287577.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-192.5619, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-23.0248, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-361.2686, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9955],
        [0.9907],
        [0.9774],
        ...,
        [0.9597],
        [0.9586],
        [0.9575]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359438.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9955],
        [0.9907],
        [0.9774],
        ...,
        [0.9596],
        [0.9585],
        [0.9574]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359423.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.8081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0064,  0.0257,  0.0174,  ...,  0.0236,  0.0222,  0.0236],
        [ 0.0116,  0.0447,  0.0309,  ...,  0.0418,  0.0387,  0.0414],
        [ 0.0060,  0.0244,  0.0165,  ...,  0.0224,  0.0211,  0.0225],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(921.2142, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.5218, device='cuda:0')



h[100].sum tensor(18.6484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8936, device='cuda:0')



h[200].sum tensor(22.1540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.8890, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0317, 0.1251, 0.0854,  ..., 0.1158, 0.1081, 0.1154],
        [0.0328, 0.1289, 0.0881,  ..., 0.1195, 0.1114, 0.1190],
        [0.0346, 0.1353, 0.0927,  ..., 0.1256, 0.1169, 0.1249],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30659.6973, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2666, 0.0000,  ..., 0.1099, 0.0000, 0.0807],
        [0.0000, 0.3097, 0.0000,  ..., 0.1268, 0.0000, 0.0937],
        [0.0000, 0.3139, 0.0000,  ..., 0.1285, 0.0000, 0.0946],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(158532.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-64.7541, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.2913, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-166.8570, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9955],
        [0.9907],
        [0.9774],
        ...,
        [0.9596],
        [0.9585],
        [0.9574]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359423.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9955],
        [0.9906],
        [0.9773],
        ...,
        [0.9595],
        [0.9584],
        [0.9573]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359408.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5967],
        [0.6255],
        [0.6196],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.3888, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0083,  0.0325,  0.0222,  ...,  0.0302,  0.0281,  0.0300],
        [ 0.0166,  0.0629,  0.0437,  ...,  0.0592,  0.0544,  0.0582],
        [ 0.0162,  0.0615,  0.0428,  ...,  0.0579,  0.0532,  0.0570],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(960.3600, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0192, device='cuda:0')



h[100].sum tensor(18.7552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0519, device='cuda:0')



h[200].sum tensor(22.3809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.4500, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0411, 0.1592, 0.1096,  ..., 0.1484, 0.1376, 0.1471],
        [0.0549, 0.2096, 0.1453,  ..., 0.1966, 0.1813, 0.1940],
        [0.0572, 0.2179, 0.1512,  ..., 0.2046, 0.1885, 0.2017],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32378.1504, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.8595e-01, 0.0000e+00,  ..., 1.5466e-01, 0.0000e+00,
         1.2064e-01],
        [0.0000e+00, 4.9696e-01, 0.0000e+00,  ..., 1.9716e-01, 0.0000e+00,
         1.5645e-01],
        [0.0000e+00, 5.0974e-01, 0.0000e+00,  ..., 2.0217e-01, 0.0000e+00,
         1.6016e-01],
        ...,
        [0.0000e+00, 3.6794e-04, 0.0000e+00,  ..., 4.8970e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6822e-04, 0.0000e+00,  ..., 4.8965e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6822e-04, 0.0000e+00,  ..., 4.8962e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(170755.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-72.4357, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.8084, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-179.2110, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9955],
        [0.9906],
        [0.9773],
        ...,
        [0.9595],
        [0.9584],
        [0.9573]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359408.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 160.0 event: 800 loss: tensor(1085.0101, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9955],
        [0.9906],
        [0.9773],
        ...,
        [0.9594],
        [0.9583],
        [0.9572]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359393., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5879],
        [0.4434],
        [0.4792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(307.9297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0031,  0.0137,  0.0089,  ...,  0.0121,  0.0118,  0.0125],
        [ 0.0082,  0.0323,  0.0221,  ...,  0.0299,  0.0279,  0.0298],
        [ 0.0070,  0.0277,  0.0188,  ...,  0.0256,  0.0240,  0.0256],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1483.5320, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.9533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.4456, device='cuda:0')



h[100].sum tensor(20.2258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7332, device='cuda:0')



h[200].sum tensor(25.5043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.9533, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0222, 0.0903, 0.0608,  ..., 0.0826, 0.0780, 0.0831],
        [0.0220, 0.0895, 0.0602,  ..., 0.0818, 0.0772, 0.0823],
        [0.0229, 0.0926, 0.0624,  ..., 0.0848, 0.0799, 0.0852],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41013.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1891, 0.0000,  ..., 0.0802, 0.0000, 0.0555],
        [0.0000, 0.2103, 0.0000,  ..., 0.0886, 0.0000, 0.0620],
        [0.0000, 0.2066, 0.0000,  ..., 0.0872, 0.0000, 0.0607],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(211863.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-113.9629, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.9253, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-241.7423, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9955],
        [0.9906],
        [0.9773],
        ...,
        [0.9594],
        [0.9583],
        [0.9572]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359393., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9955],
        [0.9906],
        [0.9772],
        ...,
        [0.9593],
        [0.9582],
        [0.9571]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359377.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.6704]], device='cuda:0') 
g.ndata[nfet].sum tensor(279.4607, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0044,  0.0182,  0.0121,  ...,  0.0165,  0.0157,  0.0167],
        [ 0.0039,  0.0165,  0.0109,  ...,  0.0148,  0.0142,  0.0151],
        [ 0.0043,  0.0179,  0.0119,  ...,  0.0162,  0.0154,  0.0164],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0050,  0.0204,  0.0137,  ...,  0.0186,  0.0176,  0.0188],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1315.3093, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.9332, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.9082, device='cuda:0')



h[100].sum tensor(19.7469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9257, device='cuda:0')



h[200].sum tensor(24.4872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.0915, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0147, 0.0630, 0.0414,  ..., 0.0565, 0.0543, 0.0577],
        [0.0151, 0.0644, 0.0424,  ..., 0.0579, 0.0555, 0.0590],
        [0.0145, 0.0620, 0.0408,  ..., 0.0556, 0.0534, 0.0568],
        ...,
        [0.0047, 0.0206, 0.0130,  ..., 0.0176, 0.0175, 0.0183],
        [0.0038, 0.0171, 0.0105,  ..., 0.0143, 0.0145, 0.0150],
        [0.0169, 0.0706, 0.0469,  ..., 0.0639, 0.0609, 0.0648]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37658.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1613, 0.0000,  ..., 0.0703, 0.0000, 0.0445],
        [0.0000, 0.1594, 0.0000,  ..., 0.0694, 0.0000, 0.0444],
        [0.0000, 0.1551, 0.0000,  ..., 0.0676, 0.0000, 0.0431],
        ...,
        [0.0000, 0.0428, 0.0000,  ..., 0.0218, 0.0000, 0.0111],
        [0.0000, 0.0550, 0.0000,  ..., 0.0267, 0.0000, 0.0146],
        [0.0000, 0.0940, 0.0000,  ..., 0.0423, 0.0000, 0.0263]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(193362., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-97.3130, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.4578, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-216.7642, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9955],
        [0.9906],
        [0.9772],
        ...,
        [0.9593],
        [0.9582],
        [0.9571]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359377.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9955],
        [0.9906],
        [0.9772],
        ...,
        [0.9593],
        [0.9582],
        [0.9571]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359377.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.2406, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1554.3068, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5566, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.3646, device='cuda:0')



h[100].sum tensor(20.4197, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0256, device='cuda:0')



h[200].sum tensor(25.9161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.9897, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42117.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.6849e-02, 0.0000e+00,  ..., 1.2279e-02, 0.0000e+00,
         2.8102e-03],
        [0.0000e+00, 7.4801e-03, 0.0000e+00,  ..., 8.3901e-03, 0.0000e+00,
         3.7566e-04],
        [0.0000e+00, 4.0349e-03, 0.0000e+00,  ..., 6.9029e-03, 0.0000e+00,
         7.4194e-05],
        ...,
        [0.0000e+00, 3.6810e-04, 0.0000e+00,  ..., 4.8953e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6838e-04, 0.0000e+00,  ..., 4.8948e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6838e-04, 0.0000e+00,  ..., 4.8945e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(212652.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-119.1065, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.4238, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-248.9174, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9955],
        [0.9906],
        [0.9772],
        ...,
        [0.9593],
        [0.9582],
        [0.9571]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359377.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9955],
        [0.9905],
        [0.9771],
        ...,
        [0.9592],
        [0.9581],
        [0.9570]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359362.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(300.6637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0038,  0.0163,  0.0107,  ...,  0.0146,  0.0140,  0.0149],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1450.3224, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.7980, device='cuda:0')



h[100].sum tensor(20.1217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5271, device='cuda:0')



h[200].sum tensor(25.2833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.2229, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0068, 0.0303, 0.0193,  ..., 0.0263, 0.0259, 0.0273],
        [0.0092, 0.0409, 0.0263,  ..., 0.0359, 0.0352, 0.0372],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0044, 0.0213, 0.0130,  ..., 0.0178, 0.0182, 0.0190]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38737.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0324, 0.0000,  ..., 0.0183, 0.0000, 0.0076],
        [0.0000, 0.0900, 0.0000,  ..., 0.0414, 0.0000, 0.0245],
        [0.0000, 0.1375, 0.0000,  ..., 0.0605, 0.0000, 0.0380],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0083, 0.0000, 0.0013],
        [0.0000, 0.0253, 0.0000,  ..., 0.0152, 0.0000, 0.0053],
        [0.0000, 0.0644, 0.0000,  ..., 0.0312, 0.0000, 0.0157]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(193841.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-102.9809, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.1437, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-225.0305, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9955],
        [0.9905],
        [0.9771],
        ...,
        [0.9592],
        [0.9581],
        [0.9570]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359362.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9954],
        [0.9905],
        [0.9771],
        ...,
        [0.9592],
        [0.9580],
        [0.9569]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359347.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.4070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0034,  0.0149,  0.0097,  ...,  0.0133,  0.0128,  0.0136],
        [ 0.0100,  0.0389,  0.0267,  ...,  0.0362,  0.0336,  0.0359],
        [ 0.0013,  0.0069,  0.0041,  ...,  0.0057,  0.0059,  0.0062],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(943.7604, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7535, device='cuda:0')



h[100].sum tensor(18.6938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.9673, device='cuda:0')



h[200].sum tensor(22.2504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.1502, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0352, 0.1376, 0.0943,  ..., 0.1278, 0.1189, 0.1271],
        [0.0167, 0.0701, 0.0465,  ..., 0.0633, 0.0605, 0.0643],
        [0.0187, 0.0774, 0.0517,  ..., 0.0703, 0.0668, 0.0711],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29637.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2930, 0.0000,  ..., 0.1207, 0.0000, 0.0880],
        [0.0000, 0.2284, 0.0000,  ..., 0.0961, 0.0000, 0.0664],
        [0.0000, 0.1890, 0.0000,  ..., 0.0805, 0.0000, 0.0541],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(150001.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-60.0342, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.0125, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-159.7829, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9954],
        [0.9905],
        [0.9771],
        ...,
        [0.9592],
        [0.9580],
        [0.9569]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359347.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9954],
        [0.9905],
        [0.9770],
        ...,
        [0.9591],
        [0.9579],
        [0.9568]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359332.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.1829, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0030,  0.0131,  0.0085,  ...,  0.0116,  0.0113,  0.0120],
        [ 0.0065,  0.0259,  0.0175,  ...,  0.0238,  0.0223,  0.0238],
        [ 0.0030,  0.0132,  0.0085,  ...,  0.0117,  0.0114,  0.0120],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1584.3425, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.7160, device='cuda:0')



h[100].sum tensor(20.4873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1374, device='cuda:0')



h[200].sum tensor(26.0598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.3860, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0214, 0.0853, 0.0577,  ..., 0.0784, 0.0736, 0.0784],
        [0.0208, 0.0851, 0.0571,  ..., 0.0776, 0.0734, 0.0782],
        [0.0209, 0.0837, 0.0566,  ..., 0.0769, 0.0723, 0.0770],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44202.7305, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3311, 0.0000,  ..., 0.1346, 0.0000, 0.1017],
        [0.0000, 0.3608, 0.0000,  ..., 0.1463, 0.0000, 0.1108],
        [0.0000, 0.3425, 0.0000,  ..., 0.1390, 0.0000, 0.1054],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(224321.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-129.4125, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.7847, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-265.1320, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9954],
        [0.9905],
        [0.9770],
        ...,
        [0.9591],
        [0.9579],
        [0.9568]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359332.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9954],
        [0.9905],
        [0.9769],
        ...,
        [0.9590],
        [0.9578],
        [0.9567]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359316.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.8624, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1018.5732, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6740, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8636, device='cuda:0')



h[100].sum tensor(18.8962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.3206, device='cuda:0')



h[200].sum tensor(22.6803, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.4022, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32938.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0066, 0.0000, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(174215.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-75.1665, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.4525, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-183.0582, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9954],
        [0.9905],
        [0.9769],
        ...,
        [0.9590],
        [0.9578],
        [0.9567]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359316.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9954],
        [0.9904],
        [0.9769],
        ...,
        [0.9589],
        [0.9578],
        [0.9566]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359301.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(201.3741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(911.3495, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.9484, device='cuda:0')



h[100].sum tensor(18.5922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7111, device='cuda:0')



h[200].sum tensor(22.0347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.2422, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30084.7930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(158288.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-61.7281, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.9355, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-162.5673, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9954],
        [0.9904],
        [0.9769],
        ...,
        [0.9589],
        [0.9578],
        [0.9566]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359301.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9954],
        [0.9904],
        [0.9768],
        ...,
        [0.9588],
        [0.9577],
        [0.9565]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359286.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5435],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(498.5137, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0044,  0.0184,  0.0122,  ...,  0.0167,  0.0159,  0.0169],
        [ 0.0078,  0.0307,  0.0209,  ...,  0.0284,  0.0265,  0.0283],
        [ 0.0051,  0.0210,  0.0141,  ...,  0.0192,  0.0182,  0.0193],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2573.1475, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.7945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(44.4323, device='cuda:0')



h[100].sum tensor(23.2356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.1383, device='cuda:0')



h[200].sum tensor(31.8971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(50.1109, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0120, 0.0512, 0.0336,  ..., 0.0458, 0.0441, 0.0468],
        [0.0178, 0.0740, 0.0493,  ..., 0.0671, 0.0639, 0.0680],
        [0.0357, 0.1394, 0.0956,  ..., 0.1295, 0.1205, 0.1287],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(68239.4609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.9371e-01, 0.0000e+00,  ..., 8.2458e-02, 0.0000e+00,
         5.5493e-02],
        [0.0000e+00, 2.6959e-01, 0.0000e+00,  ..., 1.1177e-01, 0.0000e+00,
         7.9982e-02],
        [0.0000e+00, 3.7478e-01, 0.0000e+00,  ..., 1.5180e-01, 0.0000e+00,
         1.1501e-01],
        ...,
        [0.0000e+00, 3.6858e-04, 0.0000e+00,  ..., 4.8902e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6886e-04, 0.0000e+00,  ..., 4.8897e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6886e-04, 0.0000e+00,  ..., 4.8894e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(381760.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-243.9050, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-13.7343, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-439.1675, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9954],
        [0.9904],
        [0.9768],
        ...,
        [0.9588],
        [0.9577],
        [0.9565]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359286.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9954],
        [0.9904],
        [0.9768],
        ...,
        [0.9587],
        [0.9576],
        [0.9565]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359271.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(298.1101, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0038,  0.0162,  0.0107,  ...,  0.0146,  0.0140,  0.0148],
        [ 0.0014,  0.0076,  0.0046,  ...,  0.0063,  0.0065,  0.0068],
        [ 0.0070,  0.0279,  0.0189,  ...,  0.0257,  0.0241,  0.0257],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1462.8881, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.5704, device='cuda:0')



h[100].sum tensor(20.1258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4547, device='cuda:0')



h[200].sum tensor(25.2920, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.9662, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0076, 0.0368, 0.0229,  ..., 0.0315, 0.0316, 0.0334],
        [0.0151, 0.0645, 0.0425,  ..., 0.0579, 0.0556, 0.0591],
        [0.0081, 0.0367, 0.0233,  ..., 0.0319, 0.0315, 0.0333],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43669.3203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1556, 0.0000,  ..., 0.0683, 0.0000, 0.0419],
        [0.0000, 0.1532, 0.0000,  ..., 0.0673, 0.0000, 0.0415],
        [0.0000, 0.1259, 0.0000,  ..., 0.0563, 0.0000, 0.0335],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(239105.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-126.3044, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-36.9493, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-260.2691, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9954],
        [0.9904],
        [0.9768],
        ...,
        [0.9587],
        [0.9576],
        [0.9565]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359271.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 170.0 event: 850 loss: tensor(590.8610, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9953],
        [0.9903],
        [0.9767],
        ...,
        [0.9586],
        [0.9575],
        [0.9564]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359256., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(293.6479, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0027,  0.0121,  0.0078,  ...,  0.0107,  0.0104,  0.0111],
        [ 0.0028,  0.0124,  0.0080,  ...,  0.0109,  0.0106,  0.0113],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1420.6575, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.4095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.1727, device='cuda:0')



h[100].sum tensor(20.0029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3281, device='cuda:0')



h[200].sum tensor(25.0308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.5176, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0156, 0.0661, 0.0436,  ..., 0.0595, 0.0570, 0.0606],
        [0.0048, 0.0230, 0.0141,  ..., 0.0194, 0.0197, 0.0205],
        [0.0027, 0.0135, 0.0079,  ..., 0.0108, 0.0114, 0.0117],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41719.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1332, 0.0000,  ..., 0.0586, 0.0000, 0.0374],
        [0.0000, 0.0745, 0.0000,  ..., 0.0353, 0.0000, 0.0196],
        [0.0000, 0.0389, 0.0000,  ..., 0.0210, 0.0000, 0.0092],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(227110.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-117.3728, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.3610, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-246.6810, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9953],
        [0.9903],
        [0.9767],
        ...,
        [0.9586],
        [0.9575],
        [0.9564]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359256., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9953],
        [0.9903],
        [0.9766],
        ...,
        [0.9585],
        [0.9574],
        [0.9563]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359240.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5786],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.4822, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0093,  0.0363,  0.0249,  ...,  0.0337,  0.0313,  0.0335],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1385.7786, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.4449, device='cuda:0')



h[100].sum tensor(19.9006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0965, device='cuda:0')



h[200].sum tensor(24.8137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.6968, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0201, 0.0827, 0.0554,  ..., 0.0754, 0.0714, 0.0761],
        [0.0069, 0.0308, 0.0196,  ..., 0.0268, 0.0264, 0.0278],
        [0.0209, 0.0854, 0.0573,  ..., 0.0779, 0.0737, 0.0785],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40473.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1349, 0.0000,  ..., 0.0589, 0.0000, 0.0386],
        [0.0000, 0.1074, 0.0000,  ..., 0.0481, 0.0000, 0.0301],
        [0.0000, 0.1364, 0.0000,  ..., 0.0594, 0.0000, 0.0392],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(209932.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-111.6996, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.4165, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-237.8339, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9953],
        [0.9903],
        [0.9766],
        ...,
        [0.9585],
        [0.9574],
        [0.9563]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359240.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9953],
        [0.9903],
        [0.9766],
        ...,
        [0.9584],
        [0.9573],
        [0.9562]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359225.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3091],
        [0.7192],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.6913, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0020,  0.0096,  0.0060,  ...,  0.0083,  0.0083,  0.0088],
        [ 0.0070,  0.0280,  0.0190,  ...,  0.0258,  0.0242,  0.0258],
        [ 0.0088,  0.0344,  0.0236,  ...,  0.0320,  0.0297,  0.0318],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1144.0818, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.6310, device='cuda:0')



h[100].sum tensor(19.2228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.8829, device='cuda:0')



h[200].sum tensor(23.3741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.3955, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0099, 0.0416, 0.0273,  ..., 0.0371, 0.0358, 0.0378],
        [0.0236, 0.0955, 0.0644,  ..., 0.0875, 0.0824, 0.0879],
        [0.0402, 0.1558, 0.1072,  ..., 0.1452, 0.1347, 0.1440],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35303.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.6358e-01, 0.0000e+00,  ..., 7.0114e-02, 0.0000e+00,
         4.7328e-02],
        [0.0000e+00, 2.7264e-01, 0.0000e+00,  ..., 1.1234e-01, 0.0000e+00,
         8.2105e-02],
        [0.0000e+00, 3.8852e-01, 0.0000e+00,  ..., 1.5670e-01, 0.0000e+00,
         1.2006e-01],
        ...,
        [0.0000e+00, 3.6891e-04, 0.0000e+00,  ..., 4.8868e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6918e-04, 0.0000e+00,  ..., 4.8862e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6918e-04, 0.0000e+00,  ..., 4.8860e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(186187.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-86.1339, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.5551, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-199.7795, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9953],
        [0.9903],
        [0.9766],
        ...,
        [0.9584],
        [0.9573],
        [0.9562]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359225.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9953],
        [0.9902],
        [0.9765],
        ...,
        [0.9583],
        [0.9572],
        [0.9561]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359210.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.2662, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0033,  0.0145,  0.0095,  ...,  0.0129,  0.0125,  0.0133],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1092.6570, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0583, device='cuda:0')



h[100].sum tensor(19.0757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7007, device='cuda:0')



h[200].sum tensor(23.0616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.7496, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0067, 0.0300, 0.0190,  ..., 0.0260, 0.0257, 0.0270],
        [0.0033, 0.0157, 0.0094,  ..., 0.0129, 0.0133, 0.0137],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33266.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0951, 0.0000,  ..., 0.0433, 0.0000, 0.0263],
        [0.0000, 0.0533, 0.0000,  ..., 0.0267, 0.0000, 0.0133],
        [0.0000, 0.0210, 0.0000,  ..., 0.0139, 0.0000, 0.0035],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(171509.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-77.4360, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.8921, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-185.7476, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9953],
        [0.9902],
        [0.9765],
        ...,
        [0.9583],
        [0.9572],
        [0.9561]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359210.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9953],
        [0.9902],
        [0.9764],
        ...,
        [0.9583],
        [0.9571],
        [0.9560]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359194.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2891],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.8975, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0037,  0.0157,  0.0103,  ...,  0.0141,  0.0135,  0.0144],
        [ 0.0013,  0.0071,  0.0042,  ...,  0.0058,  0.0061,  0.0064],
        [ 0.0018,  0.0090,  0.0056,  ...,  0.0077,  0.0078,  0.0082],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(988.6979, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.2428, device='cuda:0')



h[100].sum tensor(18.7829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1230, device='cuda:0')



h[200].sum tensor(22.4397, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.7021, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0053, 0.0269, 0.0164,  ..., 0.0225, 0.0230, 0.0241],
        [0.0131, 0.0571, 0.0373,  ..., 0.0509, 0.0492, 0.0523],
        [0.0058, 0.0267, 0.0168,  ..., 0.0229, 0.0229, 0.0240],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31227.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0909, 0.0000,  ..., 0.0426, 0.0000, 0.0226],
        [0.0000, 0.1183, 0.0000,  ..., 0.0535, 0.0000, 0.0308],
        [0.0000, 0.0836, 0.0000,  ..., 0.0394, 0.0000, 0.0208],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(163007.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-67.6638, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.6653, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-171.1071, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9953],
        [0.9902],
        [0.9764],
        ...,
        [0.9583],
        [0.9571],
        [0.9560]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359194.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9953],
        [0.9902],
        [0.9764],
        ...,
        [0.9582],
        [0.9570],
        [0.9559]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359179.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(233.2286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0016,  0.0081,  0.0049,  ...,  0.0068,  0.0069,  0.0073],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1085.3574, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7876, device='cuda:0')



h[100].sum tensor(19.0475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6146, device='cuda:0')



h[200].sum tensor(23.0017, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.4443, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0060, 0.0274, 0.0173,  ..., 0.0236, 0.0235, 0.0246],
        [0.0016, 0.0093, 0.0049,  ..., 0.0067, 0.0077, 0.0077],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33724.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0873, 0.0000,  ..., 0.0410, 0.0000, 0.0220],
        [0.0000, 0.0472, 0.0000,  ..., 0.0247, 0.0000, 0.0106],
        [0.0000, 0.0186, 0.0000,  ..., 0.0129, 0.0000, 0.0035],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(174943.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-79.3529, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.6864, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-188.7499, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9953],
        [0.9902],
        [0.9764],
        ...,
        [0.9582],
        [0.9570],
        [0.9559]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359179.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9952],
        [0.9902],
        [0.9763],
        ...,
        [0.9581],
        [0.9569],
        [0.9558]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359164.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(235.6510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0015,  0.0077,  0.0047,  ...,  0.0064,  0.0066,  0.0069],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1120.8552, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1710, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0035, device='cuda:0')



h[100].sum tensor(19.1420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6833, device='cuda:0')



h[200].sum tensor(23.2024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.6878, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0034, 0.0198, 0.0113,  ..., 0.0157, 0.0169, 0.0175],
        [0.0026, 0.0148, 0.0083,  ..., 0.0115, 0.0126, 0.0129],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36166.2617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0669, 0.0000,  ..., 0.0333, 0.0000, 0.0145],
        [0.0000, 0.0481, 0.0000,  ..., 0.0254, 0.0000, 0.0098],
        [0.0000, 0.0191, 0.0000,  ..., 0.0132, 0.0000, 0.0030],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(193564., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-91.1539, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.3846, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-206.5364, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9952],
        [0.9902],
        [0.9763],
        ...,
        [0.9581],
        [0.9569],
        [0.9558]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359164.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9952],
        [0.9901],
        [0.9763],
        ...,
        [0.9580],
        [0.9568],
        [0.9557]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359149.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6099],
        [0.6123],
        [0.6582],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.9604, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0083,  0.0327,  0.0223,  ...,  0.0303,  0.0282,  0.0301],
        [ 0.0137,  0.0523,  0.0362,  ...,  0.0490,  0.0452,  0.0484],
        [ 0.0096,  0.0372,  0.0256,  ...,  0.0346,  0.0322,  0.0344],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(791.5568, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9507, device='cuda:0')



h[100].sum tensor(18.2261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.0755, device='cuda:0')



h[200].sum tensor(21.2571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.9892, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0359, 0.1404, 0.0963,  ..., 0.1305, 0.1214, 0.1297],
        [0.0465, 0.1790, 0.1236,  ..., 0.1674, 0.1548, 0.1656],
        [0.0491, 0.1882, 0.1301,  ..., 0.1762, 0.1627, 0.1741],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27832.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.4571e-01, 0.0000e+00,  ..., 1.3974e-01, 0.0000e+00,
         1.0683e-01],
        [0.0000e+00, 4.3188e-01, 0.0000e+00,  ..., 1.7290e-01, 0.0000e+00,
         1.3449e-01],
        [0.0000e+00, 4.4128e-01, 0.0000e+00,  ..., 1.7673e-01, 0.0000e+00,
         1.3704e-01],
        ...,
        [0.0000e+00, 3.6930e-04, 0.0000e+00,  ..., 4.8825e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6958e-04, 0.0000e+00,  ..., 4.8820e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.6958e-04, 0.0000e+00,  ..., 4.8817e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(145841.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-51.0199, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.7675, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-146.5507, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9952],
        [0.9901],
        [0.9763],
        ...,
        [0.9580],
        [0.9568],
        [0.9557]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359149.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9952],
        [0.9901],
        [0.9762],
        ...,
        [0.9579],
        [0.9567],
        [0.9556]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359133.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.9590, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0037,  0.0158,  0.0104,  ...,  0.0141,  0.0136,  0.0145],
        [ 0.0015,  0.0078,  0.0047,  ...,  0.0066,  0.0067,  0.0071],
        [ 0.0017,  0.0084,  0.0051,  ...,  0.0071,  0.0072,  0.0076],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1253.4725, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4367, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1700, device='cuda:0')



h[100].sum tensor(19.5008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3727, device='cuda:0')



h[200].sum tensor(23.9646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.1312, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0054, 0.0269, 0.0164,  ..., 0.0225, 0.0230, 0.0241],
        [0.0124, 0.0544, 0.0354,  ..., 0.0483, 0.0469, 0.0497],
        [0.0058, 0.0267, 0.0168,  ..., 0.0229, 0.0229, 0.0240],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37487.5430, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0881, 0.0000,  ..., 0.0415, 0.0000, 0.0217],
        [0.0000, 0.1124, 0.0000,  ..., 0.0511, 0.0000, 0.0290],
        [0.0000, 0.0789, 0.0000,  ..., 0.0375, 0.0000, 0.0197],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(194436.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-96.6002, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.5200, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-215.6246, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9952],
        [0.9901],
        [0.9762],
        ...,
        [0.9579],
        [0.9567],
        [0.9556]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359133.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9952],
        [0.9901],
        [0.9761],
        ...,
        [0.9578],
        [0.9567],
        [0.9555]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359118.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2665, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0063,  0.0253,  0.0171,  ...,  0.0232,  0.0218,  0.0233],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1350.8336, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.7126, device='cuda:0')



h[100].sum tensor(19.7655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8635, device='cuda:0')



h[200].sum tensor(24.5267, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.8710, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0234, 0.0928, 0.0631,  ..., 0.0855, 0.0801, 0.0854],
        [0.0154, 0.0634, 0.0422,  ..., 0.0574, 0.0547, 0.0581],
        [0.0085, 0.0382, 0.0244,  ..., 0.0334, 0.0328, 0.0347],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38735.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2913, 0.0000,  ..., 0.1199, 0.0000, 0.0879],
        [0.0000, 0.2439, 0.0000,  ..., 0.1018, 0.0000, 0.0723],
        [0.0000, 0.1970, 0.0000,  ..., 0.0839, 0.0000, 0.0565],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(199383.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-102.8491, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.3753, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-224.7081, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9952],
        [0.9901],
        [0.9761],
        ...,
        [0.9578],
        [0.9567],
        [0.9555]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359118.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 180.0 event: 900 loss: tensor(654.5710, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9952],
        [0.9900],
        [0.9761],
        ...,
        [0.9577],
        [0.9566],
        [0.9554]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359103.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.6633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1230.5872, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1437, device='cuda:0')



h[100].sum tensor(19.4289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3643, device='cuda:0')



h[200].sum tensor(23.8118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.1015, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0017, 0.0095, 0.0051,  ..., 0.0070, 0.0080, 0.0080],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36556.1289, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0039, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0125, 0.0000,  ..., 0.0104, 0.0000, 0.0018],
        [0.0000, 0.0370, 0.0000,  ..., 0.0205, 0.0000, 0.0077],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(190322.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-93.2366, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.8590, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-209.5612, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9952],
        [0.9900],
        [0.9761],
        ...,
        [0.9577],
        [0.9566],
        [0.9554]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359103.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9952],
        [0.9900],
        [0.9760],
        ...,
        [0.9576],
        [0.9565],
        [0.9554]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359087.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3157],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(483.5917, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0021,  0.0098,  0.0062,  ...,  0.0085,  0.0085,  0.0089],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0021,  0.0098,  0.0062,  ...,  0.0085,  0.0085,  0.0089],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2572.2793, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.0227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(43.1023, device='cuda:0')



h[100].sum tensor(23.1241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.7151, device='cuda:0')



h[200].sum tensor(31.6602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(48.6109, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0043, 0.0211, 0.0128,  ..., 0.0175, 0.0180, 0.0188],
        [0.0089, 0.0416, 0.0263,  ..., 0.0361, 0.0358, 0.0378],
        [0.0016, 0.0093, 0.0049,  ..., 0.0068, 0.0078, 0.0078],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(66971.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0779, 0.0000,  ..., 0.0374, 0.0000, 0.0186],
        [0.0000, 0.0884, 0.0000,  ..., 0.0417, 0.0000, 0.0214],
        [0.0000, 0.0536, 0.0000,  ..., 0.0274, 0.0000, 0.0120],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(356758.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-238.9455, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-13.3799, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-432.1068, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9952],
        [0.9900],
        [0.9760],
        ...,
        [0.9576],
        [0.9565],
        [0.9554]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359087.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9951],
        [0.9900],
        [0.9759],
        ...,
        [0.9575],
        [0.9564],
        [0.9553]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359072.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.5162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0015,  0.0077,  0.0047,  ...,  0.0065,  0.0066,  0.0070],
        [ 0.0015,  0.0077,  0.0047,  ...,  0.0065,  0.0066,  0.0070],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1285.6143, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.2913, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7545, device='cuda:0')



h[100].sum tensor(19.5719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5586, device='cuda:0')



h[200].sum tensor(24.1156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7904, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0048, 0.0250, 0.0150,  ..., 0.0207, 0.0213, 0.0223],
        [0.0034, 0.0199, 0.0114,  ..., 0.0158, 0.0170, 0.0176],
        [0.0026, 0.0148, 0.0083,  ..., 0.0115, 0.0126, 0.0129],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36565.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0841, 0.0000,  ..., 0.0405, 0.0000, 0.0190],
        [0.0000, 0.0708, 0.0000,  ..., 0.0350, 0.0000, 0.0153],
        [0.0000, 0.0522, 0.0000,  ..., 0.0272, 0.0000, 0.0106],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(188415.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-92.4114, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.1838, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-209.2378, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9951],
        [0.9900],
        [0.9759],
        ...,
        [0.9575],
        [0.9564],
        [0.9553]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359072.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9951],
        [0.9899],
        [0.9759],
        ...,
        [0.9574],
        [0.9563],
        [0.9552]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359057.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2661],
        [0.0000],
        [0.3716],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.8510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0047,  0.0195,  0.0130,  ...,  0.0177,  0.0168,  0.0179],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1028.9786, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5952, device='cuda:0')



h[100].sum tensor(18.8614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2352, device='cuda:0')



h[200].sum tensor(22.6064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.0995, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0088, 0.0415, 0.0262,  ..., 0.0359, 0.0357, 0.0377],
        [0.0032, 0.0171, 0.0100,  ..., 0.0137, 0.0146, 0.0151],
        [0.0110, 0.0494, 0.0318,  ..., 0.0435, 0.0425, 0.0450],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32360.9395, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0988, 0.0000,  ..., 0.0460, 0.0000, 0.0243],
        [0.0000, 0.0883, 0.0000,  ..., 0.0417, 0.0000, 0.0214],
        [0.0000, 0.1144, 0.0000,  ..., 0.0521, 0.0000, 0.0292],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(169949.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-72.4938, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.0121, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-178.8135, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9951],
        [0.9899],
        [0.9759],
        ...,
        [0.9574],
        [0.9563],
        [0.9552]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359057.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9951],
        [0.9899],
        [0.9758],
        ...,
        [0.9573],
        [0.9562],
        [0.9551]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359041.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5142],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.0995, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0042,  0.0178,  0.0118,  ...,  0.0161,  0.0154,  0.0163],
        [ 0.0037,  0.0158,  0.0104,  ...,  0.0141,  0.0136,  0.0144],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1277.0967, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.3572, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7173, device='cuda:0')



h[100].sum tensor(19.5397, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5468, device='cuda:0')



h[200].sum tensor(24.0471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7485, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0170, 0.0715, 0.0474,  ..., 0.0646, 0.0616, 0.0656],
        [0.0071, 0.0315, 0.0201,  ..., 0.0274, 0.0270, 0.0284],
        [0.0036, 0.0168, 0.0103,  ..., 0.0140, 0.0143, 0.0148],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35185.4258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1316, 0.0000,  ..., 0.0578, 0.0000, 0.0373],
        [0.0000, 0.0877, 0.0000,  ..., 0.0404, 0.0000, 0.0239],
        [0.0000, 0.0508, 0.0000,  ..., 0.0256, 0.0000, 0.0128],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(174093.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-86.7592, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.8831, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-200.0188, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9951],
        [0.9899],
        [0.9758],
        ...,
        [0.9573],
        [0.9562],
        [0.9551]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359041.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9951],
        [0.9899],
        [0.9758],
        ...,
        [0.9573],
        [0.9561],
        [0.9550]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359026.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.5199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0014,  0.0074,  0.0044,  ...,  0.0061,  0.0063,  0.0066],
        [ 0.0041,  0.0174,  0.0115,  ...,  0.0157,  0.0150,  0.0159],
        [ 0.0020,  0.0096,  0.0060,  ...,  0.0082,  0.0082,  0.0087],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(945.7432, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2288, device='cuda:0')



h[100].sum tensor(18.6256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8004, device='cuda:0')



h[200].sum tensor(22.1055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.5584, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0113, 0.0505, 0.0326,  ..., 0.0445, 0.0434, 0.0461],
        [0.0061, 0.0314, 0.0191,  ..., 0.0263, 0.0270, 0.0284],
        [0.0114, 0.0507, 0.0328,  ..., 0.0448, 0.0437, 0.0463],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30689.1133, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 9.9695e-02, 0.0000e+00,  ..., 4.6199e-02, 0.0000e+00,
         2.4992e-02],
        [0.0000e+00, 1.0286e-01, 0.0000e+00,  ..., 4.7753e-02, 0.0000e+00,
         2.5102e-02],
        [0.0000e+00, 1.2535e-01, 0.0000e+00,  ..., 5.6530e-02, 0.0000e+00,
         3.2175e-02],
        ...,
        [0.0000e+00, 1.8820e-03, 0.0000e+00,  ..., 5.4846e-03, 0.0000e+00,
         2.4457e-05],
        [0.0000e+00, 3.7022e-04, 0.0000e+00,  ..., 4.8752e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.7022e-04, 0.0000e+00,  ..., 4.8749e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(161445.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-64.6382, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.4472, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-166.7728, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9951],
        [0.9899],
        [0.9758],
        ...,
        [0.9573],
        [0.9561],
        [0.9550]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359026.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9951],
        [0.9898],
        [0.9757],
        ...,
        [0.9572],
        [0.9560],
        [0.9549]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(359011.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5524, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(795.2604, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9143, device='cuda:0')



h[100].sum tensor(18.2094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.0639, device='cuda:0')



h[200].sum tensor(21.2217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.9482, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28291.7109, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.1559e-04, 0.0000e+00,  ..., 5.3587e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 1.6659e-03, 0.0000e+00,  ..., 5.9044e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.7108e-03, 0.0000e+00,  ..., 6.7327e-03, 0.0000e+00,
         9.7979e-05],
        ...,
        [0.0000e+00, 3.7002e-04, 0.0000e+00,  ..., 4.8748e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.7030e-04, 0.0000e+00,  ..., 4.8743e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.7030e-04, 0.0000e+00,  ..., 4.8741e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(150515.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.4203, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.5863, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-149.4813, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9951],
        [0.9898],
        [0.9757],
        ...,
        [0.9572],
        [0.9560],
        [0.9549]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(359011.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9951],
        [0.9898],
        [0.9756],
        ...,
        [0.9571],
        [0.9559],
        [0.9548]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358995.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.8988, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(923.1407, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3683, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8169, device='cuda:0')



h[100].sum tensor(18.5569, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.6693, device='cuda:0')



h[200].sum tensor(21.9596, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.0939, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30056.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(156379.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-61.8741, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.6721, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-162.7256, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9951],
        [0.9898],
        [0.9756],
        ...,
        [0.9571],
        [0.9559],
        [0.9548]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358995.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9951],
        [0.9898],
        [0.9756],
        ...,
        [0.9571],
        [0.9559],
        [0.9548]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358995.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(177.9586, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0039,  0.0166,  0.0109,  ...,  0.0149,  0.0143,  0.0152],
        [ 0.0061,  0.0245,  0.0166,  ...,  0.0225,  0.0212,  0.0226],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(794.1980, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.8614, device='cuda:0')



h[100].sum tensor(18.2036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.0471, device='cuda:0')



h[200].sum tensor(21.2092, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.8885, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0103, 0.0469, 0.0300,  ..., 0.0411, 0.0403, 0.0427],
        [0.0091, 0.0387, 0.0253,  ..., 0.0344, 0.0333, 0.0351],
        [0.0087, 0.0371, 0.0241,  ..., 0.0328, 0.0319, 0.0336],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27717.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0974, 0.0000,  ..., 0.0450, 0.0000, 0.0250],
        [0.0000, 0.1128, 0.0000,  ..., 0.0508, 0.0000, 0.0303],
        [0.0000, 0.1257, 0.0000,  ..., 0.0559, 0.0000, 0.0342],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(146845.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-50.7939, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.8560, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-145.7132, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9951],
        [0.9898],
        [0.9756],
        ...,
        [0.9571],
        [0.9559],
        [0.9548]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358995.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9950],
        [0.9898],
        [0.9756],
        ...,
        [0.9570],
        [0.9558],
        [0.9547]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358980.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.3387, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1018.4390, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1930, device='cuda:0')



h[100].sum tensor(18.8144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1072, device='cuda:0')



h[200].sum tensor(22.5066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.6460, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34039.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0039, 0.0000,  ..., 0.0069, 0.0000, 0.0002],
        [0.0000, 0.0115, 0.0000,  ..., 0.0100, 0.0000, 0.0014],
        [0.0000, 0.0152, 0.0000,  ..., 0.0116, 0.0000, 0.0020],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(183997.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.9255, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.9479, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.6474, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9950],
        [0.9898],
        [0.9756],
        ...,
        [0.9570],
        [0.9558],
        [0.9547]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358980.4688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 190.0 event: 950 loss: tensor(557.8589, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9950],
        [0.9898],
        [0.9755],
        ...,
        [0.9569],
        [0.9557],
        [0.9546]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358965.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.0538, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056,  0.0229,  0.0154,  ...,  0.0210,  0.0198,  0.0211],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1044.0354, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5242, device='cuda:0')



h[100].sum tensor(18.8808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2126, device='cuda:0')



h[200].sum tensor(22.6477, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.0194, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0181, 0.0714, 0.0484,  ..., 0.0656, 0.0616, 0.0655],
        [0.0101, 0.0423, 0.0278,  ..., 0.0378, 0.0364, 0.0385],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33552.5547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1802, 0.0000,  ..., 0.0757, 0.0000, 0.0544],
        [0.0000, 0.1254, 0.0000,  ..., 0.0546, 0.0000, 0.0367],
        [0.0000, 0.0630, 0.0000,  ..., 0.0301, 0.0000, 0.0173],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(180198.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-78.5492, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.6881, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-187.7265, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9950],
        [0.9898],
        [0.9755],
        ...,
        [0.9569],
        [0.9557],
        [0.9546]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358965.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9950],
        [0.9897],
        [0.9754],
        ...,
        [0.9568],
        [0.9557],
        [0.9545]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358949.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.6575, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0065,  0.0260,  0.0176,  ...,  0.0239,  0.0224,  0.0239],
        [ 0.0026,  0.0119,  0.0076,  ...,  0.0104,  0.0102,  0.0108],
        [ 0.0035,  0.0152,  0.0099,  ...,  0.0136,  0.0131,  0.0139],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1404.5066, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.2823, device='cuda:0')



h[100].sum tensor(19.8617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0448, device='cuda:0')



h[200].sum tensor(24.7311, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.5134, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0180, 0.0750, 0.0499,  ..., 0.0680, 0.0647, 0.0689],
        [0.0180, 0.0751, 0.0500,  ..., 0.0680, 0.0648, 0.0689],
        [0.0054, 0.0249, 0.0155,  ..., 0.0212, 0.0213, 0.0223],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39975.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2755, 0.0000,  ..., 0.1143, 0.0000, 0.0818],
        [0.0000, 0.2215, 0.0000,  ..., 0.0935, 0.0000, 0.0644],
        [0.0000, 0.1427, 0.0000,  ..., 0.0625, 0.0000, 0.0398],
        ...,
        [0.0000, 0.0027, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0079, 0.0000, 0.0003],
        [0.0000, 0.0098, 0.0000,  ..., 0.0089, 0.0000, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(207091.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-109.1221, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.0678, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-233.9140, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9950],
        [0.9897],
        [0.9754],
        ...,
        [0.9568],
        [0.9557],
        [0.9545]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358949.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9950],
        [0.9897],
        [0.9754],
        ...,
        [0.9567],
        [0.9556],
        [0.9544]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358934.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8577, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1082.0653, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5079, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3088, device='cuda:0')



h[100].sum tensor(18.9773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4622, device='cuda:0')



h[200].sum tensor(22.8527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.9044, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32527.3359, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.9594e-02, 0.0000e+00,  ..., 2.1276e-02, 0.0000e+00,
         9.7732e-03],
        [0.0000e+00, 1.2165e-02, 0.0000e+00,  ..., 1.0287e-02, 0.0000e+00,
         1.6635e-03],
        [0.0000e+00, 3.0239e-03, 0.0000e+00,  ..., 6.4566e-03, 0.0000e+00,
         2.7074e-05],
        ...,
        [0.0000e+00, 3.7042e-04, 0.0000e+00,  ..., 4.8706e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.7070e-04, 0.0000e+00,  ..., 4.8701e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.7070e-04, 0.0000e+00,  ..., 4.8698e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(166000.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-74.0091, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.2565, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-180.8134, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9950],
        [0.9897],
        [0.9754],
        ...,
        [0.9567],
        [0.9556],
        [0.9544]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358934.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9950],
        [0.9897],
        [0.9753],
        ...,
        [0.9566],
        [0.9555],
        [0.9543]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358918.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3296],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.5911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0032,  0.0139,  0.0090,  ...,  0.0123,  0.0120,  0.0127],
        [ 0.0022,  0.0103,  0.0065,  ...,  0.0089,  0.0088,  0.0093],
        [ 0.0014,  0.0073,  0.0044,  ...,  0.0060,  0.0062,  0.0066],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(916.0498, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7895, device='cuda:0')



h[100].sum tensor(18.5211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.6606, device='cuda:0')



h[200].sum tensor(21.8837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.0630, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0107, 0.0483, 0.0310,  ..., 0.0424, 0.0415, 0.0440],
        [0.0072, 0.0355, 0.0219,  ..., 0.0302, 0.0304, 0.0321],
        [0.0053, 0.0285, 0.0170,  ..., 0.0236, 0.0244, 0.0257],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29435.7617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1091, 0.0000,  ..., 0.0499, 0.0000, 0.0279],
        [0.0000, 0.1088, 0.0000,  ..., 0.0502, 0.0000, 0.0267],
        [0.0000, 0.0990, 0.0000,  ..., 0.0464, 0.0000, 0.0234],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(154759.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-58.7458, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.3581, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-158.0291, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9950],
        [0.9897],
        [0.9753],
        ...,
        [0.9566],
        [0.9555],
        [0.9543]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358918.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9950],
        [0.9896],
        [0.9752],
        ...,
        [0.9565],
        [0.9554],
        [0.9542]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358903.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.1165, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0048,  0.0198,  0.0132,  ...,  0.0180,  0.0171,  0.0182],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1572.8325, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7889, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5514, device='cuda:0')



h[100].sum tensor(20.3061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7668, device='cuda:0')



h[200].sum tensor(25.6750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0725, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0048, 0.0209, 0.0131,  ..., 0.0179, 0.0178, 0.0186],
        [0.0163, 0.0648, 0.0438,  ..., 0.0594, 0.0559, 0.0594],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43109.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0284, 0.0000,  ..., 0.0165, 0.0000, 0.0070],
        [0.0000, 0.1018, 0.0000,  ..., 0.0450, 0.0000, 0.0299],
        [0.0000, 0.2239, 0.0000,  ..., 0.0922, 0.0000, 0.0688],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(224309.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-124.1562, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-36.5574, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-257.4115, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9950],
        [0.9896],
        [0.9752],
        ...,
        [0.9565],
        [0.9554],
        [0.9542]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358903.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9949],
        [0.9896],
        [0.9752],
        ...,
        [0.9564],
        [0.9553],
        [0.9541]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358888.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.4089, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1080.9684, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5362, device='cuda:0')



h[100].sum tensor(18.9632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5346, device='cuda:0')



h[200].sum tensor(22.8227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.1608, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33018.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0061, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(176582.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-75.6271, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.4923, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-183.3420, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9949],
        [0.9896],
        [0.9752],
        ...,
        [0.9564],
        [0.9553],
        [0.9541]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358888.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9949],
        [0.9896],
        [0.9751],
        ...,
        [0.9564],
        [0.9552],
        [0.9541]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358872.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.3199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0058,  0.0234,  0.0158,  ...,  0.0214,  0.0202,  0.0216],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(707.1212, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.6228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.3784, device='cuda:0')



h[100].sum tensor(17.9438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.5752, device='cuda:0')



h[200].sum tensor(20.6574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.2160, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0161, 0.0661, 0.0441,  ..., 0.0600, 0.0570, 0.0606],
        [0.0104, 0.0432, 0.0285,  ..., 0.0387, 0.0372, 0.0393],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26118.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1808, 0.0000,  ..., 0.0769, 0.0000, 0.0528],
        [0.0000, 0.1170, 0.0000,  ..., 0.0516, 0.0000, 0.0337],
        [0.0000, 0.0373, 0.0000,  ..., 0.0200, 0.0000, 0.0098],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(138794.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-43.1318, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-52.2207, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-134.2281, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9949],
        [0.9896],
        [0.9751],
        ...,
        [0.9564],
        [0.9552],
        [0.9541]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358872.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9949],
        [0.9895],
        [0.9751],
        ...,
        [0.9563],
        [0.9551],
        [0.9540]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358857.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(480.4238, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0082,  0.0050,  ...,  0.0069,  0.0070,  0.0074],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2532.5474, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.4896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(42.8199, device='cuda:0')



h[100].sum tensor(22.8959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(13.6252, device='cuda:0')



h[200].sum tensor(31.1756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(48.2925, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0118, 0.0522, 0.0338,  ..., 0.0461, 0.0449, 0.0476],
        [0.0012, 0.0080, 0.0040,  ..., 0.0055, 0.0066, 0.0065],
        [0.0116, 0.0497, 0.0325,  ..., 0.0443, 0.0428, 0.0453],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59644.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1723, 0.0000,  ..., 0.0744, 0.0000, 0.0484],
        [0.0000, 0.1520, 0.0000,  ..., 0.0660, 0.0000, 0.0428],
        [0.0000, 0.2122, 0.0000,  ..., 0.0894, 0.0000, 0.0620],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(297589.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-203.5129, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-21.5473, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-377.0214, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9949],
        [0.9895],
        [0.9751],
        ...,
        [0.9563],
        [0.9551],
        [0.9540]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358857.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9949],
        [0.9895],
        [0.9750],
        ...,
        [0.9562],
        [0.9550],
        [0.9539]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358841.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3623],
        [0.4749],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.7906, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0076,  0.0301,  0.0205,  ...,  0.0278,  0.0260,  0.0278],
        [ 0.0083,  0.0326,  0.0223,  ...,  0.0302,  0.0281,  0.0301],
        [ 0.0077,  0.0304,  0.0207,  ...,  0.0281,  0.0262,  0.0280],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1380.2061, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.8485, device='cuda:0')



h[100].sum tensor(19.7636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9067, device='cuda:0')



h[200].sum tensor(24.5227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.0242, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0286, 0.1137, 0.0773,  ..., 0.1049, 0.0982, 0.1048],
        [0.0332, 0.1304, 0.0892,  ..., 0.1209, 0.1127, 0.1204],
        [0.0344, 0.1345, 0.0921,  ..., 0.1249, 0.1163, 0.1242],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39736.0352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.8048e-01, 0.0000e+00,  ..., 1.5439e-01, 0.0000e+00,
         1.1682e-01],
        [0.0000e+00, 3.8661e-01, 0.0000e+00,  ..., 1.5663e-01, 0.0000e+00,
         1.1888e-01],
        [0.0000e+00, 3.6353e-01, 0.0000e+00,  ..., 1.4772e-01, 0.0000e+00,
         1.1126e-01],
        ...,
        [0.0000e+00, 3.7090e-04, 0.0000e+00,  ..., 4.8655e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.7117e-04, 0.0000e+00,  ..., 4.8650e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.7117e-04, 0.0000e+00,  ..., 4.8647e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(208569.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-107.9540, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.3776, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-232.0367, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9949],
        [0.9895],
        [0.9750],
        ...,
        [0.9562],
        [0.9550],
        [0.9539]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358841.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9949],
        [0.9895],
        [0.9749],
        ...,
        [0.9561],
        [0.9549],
        [0.9538]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358826.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4102],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8693, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0088,  0.0344,  0.0235,  ...,  0.0319,  0.0297,  0.0317],
        [ 0.0060,  0.0241,  0.0163,  ...,  0.0221,  0.0208,  0.0222],
        [ 0.0016,  0.0083,  0.0051,  ...,  0.0070,  0.0072,  0.0075],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1217.7207, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3599, device='cuda:0')



h[100].sum tensor(19.3190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.1149, device='cuda:0')



h[200].sum tensor(23.5784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.2175, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0315, 0.1242, 0.0848,  ..., 0.1150, 0.1074, 0.1146],
        [0.0167, 0.0700, 0.0464,  ..., 0.0632, 0.0604, 0.0642],
        [0.0139, 0.0598, 0.0392,  ..., 0.0535, 0.0515, 0.0547],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35402.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3101, 0.0000,  ..., 0.1273, 0.0000, 0.0930],
        [0.0000, 0.2354, 0.0000,  ..., 0.0990, 0.0000, 0.0679],
        [0.0000, 0.1879, 0.0000,  ..., 0.0809, 0.0000, 0.0520],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(185059.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-87.0452, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.3047, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-200.6554, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9949],
        [0.9895],
        [0.9749],
        ...,
        [0.9561],
        [0.9549],
        [0.9538]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358826.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 200.0 event: 1000 loss: tensor(630.7459, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9949],
        [0.9894],
        [0.9749],
        ...,
        [0.9560],
        [0.9548],
        [0.9537]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358811.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(223.1285, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0020,  0.0096,  0.0060,  ...,  0.0082,  0.0082,  0.0087],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1061.2065, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6836, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8873, device='cuda:0')



h[100].sum tensor(18.8915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.3281, device='cuda:0')



h[200].sum tensor(22.6704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.4290, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0020, 0.0107, 0.0059,  ..., 0.0082, 0.0090, 0.0091],
        [0.0040, 0.0199, 0.0120,  ..., 0.0164, 0.0170, 0.0177],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34204.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0193, 0.0000,  ..., 0.0134, 0.0000, 0.0028],
        [0.0000, 0.0366, 0.0000,  ..., 0.0204, 0.0000, 0.0074],
        [0.0000, 0.0622, 0.0000,  ..., 0.0310, 0.0000, 0.0141],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(182850.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.2916, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.2150, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-192.2776, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9949],
        [0.9894],
        [0.9749],
        ...,
        [0.9560],
        [0.9548],
        [0.9537]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358811.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9948],
        [0.9894],
        [0.9748],
        ...,
        [0.9559],
        [0.9547],
        [0.9536]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358795.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4946],
        [0.0000],
        [0.2998],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.1536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0065,  0.0260,  0.0176,  ...,  0.0239,  0.0225,  0.0240],
        [ 0.0060,  0.0241,  0.0163,  ...,  0.0221,  0.0209,  0.0222],
        [ 0.0011,  0.0064,  0.0038,  ...,  0.0052,  0.0055,  0.0058],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1354.4049, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.0691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.2569, device='cuda:0')



h[100].sum tensor(19.6805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7185, device='cuda:0')



h[200].sum tensor(24.3462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.3570, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0295, 0.1168, 0.0795,  ..., 0.1079, 0.1009, 0.1077],
        [0.0211, 0.0861, 0.0578,  ..., 0.0785, 0.0743, 0.0791],
        [0.0270, 0.1077, 0.0732,  ..., 0.0993, 0.0931, 0.0993],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37124.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3297, 0.0000,  ..., 0.1350, 0.0000, 0.0997],
        [0.0000, 0.3054, 0.0000,  ..., 0.1258, 0.0000, 0.0916],
        [0.0000, 0.3027, 0.0000,  ..., 0.1248, 0.0000, 0.0908],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(189674.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-95.7133, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.6081, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-213.3419, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9948],
        [0.9894],
        [0.9748],
        ...,
        [0.9559],
        [0.9547],
        [0.9536]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358795.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9948],
        [0.9894],
        [0.9747],
        ...,
        [0.9558],
        [0.9546],
        [0.9535]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358780.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3232],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.5454, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0021,  0.0101,  0.0063,  ...,  0.0087,  0.0087,  0.0091],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1207.4641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1527, device='cuda:0')



h[100].sum tensor(19.2793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0490, device='cuda:0')



h[200].sum tensor(23.4941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.9839, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0113, 0.0063,  ..., 0.0087, 0.0095, 0.0096],
        [0.0016, 0.0095, 0.0051,  ..., 0.0070, 0.0079, 0.0080],
        [0.0089, 0.0415, 0.0262,  ..., 0.0360, 0.0357, 0.0377],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36579.4570, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0318, 0.0000,  ..., 0.0184, 0.0000, 0.0061],
        [0.0000, 0.0480, 0.0000,  ..., 0.0252, 0.0000, 0.0102],
        [0.0000, 0.0799, 0.0000,  ..., 0.0384, 0.0000, 0.0186],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(193774.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-93.3570, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.7411, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-209.8461, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9948],
        [0.9894],
        [0.9747],
        ...,
        [0.9558],
        [0.9546],
        [0.9535]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358780.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9948],
        [0.9893],
        [0.9747],
        ...,
        [0.9557],
        [0.9546],
        [0.9534]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358764.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(269.6230, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1338.8059, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.0314, device='cuda:0')



h[100].sum tensor(19.6297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.6467, device='cuda:0')



h[200].sum tensor(24.2383, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.1026, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35230.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0009, 0.0000,  ..., 0.0056, 0.0000, 0.0000],
        [0.0000, 0.0025, 0.0000,  ..., 0.0062, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(174846.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-86.5417, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.0259, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-200.0323, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9948],
        [0.9893],
        [0.9747],
        ...,
        [0.9557],
        [0.9546],
        [0.9534]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358764.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9948],
        [0.9893],
        [0.9746],
        ...,
        [0.9556],
        [0.9545],
        [0.9533]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358749.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.8645, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0080,  0.0049,  ...,  0.0067,  0.0069,  0.0072],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1038.1335, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8398, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3290, device='cuda:0')



h[100].sum tensor(18.8151, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1505, device='cuda:0')



h[200].sum tensor(22.5082, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.7993, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0015, 0.0092, 0.0048,  ..., 0.0067, 0.0077, 0.0077],
        [0.0058, 0.0266, 0.0167,  ..., 0.0228, 0.0228, 0.0239],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32824.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0728, 0.0000,  ..., 0.0345, 0.0000, 0.0192],
        [0.0000, 0.0858, 0.0000,  ..., 0.0401, 0.0000, 0.0223],
        [0.0000, 0.1081, 0.0000,  ..., 0.0492, 0.0000, 0.0283],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0080, 0.0000, 0.0012],
        [0.0000, 0.0251, 0.0000,  ..., 0.0150, 0.0000, 0.0054]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(173571.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-74.6707, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.4787, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-182.2450, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9948],
        [0.9893],
        [0.9746],
        ...,
        [0.9556],
        [0.9545],
        [0.9533]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358749.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9948],
        [0.9893],
        [0.9745],
        ...,
        [0.9555],
        [0.9544],
        [0.9532]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358733.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2532],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.7178, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0017,  0.0084,  0.0052,  ...,  0.0071,  0.0072,  0.0076],
        [ 0.0015,  0.0080,  0.0048,  ...,  0.0067,  0.0068,  0.0072],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1133.2129, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6529, device='cuda:0')



h[100].sum tensor(19.0677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5717, device='cuda:0')



h[200].sum tensor(23.0445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.2924, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0094, 0.0436, 0.0277,  ..., 0.0379, 0.0375, 0.0396],
        [0.0046, 0.0223, 0.0136,  ..., 0.0187, 0.0191, 0.0199],
        [0.0015, 0.0091, 0.0048,  ..., 0.0066, 0.0076, 0.0076],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33769.5898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0903, 0.0000,  ..., 0.0426, 0.0000, 0.0218],
        [0.0000, 0.0649, 0.0000,  ..., 0.0321, 0.0000, 0.0149],
        [0.0000, 0.0341, 0.0000,  ..., 0.0194, 0.0000, 0.0067],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(176124.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-79.9348, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.2713, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-189.5555, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9948],
        [0.9893],
        [0.9745],
        ...,
        [0.9555],
        [0.9544],
        [0.9532]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358733.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9948],
        [0.9892],
        [0.9745],
        ...,
        [0.9554],
        [0.9543],
        [0.9531]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358718.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.5081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1045.2621, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3864, device='cuda:0')



h[100].sum tensor(18.8274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1687, device='cuda:0')



h[200].sum tensor(22.5342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.8640, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32242.0898, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0126, 0.0000,  ..., 0.0107, 0.0000, 0.0009],
        [0.0000, 0.0109, 0.0000,  ..., 0.0099, 0.0000, 0.0006],
        [0.0000, 0.0120, 0.0000,  ..., 0.0102, 0.0000, 0.0008],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(168688.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-72.1855, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.8762, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-178.2307, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9948],
        [0.9892],
        [0.9745],
        ...,
        [0.9554],
        [0.9543],
        [0.9531]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358718.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9947],
        [0.9892],
        [0.9744],
        ...,
        [0.9554],
        [0.9542],
        [0.9530]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358702.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(362.7321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1849.8105, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.3987, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.3301, device='cuda:0')



h[100].sum tensor(20.9856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.2874, device='cuda:0')



h[200].sum tensor(27.1181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.4620, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47613.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(245236.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-145.8176, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-32.2333, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-290.3601, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9947],
        [0.9892],
        [0.9744],
        ...,
        [0.9554],
        [0.9542],
        [0.9530]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358702.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9947],
        [0.9892],
        [0.9744],
        ...,
        [0.9553],
        [0.9541],
        [0.9529]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358687.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(207.0400, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0019,  0.0094,  0.0059,  ...,  0.0081,  0.0081,  0.0085],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(983.2098, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.4534, device='cuda:0')



h[100].sum tensor(18.6540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8718, device='cuda:0')



h[200].sum tensor(22.1658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.8118, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0041, 0.0204, 0.0123,  ..., 0.0169, 0.0174, 0.0181],
        [0.0019, 0.0106, 0.0058,  ..., 0.0080, 0.0089, 0.0090],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30328.0488, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0676, 0.0000,  ..., 0.0330, 0.0000, 0.0163],
        [0.0000, 0.0354, 0.0000,  ..., 0.0198, 0.0000, 0.0077],
        [0.0000, 0.0103, 0.0000,  ..., 0.0094, 0.0000, 0.0018],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(157357.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-62.8927, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.5020, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.5320, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9947],
        [0.9892],
        [0.9744],
        ...,
        [0.9553],
        [0.9541],
        [0.9529]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358687.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9947],
        [0.9892],
        [0.9743],
        ...,
        [0.9552],
        [0.9540],
        [0.9529]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358671.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2281, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0040,  0.0169,  0.0111,  ...,  0.0152,  0.0145,  0.0155],
        [ 0.0034,  0.0147,  0.0096,  ...,  0.0131,  0.0126,  0.0134],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1142.1755, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.8766, device='cuda:0')



h[100].sum tensor(19.0769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6429, device='cuda:0')



h[200].sum tensor(23.0642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.5447, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0040, 0.0180, 0.0111,  ..., 0.0151, 0.0153, 0.0159],
        [0.0065, 0.0292, 0.0185,  ..., 0.0252, 0.0250, 0.0263],
        [0.0214, 0.0873, 0.0587,  ..., 0.0797, 0.0753, 0.0803],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32022.5820, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0531, 0.0000,  ..., 0.0265, 0.0000, 0.0138],
        [0.0000, 0.0964, 0.0000,  ..., 0.0437, 0.0000, 0.0269],
        [0.0000, 0.1771, 0.0000,  ..., 0.0752, 0.0000, 0.0523],
        ...,
        [0.0000, 0.0228, 0.0000,  ..., 0.0139, 0.0000, 0.0047],
        [0.0000, 0.0173, 0.0000,  ..., 0.0117, 0.0000, 0.0036],
        [0.0000, 0.0060, 0.0000,  ..., 0.0071, 0.0000, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(161775.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-71.3739, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.8624, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-176.9159, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9947],
        [0.9892],
        [0.9743],
        ...,
        [0.9552],
        [0.9540],
        [0.9529]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358671.9062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 210.0 event: 1050 loss: tensor(614.3331, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9947],
        [0.9891],
        [0.9742],
        ...,
        [0.9551],
        [0.9539],
        [0.9528]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358656.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(157.4655, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(693.8528, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.7689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.0348, device='cuda:0')



h[100].sum tensor(17.8724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.4659, device='cuda:0')



h[200].sum tensor(20.5058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(15.8285, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26111.7559, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0045, 0.0000,  ..., 0.0071, 0.0000, 0.0002],
        [0.0000, 0.0048, 0.0000,  ..., 0.0072, 0.0000, 0.0005],
        [0.0000, 0.0100, 0.0000,  ..., 0.0092, 0.0000, 0.0020],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(141398.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-42.6451, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-52.4289, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-133.8857, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9947],
        [0.9891],
        [0.9742],
        ...,
        [0.9551],
        [0.9539],
        [0.9528]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358656.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9947],
        [0.9891],
        [0.9742],
        ...,
        [0.9550],
        [0.9538],
        [0.9527]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358640.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2578],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(247.3483, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0081,  0.0049,  ...,  0.0068,  0.0070,  0.0073],
        [ 0.0016,  0.0083,  0.0051,  ...,  0.0070,  0.0071,  0.0075],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1217.9590, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.0460, device='cuda:0')



h[100].sum tensor(19.2724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0150, device='cuda:0')



h[200].sum tensor(23.4793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.8636, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0016, 0.0093, 0.0049,  ..., 0.0068, 0.0078, 0.0078],
        [0.0059, 0.0271, 0.0170,  ..., 0.0232, 0.0232, 0.0243],
        [0.0144, 0.0619, 0.0407,  ..., 0.0555, 0.0533, 0.0567],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35468.9336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0448, 0.0000,  ..., 0.0237, 0.0000, 0.0099],
        [0.0000, 0.0881, 0.0000,  ..., 0.0412, 0.0000, 0.0223],
        [0.0000, 0.1332, 0.0000,  ..., 0.0593, 0.0000, 0.0354],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(182093.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-87.9939, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.6985, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-201.8884, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9947],
        [0.9891],
        [0.9742],
        ...,
        [0.9550],
        [0.9538],
        [0.9527]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358640.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9947],
        [0.9891],
        [0.9741],
        ...,
        [0.9549],
        [0.9537],
        [0.9526]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358625.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(259.3258, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1277.3536, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5872, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1136, device='cuda:0')



h[100].sum tensor(19.4273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3547, device='cuda:0')



h[200].sum tensor(23.8084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.0676, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37011.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0018, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(197389.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-95.4979, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.1300, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-213.2744, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9947],
        [0.9891],
        [0.9741],
        ...,
        [0.9549],
        [0.9537],
        [0.9526]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358625.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9946],
        [0.9890],
        [0.9740],
        ...,
        [0.9548],
        [0.9536],
        [0.9525]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358609.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5103],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.5892, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0037,  0.0157,  0.0103,  ...,  0.0140,  0.0135,  0.0143],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1124.4150, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5523, device='cuda:0')



h[100].sum tensor(19.0148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5397, device='cuda:0')



h[200].sum tensor(22.9323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.1789, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0036, 0.0168, 0.0102,  ..., 0.0140, 0.0143, 0.0148],
        [0.0061, 0.0259, 0.0167,  ..., 0.0227, 0.0222, 0.0232],
        [0.0293, 0.1160, 0.0790,  ..., 0.1072, 0.1003, 0.1070],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32761.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1229, 0.0000,  ..., 0.0541, 0.0000, 0.0350],
        [0.0000, 0.1846, 0.0000,  ..., 0.0780, 0.0000, 0.0549],
        [0.0000, 0.2941, 0.0000,  ..., 0.1205, 0.0000, 0.0895],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(168673.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-74.7033, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.4375, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-181.9110, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9946],
        [0.9890],
        [0.9740],
        ...,
        [0.9548],
        [0.9536],
        [0.9525]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358609.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9946],
        [0.9890],
        [0.9740],
        ...,
        [0.9547],
        [0.9535],
        [0.9524]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358594.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2837],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8247, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0194,  0.0732,  0.0511,  ...,  0.0691,  0.0634,  0.0679],
        [ 0.0074,  0.0293,  0.0200,  ...,  0.0271,  0.0253,  0.0270],
        [ 0.0036,  0.0156,  0.0102,  ...,  0.0139,  0.0134,  0.0143],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1245.2501, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7789, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1776, device='cuda:0')



h[100].sum tensor(19.3336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0569, device='cuda:0')



h[200].sum tensor(23.6094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.0120, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0314, 0.1239, 0.0846,  ..., 0.1147, 0.1070, 0.1143],
        [0.0405, 0.1571, 0.1081,  ..., 0.1464, 0.1358, 0.1452],
        [0.0225, 0.0912, 0.0615,  ..., 0.0835, 0.0788, 0.0839],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36566.9336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.9789e-01, 0.0000e+00,  ..., 1.6039e-01, 0.0000e+00,
         1.2291e-01],
        [0.0000e+00, 4.0328e-01, 0.0000e+00,  ..., 1.6246e-01, 0.0000e+00,
         1.2428e-01],
        [0.0000e+00, 3.0153e-01, 0.0000e+00,  ..., 1.2404e-01, 0.0000e+00,
         9.0270e-02],
        ...,
        [0.0000e+00, 3.7216e-04, 0.0000e+00,  ..., 4.8519e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.7244e-04, 0.0000e+00,  ..., 4.8514e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.7244e-04, 0.0000e+00,  ..., 4.8511e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(191725.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-93.1370, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.0907, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-209.2122, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9946],
        [0.9890],
        [0.9740],
        ...,
        [0.9547],
        [0.9535],
        [0.9524]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358594.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9946],
        [0.9890],
        [0.9739],
        ...,
        [0.9546],
        [0.9535],
        [0.9523]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358578.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.6937, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0107,  0.0414,  0.0285,  ...,  0.0386,  0.0358,  0.0382],
        [ 0.0075,  0.0298,  0.0203,  ...,  0.0276,  0.0258,  0.0275],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0037,  0.0157,  0.0103,  ...,  0.0140,  0.0135,  0.0144]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1941.6929, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.9886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.0397, device='cuda:0')



h[100].sum tensor(21.1859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.5132, device='cuda:0')



h[200].sum tensor(27.5437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.2623, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0402, 0.1560, 0.1074,  ..., 0.1454, 0.1349, 0.1442],
        [0.0189, 0.0763, 0.0513,  ..., 0.0697, 0.0658, 0.0700],
        [0.0100, 0.0419, 0.0275,  ..., 0.0374, 0.0360, 0.0381],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0035, 0.0159, 0.0097,  ..., 0.0132, 0.0136, 0.0140],
        [0.0061, 0.0276, 0.0175,  ..., 0.0238, 0.0236, 0.0248]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50375.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.4035, 0.0000,  ..., 0.1618, 0.0000, 0.1257],
        [0.0000, 0.2474, 0.0000,  ..., 0.1018, 0.0000, 0.0753],
        [0.0000, 0.1294, 0.0000,  ..., 0.0561, 0.0000, 0.0379],
        ...,
        [0.0000, 0.0174, 0.0000,  ..., 0.0118, 0.0000, 0.0036],
        [0.0000, 0.0458, 0.0000,  ..., 0.0230, 0.0000, 0.0118],
        [0.0000, 0.0832, 0.0000,  ..., 0.0379, 0.0000, 0.0230]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(266713.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-159.1608, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-30.3920, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-309.3697, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9946],
        [0.9890],
        [0.9739],
        ...,
        [0.9546],
        [0.9535],
        [0.9523]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358578.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9946],
        [0.9889],
        [0.9738],
        ...,
        [0.9545],
        [0.9534],
        [0.9522]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358563.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(317.1766, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1634.7241, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.6728, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.2698, device='cuda:0')



h[100].sum tensor(20.3629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9954, device='cuda:0')



h[200].sum tensor(25.7955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.8828, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40216.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0204, 0.0000,  ..., 0.0138, 0.0000, 0.0034],
        [0.0000, 0.0205, 0.0000,  ..., 0.0138, 0.0000, 0.0034],
        [0.0000, 0.0200, 0.0000,  ..., 0.0135, 0.0000, 0.0032],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0049, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(196309.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-110.5975, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.1489, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-236.5607, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9946],
        [0.9889],
        [0.9738],
        ...,
        [0.9545],
        [0.9534],
        [0.9522]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358563.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9946],
        [0.9889],
        [0.9738],
        ...,
        [0.9545],
        [0.9533],
        [0.9521]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358547.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6777],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(340.8691, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0050,  0.0207,  0.0138,  ...,  0.0188,  0.0178,  0.0190],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1836.2888, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.5857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.3815, device='cuda:0')



h[100].sum tensor(20.8941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.6673, device='cuda:0')



h[200].sum tensor(26.9239, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.2643, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0180, 0.0749, 0.0499,  ..., 0.0679, 0.0646, 0.0688],
        [0.0040, 0.0181, 0.0112,  ..., 0.0152, 0.0154, 0.0160],
        [0.0050, 0.0217, 0.0137,  ..., 0.0186, 0.0185, 0.0193],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47760.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1042, 0.0000,  ..., 0.0468, 0.0000, 0.0293],
        [0.0000, 0.0609, 0.0000,  ..., 0.0295, 0.0000, 0.0163],
        [0.0000, 0.0472, 0.0000,  ..., 0.0240, 0.0000, 0.0123],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(247695.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-145.7869, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-32.4964, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-290.8047, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9946],
        [0.9889],
        [0.9738],
        ...,
        [0.9545],
        [0.9533],
        [0.9521]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358547.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9945],
        [0.9889],
        [0.9737],
        ...,
        [0.9544],
        [0.9532],
        [0.9520]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358532.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.3102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0019,  0.0091,  0.0057,  ...,  0.0078,  0.0078,  0.0083],
        [ 0.0042,  0.0176,  0.0117,  ...,  0.0159,  0.0152,  0.0162],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(878.1755, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.8066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6949, device='cuda:0')



h[100].sum tensor(18.3427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.3123, device='cuda:0')



h[200].sum tensor(21.5046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.8285, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0104, 0.0454, 0.0295,  ..., 0.0402, 0.0391, 0.0414],
        [0.0052, 0.0243, 0.0150,  ..., 0.0206, 0.0208, 0.0217],
        [0.0148, 0.0633, 0.0417,  ..., 0.0568, 0.0546, 0.0580],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28672.9102, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1631, 0.0000,  ..., 0.0706, 0.0000, 0.0459],
        [0.0000, 0.1088, 0.0000,  ..., 0.0491, 0.0000, 0.0295],
        [0.0000, 0.1029, 0.0000,  ..., 0.0466, 0.0000, 0.0280],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(151591.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-55.4021, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.8872, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-152.7223, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9945],
        [0.9889],
        [0.9737],
        ...,
        [0.9544],
        [0.9532],
        [0.9520]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358532.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9945],
        [0.9888],
        [0.9736],
        ...,
        [0.9543],
        [0.9531],
        [0.9519]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358516.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.2456, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1405.5160, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.9479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.2651, device='cuda:0')



h[100].sum tensor(19.7397, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7211, device='cuda:0')



h[200].sum tensor(24.4720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.3663, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40140.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(213154.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-109.3614, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.5676, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-234.0141, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9945],
        [0.9888],
        [0.9736],
        ...,
        [0.9543],
        [0.9531],
        [0.9519]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358516.7188, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 220.0 event: 1100 loss: tensor(630.7459, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9945],
        [0.9888],
        [0.9736],
        ...,
        [0.9542],
        [0.9530],
        [0.9518]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358501.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5635],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9114, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0040,  0.0169,  0.0112,  ...,  0.0152,  0.0146,  0.0155],
        [ 0.0041,  0.0172,  0.0114,  ...,  0.0156,  0.0149,  0.0158],
        [ 0.0056,  0.0228,  0.0153,  ...,  0.0208,  0.0196,  0.0209],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1238.3634, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8637, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1853, device='cuda:0')



h[100].sum tensor(19.2922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0593, device='cuda:0')



h[200].sum tensor(23.5213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.0207, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0183, 0.0760, 0.0507,  ..., 0.0689, 0.0656, 0.0698],
        [0.0172, 0.0721, 0.0479,  ..., 0.0652, 0.0622, 0.0661],
        [0.0189, 0.0763, 0.0514,  ..., 0.0698, 0.0658, 0.0701],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34333.5078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1727, 0.0000,  ..., 0.0740, 0.0000, 0.0500],
        [0.0000, 0.2021, 0.0000,  ..., 0.0855, 0.0000, 0.0591],
        [0.0000, 0.2133, 0.0000,  ..., 0.0894, 0.0000, 0.0633],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(174787.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-83.0591, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.5267, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-193.9480, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9945],
        [0.9888],
        [0.9736],
        ...,
        [0.9542],
        [0.9530],
        [0.9518]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358501.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9945],
        [0.9888],
        [0.9735],
        ...,
        [0.9541],
        [0.9529],
        [0.9517]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358485.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.3855],
        [0.6333]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.8981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0069,  0.0275,  0.0187,  ...,  0.0254,  0.0238,  0.0254],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [ 0.0026,  0.0119,  0.0077,  ...,  0.0105,  0.0103,  0.0109],
        [ 0.0065,  0.0259,  0.0175,  ...,  0.0238,  0.0224,  0.0238],
        [ 0.0044,  0.0185,  0.0123,  ...,  0.0167,  0.0159,  0.0170]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1402.8997, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.9795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.4124, device='cuda:0')



h[100].sum tensor(19.7243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7680, device='cuda:0')



h[200].sum tensor(24.4392, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.5324, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0258, 0.1016, 0.0693,  ..., 0.0939, 0.0878, 0.0936],
        [0.0161, 0.0662, 0.0442,  ..., 0.0601, 0.0570, 0.0606],
        [0.0069, 0.0304, 0.0194,  ..., 0.0265, 0.0261, 0.0274],
        ...,
        [0.0111, 0.0455, 0.0302,  ..., 0.0410, 0.0392, 0.0415],
        [0.0192, 0.0789, 0.0529,  ..., 0.0719, 0.0681, 0.0725],
        [0.0229, 0.0925, 0.0625,  ..., 0.0849, 0.0798, 0.0851]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37119.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3581, 0.0000,  ..., 0.1450, 0.0000, 0.1106],
        [0.0000, 0.2728, 0.0000,  ..., 0.1125, 0.0000, 0.0826],
        [0.0000, 0.2184, 0.0000,  ..., 0.0914, 0.0000, 0.0651],
        ...,
        [0.0000, 0.1199, 0.0000,  ..., 0.0525, 0.0000, 0.0340],
        [0.0000, 0.1684, 0.0000,  ..., 0.0718, 0.0000, 0.0485],
        [0.0000, 0.1725, 0.0000,  ..., 0.0732, 0.0000, 0.0503]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(186979.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-96.0581, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.0673, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-214.0414, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9945],
        [0.9888],
        [0.9735],
        ...,
        [0.9541],
        [0.9529],
        [0.9517]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358485.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9945],
        [0.9887],
        [0.9734],
        ...,
        [0.9540],
        [0.9528],
        [0.9517]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358470.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6411],
        [0.5576],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.6858, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0094,  0.0366,  0.0252,  ...,  0.0341,  0.0317,  0.0338],
        [ 0.0102,  0.0395,  0.0272,  ...,  0.0369,  0.0342,  0.0365],
        [ 0.0093,  0.0364,  0.0250,  ...,  0.0338,  0.0314,  0.0336],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1126.5984, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3827, device='cuda:0')



h[100].sum tensor(18.9887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4857, device='cuda:0')



h[200].sum tensor(22.8768, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.9876, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0329, 0.1292, 0.0883,  ..., 0.1197, 0.1116, 0.1192],
        [0.0337, 0.1322, 0.0905,  ..., 0.1227, 0.1143, 0.1221],
        [0.0307, 0.1212, 0.0827,  ..., 0.1121, 0.1047, 0.1118],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32828.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3173, 0.0000,  ..., 0.1292, 0.0000, 0.0976],
        [0.0000, 0.3031, 0.0000,  ..., 0.1235, 0.0000, 0.0933],
        [0.0000, 0.2641, 0.0000,  ..., 0.1084, 0.0000, 0.0808],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(169968.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-75.1270, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.4565, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-182.1764, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9945],
        [0.9887],
        [0.9734],
        ...,
        [0.9540],
        [0.9528],
        [0.9517]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358470.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9945],
        [0.9887],
        [0.9734],
        ...,
        [0.9539],
        [0.9527],
        [0.9516]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358454.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3132],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.8899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0020,  0.0098,  0.0061,  ...,  0.0084,  0.0084,  0.0089],
        [ 0.0035,  0.0150,  0.0098,  ...,  0.0134,  0.0130,  0.0138],
        [ 0.0080,  0.0314,  0.0215,  ...,  0.0291,  0.0272,  0.0290],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1277.4866, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6754, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6291, device='cuda:0')



h[100].sum tensor(19.3842, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2005, device='cuda:0')



h[200].sum tensor(23.7169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.5211, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0076, 0.0352, 0.0222,  ..., 0.0305, 0.0302, 0.0319],
        [0.0189, 0.0782, 0.0522,  ..., 0.0710, 0.0675, 0.0719],
        [0.0243, 0.0978, 0.0661,  ..., 0.0898, 0.0845, 0.0901],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35881.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1310, 0.0000,  ..., 0.0583, 0.0000, 0.0348],
        [0.0000, 0.2114, 0.0000,  ..., 0.0896, 0.0000, 0.0605],
        [0.0000, 0.2649, 0.0000,  ..., 0.1102, 0.0000, 0.0781],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(184468.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-89.6341, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.6526, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-204.4175, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9945],
        [0.9887],
        [0.9734],
        ...,
        [0.9539],
        [0.9527],
        [0.9516]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358454.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9944],
        [0.9887],
        [0.9733],
        ...,
        [0.9538],
        [0.9526],
        [0.9515]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358438.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(228.5220, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0033,  0.0145,  0.0095,  ...,  0.0129,  0.0125,  0.0133],
        [ 0.0067,  0.0267,  0.0181,  ...,  0.0246,  0.0230,  0.0246],
        [ 0.0013,  0.0070,  0.0041,  ...,  0.0057,  0.0060,  0.0063],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1137.1344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3681, device='cuda:0')



h[100].sum tensor(19.0096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4811, device='cuda:0')



h[200].sum tensor(22.9211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.9711, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0272, 0.1084, 0.0736,  ..., 0.0999, 0.0936, 0.0999],
        [0.0138, 0.0597, 0.0391,  ..., 0.0533, 0.0514, 0.0546],
        [0.0127, 0.0557, 0.0363,  ..., 0.0495, 0.0479, 0.0509],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33291.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2276, 0.0000,  ..., 0.0960, 0.0000, 0.0660],
        [0.0000, 0.1903, 0.0000,  ..., 0.0819, 0.0000, 0.0529],
        [0.0000, 0.1587, 0.0000,  ..., 0.0697, 0.0000, 0.0423],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(173600.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-77.3837, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.7130, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.0388, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9944],
        [0.9887],
        [0.9733],
        ...,
        [0.9538],
        [0.9526],
        [0.9515]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358438.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9944],
        [0.9886],
        [0.9733],
        ...,
        [0.9537],
        [0.9525],
        [0.9514]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358423.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5347],
        [0.5410],
        [0.6685],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(249.6595, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0090,  0.0351,  0.0241,  ...,  0.0326,  0.0303,  0.0324],
        [ 0.0093,  0.0364,  0.0250,  ...,  0.0338,  0.0314,  0.0336],
        [ 0.0039,  0.0166,  0.0109,  ...,  0.0149,  0.0143,  0.0152],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1248.1382, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8497, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.2520, device='cuda:0')



h[100].sum tensor(19.2990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0806, device='cuda:0')



h[200].sum tensor(23.5359, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.0959, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0270, 0.1076, 0.0730,  ..., 0.0991, 0.0929, 0.0992],
        [0.0270, 0.1077, 0.0731,  ..., 0.0992, 0.0930, 0.0992],
        [0.0255, 0.1021, 0.0691,  ..., 0.0939, 0.0881, 0.0940],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35471.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2338, 0.0000,  ..., 0.0973, 0.0000, 0.0702],
        [0.0000, 0.2341, 0.0000,  ..., 0.0974, 0.0000, 0.0702],
        [0.0000, 0.2021, 0.0000,  ..., 0.0849, 0.0000, 0.0598],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(184881.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-87.2654, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.1212, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-201.2455, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9944],
        [0.9886],
        [0.9733],
        ...,
        [0.9537],
        [0.9525],
        [0.9514]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358423.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9944],
        [0.9886],
        [0.9732],
        ...,
        [0.9536],
        [0.9524],
        [0.9513]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358407.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2659],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.5929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0017,  0.0084,  0.0051,  ...,  0.0071,  0.0072,  0.0076],
        [ 0.0015,  0.0078,  0.0048,  ...,  0.0066,  0.0067,  0.0071],
        [ 0.0037,  0.0158,  0.0104,  ...,  0.0142,  0.0136,  0.0145],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(834.9227, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.0070, device='cuda:0')



h[100].sum tensor(18.2057, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.0934, device='cuda:0')



h[200].sum tensor(21.2138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.0528, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0270, 0.0169,  ..., 0.0231, 0.0231, 0.0242],
        [0.0124, 0.0545, 0.0354,  ..., 0.0484, 0.0470, 0.0498],
        [0.0053, 0.0267, 0.0163,  ..., 0.0224, 0.0229, 0.0240],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28023.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0834, 0.0000,  ..., 0.0394, 0.0000, 0.0207],
        [0.0000, 0.1158, 0.0000,  ..., 0.0525, 0.0000, 0.0300],
        [0.0000, 0.0896, 0.0000,  ..., 0.0420, 0.0000, 0.0223],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(147949.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-52.6851, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.2655, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-148.2755, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9944],
        [0.9886],
        [0.9732],
        ...,
        [0.9536],
        [0.9524],
        [0.9513]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358407.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9944],
        [0.9886],
        [0.9731],
        ...,
        [0.9535],
        [0.9524],
        [0.9512]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358392.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.1415, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1446.3130, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.0580, device='cuda:0')



h[100].sum tensor(19.8135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9734, device='cuda:0')



h[200].sum tensor(24.6286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.2605, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38822.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0054, 0.0000,  ..., 0.0075, 0.0000, 0.0003],
        [0.0000, 0.0068, 0.0000,  ..., 0.0081, 0.0000, 0.0006],
        [0.0000, 0.0080, 0.0000,  ..., 0.0086, 0.0000, 0.0009],
        ...,
        [0.0000, 0.0095, 0.0000,  ..., 0.0085, 0.0000, 0.0017],
        [0.0000, 0.0025, 0.0000,  ..., 0.0057, 0.0000, 0.0002],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(199732.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-103.6427, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.8825, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-225.7487, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9944],
        [0.9886],
        [0.9731],
        ...,
        [0.9535],
        [0.9524],
        [0.9512]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358392.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9944],
        [0.9885],
        [0.9731],
        ...,
        [0.9535],
        [0.9523],
        [0.9511]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358376.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.9000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0039,  0.0164,  0.0108,  ...,  0.0147,  0.0141,  0.0150],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(978.9725, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2626, device='cuda:0')



h[100].sum tensor(18.5794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8111, device='cuda:0')



h[200].sum tensor(22.0075, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.5967, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0057, 0.0264, 0.0165,  ..., 0.0225, 0.0226, 0.0236],
        [0.0038, 0.0175, 0.0108,  ..., 0.0147, 0.0149, 0.0154],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31501.6289, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1277, 0.0000,  ..., 0.0558, 0.0000, 0.0367],
        [0.0000, 0.0905, 0.0000,  ..., 0.0409, 0.0000, 0.0258],
        [0.0000, 0.0593, 0.0000,  ..., 0.0284, 0.0000, 0.0165],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(166601.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-69.4047, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.0425, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-173.5397, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9944],
        [0.9885],
        [0.9731],
        ...,
        [0.9535],
        [0.9523],
        [0.9511]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358376.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9944],
        [0.9885],
        [0.9730],
        ...,
        [0.9534],
        [0.9522],
        [0.9510]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358360.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(250.8436, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0020,  0.0097,  0.0060,  ...,  0.0083,  0.0083,  0.0088],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1262.7260, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3576, device='cuda:0')



h[100].sum tensor(19.3223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.1141, device='cuda:0')



h[200].sum tensor(23.5854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.2149, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0030, 0.0164, 0.0094,  ..., 0.0130, 0.0139, 0.0144],
        [0.0020, 0.0108, 0.0060,  ..., 0.0083, 0.0091, 0.0092],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38376.1992, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0463, 0.0000,  ..., 0.0245, 0.0000, 0.0100],
        [0.0000, 0.0300, 0.0000,  ..., 0.0176, 0.0000, 0.0060],
        [0.0000, 0.0150, 0.0000,  ..., 0.0114, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(212194.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-101.6342, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.2731, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-222.5367, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9944],
        [0.9885],
        [0.9730],
        ...,
        [0.9534],
        [0.9522],
        [0.9510]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358360.9375, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 230.0 event: 1150 loss: tensor(602.6853, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9943],
        [0.9885],
        [0.9729],
        ...,
        [0.9533],
        [0.9521],
        [0.9509]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358345.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.6434, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0036,  0.0156,  0.0103,  ...,  0.0140,  0.0135,  0.0143],
        [ 0.0067,  0.0268,  0.0182,  ...,  0.0247,  0.0232,  0.0247],
        [ 0.0018,  0.0089,  0.0055,  ...,  0.0076,  0.0077,  0.0081],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1264.4219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1615, device='cuda:0')



h[100].sum tensor(19.3230, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0517, device='cuda:0')



h[200].sum tensor(23.5869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.9938, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0307, 0.1213, 0.0827,  ..., 0.1122, 0.1048, 0.1119],
        [0.0206, 0.0843, 0.0565,  ..., 0.0769, 0.0728, 0.0775],
        [0.0112, 0.0461, 0.0305,  ..., 0.0414, 0.0397, 0.0420],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37407.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2770, 0.0000,  ..., 0.1144, 0.0000, 0.0832],
        [0.0000, 0.2304, 0.0000,  ..., 0.0964, 0.0000, 0.0679],
        [0.0000, 0.1521, 0.0000,  ..., 0.0655, 0.0000, 0.0438],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(202235.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-96.6059, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.6973, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-214.7852, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9943],
        [0.9885],
        [0.9729],
        ...,
        [0.9533],
        [0.9521],
        [0.9509]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358345.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9943],
        [0.9884],
        [0.9729],
        ...,
        [0.9532],
        [0.9520],
        [0.9508]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358329.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.9091, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0034,  0.0148,  0.0097,  ...,  0.0132,  0.0128,  0.0136],
        [ 0.0072,  0.0285,  0.0194,  ...,  0.0263,  0.0246,  0.0263],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1479.6044, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.4829, device='cuda:0')



h[100].sum tensor(19.8839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.1086, device='cuda:0')



h[200].sum tensor(24.7781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.7397, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0061, 0.0277, 0.0175,  ..., 0.0238, 0.0237, 0.0249],
        [0.0123, 0.0523, 0.0344,  ..., 0.0468, 0.0450, 0.0478],
        [0.0186, 0.0770, 0.0514,  ..., 0.0699, 0.0664, 0.0707],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40522.5586, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0881, 0.0000,  ..., 0.0406, 0.0000, 0.0238],
        [0.0000, 0.1589, 0.0000,  ..., 0.0685, 0.0000, 0.0457],
        [0.0000, 0.2228, 0.0000,  ..., 0.0935, 0.0000, 0.0657],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0080, 0.0000, 0.0012],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(212616.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-111.9437, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.4654, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-237.8337, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9943],
        [0.9884],
        [0.9729],
        ...,
        [0.9532],
        [0.9520],
        [0.9508]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358329.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9943],
        [0.9884],
        [0.9728],
        ...,
        [0.9531],
        [0.9519],
        [0.9507]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358314.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2751],
        [0.7080],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(305.9062, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0017,  0.0086,  0.0053,  ...,  0.0073,  0.0074,  0.0078],
        [ 0.0053,  0.0216,  0.0145,  ...,  0.0197,  0.0186,  0.0198],
        [ 0.0066,  0.0263,  0.0178,  ...,  0.0242,  0.0227,  0.0242],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1606.5088, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.9810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.2653, device='cuda:0')



h[100].sum tensor(20.2122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.6758, device='cuda:0')



h[200].sum tensor(25.4756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.7499, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0102, 0.0444, 0.0288,  ..., 0.0393, 0.0382, 0.0404],
        [0.0195, 0.0803, 0.0537,  ..., 0.0731, 0.0693, 0.0738],
        [0.0292, 0.1156, 0.0787,  ..., 0.1068, 0.0999, 0.1066],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42078.6484, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1755, 0.0000,  ..., 0.0751, 0.0000, 0.0506],
        [0.0000, 0.2185, 0.0000,  ..., 0.0918, 0.0000, 0.0640],
        [0.0000, 0.2475, 0.0000,  ..., 0.1029, 0.0000, 0.0736],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(214764.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-119.8273, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.7480, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-249.5403, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9943],
        [0.9884],
        [0.9728],
        ...,
        [0.9531],
        [0.9519],
        [0.9507]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358314.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9943],
        [0.9884],
        [0.9727],
        ...,
        [0.9530],
        [0.9518],
        [0.9506]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358298.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.0836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(959.7508, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8334, device='cuda:0')



h[100].sum tensor(18.5138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.6745, device='cuda:0')



h[200].sum tensor(21.8682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.1125, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30287.9043, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(161196.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-62.6457, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.7135, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-163.9191, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9943],
        [0.9884],
        [0.9727],
        ...,
        [0.9530],
        [0.9518],
        [0.9506]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358298.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9943],
        [0.9883],
        [0.9727],
        ...,
        [0.9529],
        [0.9517],
        [0.9505]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358282.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.8580, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(927.7045, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1002, device='cuda:0')



h[100].sum tensor(18.4270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4413, device='cuda:0')



h[200].sum tensor(21.6839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.2857, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29348.5430, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(154057.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-58.7668, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.2758, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-157.5486, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9943],
        [0.9883],
        [0.9727],
        ...,
        [0.9529],
        [0.9517],
        [0.9505]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358282.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9943],
        [0.9883],
        [0.9726],
        ...,
        [0.9528],
        [0.9516],
        [0.9504]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358267.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8218, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0019,  0.0091,  0.0057,  ...,  0.0078,  0.0078,  0.0083],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1142.7563, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3056, device='cuda:0')



h[100].sum tensor(18.9863, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4612, device='cuda:0')



h[200].sum tensor(22.8717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.9008, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0025, 0.0145, 0.0081,  ..., 0.0112, 0.0123, 0.0126],
        [0.0019, 0.0103, 0.0056,  ..., 0.0077, 0.0086, 0.0087],
        [0.0014, 0.0087, 0.0045,  ..., 0.0062, 0.0072, 0.0072],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33096.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0516, 0.0000,  ..., 0.0269, 0.0000, 0.0107],
        [0.0000, 0.0364, 0.0000,  ..., 0.0205, 0.0000, 0.0071],
        [0.0000, 0.0304, 0.0000,  ..., 0.0180, 0.0000, 0.0056],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(171553.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-76.3321, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.9441, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-184.5552, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9943],
        [0.9883],
        [0.9726],
        ...,
        [0.9528],
        [0.9516],
        [0.9504]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358267.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9942],
        [0.9883],
        [0.9725],
        ...,
        [0.9527],
        [0.9515],
        [0.9504]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358251.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.4836, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0013,  0.0071,  0.0042,  ...,  0.0058,  0.0061,  0.0064],
        [ 0.0013,  0.0071,  0.0042,  ...,  0.0058,  0.0061,  0.0064],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1566.0117, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.9602, device='cuda:0')



h[100].sum tensor(20.0882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5787, device='cuda:0')



h[200].sum tensor(25.2120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.4058, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0050, 0.0257, 0.0155,  ..., 0.0214, 0.0220, 0.0231],
        [0.0034, 0.0197, 0.0113,  ..., 0.0156, 0.0168, 0.0174],
        [0.0022, 0.0136, 0.0075,  ..., 0.0104, 0.0115, 0.0118],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43066.1641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0761, 0.0000,  ..., 0.0370, 0.0000, 0.0174],
        [0.0000, 0.0669, 0.0000,  ..., 0.0333, 0.0000, 0.0146],
        [0.0000, 0.0478, 0.0000,  ..., 0.0252, 0.0000, 0.0099],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(231335.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-124.3390, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-36.3504, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-257.3980, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9942],
        [0.9883],
        [0.9725],
        ...,
        [0.9527],
        [0.9515],
        [0.9504]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358251.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9942],
        [0.9883],
        [0.9725],
        ...,
        [0.9527],
        [0.9515],
        [0.9504]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358251.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(186.3765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(899.9296, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6116, device='cuda:0')



h[100].sum tensor(18.3488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.2858, device='cuda:0')



h[200].sum tensor(21.5176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.7347, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30294.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0018, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(163112.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-62.7558, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.6322, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.0448, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9942],
        [0.9883],
        [0.9725],
        ...,
        [0.9527],
        [0.9515],
        [0.9504]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358251.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9942],
        [0.9883],
        [0.9725],
        ...,
        [0.9526],
        [0.9514],
        [0.9503]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358235.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7712, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1081.6799, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4098, device='cuda:0')



h[100].sum tensor(18.8201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1762, device='cuda:0')



h[200].sum tensor(22.5188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.8905, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32710.8711, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0014, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0069, 0.0000, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0097, 0.0000, 0.0012],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(173704.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-74.0345, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.5137, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-181.4290, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9942],
        [0.9883],
        [0.9725],
        ...,
        [0.9526],
        [0.9514],
        [0.9503]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358235.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9942],
        [0.9882],
        [0.9724],
        ...,
        [0.9525],
        [0.9513],
        [0.9502]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358220.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(428.8805, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2399.1650, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-3.8095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.2259, device='cuda:0')



h[100].sum tensor(22.2509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.1634, device='cuda:0')



h[200].sum tensor(29.8055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.1113, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(59600.3086, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0086, 0.0000, 0.0014],
        [0.0000, 0.0018, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0074, 0.0000, 0.0005],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(315185.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-202.9458, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-21.2769, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-376.9758, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9942],
        [0.9882],
        [0.9724],
        ...,
        [0.9525],
        [0.9513],
        [0.9502]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358220.1875, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 240.0 event: 1200 loss: tensor(582.2134, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9942],
        [0.9882],
        [0.9723],
        ...,
        [0.9525],
        [0.9513],
        [0.9501]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358204.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(291.8463, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0025,  0.0115,  0.0073,  ...,  0.0101,  0.0099,  0.0105],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1547.3945, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3613, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0121, device='cuda:0')



h[100].sum tensor(20.0264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2770, device='cuda:0')



h[200].sum tensor(25.0808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.3365, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0066, 0.0296, 0.0188,  ..., 0.0256, 0.0253, 0.0266],
        [0.0025, 0.0127, 0.0073,  ..., 0.0100, 0.0107, 0.0109],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42213.7070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1095, 0.0000,  ..., 0.0491, 0.0000, 0.0304],
        [0.0000, 0.0489, 0.0000,  ..., 0.0249, 0.0000, 0.0123],
        [0.0000, 0.0137, 0.0000,  ..., 0.0107, 0.0000, 0.0030],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(222413.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-120.7245, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-36.9355, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-251.4691, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9942],
        [0.9882],
        [0.9723],
        ...,
        [0.9525],
        [0.9513],
        [0.9501]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358204.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9942],
        [0.9882],
        [0.9723],
        ...,
        [0.9524],
        [0.9512],
        [0.9500]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358188.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(357.3451, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0022,  0.0103,  0.0065,  ...,  0.0089,  0.0089,  0.0094],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1917.8640, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.3981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.8500, device='cuda:0')



h[100].sum tensor(20.9858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.1346, device='cuda:0')



h[200].sum tensor(27.1186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.9205, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0022, 0.0114, 0.0064,  ..., 0.0088, 0.0096, 0.0098],
        [0.0017, 0.0096, 0.0051,  ..., 0.0071, 0.0080, 0.0081],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47436.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0077, 0.0000,  ..., 0.0084, 0.0000, 0.0010],
        [0.0000, 0.0242, 0.0000,  ..., 0.0152, 0.0000, 0.0044],
        [0.0000, 0.0363, 0.0000,  ..., 0.0204, 0.0000, 0.0071],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(249779.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-145.4398, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-32.9220, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-288.2551, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9942],
        [0.9882],
        [0.9723],
        ...,
        [0.9524],
        [0.9512],
        [0.9500]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358188.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9941],
        [0.9881],
        [0.9722],
        ...,
        [0.9523],
        [0.9511],
        [0.9499]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358173.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.8761, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0033,  0.0144,  0.0094,  ...,  0.0128,  0.0124,  0.0131],
        [ 0.0062,  0.0251,  0.0170,  ...,  0.0231,  0.0217,  0.0231],
        [ 0.0117,  0.0450,  0.0311,  ...,  0.0421,  0.0389,  0.0416],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1240.2319, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.7366, device='cuda:0')



h[100].sum tensor(19.2194, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9165, device='cuda:0')



h[200].sum tensor(23.3667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.5146, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0114, 0.0470, 0.0311,  ..., 0.0422, 0.0404, 0.0428],
        [0.0270, 0.1077, 0.0731,  ..., 0.0992, 0.0930, 0.0992],
        [0.0334, 0.1309, 0.0896,  ..., 0.1215, 0.1132, 0.1209],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35291.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1586, 0.0000,  ..., 0.0682, 0.0000, 0.0453],
        [0.0000, 0.2657, 0.0000,  ..., 0.1099, 0.0000, 0.0793],
        [0.0000, 0.3416, 0.0000,  ..., 0.1390, 0.0000, 0.1041],
        ...,
        [0.0000, 0.0124, 0.0000,  ..., 0.0097, 0.0000, 0.0026],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(183862.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-87.3180, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.9486, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-200.4201, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9941],
        [0.9881],
        [0.9722],
        ...,
        [0.9523],
        [0.9511],
        [0.9499]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358173.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9941],
        [0.9881],
        [0.9721],
        ...,
        [0.9522],
        [0.9510],
        [0.9498]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358157.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4133],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.4471, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0029,  0.0128,  0.0082,  ...,  0.0113,  0.0110,  0.0116],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0029,  0.0128,  0.0082,  ...,  0.0113,  0.0110,  0.0116],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1457.6030, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.0852, device='cuda:0')



h[100].sum tensor(19.7802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9821, device='cuda:0')



h[200].sum tensor(24.5579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.2912, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0022, 0.0117, 0.0066,  ..., 0.0091, 0.0099, 0.0100],
        [0.0101, 0.0462, 0.0296,  ..., 0.0405, 0.0398, 0.0421],
        [0.0022, 0.0116, 0.0066,  ..., 0.0090, 0.0098, 0.0099],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40294.5234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0382, 0.0000,  ..., 0.0209, 0.0000, 0.0085],
        [0.0000, 0.0719, 0.0000,  ..., 0.0348, 0.0000, 0.0175],
        [0.0000, 0.0602, 0.0000,  ..., 0.0301, 0.0000, 0.0138],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(215798.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-110.3352, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.4191, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-236.5769, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9941],
        [0.9881],
        [0.9721],
        ...,
        [0.9522],
        [0.9510],
        [0.9498]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358157.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9941],
        [0.9881],
        [0.9721],
        ...,
        [0.9521],
        [0.9509],
        [0.9497]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358141.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2544],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3783, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0080,  0.0049,  ...,  0.0067,  0.0069,  0.0072],
        [ 0.0015,  0.0076,  0.0046,  ...,  0.0064,  0.0065,  0.0069],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1059.0929, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9889, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9292, device='cuda:0')



h[100].sum tensor(18.7423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0232, device='cuda:0')



h[200].sum tensor(22.3534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.3484, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0016, 0.0092, 0.0048,  ..., 0.0067, 0.0077, 0.0077],
        [0.0057, 0.0263, 0.0164,  ..., 0.0224, 0.0225, 0.0235],
        [0.0119, 0.0525, 0.0340,  ..., 0.0465, 0.0452, 0.0480],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32321.0234, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0428, 0.0000,  ..., 0.0230, 0.0000, 0.0092],
        [0.0000, 0.0799, 0.0000,  ..., 0.0380, 0.0000, 0.0196],
        [0.0000, 0.1108, 0.0000,  ..., 0.0506, 0.0000, 0.0283],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(171382.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-72.6234, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.7736, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-178.7085, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9941],
        [0.9881],
        [0.9721],
        ...,
        [0.9521],
        [0.9509],
        [0.9497]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358141.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9941],
        [0.9880],
        [0.9720],
        ...,
        [0.9520],
        [0.9508],
        [0.9496]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358126.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.4900, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1517.2054, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.8021, device='cuda:0')



h[100].sum tensor(19.9265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2102, device='cuda:0')



h[200].sum tensor(24.8686, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.0997, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38300.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(192378.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-101.2019, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.1495, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-222.2874, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9941],
        [0.9880],
        [0.9720],
        ...,
        [0.9520],
        [0.9508],
        [0.9496]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358126.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9941],
        [0.9880],
        [0.9720],
        ...,
        [0.9519],
        [0.9507],
        [0.9495]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358110.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(348.9005, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1898.9399, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.5518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.0973, device='cuda:0')



h[100].sum tensor(20.9107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.8951, device='cuda:0')



h[200].sum tensor(26.9591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.0717, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48339.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0192, 0.0000,  ..., 0.0132, 0.0000, 0.0037],
        [0.0000, 0.0155, 0.0000,  ..., 0.0115, 0.0000, 0.0024],
        [0.0000, 0.0501, 0.0000,  ..., 0.0250, 0.0000, 0.0137],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(259719.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-149.0695, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-32.1385, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-294.7293, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9941],
        [0.9880],
        [0.9720],
        ...,
        [0.9519],
        [0.9507],
        [0.9495]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358110.4062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9941],
        [0.9880],
        [0.9719],
        ...,
        [0.9518],
        [0.9506],
        [0.9494]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358094.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6138],
        [0.8994],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(335.8342, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0045,  0.0188,  0.0125,  ...,  0.0170,  0.0162,  0.0172],
        [ 0.0068,  0.0273,  0.0185,  ...,  0.0251,  0.0236,  0.0251],
        [ 0.0045,  0.0188,  0.0125,  ...,  0.0170,  0.0162,  0.0172],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1807.0338, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.0488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.9327, device='cuda:0')



h[100].sum tensor(20.6678, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.5245, device='cuda:0')



h[200].sum tensor(26.4432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.7582, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0247, 0.0995, 0.0673,  ..., 0.0914, 0.0860, 0.0917],
        [0.0266, 0.1061, 0.0720,  ..., 0.0977, 0.0916, 0.0978],
        [0.0464, 0.1786, 0.1234,  ..., 0.1670, 0.1544, 0.1652],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45189.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.8838e-01, 0.0000e+00,  ..., 1.1884e-01, 0.0000e+00,
         8.6846e-02],
        [0.0000e+00, 3.3505e-01, 0.0000e+00,  ..., 1.3654e-01, 0.0000e+00,
         1.0228e-01],
        [0.0000e+00, 4.2839e-01, 0.0000e+00,  ..., 1.7185e-01, 0.0000e+00,
         1.3311e-01],
        ...,
        [0.0000e+00, 3.7467e-04, 0.0000e+00,  ..., 4.8248e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.7495e-04, 0.0000e+00,  ..., 4.8243e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 4.1101e-03, 0.0000e+00,  ..., 6.3829e-03, 0.0000e+00,
         2.6632e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(229462.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-134.4077, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-34.9687, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-272.0050, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9941],
        [0.9880],
        [0.9719],
        ...,
        [0.9518],
        [0.9506],
        [0.9494]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358094.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9940],
        [0.9879],
        [0.9718],
        ...,
        [0.9517],
        [0.9505],
        [0.9493]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358078.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(277.2692, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1470.7924, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.7129, device='cuda:0')



h[100].sum tensor(19.7939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8636, device='cuda:0')



h[200].sum tensor(24.5870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.8712, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39571.0430, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0076, 0.0000,  ..., 0.0083, 0.0000, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(205749.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-107.2316, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.3811, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-230.9069, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9940],
        [0.9879],
        [0.9718],
        ...,
        [0.9517],
        [0.9505],
        [0.9493]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358078.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9940],
        [0.9879],
        [0.9718],
        ...,
        [0.9516],
        [0.9504],
        [0.9492]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358063.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.6738],
        [0.4446],
        [0.3091],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(190.2925, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0105,  0.0405,  0.0279,  ...,  0.0378,  0.0350,  0.0375],
        [ 0.0116,  0.0445,  0.0307,  ...,  0.0416,  0.0385,  0.0412],
        [ 0.0072,  0.0285,  0.0194,  ...,  0.0263,  0.0246,  0.0263],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(923.6168, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.9607, device='cuda:0')



h[100].sum tensor(18.3770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.3969, device='cuda:0')



h[200].sum tensor(21.5775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.1283, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0487, 0.1869, 0.1292,  ..., 0.1749, 0.1616, 0.1728],
        [0.0402, 0.1560, 0.1074,  ..., 0.1454, 0.1349, 0.1442],
        [0.0298, 0.1180, 0.0805,  ..., 0.1091, 0.1020, 0.1089],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29033.7051, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 4.4365e-01, 0.0000e+00,  ..., 1.7762e-01, 0.0000e+00,
         1.3808e-01],
        [0.0000e+00, 4.0148e-01, 0.0000e+00,  ..., 1.6169e-01, 0.0000e+00,
         1.2406e-01],
        [0.0000e+00, 3.3009e-01, 0.0000e+00,  ..., 1.3454e-01, 0.0000e+00,
         1.0057e-01],
        ...,
        [0.0000e+00, 3.7482e-04, 0.0000e+00,  ..., 4.8231e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.7510e-04, 0.0000e+00,  ..., 4.8226e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.7510e-04, 0.0000e+00,  ..., 4.8223e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(152770.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-57.0556, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.6158, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-155.1249, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9940],
        [0.9879],
        [0.9718],
        ...,
        [0.9516],
        [0.9504],
        [0.9492]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358063.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 250.0 event: 1250 loss: tensor(622.2748, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9940],
        [0.9879],
        [0.9717],
        ...,
        [0.9515],
        [0.9503],
        [0.9491]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358047.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.0486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0011,  0.0064,  0.0038,  ...,  0.0052,  0.0055,  0.0058],
        [ 0.0011,  0.0064,  0.0038,  ...,  0.0052,  0.0055,  0.0058],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1123.2952, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9693, device='cuda:0')



h[100].sum tensor(18.8893, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.3542, device='cuda:0')



h[200].sum tensor(22.6657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.5215, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0019, 0.0125, 0.0067,  ..., 0.0093, 0.0106, 0.0108],
        [0.0019, 0.0125, 0.0067,  ..., 0.0093, 0.0106, 0.0108],
        [0.0019, 0.0124, 0.0066,  ..., 0.0093, 0.0105, 0.0107],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31787.5215, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0362, 0.0000,  ..., 0.0208, 0.0000, 0.0055],
        [0.0000, 0.0394, 0.0000,  ..., 0.0221, 0.0000, 0.0061],
        [0.0000, 0.0361, 0.0000,  ..., 0.0206, 0.0000, 0.0054],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(165811.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-69.3863, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.4964, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-174.5529, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9940],
        [0.9879],
        [0.9717],
        ...,
        [0.9515],
        [0.9503],
        [0.9491]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358047.5312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9940],
        [0.9878],
        [0.9716],
        ...,
        [0.9515],
        [0.9502],
        [0.9491]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358031.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.3246, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0034,  0.0146,  0.0096,  ...,  0.0130,  0.0126,  0.0134],
        [ 0.0069,  0.0276,  0.0187,  ...,  0.0254,  0.0238,  0.0254],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(870.6731, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2505, device='cuda:0')



h[100].sum tensor(18.2350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.1709, device='cuda:0')



h[200].sum tensor(21.2760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.3274, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0034, 0.0158, 0.0095,  ..., 0.0130, 0.0134, 0.0138],
        [0.0124, 0.0508, 0.0338,  ..., 0.0459, 0.0437, 0.0463],
        [0.0308, 0.1217, 0.0831,  ..., 0.1127, 0.1052, 0.1123],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28030.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0696, 0.0000,  ..., 0.0328, 0.0000, 0.0193],
        [0.0000, 0.1605, 0.0000,  ..., 0.0682, 0.0000, 0.0478],
        [0.0000, 0.2865, 0.0000,  ..., 0.1171, 0.0000, 0.0878],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(148607.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-52.2896, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.3834, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-148.0883, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9940],
        [0.9878],
        [0.9716],
        ...,
        [0.9515],
        [0.9502],
        [0.9491]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358031.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9940],
        [0.9878],
        [0.9716],
        ...,
        [0.9514],
        [0.9501],
        [0.9490]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358016.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(256.8682, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0021,  0.0101,  0.0063,  ...,  0.0087,  0.0087,  0.0091],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0038,  0.0162,  0.0107,  ...,  0.0145,  0.0140,  0.0148],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1326.8165, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.6288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.8945, device='cuda:0')



h[100].sum tensor(19.4070, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2850, device='cuda:0')



h[200].sum tensor(23.7652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.8205, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0120, 0.0512, 0.0336,  ..., 0.0458, 0.0441, 0.0467],
        [0.0105, 0.0475, 0.0305,  ..., 0.0417, 0.0409, 0.0433],
        [0.0076, 0.0333, 0.0214,  ..., 0.0292, 0.0285, 0.0301],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0047, 0.0206, 0.0130,  ..., 0.0177, 0.0176, 0.0184]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34900.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1829, 0.0000,  ..., 0.0785, 0.0000, 0.0523],
        [0.0000, 0.1430, 0.0000,  ..., 0.0630, 0.0000, 0.0390],
        [0.0000, 0.1280, 0.0000,  ..., 0.0567, 0.0000, 0.0351],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0060, 0.0000, 0.0005],
        [0.0000, 0.0166, 0.0000,  ..., 0.0113, 0.0000, 0.0041],
        [0.0000, 0.0557, 0.0000,  ..., 0.0267, 0.0000, 0.0152]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(180141.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-84.9436, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.3246, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-197.5268, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9940],
        [0.9878],
        [0.9716],
        ...,
        [0.9514],
        [0.9501],
        [0.9490]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358016.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9940],
        [0.9878],
        [0.9715],
        ...,
        [0.9513],
        [0.9501],
        [0.9489]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(358000.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.8889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0041,  0.0174,  0.0116,  ...,  0.0157,  0.0150,  0.0160],
        [ 0.0035,  0.0152,  0.0100,  ...,  0.0136,  0.0131,  0.0139],
        [ 0.0035,  0.0149,  0.0098,  ...,  0.0133,  0.0129,  0.0137],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1303.7805, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1833, device='cuda:0')



h[100].sum tensor(19.3440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0587, device='cuda:0')



h[200].sum tensor(23.6315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.0184, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0147, 0.0628, 0.0413,  ..., 0.0563, 0.0541, 0.0575],
        [0.0200, 0.0821, 0.0549,  ..., 0.0747, 0.0708, 0.0754],
        [0.0188, 0.0777, 0.0519,  ..., 0.0706, 0.0670, 0.0714],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35628.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1959, 0.0000,  ..., 0.0838, 0.0000, 0.0556],
        [0.0000, 0.2429, 0.0000,  ..., 0.1021, 0.0000, 0.0707],
        [0.0000, 0.2463, 0.0000,  ..., 0.1034, 0.0000, 0.0720],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(185466.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-88.4539, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.7708, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-202.6136, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9940],
        [0.9878],
        [0.9715],
        ...,
        [0.9513],
        [0.9501],
        [0.9489]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(358000.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9939],
        [0.9877],
        [0.9714],
        ...,
        [0.9512],
        [0.9500],
        [0.9488]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357984.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(209.9396, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0015,  0.0078,  0.0047,  ...,  0.0065,  0.0067,  0.0070],
        [ 0.0015,  0.0079,  0.0048,  ...,  0.0067,  0.0068,  0.0072],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1049.7302, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.7118, device='cuda:0')



h[100].sum tensor(18.6874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.9541, device='cuda:0')



h[200].sum tensor(22.2369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.1032, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0120, 0.0529, 0.0343,  ..., 0.0468, 0.0455, 0.0483],
        [0.0057, 0.0264, 0.0165,  ..., 0.0226, 0.0226, 0.0237],
        [0.0015, 0.0091, 0.0048,  ..., 0.0066, 0.0076, 0.0076],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30255.3867, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1134, 0.0000,  ..., 0.0517, 0.0000, 0.0291],
        [0.0000, 0.0821, 0.0000,  ..., 0.0390, 0.0000, 0.0202],
        [0.0000, 0.0473, 0.0000,  ..., 0.0248, 0.0000, 0.0102],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(154930.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-63.4683, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.3239, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.2124, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9939],
        [0.9877],
        [0.9714],
        ...,
        [0.9512],
        [0.9500],
        [0.9488]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357984.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9939],
        [0.9877],
        [0.9714],
        ...,
        [0.9511],
        [0.9499],
        [0.9487]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357968.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.4297, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1508.9116, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.2619, device='cuda:0')



h[100].sum tensor(19.8635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0383, device='cuda:0')



h[200].sum tensor(24.7349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.4905, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0017, 0.0117, 0.0061,  ..., 0.0085, 0.0099, 0.0100],
        [0.0009, 0.0067, 0.0030,  ..., 0.0043, 0.0055, 0.0053],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38915.7773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0280, 0.0000,  ..., 0.0173, 0.0000, 0.0038],
        [0.0000, 0.0198, 0.0000,  ..., 0.0137, 0.0000, 0.0022],
        [0.0000, 0.0102, 0.0000,  ..., 0.0095, 0.0000, 0.0008],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(202163.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-104.2800, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.5227, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-226.7939, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9939],
        [0.9877],
        [0.9714],
        ...,
        [0.9511],
        [0.9499],
        [0.9487]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357968.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9939],
        [0.9877],
        [0.9714],
        ...,
        [0.9511],
        [0.9499],
        [0.9487]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357968.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.3491, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1079.7400, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9266, device='cuda:0')



h[100].sum tensor(18.7615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0224, device='cuda:0')



h[200].sum tensor(22.3942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.3454, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33103.0586, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(177111.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-76.2645, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.2043, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-184.1418, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9939],
        [0.9877],
        [0.9714],
        ...,
        [0.9511],
        [0.9499],
        [0.9487]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357968.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9939],
        [0.9877],
        [0.9713],
        ...,
        [0.9510],
        [0.9498],
        [0.9486]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357953.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(198.2332, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(983.0488, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.6684, device='cuda:0')



h[100].sum tensor(18.5104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.6221, device='cuda:0')



h[200].sum tensor(21.8608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.9265, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0016, 0.0093, 0.0049,  ..., 0.0068, 0.0078, 0.0078],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29896.5527, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0474, 0.0000,  ..., 0.0245, 0.0000, 0.0113],
        [0.0000, 0.0177, 0.0000,  ..., 0.0125, 0.0000, 0.0030],
        [0.0000, 0.0124, 0.0000,  ..., 0.0103, 0.0000, 0.0016],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(156421.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-61.5739, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.5735, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-161.7435, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9939],
        [0.9877],
        [0.9713],
        ...,
        [0.9510],
        [0.9498],
        [0.9486]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357953.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9939],
        [0.9876],
        [0.9712],
        ...,
        [0.9509],
        [0.9497],
        [0.9485]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357937.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(254.1191, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1313.4399, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6495, device='cuda:0')



h[100].sum tensor(19.3544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2070, device='cuda:0')



h[200].sum tensor(23.6535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.5442, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34110.0703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(171847.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-81.0666, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.1061, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.6729, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9939],
        [0.9876],
        [0.9712],
        ...,
        [0.9509],
        [0.9497],
        [0.9485]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357937.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9939],
        [0.9876],
        [0.9712],
        ...,
        [0.9508],
        [0.9496],
        [0.9484]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357921.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7982, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0060,  0.0242,  0.0164,  ...,  0.0222,  0.0209,  0.0223],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1181.0471, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.4818, device='cuda:0')



h[100].sum tensor(19.0117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5173, device='cuda:0')



h[200].sum tensor(22.9257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.0994, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0107, 0.0446, 0.0294,  ..., 0.0400, 0.0384, 0.0406],
        [0.0218, 0.0868, 0.0588,  ..., 0.0798, 0.0749, 0.0798],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33117.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0569, 0.0000,  ..., 0.0279, 0.0000, 0.0156],
        [0.0000, 0.1402, 0.0000,  ..., 0.0607, 0.0000, 0.0409],
        [0.0000, 0.2241, 0.0000,  ..., 0.0934, 0.0000, 0.0670],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(170584.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-76.6997, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.8062, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-184.8170, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9939],
        [0.9876],
        [0.9712],
        ...,
        [0.9508],
        [0.9496],
        [0.9484]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357921.5000, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 260.0 event: 1300 loss: tensor(552.5645, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9938],
        [0.9876],
        [0.9711],
        ...,
        [0.9507],
        [0.9495],
        [0.9483]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357905.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.3192, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0013,  0.0072,  0.0043,  ...,  0.0059,  0.0061,  0.0064],
        [ 0.0017,  0.0086,  0.0053,  ...,  0.0073,  0.0074,  0.0078],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1617.3379, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.9456, device='cuda:0')



h[100].sum tensor(20.1247, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5740, device='cuda:0')



h[200].sum tensor(25.2896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.3893, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0127, 0.0555, 0.0361,  ..., 0.0493, 0.0478, 0.0507],
        [0.0058, 0.0264, 0.0165,  ..., 0.0226, 0.0226, 0.0237],
        [0.0034, 0.0176, 0.0103,  ..., 0.0142, 0.0150, 0.0155],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43424.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1196, 0.0000,  ..., 0.0541, 0.0000, 0.0310],
        [0.0000, 0.0889, 0.0000,  ..., 0.0417, 0.0000, 0.0221],
        [0.0000, 0.0661, 0.0000,  ..., 0.0324, 0.0000, 0.0156],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(237946.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-126.4112, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.9951, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-259.9732, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9938],
        [0.9876],
        [0.9711],
        ...,
        [0.9507],
        [0.9495],
        [0.9483]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357905.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9938],
        [0.9875],
        [0.9710],
        ...,
        [0.9506],
        [0.9494],
        [0.9482]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357889.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.8813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1739.5437, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.5297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.2435, device='cuda:0')



h[100].sum tensor(20.4328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9870, device='cuda:0')



h[200].sum tensor(25.9440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.8531, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44094.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0069, 0.0000, 0.0002],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0059, 0.0000, 0.0004],
        [0.0000, 0.0123, 0.0000,  ..., 0.0096, 0.0000, 0.0027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(227130.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-129.4299, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.3039, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-264.9574, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9938],
        [0.9875],
        [0.9710],
        ...,
        [0.9506],
        [0.9494],
        [0.9482]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357889.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9938],
        [0.9875],
        [0.9710],
        ...,
        [0.9505],
        [0.9493],
        [0.9481]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357874.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.6022, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0018,  0.0091,  0.0056,  ...,  0.0077,  0.0078,  0.0082],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1125.9110, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.6622, device='cuda:0')



h[100].sum tensor(18.8611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2565, device='cuda:0')



h[200].sum tensor(22.6058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.1750, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0018, 0.0102, 0.0056,  ..., 0.0077, 0.0086, 0.0086],
        [0.0014, 0.0086, 0.0044,  ..., 0.0061, 0.0072, 0.0071],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34008.9453, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0081, 0.0000,  ..., 0.0086, 0.0000, 0.0007],
        [0.0000, 0.0208, 0.0000,  ..., 0.0139, 0.0000, 0.0034],
        [0.0000, 0.0263, 0.0000,  ..., 0.0162, 0.0000, 0.0047],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(182446.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.8367, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.1390, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.0248, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9938],
        [0.9875],
        [0.9710],
        ...,
        [0.9505],
        [0.9493],
        [0.9481]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357874.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9938],
        [0.9875],
        [0.9709],
        ...,
        [0.9505],
        [0.9492],
        [0.9480]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357858.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(329.6671, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1796.1685, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.2527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.3831, device='cuda:0')



h[100].sum tensor(20.5682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.3496, device='cuda:0')



h[200].sum tensor(26.2315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.1383, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44055.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0247, 0.0000,  ..., 0.0146, 0.0000, 0.0053],
        [0.0000, 0.0235, 0.0000,  ..., 0.0141, 0.0000, 0.0050],
        [0.0000, 0.0177, 0.0000,  ..., 0.0118, 0.0000, 0.0037]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(231839.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-128.8373, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.6749, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-264.2074, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9938],
        [0.9875],
        [0.9709],
        ...,
        [0.9505],
        [0.9492],
        [0.9480]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357858.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9938],
        [0.9874],
        [0.9708],
        ...,
        [0.9504],
        [0.9491],
        [0.9479]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357842.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(245.8719, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0025,  0.0113,  0.0072,  ...,  0.0098,  0.0097,  0.0103]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1284.3542, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9144, device='cuda:0')



h[100].sum tensor(19.2588, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9731, device='cuda:0')



h[200].sum tensor(23.4504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.7152, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0023, 0.0118, 0.0067,  ..., 0.0092, 0.0099, 0.0101],
        [0.0018, 0.0099, 0.0054,  ..., 0.0075, 0.0083, 0.0084]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35632.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0109, 0.0000,  ..., 0.0097, 0.0000, 0.0017],
        [0.0000, 0.0054, 0.0000,  ..., 0.0075, 0.0000, 0.0004],
        [0.0000, 0.0104, 0.0000,  ..., 0.0096, 0.0000, 0.0014],
        ...,
        [0.0000, 0.0076, 0.0000,  ..., 0.0078, 0.0000, 0.0011],
        [0.0000, 0.0234, 0.0000,  ..., 0.0143, 0.0000, 0.0045],
        [0.0000, 0.0301, 0.0000,  ..., 0.0171, 0.0000, 0.0062]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(183888.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-88.6549, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.5025, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-203.0350, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9938],
        [0.9874],
        [0.9708],
        ...,
        [0.9504],
        [0.9491],
        [0.9479]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357842.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9938],
        [0.9874],
        [0.9708],
        ...,
        [0.9503],
        [0.9490],
        [0.9478]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357826.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.3820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1134.1575, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.6425, device='cuda:0')



h[100].sum tensor(18.8728, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2502, device='cuda:0')



h[200].sum tensor(22.6306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.1529, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32849.2461, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0158, 0.0000,  ..., 0.0118, 0.0000, 0.0025],
        [0.0000, 0.0070, 0.0000,  ..., 0.0082, 0.0000, 0.0006],
        [0.0000, 0.0074, 0.0000,  ..., 0.0083, 0.0000, 0.0007],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(172856.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-74.9473, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.5303, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-182.1409, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9938],
        [0.9874],
        [0.9708],
        ...,
        [0.9503],
        [0.9490],
        [0.9478]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357826.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9937],
        [0.9874],
        [0.9707],
        ...,
        [0.9502],
        [0.9490],
        [0.9478]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357810.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(182.6980, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(904.5643, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.9237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.2838, device='cuda:0')



h[100].sum tensor(18.2854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.1815, device='cuda:0')



h[200].sum tensor(21.3831, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.3649, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28942.9805, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(156343.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-56.1740, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.8454, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-154.2607, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9937],
        [0.9874],
        [0.9707],
        ...,
        [0.9502],
        [0.9490],
        [0.9478]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357810.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9937],
        [0.9873],
        [0.9706],
        ...,
        [0.9501],
        [0.9489],
        [0.9477]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357795.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4155],
        [0.3840],
        [0.4084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.0765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0115,  0.0443,  0.0306,  ...,  0.0415,  0.0383,  0.0410],
        [ 0.0090,  0.0350,  0.0240,  ...,  0.0325,  0.0302,  0.0323],
        [ 0.0075,  0.0298,  0.0203,  ...,  0.0276,  0.0258,  0.0275],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1505.5536, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.2305, device='cuda:0')



h[100].sum tensor(19.8108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0283, device='cuda:0')



h[200].sum tensor(24.6229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.4550, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0333, 0.1306, 0.0893,  ..., 0.1211, 0.1128, 0.1205],
        [0.0420, 0.1626, 0.1120,  ..., 0.1517, 0.1406, 0.1503],
        [0.0440, 0.1698, 0.1172,  ..., 0.1587, 0.1469, 0.1570],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36689.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.9941e-01, 0.0000e+00,  ..., 1.6110e-01, 0.0000e+00,
         1.2326e-01],
        [0.0000e+00, 4.7188e-01, 0.0000e+00,  ..., 1.8852e-01, 0.0000e+00,
         1.4736e-01],
        [0.0000e+00, 4.9612e-01, 0.0000e+00,  ..., 1.9758e-01, 0.0000e+00,
         1.5560e-01],
        ...,
        [0.0000e+00, 3.7614e-04, 0.0000e+00,  ..., 4.8087e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.7642e-04, 0.0000e+00,  ..., 4.8082e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.7642e-04, 0.0000e+00,  ..., 4.8080e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(180631.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-94.2642, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.2330, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-211.0576, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9937],
        [0.9873],
        [0.9706],
        ...,
        [0.9501],
        [0.9489],
        [0.9477]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357795.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9937],
        [0.9873],
        [0.9706],
        ...,
        [0.9500],
        [0.9488],
        [0.9476]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357779.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2930],
        [0.4873],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(326.8270, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0092,  0.0359,  0.0247,  ...,  0.0334,  0.0311,  0.0332],
        [ 0.0060,  0.0243,  0.0164,  ...,  0.0223,  0.0210,  0.0224],
        [ 0.0127,  0.0488,  0.0338,  ...,  0.0457,  0.0422,  0.0451],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1775.4194, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.1299, device='cuda:0')



h[100].sum tensor(20.4923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.2691, device='cuda:0')



h[200].sum tensor(26.0704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.8528, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0258, 0.1033, 0.0700,  ..., 0.0951, 0.0892, 0.0952],
        [0.0391, 0.1518, 0.1044,  ..., 0.1414, 0.1313, 0.1403],
        [0.0203, 0.0831, 0.0557,  ..., 0.0758, 0.0717, 0.0764],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43048.8477, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2792, 0.0000,  ..., 0.1153, 0.0000, 0.0836],
        [0.0000, 0.3328, 0.0000,  ..., 0.1358, 0.0000, 0.1009],
        [0.0000, 0.2614, 0.0000,  ..., 0.1084, 0.0000, 0.0778],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(219769.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-123.6329, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.1957, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-256.0521, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9937],
        [0.9873],
        [0.9706],
        ...,
        [0.9500],
        [0.9488],
        [0.9476]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357779.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9937],
        [0.9873],
        [0.9705],
        ...,
        [0.9499],
        [0.9487],
        [0.9475]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357763.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.5704, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061,  0.0247,  0.0167,  ...,  0.0227,  0.0213,  0.0228],
        [ 0.0099,  0.0385,  0.0265,  ...,  0.0359,  0.0333,  0.0356],
        [ 0.0113,  0.0437,  0.0302,  ...,  0.0409,  0.0378,  0.0404],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1222.8936, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0854, device='cuda:0')



h[100].sum tensor(19.0856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7093, device='cuda:0')



h[200].sum tensor(23.0825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.7802, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0295, 0.1170, 0.0797,  ..., 0.1081, 0.1011, 0.1079],
        [0.0305, 0.1206, 0.0823,  ..., 0.1116, 0.1042, 0.1113],
        [0.0345, 0.1349, 0.0924,  ..., 0.1253, 0.1166, 0.1246],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32488.2168, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3542, 0.0000,  ..., 0.1439, 0.0000, 0.1086],
        [0.0000, 0.3429, 0.0000,  ..., 0.1396, 0.0000, 0.1049],
        [0.0000, 0.3214, 0.0000,  ..., 0.1310, 0.0000, 0.0984],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(166029.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-73.5135, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.4120, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-180.2062, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9937],
        [0.9873],
        [0.9705],
        ...,
        [0.9499],
        [0.9487],
        [0.9475]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357763.4688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 270.0 event: 1350 loss: tensor(642.3937, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9937],
        [0.9872],
        [0.9704],
        ...,
        [0.9498],
        [0.9486],
        [0.9474]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357747.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(218.7574, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1128.5183, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4977, device='cuda:0')



h[100].sum tensor(18.8430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2041, device='cuda:0')



h[200].sum tensor(22.5673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.9896, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32086.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0159, 0.0000,  ..., 0.0117, 0.0000, 0.0031],
        [0.0000, 0.0356, 0.0000,  ..., 0.0196, 0.0000, 0.0086],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(164807.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-71.9221, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.7225, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-177.3721, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9937],
        [0.9872],
        [0.9704],
        ...,
        [0.9498],
        [0.9486],
        [0.9474]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357747.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9936],
        [0.9872],
        [0.9704],
        ...,
        [0.9497],
        [0.9485],
        [0.9473]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357731.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(187.2643, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(939.3265, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.7698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(16.6908, device='cuda:0')



h[100].sum tensor(18.3607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.3110, device='cuda:0')



h[200].sum tensor(21.5429, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.8239, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0016, 0.0094, 0.0050,  ..., 0.0068, 0.0078, 0.0078],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0017, 0.0097, 0.0052,  ..., 0.0072, 0.0081, 0.0081],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29029.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0205, 0.0000,  ..., 0.0139, 0.0000, 0.0031],
        [0.0000, 0.0123, 0.0000,  ..., 0.0104, 0.0000, 0.0012],
        [0.0000, 0.0211, 0.0000,  ..., 0.0140, 0.0000, 0.0034],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(154714.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-57.3472, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.3864, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-155.4277, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9936],
        [0.9872],
        [0.9704],
        ...,
        [0.9497],
        [0.9485],
        [0.9473]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357731.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9936],
        [0.9872],
        [0.9703],
        ...,
        [0.9496],
        [0.9484],
        [0.9472]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357715.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(281.3146, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1496.2421, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8909, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.0734, device='cuda:0')



h[100].sum tensor(19.7676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.9783, device='cuda:0')



h[200].sum tensor(24.5312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.2779, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37509.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0014, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0063, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0083, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(190290., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-97.5155, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.0675, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-216.2405, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9936],
        [0.9872],
        [0.9703],
        ...,
        [0.9496],
        [0.9484],
        [0.9472]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357715.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9936],
        [0.9871],
        [0.9702],
        ...,
        [0.9495],
        [0.9483],
        [0.9471]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357700.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2888],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(368.3503, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0018,  0.0090,  0.0056,  ...,  0.0077,  0.0078,  0.0082],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0018,  0.0090,  0.0056,  ...,  0.0077,  0.0078,  0.0082],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2035.9218, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.1062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.8309, device='cuda:0')



h[100].sum tensor(21.1285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.4467, device='cuda:0')



h[200].sum tensor(27.4216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.0268, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0014, 0.0087, 0.0045,  ..., 0.0062, 0.0072, 0.0072],
        [0.0065, 0.0328, 0.0200,  ..., 0.0276, 0.0281, 0.0296],
        [0.0014, 0.0086, 0.0044,  ..., 0.0061, 0.0072, 0.0071],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(48140.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0392, 0.0000,  ..., 0.0216, 0.0000, 0.0080],
        [0.0000, 0.0529, 0.0000,  ..., 0.0275, 0.0000, 0.0111],
        [0.0000, 0.0394, 0.0000,  ..., 0.0218, 0.0000, 0.0073],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(247849.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-148.7075, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-32.4998, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-293.0099, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9936],
        [0.9871],
        [0.9702],
        ...,
        [0.9495],
        [0.9483],
        [0.9471]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357700.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9936],
        [0.9871],
        [0.9702],
        ...,
        [0.9495],
        [0.9482],
        [0.9470]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357684.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3040],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3164, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0020,  0.0095,  0.0059,  ...,  0.0081,  0.0082,  0.0086],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1133.2595, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7833, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5476, device='cuda:0')



h[100].sum tensor(18.8428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2200, device='cuda:0')



h[200].sum tensor(22.5669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.0458, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0058, 0.0286, 0.0175,  ..., 0.0241, 0.0245, 0.0257],
        [0.0054, 0.0252, 0.0157,  ..., 0.0215, 0.0216, 0.0226],
        [0.0082, 0.0389, 0.0244,  ..., 0.0335, 0.0335, 0.0353],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32268.9258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0765, 0.0000,  ..., 0.0372, 0.0000, 0.0174],
        [0.0000, 0.0794, 0.0000,  ..., 0.0382, 0.0000, 0.0185],
        [0.0000, 0.0798, 0.0000,  ..., 0.0384, 0.0000, 0.0184],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(169086.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-72.6888, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.7536, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-178.3423, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9936],
        [0.9871],
        [0.9702],
        ...,
        [0.9495],
        [0.9482],
        [0.9470]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357684.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9936],
        [0.9871],
        [0.9701],
        ...,
        [0.9494],
        [0.9481],
        [0.9469]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357668.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(229.7881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0072,  0.0284,  0.0193,  ...,  0.0262,  0.0245,  0.0262],
        [ 0.0045,  0.0189,  0.0126,  ...,  0.0171,  0.0163,  0.0173],
        [ 0.0054,  0.0220,  0.0148,  ...,  0.0201,  0.0190,  0.0202],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1193.0602, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.4809, device='cuda:0')



h[100].sum tensor(18.9907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5170, device='cuda:0')



h[200].sum tensor(22.8811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.0984, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0192, 0.0792, 0.0529,  ..., 0.0720, 0.0683, 0.0728],
        [0.0223, 0.0904, 0.0609,  ..., 0.0827, 0.0781, 0.0832],
        [0.0198, 0.0814, 0.0545,  ..., 0.0741, 0.0702, 0.0748],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35174.3672, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2303, 0.0000,  ..., 0.0970, 0.0000, 0.0670],
        [0.0000, 0.2280, 0.0000,  ..., 0.0960, 0.0000, 0.0665],
        [0.0000, 0.2066, 0.0000,  ..., 0.0876, 0.0000, 0.0597],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(191050.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-86.5965, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.9934, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-199.5532, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9936],
        [0.9871],
        [0.9701],
        ...,
        [0.9494],
        [0.9481],
        [0.9469]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357668.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9936],
        [0.9870],
        [0.9700],
        ...,
        [0.9493],
        [0.9480],
        [0.9468]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357652.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.4644, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1034.8843, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0455, device='cuda:0')



h[100].sum tensor(18.5886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7421, device='cuda:0')



h[200].sum tensor(22.0270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.3518, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0012, 0.0078, 0.0039,  ..., 0.0054, 0.0065, 0.0064],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30487.8242, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0456, 0.0000,  ..., 0.0242, 0.0000, 0.0099],
        [0.0000, 0.0239, 0.0000,  ..., 0.0151, 0.0000, 0.0047],
        [0.0000, 0.0085, 0.0000,  ..., 0.0087, 0.0000, 0.0013],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(160643.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-63.9629, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.1767, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-165.8017, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9936],
        [0.9870],
        [0.9700],
        ...,
        [0.9493],
        [0.9480],
        [0.9468]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357652.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9935],
        [0.9870],
        [0.9700],
        ...,
        [0.9492],
        [0.9479],
        [0.9467]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357636.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(418.1561, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2323.1821, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.6663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.2701, device='cuda:0')



h[100].sum tensor(21.8321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.8593, device='cuda:0')



h[200].sum tensor(28.9162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.0333, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0097, 0.0407, 0.0267,  ..., 0.0363, 0.0350, 0.0370],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(51165.5508, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1553, 0.0000,  ..., 0.0662, 0.0000, 0.0461],
        [0.0000, 0.0479, 0.0000,  ..., 0.0241, 0.0000, 0.0133],
        [0.0000, 0.0128, 0.0000,  ..., 0.0103, 0.0000, 0.0025],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(252447.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-162.9827, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-28.9757, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-315.9048, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9935],
        [0.9870],
        [0.9700],
        ...,
        [0.9492],
        [0.9479],
        [0.9467]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357636.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9935],
        [0.9870],
        [0.9699],
        ...,
        [0.9491],
        [0.9478],
        [0.9466]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357620.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.0256, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0019,  0.0093,  0.0058,  ...,  0.0079,  0.0079,  0.0084],
        [ 0.0051,  0.0208,  0.0139,  ...,  0.0190,  0.0180,  0.0191],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1314.6656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3738, device='cuda:0')



h[100].sum tensor(19.2874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.1193, device='cuda:0')



h[200].sum tensor(23.5113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.2332, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0242, 0.0974, 0.0658,  ..., 0.0893, 0.0841, 0.0897],
        [0.0120, 0.0510, 0.0335,  ..., 0.0456, 0.0439, 0.0466],
        [0.0125, 0.0527, 0.0347,  ..., 0.0472, 0.0454, 0.0482],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33786.9336, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2481, 0.0000,  ..., 0.1039, 0.0000, 0.0726],
        [0.0000, 0.1800, 0.0000,  ..., 0.0774, 0.0000, 0.0509],
        [0.0000, 0.1399, 0.0000,  ..., 0.0615, 0.0000, 0.0387],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(171210.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-79.5643, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.2674, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-189.5375, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9935],
        [0.9870],
        [0.9699],
        ...,
        [0.9491],
        [0.9478],
        [0.9466]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357620.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9935],
        [0.9869],
        [0.9698],
        ...,
        [0.9490],
        [0.9478],
        [0.9465]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357604.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.7970, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(965.1237, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0948, device='cuda:0')



h[100].sum tensor(18.4048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4395, device='cuda:0')



h[200].sum tensor(21.6366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.2795, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0052, 0.0242, 0.0150,  ..., 0.0205, 0.0207, 0.0216],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30063.2617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0059, 0.0000,  ..., 0.0076, 0.0000, 0.0009],
        [0.0000, 0.0364, 0.0000,  ..., 0.0197, 0.0000, 0.0096],
        [0.0000, 0.0995, 0.0000,  ..., 0.0449, 0.0000, 0.0275],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(161224.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-61.9315, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.7685, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-162.3903, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9935],
        [0.9869],
        [0.9698],
        ...,
        [0.9490],
        [0.9478],
        [0.9465]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357604.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 280.0 event: 1400 loss: tensor(622.9807, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9935],
        [0.9869],
        [0.9698],
        ...,
        [0.9489],
        [0.9477],
        [0.9465]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357588.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.1177, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1175.0222, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.9755, device='cuda:0')



h[100].sum tensor(18.9297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.3562, device='cuda:0')



h[200].sum tensor(22.7514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.5284, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0051, 0.0240, 0.0148,  ..., 0.0203, 0.0205, 0.0214],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33320.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0141, 0.0000,  ..., 0.0109, 0.0000, 0.0030],
        [0.0000, 0.0484, 0.0000,  ..., 0.0246, 0.0000, 0.0126],
        [0.0000, 0.1022, 0.0000,  ..., 0.0463, 0.0000, 0.0276],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(173191.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-78.1525, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.3321, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.6323, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9935],
        [0.9869],
        [0.9698],
        ...,
        [0.9489],
        [0.9477],
        [0.9465]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357588.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9935],
        [0.9869],
        [0.9697],
        ...,
        [0.9488],
        [0.9476],
        [0.9464]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357573.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.7162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1280.3439, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.4549, device='cuda:0')



h[100].sum tensor(19.1911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.8269, device='cuda:0')



h[200].sum tensor(23.3066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.1969, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34633.2266, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0363, 0.0000,  ..., 0.0199, 0.0000, 0.0086],
        [0.0000, 0.0414, 0.0000,  ..., 0.0219, 0.0000, 0.0100],
        [0.0000, 0.0407, 0.0000,  ..., 0.0216, 0.0000, 0.0098],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(176984., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-84.2476, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.1196, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-196.1463, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9935],
        [0.9869],
        [0.9697],
        ...,
        [0.9488],
        [0.9476],
        [0.9464]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357573.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9934],
        [0.9868],
        [0.9696],
        ...,
        [0.9487],
        [0.9475],
        [0.9463]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357557.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2898],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.2838, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0018,  0.0091,  0.0056,  ...,  0.0077,  0.0078,  0.0082],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0018,  0.0091,  0.0056,  ...,  0.0077,  0.0078,  0.0082],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1237.5651, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2970, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.8816, device='cuda:0')



h[100].sum tensor(19.0804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6445, device='cuda:0')



h[200].sum tensor(23.0717, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.5503, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0031, 0.0149, 0.0089,  ..., 0.0121, 0.0126, 0.0130],
        [0.0099, 0.0453, 0.0289,  ..., 0.0396, 0.0390, 0.0412],
        [0.0031, 0.0148, 0.0088,  ..., 0.0120, 0.0125, 0.0129],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34061.8398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0553, 0.0000,  ..., 0.0280, 0.0000, 0.0127],
        [0.0000, 0.0847, 0.0000,  ..., 0.0401, 0.0000, 0.0205],
        [0.0000, 0.0568, 0.0000,  ..., 0.0286, 0.0000, 0.0130],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(176540.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.8494, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.0840, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.3705, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9934],
        [0.9868],
        [0.9696],
        ...,
        [0.9487],
        [0.9475],
        [0.9463]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357557.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9934],
        [0.9868],
        [0.9696],
        ...,
        [0.9486],
        [0.9474],
        [0.9462]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357541.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2966],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(196.6265, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0048,  0.0197,  0.0132,  ...,  0.0179,  0.0170,  0.0181],
        [ 0.0019,  0.0093,  0.0058,  ...,  0.0079,  0.0080,  0.0084],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1005.0942, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5252, device='cuda:0')



h[100].sum tensor(18.4945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.5765, device='cuda:0')



h[200].sum tensor(21.8272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.7650, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0057, 0.0262, 0.0163,  ..., 0.0223, 0.0224, 0.0234],
        [0.0062, 0.0280, 0.0177,  ..., 0.0241, 0.0240, 0.0252],
        [0.0097, 0.0445, 0.0283,  ..., 0.0388, 0.0383, 0.0405],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29545.8809, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1059, 0.0000,  ..., 0.0478, 0.0000, 0.0285],
        [0.0000, 0.0985, 0.0000,  ..., 0.0453, 0.0000, 0.0254],
        [0.0000, 0.1102, 0.0000,  ..., 0.0504, 0.0000, 0.0278],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(154995.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-59.4406, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.1370, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-158.8085, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9934],
        [0.9868],
        [0.9696],
        ...,
        [0.9486],
        [0.9474],
        [0.9462]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357541.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9934],
        [0.9868],
        [0.9695],
        ...,
        [0.9485],
        [0.9473],
        [0.9461]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357525.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(359.7727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0029,  0.0129,  0.0083,  ...,  0.0114,  0.0111,  0.0118],
        [ 0.0041,  0.0173,  0.0114,  ...,  0.0156,  0.0149,  0.0159],
        [ 0.0040,  0.0170,  0.0112,  ...,  0.0153,  0.0147,  0.0156],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2019.3507, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.3035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.0664, device='cuda:0')



h[100].sum tensor(21.0320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.2035, device='cuda:0')



h[200].sum tensor(27.2168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.1645, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0063, 0.0286, 0.0181,  ..., 0.0247, 0.0245, 0.0257],
        [0.0101, 0.0442, 0.0286,  ..., 0.0391, 0.0380, 0.0402],
        [0.0232, 0.0936, 0.0632,  ..., 0.0858, 0.0808, 0.0862],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47699.7773, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1200, 0.0000,  ..., 0.0539, 0.0000, 0.0319],
        [0.0000, 0.1653, 0.0000,  ..., 0.0714, 0.0000, 0.0466],
        [0.0000, 0.2340, 0.0000,  ..., 0.0974, 0.0000, 0.0696],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(240889.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-146.9189, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-31.9159, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-291.1124, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9934],
        [0.9868],
        [0.9695],
        ...,
        [0.9485],
        [0.9473],
        [0.9461]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357525.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9934],
        [0.9867],
        [0.9694],
        ...,
        [0.9484],
        [0.9472],
        [0.9460]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357509.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.9802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0035,  0.0152,  0.0099,  ...,  0.0136,  0.0131,  0.0139],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1039.8975, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3286, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1807, device='cuda:0')



h[100].sum tensor(18.5763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7851, device='cuda:0')



h[200].sum tensor(22.0008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.5042, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0035, 0.0162, 0.0098,  ..., 0.0134, 0.0138, 0.0142],
        [0.0027, 0.0135, 0.0079,  ..., 0.0108, 0.0114, 0.0117],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30028.3320, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0139, 0.0000,  ..., 0.0108, 0.0000, 0.0026],
        [0.0000, 0.0416, 0.0000,  ..., 0.0218, 0.0000, 0.0104],
        [0.0000, 0.0748, 0.0000,  ..., 0.0350, 0.0000, 0.0205],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(157111.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-62.1831, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.3565, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-162.7843, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9934],
        [0.9867],
        [0.9694],
        ...,
        [0.9484],
        [0.9472],
        [0.9460]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357509.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9934],
        [0.9867],
        [0.9694],
        ...,
        [0.9484],
        [0.9471],
        [0.9459]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357493.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2739],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.1105, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056,  0.0226,  0.0152,  ...,  0.0207,  0.0195,  0.0208],
        [ 0.0136,  0.0518,  0.0359,  ...,  0.0486,  0.0448,  0.0480],
        [ 0.0052,  0.0214,  0.0144,  ...,  0.0195,  0.0185,  0.0197],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1353.6284, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.3814, device='cuda:0')



h[100].sum tensor(19.3579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.1217, device='cuda:0')



h[200].sum tensor(23.6610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.2418, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0274, 0.1094, 0.0743,  ..., 0.1008, 0.0945, 0.1008],
        [0.0211, 0.0863, 0.0579,  ..., 0.0788, 0.0745, 0.0793],
        [0.0219, 0.0872, 0.0591,  ..., 0.0802, 0.0752, 0.0802],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35226.8203, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2471, 0.0000,  ..., 0.1026, 0.0000, 0.0739],
        [0.0000, 0.2404, 0.0000,  ..., 0.1000, 0.0000, 0.0719],
        [0.0000, 0.2121, 0.0000,  ..., 0.0884, 0.0000, 0.0637],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(179576.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-86.7518, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.0403, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-199.7781, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9934],
        [0.9867],
        [0.9694],
        ...,
        [0.9484],
        [0.9471],
        [0.9459]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357493.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9934],
        [0.9866],
        [0.9693],
        ...,
        [0.9483],
        [0.9470],
        [0.9458]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357477.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(217.7480, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0039,  0.0166,  0.0110,  ...,  0.0149,  0.0143,  0.0152],
        [ 0.0020,  0.0098,  0.0061,  ...,  0.0084,  0.0084,  0.0089],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1133.5090, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8613, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.4078, device='cuda:0')



h[100].sum tensor(18.8046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1755, device='cuda:0')



h[200].sum tensor(22.4859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.8881, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0127, 0.0555, 0.0361,  ..., 0.0493, 0.0478, 0.0507],
        [0.0070, 0.0309, 0.0197,  ..., 0.0269, 0.0265, 0.0279],
        [0.0020, 0.0109, 0.0060,  ..., 0.0083, 0.0091, 0.0092],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31649.9746, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1223, 0.0000,  ..., 0.0550, 0.0000, 0.0321],
        [0.0000, 0.0864, 0.0000,  ..., 0.0403, 0.0000, 0.0222],
        [0.0000, 0.0440, 0.0000,  ..., 0.0231, 0.0000, 0.0101],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(163821.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-69.6770, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.9873, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-174.3721, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9934],
        [0.9866],
        [0.9693],
        ...,
        [0.9483],
        [0.9470],
        [0.9458]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357477.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9933],
        [0.9866],
        [0.9692],
        ...,
        [0.9482],
        [0.9469],
        [0.9457]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357461.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(199.0267, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1028.9729, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.7392, device='cuda:0')



h[100].sum tensor(18.5408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.6446, device='cuda:0')



h[200].sum tensor(21.9255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.0063, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29398.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(152171.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-58.8706, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.1419, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-157.9485, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9933],
        [0.9866],
        [0.9692],
        ...,
        [0.9482],
        [0.9469],
        [0.9457]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357461.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9933],
        [0.9866],
        [0.9692],
        ...,
        [0.9481],
        [0.9468],
        [0.9456]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357445.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.9727, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1287.8994, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.7452, device='cuda:0')



h[100].sum tensor(19.1838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9193, device='cuda:0')



h[200].sum tensor(23.2912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.5243, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34946.6641, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.1691e-04, 0.0000e+00,  ..., 5.3467e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 9.4306e-04, 0.0000e+00,  ..., 5.5899e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.4803e-03, 0.0000e+00,  ..., 6.2060e-03, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 3.0102e-03, 0.0000e+00,  ..., 5.9136e-03, 0.0000e+00,
         1.1582e-05],
        [0.0000e+00, 8.2754e-03, 0.0000e+00,  ..., 8.1600e-03, 0.0000e+00,
         5.5981e-04],
        [0.0000e+00, 1.0908e-02, 0.0000e+00,  ..., 9.2831e-03, 0.0000e+00,
         9.8062e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(182244.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-84.8209, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.3187, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-197.7252, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9933],
        [0.9866],
        [0.9692],
        ...,
        [0.9481],
        [0.9468],
        [0.9456]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357445.6562, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 290.0 event: 1450 loss: tensor(615.7450, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9933],
        [0.9865],
        [0.9691],
        ...,
        [0.9480],
        [0.9467],
        [0.9455]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357429.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(164.5758, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0027,  0.0120,  0.0077,  ...,  0.0105,  0.0103,  0.0109],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(811.4362, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.5212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.6686, device='cuda:0')



h[100].sum tensor(17.9934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.6675, device='cuda:0')



h[200].sum tensor(20.7629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.5432, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0026, 0.0131, 0.0076,  ..., 0.0104, 0.0111, 0.0113],
        [0.0094, 0.0395, 0.0258,  ..., 0.0351, 0.0339, 0.0359],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27043.6211, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0280, 0.0000,  ..., 0.0167, 0.0000, 0.0056],
        [0.0000, 0.0587, 0.0000,  ..., 0.0287, 0.0000, 0.0150],
        [0.0000, 0.1326, 0.0000,  ..., 0.0576, 0.0000, 0.0383],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(146732.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-47.8177, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-51.1425, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-141.0546, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9933],
        [0.9865],
        [0.9691],
        ...,
        [0.9480],
        [0.9467],
        [0.9455]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357429.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9933],
        [0.9865],
        [0.9690],
        ...,
        [0.9479],
        [0.9467],
        [0.9454]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357413.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(261.4150, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0091,  0.0355,  0.0244,  ...,  0.0330,  0.0307,  0.0328],
        [ 0.0083,  0.0325,  0.0222,  ...,  0.0302,  0.0281,  0.0300],
        [ 0.0077,  0.0305,  0.0208,  ...,  0.0282,  0.0264,  0.0282],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1416.0620, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.2998, device='cuda:0')



h[100].sum tensor(19.4964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.4140, device='cuda:0')



h[200].sum tensor(23.9551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.2776, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0184, 0.0745, 0.0501,  ..., 0.0681, 0.0643, 0.0684],
        [0.0261, 0.1045, 0.0708,  ..., 0.0962, 0.0902, 0.0963],
        [0.0282, 0.1119, 0.0761,  ..., 0.1033, 0.0967, 0.1032],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36376.0547, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2671, 0.0000,  ..., 0.1106, 0.0000, 0.0800],
        [0.0000, 0.2968, 0.0000,  ..., 0.1223, 0.0000, 0.0893],
        [0.0000, 0.3026, 0.0000,  ..., 0.1247, 0.0000, 0.0909],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(189794.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-92.2729, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.8438, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-208.3291, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9933],
        [0.9865],
        [0.9690],
        ...,
        [0.9479],
        [0.9467],
        [0.9454]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357413.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9933],
        [0.9865],
        [0.9690],
        ...,
        [0.9479],
        [0.9467],
        [0.9454]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357413.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2571],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(208.9922, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0043,  0.0180,  0.0120,  ...,  0.0163,  0.0156,  0.0165],
        [ 0.0016,  0.0081,  0.0049,  ...,  0.0068,  0.0069,  0.0073],
        [ 0.0015,  0.0079,  0.0048,  ...,  0.0066,  0.0067,  0.0071],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1088.1007, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.6274, device='cuda:0')



h[100].sum tensor(18.6799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.9272, device='cuda:0')



h[200].sum tensor(22.2210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.0080, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0073, 0.0360, 0.0223,  ..., 0.0307, 0.0309, 0.0326],
        [0.0109, 0.0489, 0.0314,  ..., 0.0430, 0.0421, 0.0446],
        [0.0085, 0.0401, 0.0252,  ..., 0.0346, 0.0345, 0.0364],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31078.0840, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1339, 0.0000,  ..., 0.0604, 0.0000, 0.0339],
        [0.0000, 0.1402, 0.0000,  ..., 0.0628, 0.0000, 0.0361],
        [0.0000, 0.1228, 0.0000,  ..., 0.0558, 0.0000, 0.0310],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(163126.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-66.7736, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.5469, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-170.1477, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9933],
        [0.9865],
        [0.9690],
        ...,
        [0.9479],
        [0.9467],
        [0.9454]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357413.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9933],
        [0.9865],
        [0.9690],
        ...,
        [0.9478],
        [0.9466],
        [0.9453]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357397.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(220.9767, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0063,  0.0253,  0.0171,  ...,  0.0233,  0.0219,  0.0233],
        [ 0.0026,  0.0117,  0.0075,  ...,  0.0102,  0.0100,  0.0106],
        [ 0.0029,  0.0128,  0.0083,  ...,  0.0113,  0.0110,  0.0117],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1165.1941, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.6955, device='cuda:0')



h[100].sum tensor(18.8689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2671, device='cuda:0')



h[200].sum tensor(22.6224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.2127, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0146, 0.0626, 0.0411,  ..., 0.0561, 0.0539, 0.0573],
        [0.0161, 0.0679, 0.0449,  ..., 0.0612, 0.0586, 0.0623],
        [0.0091, 0.0403, 0.0259,  ..., 0.0354, 0.0347, 0.0367],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32238.3613, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1401, 0.0000,  ..., 0.0616, 0.0000, 0.0388],
        [0.0000, 0.1397, 0.0000,  ..., 0.0613, 0.0000, 0.0390],
        [0.0000, 0.1136, 0.0000,  ..., 0.0509, 0.0000, 0.0311],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(168329.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-71.9794, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.8505, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-178.0670, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9933],
        [0.9865],
        [0.9690],
        ...,
        [0.9478],
        [0.9466],
        [0.9453]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357397.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9933],
        [0.9865],
        [0.9690],
        ...,
        [0.9478],
        [0.9466],
        [0.9453]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357397.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2844],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(302.6323, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0031,  0.0135,  0.0088,  ...,  0.0120,  0.0117,  0.0124],
        [ 0.0018,  0.0089,  0.0055,  ...,  0.0076,  0.0077,  0.0081],
        [ 0.0013,  0.0070,  0.0042,  ...,  0.0058,  0.0060,  0.0063],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1656.0354, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.9735, device='cuda:0')



h[100].sum tensor(20.0898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.5829, device='cuda:0')



h[200].sum tensor(25.2154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.4208, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0107, 0.0462, 0.0301,  ..., 0.0410, 0.0398, 0.0421],
        [0.0088, 0.0395, 0.0253,  ..., 0.0346, 0.0340, 0.0359],
        [0.0160, 0.0677, 0.0448,  ..., 0.0610, 0.0583, 0.0620],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39972.9258, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2097, 0.0000,  ..., 0.0886, 0.0000, 0.0608],
        [0.0000, 0.1695, 0.0000,  ..., 0.0733, 0.0000, 0.0473],
        [0.0000, 0.1821, 0.0000,  ..., 0.0784, 0.0000, 0.0511],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(200088.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-110.0317, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.2840, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-234.7105, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9933],
        [0.9865],
        [0.9690],
        ...,
        [0.9478],
        [0.9466],
        [0.9453]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357397.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9932],
        [0.9864],
        [0.9689],
        ...,
        [0.9477],
        [0.9465],
        [0.9452]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357381.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.4102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(907.8650, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9907, device='cuda:0')



h[100].sum tensor(18.2264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.0882, device='cuda:0')



h[200].sum tensor(21.2578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(18.0344, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0116, 0.0477, 0.0316,  ..., 0.0430, 0.0411, 0.0435],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28232.3633, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1268, 0.0000,  ..., 0.0557, 0.0000, 0.0363],
        [0.0000, 0.0458, 0.0000,  ..., 0.0236, 0.0000, 0.0119],
        [0.0000, 0.0186, 0.0000,  ..., 0.0127, 0.0000, 0.0038],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(149128.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-53.5352, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.1901, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-149.4478, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9932],
        [0.9864],
        [0.9689],
        ...,
        [0.9477],
        [0.9465],
        [0.9452]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357381.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9932],
        [0.9864],
        [0.9689],
        ...,
        [0.9477],
        [0.9465],
        [0.9452]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357381.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.7050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1615.8481, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.4442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0886, device='cuda:0')



h[100].sum tensor(19.9859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3014, device='cuda:0')



h[200].sum tensor(24.9948, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.4229, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0052, 0.0245, 0.0152,  ..., 0.0208, 0.0209, 0.0219],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40976.2930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0140, 0.0000,  ..., 0.0109, 0.0000, 0.0024],
        [0.0000, 0.0267, 0.0000,  ..., 0.0160, 0.0000, 0.0064],
        [0.0000, 0.0759, 0.0000,  ..., 0.0359, 0.0000, 0.0195],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(214745.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-113.9397, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.0631, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-240.9824, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9932],
        [0.9864],
        [0.9689],
        ...,
        [0.9477],
        [0.9465],
        [0.9452]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357381.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9932],
        [0.9864],
        [0.9688],
        ...,
        [0.9476],
        [0.9464],
        [0.9452]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357365.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(322.9968, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0015,  0.0079,  0.0048,  ...,  0.0066,  0.0068,  0.0071],
        [ 0.0016,  0.0083,  0.0051,  ...,  0.0070,  0.0071,  0.0075],
        [ 0.0037,  0.0158,  0.0104,  ...,  0.0141,  0.0136,  0.0144],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1811.2700, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.7885, device='cuda:0')



h[100].sum tensor(20.4672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.1605, device='cuda:0')



h[200].sum tensor(26.0172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.4678, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0270, 0.0170,  ..., 0.0232, 0.0231, 0.0242],
        [0.0120, 0.0530, 0.0343,  ..., 0.0469, 0.0456, 0.0484],
        [0.0092, 0.0426, 0.0270,  ..., 0.0371, 0.0367, 0.0388],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44540.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1227, 0.0000,  ..., 0.0549, 0.0000, 0.0328],
        [0.0000, 0.1303, 0.0000,  ..., 0.0584, 0.0000, 0.0341],
        [0.0000, 0.1337, 0.0000,  ..., 0.0596, 0.0000, 0.0353],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(234546.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-131.6048, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.2944, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-267.5091, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9932],
        [0.9864],
        [0.9688],
        ...,
        [0.9476],
        [0.9464],
        [0.9452]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357365.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9932],
        [0.9864],
        [0.9688],
        ...,
        [0.9475],
        [0.9463],
        [0.9451]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357349.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.6172, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0035,  0.0151,  0.0099,  ...,  0.0135,  0.0131,  0.0139],
        [ 0.0082,  0.0322,  0.0220,  ...,  0.0299,  0.0279,  0.0298],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1310.7543, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0089, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8026, device='cuda:0')



h[100].sum tensor(19.2212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9376, device='cuda:0')



h[200].sum tensor(23.3706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.5890, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0091, 0.0386, 0.0252,  ..., 0.0343, 0.0332, 0.0351],
        [0.0160, 0.0658, 0.0439,  ..., 0.0597, 0.0567, 0.0603],
        [0.0123, 0.0520, 0.0342,  ..., 0.0466, 0.0448, 0.0475],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34491.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1444, 0.0000,  ..., 0.0630, 0.0000, 0.0409],
        [0.0000, 0.1824, 0.0000,  ..., 0.0778, 0.0000, 0.0528],
        [0.0000, 0.1683, 0.0000,  ..., 0.0724, 0.0000, 0.0482],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(179212., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-83.0718, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.6958, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-194.4999, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9932],
        [0.9864],
        [0.9688],
        ...,
        [0.9475],
        [0.9463],
        [0.9451]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357349.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9932],
        [0.9863],
        [0.9687],
        ...,
        [0.9474],
        [0.9462],
        [0.9450]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357333.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0034,  0.0149,  0.0097,  ...,  0.0133,  0.0128,  0.0136],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1753.9285, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.1001, device='cuda:0')



h[100].sum tensor(20.3165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9414, device='cuda:0')



h[200].sum tensor(25.6971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.6914, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0122, 0.0539, 0.0350,  ..., 0.0478, 0.0464, 0.0492],
        [0.0027, 0.0134, 0.0078,  ..., 0.0107, 0.0113, 0.0116],
        [0.0034, 0.0159, 0.0096,  ..., 0.0131, 0.0135, 0.0139],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41288.4648, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0955, 0.0000,  ..., 0.0441, 0.0000, 0.0249],
        [0.0000, 0.0584, 0.0000,  ..., 0.0290, 0.0000, 0.0143],
        [0.0000, 0.0742, 0.0000,  ..., 0.0352, 0.0000, 0.0193],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(208619.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-115.8810, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.3512, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-243.8885, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9932],
        [0.9863],
        [0.9687],
        ...,
        [0.9474],
        [0.9462],
        [0.9450]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357333.9062, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 300.0 event: 1500 loss: tensor(434.1452, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9932],
        [0.9863],
        [0.9686],
        ...,
        [0.9474],
        [0.9461],
        [0.9449]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357317.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.1709, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1413.7809, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.5000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1889, device='cuda:0')



h[100].sum tensor(19.4699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3787, device='cuda:0')



h[200].sum tensor(23.8989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.1525, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36872.8008, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0015, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0069, 0.0000, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(190951.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-94.7332, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.4535, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-211.8006, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9932],
        [0.9863],
        [0.9686],
        ...,
        [0.9474],
        [0.9461],
        [0.9449]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357317.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9931],
        [0.9863],
        [0.9686],
        ...,
        [0.9473],
        [0.9460],
        [0.9448]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357301.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.9764, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0066,  0.0264,  0.0179,  ...,  0.0243,  0.0228,  0.0244],
        [ 0.0037,  0.0160,  0.0105,  ...,  0.0143,  0.0138,  0.0146],
        [ 0.0016,  0.0081,  0.0049,  ...,  0.0068,  0.0069,  0.0073],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(995.6476, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1999, device='cuda:0')



h[100].sum tensor(18.4318, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4730, device='cuda:0')



h[200].sum tensor(21.6939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.3981, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0184, 0.0763, 0.0508,  ..., 0.0692, 0.0658, 0.0700],
        [0.0177, 0.0740, 0.0492,  ..., 0.0670, 0.0638, 0.0679],
        [0.0152, 0.0648, 0.0427,  ..., 0.0582, 0.0558, 0.0594],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29160.6367, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2619, 0.0000,  ..., 0.1094, 0.0000, 0.0769],
        [0.0000, 0.2372, 0.0000,  ..., 0.0999, 0.0000, 0.0687],
        [0.0000, 0.2155, 0.0000,  ..., 0.0916, 0.0000, 0.0614],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(153063.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-57.4473, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.3288, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-156.2052, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9931],
        [0.9863],
        [0.9686],
        ...,
        [0.9473],
        [0.9460],
        [0.9448]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357301.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9931],
        [0.9862],
        [0.9685],
        ...,
        [0.9472],
        [0.9459],
        [0.9447]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357285.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0052,  0.0214,  0.0144,  ...,  0.0195,  0.0185,  0.0197],
        [ 0.0055,  0.0225,  0.0151,  ...,  0.0205,  0.0194,  0.0207],
        [ 0.0077,  0.0304,  0.0208,  ...,  0.0282,  0.0263,  0.0281],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1678.3359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.1755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.6980, device='cuda:0')



h[100].sum tensor(20.1172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4953, device='cuda:0')



h[200].sum tensor(25.2737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.1101, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0295, 0.1168, 0.0796,  ..., 0.1080, 0.1010, 0.1078],
        [0.0219, 0.0872, 0.0591,  ..., 0.0802, 0.0753, 0.0802],
        [0.0115, 0.0475, 0.0315,  ..., 0.0428, 0.0409, 0.0433],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0019, 0.0103, 0.0057,  ..., 0.0079, 0.0087, 0.0088]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40702.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3552, 0.0000,  ..., 0.1439, 0.0000, 0.1093],
        [0.0000, 0.2689, 0.0000,  ..., 0.1107, 0.0000, 0.0815],
        [0.0000, 0.1771, 0.0000,  ..., 0.0752, 0.0000, 0.0523],
        ...,
        [0.0000, 0.0018, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0085, 0.0000, 0.0016],
        [0.0000, 0.0351, 0.0000,  ..., 0.0190, 0.0000, 0.0080]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(208363.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-112.2351, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.2895, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-239.0508, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9931],
        [0.9862],
        [0.9685],
        ...,
        [0.9472],
        [0.9459],
        [0.9447]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357285.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9931],
        [0.9862],
        [0.9684],
        ...,
        [0.9471],
        [0.9458],
        [0.9446]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357269.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4307],
        [0.2561],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.4669, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056,  0.0226,  0.0152,  ...,  0.0206,  0.0195,  0.0208],
        [ 0.0030,  0.0133,  0.0086,  ...,  0.0118,  0.0114,  0.0121],
        [ 0.0016,  0.0081,  0.0049,  ...,  0.0068,  0.0069,  0.0073],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1166.9999, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5610, device='cuda:0')



h[100].sum tensor(18.8500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2243, device='cuda:0')



h[200].sum tensor(22.5822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.0609, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0156, 0.0662, 0.0437,  ..., 0.0596, 0.0571, 0.0607],
        [0.0123, 0.0541, 0.0351,  ..., 0.0480, 0.0466, 0.0494],
        [0.0062, 0.0317, 0.0193,  ..., 0.0266, 0.0272, 0.0286],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32025.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1451, 0.0000,  ..., 0.0637, 0.0000, 0.0397],
        [0.0000, 0.1308, 0.0000,  ..., 0.0583, 0.0000, 0.0346],
        [0.0000, 0.1076, 0.0000,  ..., 0.0495, 0.0000, 0.0264],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(165546.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-71.4160, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.7594, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-176.8864, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9931],
        [0.9862],
        [0.9684],
        ...,
        [0.9471],
        [0.9458],
        [0.9446]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357269.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9931],
        [0.9862],
        [0.9684],
        ...,
        [0.9470],
        [0.9457],
        [0.9445]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357253.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.2531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0052,  0.0214,  0.0143,  ...,  0.0195,  0.0184,  0.0197],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1300.9042, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5919, device='cuda:0')



h[100].sum tensor(19.1776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.8705, device='cuda:0')



h[200].sum tensor(23.2781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.3514, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0069, 0.0305, 0.0194,  ..., 0.0265, 0.0262, 0.0275],
        [0.0137, 0.0573, 0.0380,  ..., 0.0517, 0.0494, 0.0525],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33750.6836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0290, 0.0000,  ..., 0.0169, 0.0000, 0.0072],
        [0.0000, 0.0948, 0.0000,  ..., 0.0430, 0.0000, 0.0262],
        [0.0000, 0.1686, 0.0000,  ..., 0.0720, 0.0000, 0.0491],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(176340.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-79.1826, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.4953, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-188.9571, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9931],
        [0.9862],
        [0.9684],
        ...,
        [0.9470],
        [0.9457],
        [0.9445]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357253.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9931],
        [0.9861],
        [0.9683],
        ...,
        [0.9469],
        [0.9456],
        [0.9444]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357237.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.7903, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0043,  0.0181,  0.0120,  ...,  0.0164,  0.0156,  0.0166],
        [ 0.0038,  0.0162,  0.0107,  ...,  0.0146,  0.0140,  0.0149],
        [ 0.0039,  0.0164,  0.0108,  ...,  0.0148,  0.0142,  0.0150],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1568.8535, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.2941, device='cuda:0')



h[100].sum tensor(19.8353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0485, device='cuda:0')



h[200].sum tensor(24.6750, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.5267, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0169, 0.0708, 0.0469,  ..., 0.0639, 0.0610, 0.0649],
        [0.0106, 0.0479, 0.0307,  ..., 0.0420, 0.0412, 0.0436],
        [0.0109, 0.0490, 0.0315,  ..., 0.0431, 0.0422, 0.0447],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40664.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1914, 0.0000,  ..., 0.0812, 0.0000, 0.0559],
        [0.0000, 0.1413, 0.0000,  ..., 0.0621, 0.0000, 0.0390],
        [0.0000, 0.1194, 0.0000,  ..., 0.0535, 0.0000, 0.0322],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(214445.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-112.6233, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.8202, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-239.4916, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9931],
        [0.9861],
        [0.9683],
        ...,
        [0.9469],
        [0.9456],
        [0.9444]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357237.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9931],
        [0.9861],
        [0.9682],
        ...,
        [0.9468],
        [0.9455],
        [0.9443]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357221.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3093],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.0848, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0020,  0.0097,  0.0060,  ...,  0.0083,  0.0083,  0.0088],
        [ 0.0012,  0.0065,  0.0038,  ...,  0.0053,  0.0056,  0.0058],
        [ 0.0037,  0.0158,  0.0104,  ...,  0.0141,  0.0136,  0.0144],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(999.1870, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2096, device='cuda:0')



h[100].sum tensor(18.4278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4761, device='cuda:0')



h[200].sum tensor(21.6855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.4090, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0059, 0.0270, 0.0169,  ..., 0.0231, 0.0231, 0.0242],
        [0.0155, 0.0657, 0.0433,  ..., 0.0590, 0.0566, 0.0602],
        [0.0087, 0.0390, 0.0249,  ..., 0.0341, 0.0335, 0.0354],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29352.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0975, 0.0000,  ..., 0.0447, 0.0000, 0.0257],
        [0.0000, 0.1457, 0.0000,  ..., 0.0641, 0.0000, 0.0399],
        [0.0000, 0.1256, 0.0000,  ..., 0.0561, 0.0000, 0.0337],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(155406.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-59.0030, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.0895, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-157.6575, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9931],
        [0.9861],
        [0.9682],
        ...,
        [0.9468],
        [0.9455],
        [0.9443]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357221.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9930],
        [0.9861],
        [0.9682],
        ...,
        [0.9467],
        [0.9455],
        [0.9442]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357205.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(316.2621, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0045,  0.0187,  0.0125,  ...,  0.0169,  0.0161,  0.0172],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1796.9155, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.6184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.1883, device='cuda:0')



h[100].sum tensor(20.3895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9695, device='cuda:0')



h[200].sum tensor(25.8520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.7908, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0036, 0.0165, 0.0100,  ..., 0.0137, 0.0140, 0.0145],
        [0.0045, 0.0198, 0.0124,  ..., 0.0168, 0.0169, 0.0175],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42992.8789, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0904, 0.0000,  ..., 0.0414, 0.0000, 0.0248],
        [0.0000, 0.0600, 0.0000,  ..., 0.0293, 0.0000, 0.0156],
        [0.0000, 0.0383, 0.0000,  ..., 0.0207, 0.0000, 0.0090],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(223097.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-124.0698, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-36.5518, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-256.5320, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9930],
        [0.9861],
        [0.9682],
        ...,
        [0.9467],
        [0.9455],
        [0.9442]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357205.8438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9930],
        [0.9860],
        [0.9681],
        ...,
        [0.9466],
        [0.9454],
        [0.9441]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357189.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(243.2311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1334.7504, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.6791, device='cuda:0')



h[100].sum tensor(19.2483, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.8982, device='cuda:0')



h[200].sum tensor(23.4282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.4497, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0015, 0.0088, 0.0046,  ..., 0.0064, 0.0074, 0.0074],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35436.9141, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0090, 0.0000, 0.0015],
        [0.0000, 0.0284, 0.0000,  ..., 0.0169, 0.0000, 0.0056],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(184160.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-87.8881, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.8744, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-201.2273, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9930],
        [0.9860],
        [0.9681],
        ...,
        [0.9466],
        [0.9454],
        [0.9441]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357189.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9930],
        [0.9860],
        [0.9680],
        ...,
        [0.9465],
        [0.9453],
        [0.9440]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357173.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2646],
        [0.0000],
        [0.2510],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.5781, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0037,  0.0158,  0.0104,  ...,  0.0142,  0.0136,  0.0145],
        [ 0.0019,  0.0094,  0.0059,  ...,  0.0081,  0.0081,  0.0085],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1575.3848, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.4534, device='cuda:0')



h[100].sum tensor(19.8366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0992, device='cuda:0')



h[200].sum tensor(24.6778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.7065, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0078, 0.0377, 0.0235,  ..., 0.0323, 0.0324, 0.0342],
        [0.0043, 0.0231, 0.0137,  ..., 0.0189, 0.0197, 0.0206],
        [0.0132, 0.0572, 0.0374,  ..., 0.0510, 0.0493, 0.0524],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40183.6055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0838, 0.0000,  ..., 0.0399, 0.0000, 0.0199],
        [0.0000, 0.0915, 0.0000,  ..., 0.0429, 0.0000, 0.0222],
        [0.0000, 0.1506, 0.0000,  ..., 0.0661, 0.0000, 0.0407],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(210048.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-110.4694, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.5881, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-235.5772, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9930],
        [0.9860],
        [0.9680],
        ...,
        [0.9465],
        [0.9453],
        [0.9440]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357173.7500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 310.0 event: 1550 loss: tensor(641.1583, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9930],
        [0.9860],
        [0.9680],
        ...,
        [0.9464],
        [0.9452],
        [0.9439]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357157.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(191.1257, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(993.3958, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.0349, device='cuda:0')



h[100].sum tensor(18.4035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4205, device='cuda:0')



h[200].sum tensor(21.6339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.2121, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29042.0742, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0027, 0.0000,  ..., 0.0064, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(153049.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-57.1759, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.5039, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-155.2485, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9930],
        [0.9860],
        [0.9680],
        ...,
        [0.9464],
        [0.9452],
        [0.9439]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357157.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9930],
        [0.9859],
        [0.9679],
        ...,
        [0.9463],
        [0.9451],
        [0.9439]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357141.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(432.5950, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2486.0984, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.1915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.5570, device='cuda:0')



h[100].sum tensor(22.0642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(12.2688, device='cuda:0')



h[200].sum tensor(29.4090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.4847, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52776.5703, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(262086.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-171.5388, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-26.9057, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-328.3034, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9930],
        [0.9859],
        [0.9679],
        ...,
        [0.9463],
        [0.9451],
        [0.9439]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357141.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9929],
        [0.9859],
        [0.9678],
        ...,
        [0.9463],
        [0.9450],
        [0.9438]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357125.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(166.4302, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0083,  0.0050,  ...,  0.0070,  0.0071,  0.0075],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(826.4531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.5298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.8338, device='cuda:0')



h[100].sum tensor(17.9892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.7201, device='cuda:0')



h[200].sum tensor(20.7540, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.7296, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0016, 0.0094, 0.0050,  ..., 0.0069, 0.0079, 0.0079],
        [0.0055, 0.0254, 0.0159,  ..., 0.0217, 0.0218, 0.0228],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26532.1914, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0169, 0.0000,  ..., 0.0121, 0.0000, 0.0037],
        [0.0000, 0.0502, 0.0000,  ..., 0.0257, 0.0000, 0.0120],
        [0.0000, 0.0938, 0.0000,  ..., 0.0434, 0.0000, 0.0242],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(141942.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-45.3369, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-51.4816, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-137.5086, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9929],
        [0.9859],
        [0.9678],
        ...,
        [0.9463],
        [0.9450],
        [0.9438]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357125.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9929],
        [0.9859],
        [0.9678],
        ...,
        [0.9462],
        [0.9449],
        [0.9437]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357109.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2493],
        [0.6694],
        [0.2465],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(221.8746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0087,  0.0340,  0.0233,  ...,  0.0316,  0.0294,  0.0314],
        [ 0.0073,  0.0290,  0.0197,  ...,  0.0268,  0.0250,  0.0267],
        [ 0.0070,  0.0278,  0.0189,  ...,  0.0256,  0.0240,  0.0256],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1191.9180, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7756, device='cuda:0')



h[100].sum tensor(18.8825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2926, device='cuda:0')



h[200].sum tensor(22.6512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.3030, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0221, 0.0900, 0.0606,  ..., 0.0823, 0.0777, 0.0828],
        [0.0313, 0.1236, 0.0844,  ..., 0.1144, 0.1068, 0.1140],
        [0.0210, 0.0858, 0.0577,  ..., 0.0784, 0.0741, 0.0789],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32483.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2191, 0.0000,  ..., 0.0924, 0.0000, 0.0634],
        [0.0000, 0.2536, 0.0000,  ..., 0.1056, 0.0000, 0.0749],
        [0.0000, 0.2128, 0.0000,  ..., 0.0898, 0.0000, 0.0619],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(168039.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-73.9027, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.2698, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-180.2663, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9929],
        [0.9859],
        [0.9678],
        ...,
        [0.9462],
        [0.9449],
        [0.9437]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357109.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9929],
        [0.9858],
        [0.9677],
        ...,
        [0.9461],
        [0.9448],
        [0.9436]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357093.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.6370, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1677.4385, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.3500, device='cuda:0')



h[100].sum tensor(20.0682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3845, device='cuda:0')



h[200].sum tensor(25.1695, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.7176, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0040, 0.0198, 0.0119,  ..., 0.0163, 0.0169, 0.0176],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41795.9609, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0142, 0.0000,  ..., 0.0112, 0.0000, 0.0020],
        [0.0000, 0.0353, 0.0000,  ..., 0.0198, 0.0000, 0.0078],
        [0.0000, 0.0747, 0.0000,  ..., 0.0358, 0.0000, 0.0183],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(218154.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-117.7190, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.3020, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-246.9049, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9929],
        [0.9858],
        [0.9677],
        ...,
        [0.9461],
        [0.9448],
        [0.9436]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357093.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9929],
        [0.9858],
        [0.9676],
        ...,
        [0.9460],
        [0.9447],
        [0.9435]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357077.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3555],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.8713, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0063,  0.0252,  0.0170,  ...,  0.0231,  0.0217,  0.0232],
        [ 0.0033,  0.0145,  0.0095,  ...,  0.0130,  0.0125,  0.0133],
        [ 0.0050,  0.0206,  0.0138,  ...,  0.0187,  0.0178,  0.0189],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0028,  0.0127,  0.0082,  ...,  0.0112,  0.0109,  0.0116],
        [ 0.0028,  0.0127,  0.0082,  ...,  0.0112,  0.0109,  0.0116]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1233.1650, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5073, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.2209, device='cuda:0')



h[100].sum tensor(18.9777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4343, device='cuda:0')



h[200].sum tensor(22.8534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.8052, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0078, 0.0358, 0.0227,  ..., 0.0310, 0.0307, 0.0324],
        [0.0180, 0.0749, 0.0499,  ..., 0.0679, 0.0646, 0.0687],
        [0.0098, 0.0432, 0.0279,  ..., 0.0381, 0.0371, 0.0393],
        ...,
        [0.0047, 0.0223, 0.0138,  ..., 0.0188, 0.0191, 0.0199],
        [0.0047, 0.0223, 0.0138,  ..., 0.0188, 0.0191, 0.0199],
        [0.0047, 0.0223, 0.0138,  ..., 0.0188, 0.0191, 0.0199]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32229.2129, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1238, 0.0000,  ..., 0.0554, 0.0000, 0.0333],
        [0.0000, 0.1486, 0.0000,  ..., 0.0650, 0.0000, 0.0413],
        [0.0000, 0.1204, 0.0000,  ..., 0.0538, 0.0000, 0.0326],
        ...,
        [0.0000, 0.0463, 0.0000,  ..., 0.0236, 0.0000, 0.0109],
        [0.0000, 0.0561, 0.0000,  ..., 0.0276, 0.0000, 0.0135],
        [0.0000, 0.0561, 0.0000,  ..., 0.0276, 0.0000, 0.0135]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(163572.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-72.7002, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.2659, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-178.7254, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9929],
        [0.9858],
        [0.9676],
        ...,
        [0.9460],
        [0.9447],
        [0.9435]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357077.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9929],
        [0.9858],
        [0.9675],
        ...,
        [0.9459],
        [0.9446],
        [0.9434]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357061.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(192.1000, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1007.1713, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.1218, device='cuda:0')



h[100].sum tensor(18.4224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4481, device='cuda:0')



h[200].sum tensor(21.6741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.3100, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28945.8555, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0079, 0.0000, 0.0005],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(150540.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-56.9989, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.2664, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-154.9739, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9929],
        [0.9858],
        [0.9675],
        ...,
        [0.9459],
        [0.9446],
        [0.9434]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357061.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9928],
        [0.9857],
        [0.9675],
        ...,
        [0.9458],
        [0.9445],
        [0.9433]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357045.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.3430],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(227.8774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [ 0.0012,  0.0066,  0.0039,  ...,  0.0054,  0.0057,  0.0059],
        [ 0.0023,  0.0107,  0.0068,  ...,  0.0093,  0.0092,  0.0097],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1233.0320, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.5199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.3106, device='cuda:0')



h[100].sum tensor(18.9715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4628, device='cuda:0')



h[200].sum tensor(22.8403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.9064, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0123, 0.0537, 0.0350,  ..., 0.0478, 0.0463, 0.0491],
        [0.0040, 0.0199, 0.0120,  ..., 0.0165, 0.0170, 0.0177],
        [0.0021, 0.0111, 0.0063,  ..., 0.0086, 0.0094, 0.0095]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34755.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0220, 0.0000,  ..., 0.0143, 0.0000, 0.0049],
        [0.0000, 0.0046, 0.0000,  ..., 0.0071, 0.0000, 0.0004],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.1140, 0.0000,  ..., 0.0508, 0.0000, 0.0306],
        [0.0000, 0.0736, 0.0000,  ..., 0.0345, 0.0000, 0.0190],
        [0.0000, 0.0376, 0.0000,  ..., 0.0199, 0.0000, 0.0088]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(185936.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-84.7174, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.1355, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-196.7670, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9928],
        [0.9857],
        [0.9675],
        ...,
        [0.9458],
        [0.9445],
        [0.9433]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357045.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9928],
        [0.9857],
        [0.9674],
        ...,
        [0.9457],
        [0.9444],
        [0.9432]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357029.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(172.6564, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(880.3490, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.2867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3888, device='cuda:0')



h[100].sum tensor(18.1081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.8967, device='cuda:0')



h[200].sum tensor(21.0063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.3555, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27313.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0120, 0.0000,  ..., 0.0096, 0.0000, 0.0018],
        [0.0000, 0.0042, 0.0000,  ..., 0.0064, 0.0000, 0.0003],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(144409.7344, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-48.7920, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-50.9477, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-142.8830, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9928],
        [0.9857],
        [0.9674],
        ...,
        [0.9457],
        [0.9444],
        [0.9432]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357029.1875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9928],
        [0.9856],
        [0.9673],
        ...,
        [0.9456],
        [0.9443],
        [0.9431]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(357013.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(401.0800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0045,  0.0186,  0.0123,  ...,  0.0168,  0.0160,  0.0170]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2343.7979, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-4.9906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.7481, device='cuda:0')



h[100].sum tensor(21.6737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(11.3750, device='cuda:0')



h[200].sum tensor(28.5796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.3168, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0042, 0.0185, 0.0115,  ..., 0.0157, 0.0158, 0.0164],
        [0.0075, 0.0323, 0.0209,  ..., 0.0284, 0.0278, 0.0293]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52726.7617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0090, 0.0000, 0.0015],
        [0.0000, 0.0383, 0.0000,  ..., 0.0206, 0.0000, 0.0096],
        ...,
        [0.0000, 0.0147, 0.0000,  ..., 0.0105, 0.0000, 0.0034],
        [0.0000, 0.0519, 0.0000,  ..., 0.0252, 0.0000, 0.0140],
        [0.0000, 0.0971, 0.0000,  ..., 0.0431, 0.0000, 0.0277]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(271016.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-170.7579, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-27.8832, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-326.6989, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9928],
        [0.9856],
        [0.9673],
        ...,
        [0.9456],
        [0.9443],
        [0.9431]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(357013.0938, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 320.0 event: 1600 loss: tensor(630.2165, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9928],
        [0.9856],
        [0.9673],
        ...,
        [0.9455],
        [0.9443],
        [0.9430]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356997., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(165.5991, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(839.8752, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.4974, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.7598, device='cuda:0')



h[100].sum tensor(18.0051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.6965, device='cuda:0')



h[200].sum tensor(20.7876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.6461, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0014, 0.0107, 0.0054,  ..., 0.0076, 0.0090, 0.0091],
        [0.0007, 0.0062, 0.0027,  ..., 0.0038, 0.0051, 0.0049],
        [0.0098, 0.0411, 0.0270,  ..., 0.0367, 0.0353, 0.0373],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26069.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0356, 0.0000,  ..., 0.0202, 0.0000, 0.0061],
        [0.0000, 0.0517, 0.0000,  ..., 0.0264, 0.0000, 0.0116],
        [0.0000, 0.1142, 0.0000,  ..., 0.0509, 0.0000, 0.0314],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(136749.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-43.4778, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-51.6573, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-134.5033, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9928],
        [0.9856],
        [0.9673],
        ...,
        [0.9455],
        [0.9443],
        [0.9430]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356997., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9928],
        [0.9856],
        [0.9672],
        ...,
        [0.9454],
        [0.9442],
        [0.9429]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356980.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(370.0095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2147.6172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.9886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.9788, device='cuda:0')



h[100].sum tensor(21.1860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.4938, device='cuda:0')



h[200].sum tensor(27.5437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.1936, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0057, 0.0262, 0.0164,  ..., 0.0224, 0.0225, 0.0235],
        [0.0059, 0.0270, 0.0169,  ..., 0.0232, 0.0231, 0.0242],
        [0.0019, 0.0103, 0.0056,  ..., 0.0077, 0.0086, 0.0087],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(47066.1797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0739, 0.0000,  ..., 0.0357, 0.0000, 0.0178],
        [0.0000, 0.0737, 0.0000,  ..., 0.0356, 0.0000, 0.0179],
        [0.0000, 0.0465, 0.0000,  ..., 0.0244, 0.0000, 0.0104],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(244240.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-142.8809, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-33.2307, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-285.4041, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9928],
        [0.9856],
        [0.9672],
        ...,
        [0.9454],
        [0.9442],
        [0.9429]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356980.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9927],
        [0.9855],
        [0.9671],
        ...,
        [0.9453],
        [0.9441],
        [0.9428]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356964.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(350.7477, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0017,  0.0084,  0.0051,  ...,  0.0071,  0.0072,  0.0076],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2048.9460, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.4892, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.2620, device='cuda:0')



h[100].sum tensor(20.9413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.9475, device='cuda:0')



h[200].sum tensor(27.0241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.2573, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0061, 0.0276, 0.0174,  ..., 0.0237, 0.0236, 0.0248],
        [0.0016, 0.0095, 0.0051,  ..., 0.0070, 0.0080, 0.0080],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(49322.2227, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0868, 0.0000,  ..., 0.0407, 0.0000, 0.0219],
        [0.0000, 0.0554, 0.0000,  ..., 0.0279, 0.0000, 0.0131],
        [0.0000, 0.0424, 0.0000,  ..., 0.0224, 0.0000, 0.0101],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(261232.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-154.7847, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-30.1744, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-303.1491, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9927],
        [0.9855],
        [0.9671],
        ...,
        [0.9453],
        [0.9441],
        [0.9428]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356964.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9927],
        [0.9855],
        [0.9671],
        ...,
        [0.9452],
        [0.9440],
        [0.9427]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356948.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(231.5481, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0035,  0.0150,  0.0099,  ...,  0.0134,  0.0130,  0.0138],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1277.2689, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.3354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.6378, device='cuda:0')



h[100].sum tensor(19.0616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5669, device='cuda:0')



h[200].sum tensor(23.0317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.2753, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0167, 0.0684, 0.0458,  ..., 0.0622, 0.0590, 0.0627],
        [0.0057, 0.0261, 0.0163,  ..., 0.0223, 0.0224, 0.0234],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34865.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2260, 0.0000,  ..., 0.0946, 0.0000, 0.0669],
        [0.0000, 0.1577, 0.0000,  ..., 0.0679, 0.0000, 0.0454],
        [0.0000, 0.1095, 0.0000,  ..., 0.0487, 0.0000, 0.0309],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(184472.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-84.6589, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.5712, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-196.7760, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9927],
        [0.9855],
        [0.9671],
        ...,
        [0.9452],
        [0.9440],
        [0.9427]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356948.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9927],
        [0.9855],
        [0.9670],
        ...,
        [0.9452],
        [0.9439],
        [0.9426]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356932.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(356.6199, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2082.1812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.3424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.7854, device='cuda:0')



h[100].sum tensor(21.0131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.1140, device='cuda:0')



h[200].sum tensor(27.1765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.8476, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(50679.0273, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0018, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0008, 0.0000,  ..., 0.0055, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(276865.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-161.4723, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-29.0649, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-312.7638, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9927],
        [0.9855],
        [0.9670],
        ...,
        [0.9452],
        [0.9439],
        [0.9426]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356932.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9927],
        [0.9854],
        [0.9669],
        ...,
        [0.9451],
        [0.9438],
        [0.9426]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356916.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.5977],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(236.3813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0012,  0.0068,  0.0040,  ...,  0.0056,  0.0059,  0.0061],
        [ 0.0044,  0.0183,  0.0121,  ...,  0.0165,  0.0158,  0.0168],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1288.6589, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.0686, device='cuda:0')



h[100].sum tensor(19.0834, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7040, device='cuda:0')



h[200].sum tensor(23.0779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.7612, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0239, 0.0965, 0.0651,  ..., 0.0885, 0.0833, 0.0888],
        [0.0061, 0.0278, 0.0175,  ..., 0.0239, 0.0238, 0.0249],
        [0.0043, 0.0192, 0.0120,  ..., 0.0163, 0.0164, 0.0170],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33360.9297, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1877, 0.0000,  ..., 0.0800, 0.0000, 0.0540],
        [0.0000, 0.1086, 0.0000,  ..., 0.0487, 0.0000, 0.0298],
        [0.0000, 0.0611, 0.0000,  ..., 0.0296, 0.0000, 0.0159],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(173752.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-77.5935, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.6974, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.2613, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9927],
        [0.9854],
        [0.9669],
        ...,
        [0.9451],
        [0.9438],
        [0.9426]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356916.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9927],
        [0.9854],
        [0.9669],
        ...,
        [0.9450],
        [0.9437],
        [0.9425]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356900.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(303.7697, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0022,  0.0103,  0.0065,  ...,  0.0089,  0.0089,  0.0094],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1707.3269, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.0748, device='cuda:0')



h[100].sum tensor(20.0953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.6152, device='cuda:0')



h[200].sum tensor(25.2272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.5351, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0077, 0.0374, 0.0233,  ..., 0.0320, 0.0321, 0.0339],
        [0.0017, 0.0097, 0.0052,  ..., 0.0071, 0.0081, 0.0081],
        [0.0079, 0.0341, 0.0220,  ..., 0.0300, 0.0293, 0.0309],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38144.9961, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0561, 0.0000,  ..., 0.0285, 0.0000, 0.0129],
        [0.0000, 0.0603, 0.0000,  ..., 0.0295, 0.0000, 0.0153],
        [0.0000, 0.1114, 0.0000,  ..., 0.0494, 0.0000, 0.0315],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(188140.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-100.9430, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.0388, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-221.3436, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9927],
        [0.9854],
        [0.9669],
        ...,
        [0.9450],
        [0.9437],
        [0.9425]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356900.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9927],
        [0.9854],
        [0.9668],
        ...,
        [0.9449],
        [0.9436],
        [0.9424]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356884.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.5889, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0019,  0.0092,  0.0057,  ...,  0.0079,  0.0079,  0.0083],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1172.4258, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3045, device='cuda:0')



h[100].sum tensor(18.7959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1426, device='cuda:0')



h[200].sum tensor(22.4674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.7716, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0063, 0.0285, 0.0180,  ..., 0.0246, 0.0244, 0.0257],
        [0.0019, 0.0104, 0.0057,  ..., 0.0078, 0.0087, 0.0088],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32655.2480, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1383, 0.0000,  ..., 0.0605, 0.0000, 0.0391],
        [0.0000, 0.0771, 0.0000,  ..., 0.0362, 0.0000, 0.0207],
        [0.0000, 0.0433, 0.0000,  ..., 0.0224, 0.0000, 0.0115],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(172133.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-74.3311, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.1969, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-181.3770, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9927],
        [0.9854],
        [0.9668],
        ...,
        [0.9449],
        [0.9436],
        [0.9424]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356884.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9926],
        [0.9853],
        [0.9667],
        ...,
        [0.9448],
        [0.9435],
        [0.9423]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356868., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(260.6846, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1466.7212, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.2347, device='cuda:0')



h[100].sum tensor(19.5055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.3932, device='cuda:0')



h[200].sum tensor(23.9744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.2041, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0031, 0.0169, 0.0098,  ..., 0.0135, 0.0144, 0.0148],
        [0.0007, 0.0062, 0.0027,  ..., 0.0038, 0.0051, 0.0049],
        [0.0017, 0.0098, 0.0053,  ..., 0.0072, 0.0082, 0.0082],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38224.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0573, 0.0000,  ..., 0.0292, 0.0000, 0.0122],
        [0.0000, 0.0482, 0.0000,  ..., 0.0254, 0.0000, 0.0099],
        [0.0000, 0.0519, 0.0000,  ..., 0.0268, 0.0000, 0.0111],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(202571.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-101.2768, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.2404, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-221.4805, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9926],
        [0.9853],
        [0.9667],
        ...,
        [0.9448],
        [0.9435],
        [0.9423]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356868., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9926],
        [0.9853],
        [0.9667],
        ...,
        [0.9447],
        [0.9434],
        [0.9422]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356851.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(283.3627, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0013,  0.0071,  0.0043,  ...,  0.0059,  0.0061,  0.0064],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1621.1399, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.6698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.2560, device='cuda:0')



h[100].sum tensor(19.8756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0364, device='cuda:0')



h[200].sum tensor(24.7606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.4838, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0039, 0.0197, 0.0118,  ..., 0.0162, 0.0168, 0.0174],
        [0.0050, 0.0253, 0.0153,  ..., 0.0211, 0.0217, 0.0227],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39110.3555, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0620, 0.0000,  ..., 0.0306, 0.0000, 0.0149],
        [0.0000, 0.0716, 0.0000,  ..., 0.0350, 0.0000, 0.0164],
        [0.0000, 0.0891, 0.0000,  ..., 0.0422, 0.0000, 0.0210],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(201514.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-105.7441, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.2077, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-228.2174, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9926],
        [0.9853],
        [0.9667],
        ...,
        [0.9447],
        [0.9434],
        [0.9422]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356851.8750, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 330.0 event: 1650 loss: tensor(634.4520, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9926],
        [0.9853],
        [0.9666],
        ...,
        [0.9446],
        [0.9433],
        [0.9421]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356835.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4136],
        [0.4111],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(251.7673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0089,  0.0348,  0.0239,  ...,  0.0324,  0.0301,  0.0322],
        [ 0.0058,  0.0236,  0.0159,  ...,  0.0216,  0.0204,  0.0217],
        [ 0.0028,  0.0127,  0.0082,  ...,  0.0112,  0.0109,  0.0116],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1411.8297, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.4399, device='cuda:0')



h[100].sum tensor(19.3663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.1403, device='cuda:0')



h[200].sum tensor(23.6788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.3078, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0264, 0.1055, 0.0715,  ..., 0.0971, 0.0911, 0.0972],
        [0.0258, 0.1034, 0.0701,  ..., 0.0952, 0.0893, 0.0953],
        [0.0126, 0.0514, 0.0342,  ..., 0.0465, 0.0442, 0.0469],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36180.0859, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2712, 0.0000,  ..., 0.1124, 0.0000, 0.0812],
        [0.0000, 0.2530, 0.0000,  ..., 0.1053, 0.0000, 0.0753],
        [0.0000, 0.1792, 0.0000,  ..., 0.0763, 0.0000, 0.0522],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(188597.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-91.2761, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.0941, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-206.7081, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9926],
        [0.9853],
        [0.9666],
        ...,
        [0.9446],
        [0.9433],
        [0.9421]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356835.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9926],
        [0.9852],
        [0.9665],
        ...,
        [0.9445],
        [0.9432],
        [0.9420]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356819.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(282.4510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1588.9126, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.1747, device='cuda:0')



h[100].sum tensor(19.7907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0106, device='cuda:0')



h[200].sum tensor(24.5802, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.3921, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38303.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(197831.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-101.7360, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.8382, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-222.4935, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9926],
        [0.9852],
        [0.9665],
        ...,
        [0.9445],
        [0.9432],
        [0.9420]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356819.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9926],
        [0.9852],
        [0.9665],
        ...,
        [0.9444],
        [0.9431],
        [0.9419]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356803.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.6720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1161.4945, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8662, device='cuda:0')



h[100].sum tensor(18.7560, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0032, device='cuda:0')



h[200].sum tensor(22.3826, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.2774, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32661.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0145, 0.0000,  ..., 0.0111, 0.0000, 0.0027],
        [0.0000, 0.0017, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(172825.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-74.7429, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.0108, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-181.6254, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9926],
        [0.9852],
        [0.9665],
        ...,
        [0.9444],
        [0.9431],
        [0.9419]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356803.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9925],
        [0.9852],
        [0.9664],
        ...,
        [0.9443],
        [0.9431],
        [0.9418]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356787.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4641],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(294.9598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0027,  0.0123,  0.0079,  ...,  0.0108,  0.0106,  0.0112],
        [ 0.0033,  0.0143,  0.0093,  ...,  0.0127,  0.0123,  0.0131],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1700.2339, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.2896, device='cuda:0')



h[100].sum tensor(20.0521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3653, device='cuda:0')



h[200].sum tensor(25.1354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.6495, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0304, 0.1201, 0.0819,  ..., 0.1111, 0.1038, 0.1108],
        [0.0103, 0.0429, 0.0282,  ..., 0.0383, 0.0369, 0.0390],
        [0.0032, 0.0153, 0.0092,  ..., 0.0125, 0.0130, 0.0134],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40310.3516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3806, 0.0000,  ..., 0.1533, 0.0000, 0.1180],
        [0.0000, 0.2201, 0.0000,  ..., 0.0914, 0.0000, 0.0666],
        [0.0000, 0.1109, 0.0000,  ..., 0.0490, 0.0000, 0.0319],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(206382.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-111.1639, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.1697, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-236.8348, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9925],
        [0.9852],
        [0.9664],
        ...,
        [0.9443],
        [0.9431],
        [0.9418]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356787.2188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9925],
        [0.9851],
        [0.9663],
        ...,
        [0.9442],
        [0.9430],
        [0.9417]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356771.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(241.8656, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1354.4647, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5574, device='cuda:0')



h[100].sum tensor(19.2155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.8595, device='cuda:0')



h[200].sum tensor(23.3586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.3125, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0022, 0.0134, 0.0073,  ..., 0.0102, 0.0113, 0.0116],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36580.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0618, 0.0000,  ..., 0.0300, 0.0000, 0.0161],
        [0.0000, 0.0853, 0.0000,  ..., 0.0394, 0.0000, 0.0231],
        [0.0000, 0.1134, 0.0000,  ..., 0.0508, 0.0000, 0.0307],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(193009.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-93.2530, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.7493, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-209.5691, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9925],
        [0.9851],
        [0.9663],
        ...,
        [0.9442],
        [0.9430],
        [0.9417]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356771.0625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9925],
        [0.9851],
        [0.9663],
        ...,
        [0.9442],
        [0.9429],
        [0.9416]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356754.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.5421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0060,  0.0242,  0.0163,  ...,  0.0222,  0.0209,  0.0222],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1151.3628, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9438, device='cuda:0')



h[100].sum tensor(18.7237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0279, device='cuda:0')



h[200].sum tensor(22.3140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.3648, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0059, 0.0251, 0.0161,  ..., 0.0219, 0.0215, 0.0225],
        [0.0047, 0.0207, 0.0130,  ..., 0.0177, 0.0176, 0.0184],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30618.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0174, 0.0000,  ..., 0.0121, 0.0000, 0.0043],
        [0.0000, 0.0579, 0.0000,  ..., 0.0280, 0.0000, 0.0160],
        [0.0000, 0.0805, 0.0000,  ..., 0.0369, 0.0000, 0.0229],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(157147.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-64.4770, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.9932, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-166.6961, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9925],
        [0.9851],
        [0.9663],
        ...,
        [0.9442],
        [0.9429],
        [0.9416]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356754.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9925],
        [0.9850],
        [0.9662],
        ...,
        [0.9441],
        [0.9428],
        [0.9415]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356738.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6606],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.4608, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049,  0.0202,  0.0135,  ...,  0.0183,  0.0174,  0.0185],
        [ 0.0037,  0.0159,  0.0105,  ...,  0.0143,  0.0137,  0.0146],
        [ 0.0112,  0.0433,  0.0298,  ...,  0.0404,  0.0374,  0.0400],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1525.1587, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7495, device='cuda:0')



h[100].sum tensor(19.6200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5571, device='cuda:0')



h[200].sum tensor(24.2177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7848, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0110, 0.0457, 0.0302,  ..., 0.0410, 0.0393, 0.0416],
        [0.0304, 0.1200, 0.0818,  ..., 0.1110, 0.1037, 0.1107],
        [0.0281, 0.1115, 0.0759,  ..., 0.1029, 0.0963, 0.1028],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37067.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1625, 0.0000,  ..., 0.0691, 0.0000, 0.0485],
        [0.0000, 0.2753, 0.0000,  ..., 0.1132, 0.0000, 0.0837],
        [0.0000, 0.3149, 0.0000,  ..., 0.1289, 0.0000, 0.0957],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(188861.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-96.0931, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.6869, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-213.9711, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9925],
        [0.9850],
        [0.9662],
        ...,
        [0.9441],
        [0.9428],
        [0.9415]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356738.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9925],
        [0.9850],
        [0.9661],
        ...,
        [0.9440],
        [0.9427],
        [0.9414]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356722.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(342.4047, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0027,  0.0121,  0.0078,  ...,  0.0106,  0.0104,  0.0110],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0012,  0.0065,  0.0038,  ...,  0.0053,  0.0056,  0.0059],
        ...,
        [ 0.0050,  0.0207,  0.0139,  ...,  0.0189,  0.0179,  0.0190],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2034.5634, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.6949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.5184, device='cuda:0')



h[100].sum tensor(20.8408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.7109, device='cuda:0')



h[200].sum tensor(26.8106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.4187, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0100, 0.0439, 0.0284,  ..., 0.0388, 0.0378, 0.0399],
        [0.0067, 0.0337, 0.0207,  ..., 0.0285, 0.0289, 0.0305],
        [0.0031, 0.0184, 0.0104,  ..., 0.0144, 0.0157, 0.0162],
        ...,
        [0.0037, 0.0170, 0.0105,  ..., 0.0143, 0.0145, 0.0150],
        [0.0047, 0.0205, 0.0129,  ..., 0.0176, 0.0175, 0.0182],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44231.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1663, 0.0000,  ..., 0.0722, 0.0000, 0.0461],
        [0.0000, 0.1265, 0.0000,  ..., 0.0570, 0.0000, 0.0324],
        [0.0000, 0.0905, 0.0000,  ..., 0.0428, 0.0000, 0.0211],
        ...,
        [0.0000, 0.0539, 0.0000,  ..., 0.0261, 0.0000, 0.0143],
        [0.0000, 0.0419, 0.0000,  ..., 0.0213, 0.0000, 0.0109],
        [0.0000, 0.0133, 0.0000,  ..., 0.0099, 0.0000, 0.0031]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(221363.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-130.1781, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.1682, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-265.7998, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9925],
        [0.9850],
        [0.9661],
        ...,
        [0.9440],
        [0.9427],
        [0.9414]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356722.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9924],
        [0.9850],
        [0.9660],
        ...,
        [0.9439],
        [0.9426],
        [0.9413]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356706.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(274.0854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1559.3167, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.0386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.4291, device='cuda:0')



h[100].sum tensor(19.6954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7733, device='cuda:0')



h[200].sum tensor(24.3778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.5512, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(39692.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(210711.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-108.4586, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.6478, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-232.4858, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9924],
        [0.9850],
        [0.9660],
        ...,
        [0.9439],
        [0.9426],
        [0.9413]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356706.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9924],
        [0.9849],
        [0.9660],
        ...,
        [0.9438],
        [0.9425],
        [0.9412]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356690.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(343.6942, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2035.1005, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.7097, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.6333, device='cuda:0')



h[100].sum tensor(20.8335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.7475, device='cuda:0')



h[200].sum tensor(26.7952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.5483, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45611.5391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0006, 0.0000,  ..., 0.0054, 0.0000, 0.0000],
        [0.0000, 0.0014, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0048, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(231611.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-136.4337, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-34.0374, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-275.5692, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9924],
        [0.9849],
        [0.9660],
        ...,
        [0.9438],
        [0.9425],
        [0.9412]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356690.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9924],
        [0.9849],
        [0.9660],
        ...,
        [0.9438],
        [0.9425],
        [0.9412]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356690.0938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(309.9208, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [ 0.0024,  0.0110,  0.0070,  ...,  0.0095,  0.0094,  0.0100],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1798.1545, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.8729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.6231, device='cuda:0')



h[100].sum tensor(20.2651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.7896, device='cuda:0')



h[200].sum tensor(25.5878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.1534, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0072, 0.0314, 0.0202,  ..., 0.0275, 0.0269, 0.0283],
        [0.0022, 0.0114, 0.0065,  ..., 0.0089, 0.0096, 0.0097],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(42923.1328, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0075, 0.0000,  ..., 0.0082, 0.0000, 0.0012],
        [0.0000, 0.0264, 0.0000,  ..., 0.0159, 0.0000, 0.0060],
        [0.0000, 0.0497, 0.0000,  ..., 0.0254, 0.0000, 0.0119],
        ...,
        [0.0000, 0.1136, 0.0000,  ..., 0.0500, 0.0000, 0.0321],
        [0.0000, 0.0583, 0.0000,  ..., 0.0280, 0.0000, 0.0154],
        [0.0000, 0.0182, 0.0000,  ..., 0.0119, 0.0000, 0.0044]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(230122., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-123.5644, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-36.8204, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-255.7007, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9924],
        [0.9849],
        [0.9660],
        ...,
        [0.9438],
        [0.9425],
        [0.9412]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356690.0938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9924],
        [0.9849],
        [0.9659],
        ...,
        [0.9437],
        [0.9424],
        [0.9412]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356673.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(212.1219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1145.1533, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.9063, device='cuda:0')



h[100].sum tensor(18.6958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0160, device='cuda:0')



h[200].sum tensor(22.2547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.3226, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0017, 0.0098, 0.0053,  ..., 0.0073, 0.0082, 0.0083],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30300.8066, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0346, 0.0000,  ..., 0.0195, 0.0000, 0.0071],
        [0.0000, 0.0103, 0.0000,  ..., 0.0094, 0.0000, 0.0017],
        [0.0000, 0.0033, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(157287.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-63.4560, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.1096, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.6370, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9924],
        [0.9849],
        [0.9659],
        ...,
        [0.9437],
        [0.9424],
        [0.9412]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356673.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9924],
        [0.9849],
        [0.9658],
        ...,
        [0.9436],
        [0.9423],
        [0.9411]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356657.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3706],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(273.9095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0025,  0.0115,  0.0073,  ...,  0.0100,  0.0099,  0.0105],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0081,  0.0320,  0.0218,  ...,  0.0296,  0.0276,  0.0295],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1561.3933, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.0490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.4134, device='cuda:0')



h[100].sum tensor(19.6903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7683, device='cuda:0')



h[200].sum tensor(24.3670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.5335, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0019, 0.0106, 0.0059,  ..., 0.0081, 0.0089, 0.0090],
        [0.0144, 0.0618, 0.0406,  ..., 0.0554, 0.0533, 0.0566],
        [0.0071, 0.0313, 0.0200,  ..., 0.0273, 0.0269, 0.0282],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38933.2148, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1146, 0.0000,  ..., 0.0512, 0.0000, 0.0316],
        [0.0000, 0.1535, 0.0000,  ..., 0.0668, 0.0000, 0.0429],
        [0.0000, 0.1480, 0.0000,  ..., 0.0644, 0.0000, 0.0417],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(202125.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-103.9008, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.7104, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-226.4530, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9924],
        [0.9849],
        [0.9658],
        ...,
        [0.9436],
        [0.9423],
        [0.9411]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356657.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9924],
        [0.9848],
        [0.9658],
        ...,
        [0.9435],
        [0.9422],
        [0.9410]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356641.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3145],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(200.3746, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0036,  0.0153,  0.0100,  ...,  0.0137,  0.0132,  0.0140],
        [ 0.0021,  0.0098,  0.0061,  ...,  0.0084,  0.0084,  0.0089],
        [ 0.0015,  0.0077,  0.0047,  ...,  0.0065,  0.0066,  0.0070],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1086.7478, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.8593, device='cuda:0')



h[100].sum tensor(18.5509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.6828, device='cuda:0')



h[200].sum tensor(21.9468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.1418, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0105, 0.0477, 0.0306,  ..., 0.0419, 0.0411, 0.0435],
        [0.0093, 0.0432, 0.0274,  ..., 0.0376, 0.0372, 0.0393],
        [0.0031, 0.0168, 0.0097,  ..., 0.0134, 0.0143, 0.0147],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29974.7422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1097, 0.0000,  ..., 0.0502, 0.0000, 0.0278],
        [0.0000, 0.0903, 0.0000,  ..., 0.0423, 0.0000, 0.0223],
        [0.0000, 0.0518, 0.0000,  ..., 0.0266, 0.0000, 0.0115],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(158106.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-61.8378, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.5053, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-162.1204, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9924],
        [0.9848],
        [0.9658],
        ...,
        [0.9435],
        [0.9422],
        [0.9410]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356641.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9923],
        [0.9848],
        [0.9657],
        ...,
        [0.9434],
        [0.9421],
        [0.9409]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356625.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.1964, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1506.6392, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.3307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.4586, device='cuda:0')



h[100].sum tensor(19.5527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.4645, device='cuda:0')



h[200].sum tensor(24.0747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.4566, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0075, 0.0329, 0.0212,  ..., 0.0288, 0.0282, 0.0297],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37656.4766, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0092, 0.0000,  ..., 0.0089, 0.0000, 0.0017],
        [0.0000, 0.0337, 0.0000,  ..., 0.0187, 0.0000, 0.0084],
        [0.0000, 0.1083, 0.0000,  ..., 0.0483, 0.0000, 0.0304],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(193228.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-98.3946, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.5171, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-217.6757, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9923],
        [0.9848],
        [0.9657],
        ...,
        [0.9434],
        [0.9421],
        [0.9409]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356625.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9923],
        [0.9848],
        [0.9656],
        ...,
        [0.9433],
        [0.9420],
        [0.9408]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356609., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(205.3486, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0016,  0.0082,  0.0050,  ...,  0.0069,  0.0071,  0.0074],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1116.5745, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.3026, device='cuda:0')



h[100].sum tensor(18.6172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8239, device='cuda:0')



h[200].sum tensor(22.0878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.6418, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0054, 0.0270, 0.0165,  ..., 0.0227, 0.0232, 0.0243],
        [0.0044, 0.0214, 0.0130,  ..., 0.0178, 0.0182, 0.0190],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32310.1289, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0793, 0.0000,  ..., 0.0382, 0.0000, 0.0185],
        [0.0000, 0.0615, 0.0000,  ..., 0.0307, 0.0000, 0.0139],
        [0.0000, 0.0255, 0.0000,  ..., 0.0158, 0.0000, 0.0048],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(175136.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-73.6888, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.0370, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-179.5035, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9923],
        [0.9848],
        [0.9656],
        ...,
        [0.9433],
        [0.9420],
        [0.9408]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356609., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9923],
        [0.9847],
        [0.9656],
        ...,
        [0.9432],
        [0.9419],
        [0.9407]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356592.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(271.5017, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1551.3289, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.1255, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.1988, device='cuda:0')



h[100].sum tensor(19.6529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7000, device='cuda:0')



h[200].sum tensor(24.2876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.2915, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0016, 0.0111, 0.0057,  ..., 0.0080, 0.0094, 0.0095],
        [0.0031, 0.0167, 0.0097,  ..., 0.0133, 0.0142, 0.0147],
        [0.0046, 0.0221, 0.0135,  ..., 0.0185, 0.0189, 0.0197],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38190.3047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0535, 0.0000,  ..., 0.0276, 0.0000, 0.0111],
        [0.0000, 0.0576, 0.0000,  ..., 0.0291, 0.0000, 0.0128],
        [0.0000, 0.0625, 0.0000,  ..., 0.0310, 0.0000, 0.0146],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(197628.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-101.6255, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.6554, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-222.0623, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9923],
        [0.9847],
        [0.9656],
        ...,
        [0.9432],
        [0.9419],
        [0.9407]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356592.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9923],
        [0.9847],
        [0.9655],
        ...,
        [0.9431],
        [0.9419],
        [0.9406]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356576.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.0194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0020,  0.0095,  0.0060,  ...,  0.0082,  0.0082,  0.0087],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0072,  0.0286,  0.0194,  ...,  0.0264,  0.0247,  0.0263],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1237.7697, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.7885, device='cuda:0')



h[100].sum tensor(18.9015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2967, device='cuda:0')



h[200].sum tensor(22.6915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.3175, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0015, 0.0091, 0.0048,  ..., 0.0066, 0.0076, 0.0076],
        [0.0091, 0.0385, 0.0251,  ..., 0.0342, 0.0331, 0.0350],
        [0.0057, 0.0242, 0.0155,  ..., 0.0210, 0.0207, 0.0216],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32181.2422, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0641, 0.0000,  ..., 0.0316, 0.0000, 0.0151],
        [0.0000, 0.0930, 0.0000,  ..., 0.0425, 0.0000, 0.0252],
        [0.0000, 0.0895, 0.0000,  ..., 0.0407, 0.0000, 0.0251],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(164846.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-72.5475, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.4038, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-178.1739, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9923],
        [0.9847],
        [0.9655],
        ...,
        [0.9431],
        [0.9419],
        [0.9406]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356576.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9923],
        [0.9847],
        [0.9654],
        ...,
        [0.9431],
        [0.9418],
        [0.9405]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356560.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(222.7809, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0019,  0.0092,  0.0057,  ...,  0.0078,  0.0079,  0.0083],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1240.0149, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.8563, device='cuda:0')



h[100].sum tensor(18.9041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.3183, device='cuda:0')



h[200].sum tensor(22.6971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.3941, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0040, 0.0201, 0.0121,  ..., 0.0166, 0.0171, 0.0178],
        [0.0019, 0.0103, 0.0056,  ..., 0.0078, 0.0087, 0.0087],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33163.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0918, 0.0000,  ..., 0.0424, 0.0000, 0.0239],
        [0.0000, 0.0420, 0.0000,  ..., 0.0224, 0.0000, 0.0093],
        [0.0000, 0.0133, 0.0000,  ..., 0.0107, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(172660., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-76.8808, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.6181, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-185.1791, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9923],
        [0.9847],
        [0.9654],
        ...,
        [0.9431],
        [0.9418],
        [0.9405]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356560.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9922],
        [0.9846],
        [0.9654],
        ...,
        [0.9430],
        [0.9417],
        [0.9404]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356544.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.1055, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0017,  0.0085,  0.0052,  ...,  0.0072,  0.0073,  0.0077],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1107.6952, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0136, device='cuda:0')



h[100].sum tensor(18.5862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7319, device='cuda:0')



h[200].sum tensor(22.0218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.3157, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0043, 0.0211, 0.0128,  ..., 0.0175, 0.0180, 0.0187],
        [0.0017, 0.0097, 0.0052,  ..., 0.0071, 0.0081, 0.0081],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31446.2617, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0725, 0.0000,  ..., 0.0351, 0.0000, 0.0175],
        [0.0000, 0.0434, 0.0000,  ..., 0.0231, 0.0000, 0.0096],
        [0.0000, 0.0258, 0.0000,  ..., 0.0157, 0.0000, 0.0051],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(167456.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-68.5862, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.5477, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-172.1679, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9922],
        [0.9846],
        [0.9654],
        ...,
        [0.9430],
        [0.9417],
        [0.9404]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356544.0312, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 350.0 event: 1750 loss: tensor(1201.8411, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9922],
        [0.9846],
        [0.9653],
        ...,
        [0.9429],
        [0.9416],
        [0.9403]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356527.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(299.5100, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0084,  0.0330,  0.0225,  ...,  0.0306,  0.0285,  0.0304],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0050,  0.0206,  0.0138,  ...,  0.0188,  0.0178,  0.0190],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1738.1897, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.2420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.6952, device='cuda:0')



h[100].sum tensor(20.0847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.4944, device='cuda:0')



h[200].sum tensor(25.2046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.1069, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0227, 0.0902, 0.0612,  ..., 0.0831, 0.0779, 0.0830],
        [0.0266, 0.1063, 0.0721,  ..., 0.0979, 0.0918, 0.0979],
        [0.0056, 0.0256, 0.0160,  ..., 0.0219, 0.0219, 0.0230],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40371.2578, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3351, 0.0000,  ..., 0.1363, 0.0000, 0.1029],
        [0.0000, 0.2664, 0.0000,  ..., 0.1100, 0.0000, 0.0801],
        [0.0000, 0.1505, 0.0000,  ..., 0.0653, 0.0000, 0.0424],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(204362.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-111.9033, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.8323, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-237.6570, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9922],
        [0.9846],
        [0.9653],
        ...,
        [0.9429],
        [0.9416],
        [0.9403]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356527.7812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9922],
        [0.9845],
        [0.9652],
        ...,
        [0.9428],
        [0.9415],
        [0.9402]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356511.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(246.7333, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0046,  0.0191,  0.0127,  ...,  0.0173,  0.0165,  0.0175],
        [ 0.0046,  0.0190,  0.0127,  ...,  0.0173,  0.0164,  0.0175],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1388.3601, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9522, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.9912, device='cuda:0')



h[100].sum tensor(19.2489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9976, device='cuda:0')



h[200].sum tensor(23.4295, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.8018, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0046, 0.0202, 0.0126,  ..., 0.0172, 0.0172, 0.0179],
        [0.0082, 0.0352, 0.0228,  ..., 0.0310, 0.0302, 0.0318],
        [0.0260, 0.1039, 0.0705,  ..., 0.0957, 0.0898, 0.0958],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33588.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0621, 0.0000,  ..., 0.0299, 0.0000, 0.0170],
        [0.0000, 0.1143, 0.0000,  ..., 0.0503, 0.0000, 0.0331],
        [0.0000, 0.2021, 0.0000,  ..., 0.0845, 0.0000, 0.0608],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(173772.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-78.7523, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.3021, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-188.0891, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9922],
        [0.9845],
        [0.9652],
        ...,
        [0.9428],
        [0.9415],
        [0.9402]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356511.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9922],
        [0.9845],
        [0.9652],
        ...,
        [0.9427],
        [0.9414],
        [0.9401]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356495.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.8062],
        [0.4016],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.1854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0028,  0.0124,  0.0080,  ...,  0.0109,  0.0107,  0.0113],
        [ 0.0104,  0.0401,  0.0276,  ...,  0.0374,  0.0347,  0.0371],
        [ 0.0046,  0.0189,  0.0126,  ...,  0.0172,  0.0163,  0.0174],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1039.7372, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.6539, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2185, device='cuda:0')



h[100].sum tensor(18.4173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4789, device='cuda:0')



h[200].sum tensor(21.6632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.4191, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0302, 0.1192, 0.0813,  ..., 0.1103, 0.1030, 0.1100],
        [0.0219, 0.0889, 0.0598,  ..., 0.0813, 0.0768, 0.0818],
        [0.0221, 0.0897, 0.0604,  ..., 0.0821, 0.0775, 0.0826],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29946.2832, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2315, 0.0000,  ..., 0.0965, 0.0000, 0.0690],
        [0.0000, 0.2394, 0.0000,  ..., 0.0999, 0.0000, 0.0709],
        [0.0000, 0.2341, 0.0000,  ..., 0.0978, 0.0000, 0.0693],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(159333.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-61.7888, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.3619, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-162.1628, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9922],
        [0.9845],
        [0.9652],
        ...,
        [0.9427],
        [0.9414],
        [0.9401]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356495.2812, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9922],
        [0.9845],
        [0.9651],
        ...,
        [0.9426],
        [0.9413],
        [0.9400]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356479., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(330.7854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1945.1235, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.2584, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.4827, device='cuda:0')



h[100].sum tensor(20.5654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.3814, device='cuda:0')



h[200].sum tensor(26.2257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.2507, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0018, 0.0100, 0.0054,  ..., 0.0075, 0.0084, 0.0085],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0077, 0.0335, 0.0216,  ..., 0.0294, 0.0287, 0.0302],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44201.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0649, 0.0000,  ..., 0.0317, 0.0000, 0.0158],
        [0.0000, 0.0794, 0.0000,  ..., 0.0369, 0.0000, 0.0214],
        [0.0000, 0.1710, 0.0000,  ..., 0.0724, 0.0000, 0.0507],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(231104.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-130.2983, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.1414, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-265.5984, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9922],
        [0.9845],
        [0.9651],
        ...,
        [0.9426],
        [0.9413],
        [0.9400]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356479., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9921],
        [0.9844],
        [0.9650],
        ...,
        [0.9425],
        [0.9412],
        [0.9399]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356462.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2432],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(263.5742, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0033,  0.0145,  0.0095,  ...,  0.0130,  0.0125,  0.0133],
        [ 0.0014,  0.0073,  0.0044,  ...,  0.0060,  0.0062,  0.0065],
        [ 0.0056,  0.0228,  0.0153,  ...,  0.0208,  0.0196,  0.0209],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1499.8484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.4289, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.4922, device='cuda:0')



h[100].sum tensor(19.5047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.4752, device='cuda:0')



h[200].sum tensor(23.9727, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.4946, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0105, 0.0474, 0.0304,  ..., 0.0416, 0.0408, 0.0432],
        [0.0141, 0.0605, 0.0397,  ..., 0.0541, 0.0522, 0.0554],
        [0.0095, 0.0439, 0.0280,  ..., 0.0383, 0.0378, 0.0400],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35640.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1556, 0.0000,  ..., 0.0686, 0.0000, 0.0413],
        [0.0000, 0.1625, 0.0000,  ..., 0.0711, 0.0000, 0.0439],
        [0.0000, 0.1534, 0.0000,  ..., 0.0676, 0.0000, 0.0409],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(177625.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-89.1723, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.1018, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-203.4050, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9921],
        [0.9844],
        [0.9650],
        ...,
        [0.9425],
        [0.9412],
        [0.9399]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356462.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9921],
        [0.9844],
        [0.9649],
        ...,
        [0.9424],
        [0.9411],
        [0.9399]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356446.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.2986],
        [0.4417],
        [0.4258],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(285.1504, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0049,  0.0203,  0.0136,  ...,  0.0185,  0.0175,  0.0187],
        [ 0.0073,  0.0288,  0.0196,  ...,  0.0266,  0.0249,  0.0265],
        [ 0.0063,  0.0253,  0.0171,  ...,  0.0233,  0.0219,  0.0233],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1642.7063, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.7420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.4153, device='cuda:0')



h[100].sum tensor(19.8403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.0871, device='cuda:0')



h[200].sum tensor(24.6857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.6635, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0194, 0.0801, 0.0535,  ..., 0.0728, 0.0691, 0.0736],
        [0.0237, 0.0958, 0.0647,  ..., 0.0878, 0.0827, 0.0882],
        [0.0246, 0.0989, 0.0669,  ..., 0.0909, 0.0854, 0.0911],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36693.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.9975e-01, 0.0000e+00,  ..., 8.4990e-02, 0.0000e+00,
         5.7416e-02],
        [0.0000e+00, 2.3685e-01, 0.0000e+00,  ..., 9.9418e-02, 0.0000e+00,
         6.9328e-02],
        [0.0000e+00, 2.4461e-01, 0.0000e+00,  ..., 1.0227e-01, 0.0000e+00,
         7.2137e-02],
        ...,
        [0.0000e+00, 3.2906e-03, 0.0000e+00,  ..., 5.9704e-03, 0.0000e+00,
         1.1916e-04],
        [0.0000e+00, 4.7452e-03, 0.0000e+00,  ..., 6.5861e-03, 0.0000e+00,
         2.3833e-04],
        [0.0000e+00, 3.2911e-03, 0.0000e+00,  ..., 5.9697e-03, 0.0000e+00,
         1.1917e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(180928.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-94.3431, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-42.1310, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-211.1516, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9921],
        [0.9844],
        [0.9649],
        ...,
        [0.9424],
        [0.9411],
        [0.9399]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356446.4375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9921],
        [0.9844],
        [0.9649],
        ...,
        [0.9423],
        [0.9410],
        [0.9398]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356430.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(242.3395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0107,  0.0412,  0.0284,  ...,  0.0385,  0.0356,  0.0381],
        [ 0.0038,  0.0162,  0.0107,  ...,  0.0146,  0.0140,  0.0149],
        [ 0.0055,  0.0223,  0.0150,  ...,  0.0203,  0.0192,  0.0205],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1376.7667, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.0385, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.5996, device='cuda:0')



h[100].sum tensor(19.2068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.8730, device='cuda:0')



h[200].sum tensor(23.3400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.3601, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0193, 0.0795, 0.0531,  ..., 0.0723, 0.0686, 0.0730],
        [0.0240, 0.0969, 0.0655,  ..., 0.0890, 0.0837, 0.0893],
        [0.0214, 0.0871, 0.0586,  ..., 0.0796, 0.0752, 0.0802],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33899.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2374, 0.0000,  ..., 0.0986, 0.0000, 0.0714],
        [0.0000, 0.2321, 0.0000,  ..., 0.0967, 0.0000, 0.0693],
        [0.0000, 0.2085, 0.0000,  ..., 0.0881, 0.0000, 0.0608],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(171661.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-80.8424, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.6540, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-190.8668, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9921],
        [0.9844],
        [0.9649],
        ...,
        [0.9423],
        [0.9410],
        [0.9398]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356430.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9921],
        [0.9843],
        [0.9648],
        ...,
        [0.9422],
        [0.9409],
        [0.9397]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356413.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3000],
        [0.3091],
        [0.3157],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(210.9658, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0056,  0.0227,  0.0153,  ...,  0.0208,  0.0196,  0.0209],
        [ 0.0045,  0.0188,  0.0125,  ...,  0.0170,  0.0162,  0.0173],
        [ 0.0055,  0.0224,  0.0151,  ...,  0.0205,  0.0194,  0.0206],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1161.9150, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0854, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8033, device='cuda:0')



h[100].sum tensor(18.6951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.9832, device='cuda:0')



h[200].sum tensor(22.2533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.2064, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0160, 0.0675, 0.0447,  ..., 0.0608, 0.0582, 0.0619],
        [0.0213, 0.0870, 0.0584,  ..., 0.0794, 0.0751, 0.0800],
        [0.0239, 0.0964, 0.0651,  ..., 0.0885, 0.0832, 0.0888],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30381.6660, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.2637, 0.0000,  ..., 0.1099, 0.0000, 0.0779],
        [0.0000, 0.3095, 0.0000,  ..., 0.1274, 0.0000, 0.0931],
        [0.0000, 0.3143, 0.0000,  ..., 0.1288, 0.0000, 0.0951],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(156255.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-63.5232, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.1376, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-165.0302, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9921],
        [0.9843],
        [0.9648],
        ...,
        [0.9422],
        [0.9409],
        [0.9397]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356413.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9921],
        [0.9843],
        [0.9647],
        ...,
        [0.9421],
        [0.9408],
        [0.9396]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356397.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(382.9866, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0017,  0.0086,  0.0053,  ...,  0.0072,  0.0073,  0.0077],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2274.1489, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-5.7055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.1354, device='cuda:0')



h[100].sum tensor(21.3243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(10.8618, device='cuda:0')



h[200].sum tensor(27.8375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.4980, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0039, 0.0195, 0.0117,  ..., 0.0160, 0.0167, 0.0173],
        [0.0017, 0.0097, 0.0052,  ..., 0.0072, 0.0081, 0.0082],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(52369.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0660, 0.0000,  ..., 0.0325, 0.0000, 0.0153],
        [0.0000, 0.0356, 0.0000,  ..., 0.0200, 0.0000, 0.0072],
        [0.0000, 0.0139, 0.0000,  ..., 0.0110, 0.0000, 0.0020],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0060, 0.0000, 0.0006],
        [0.0000, 0.0146, 0.0000,  ..., 0.0104, 0.0000, 0.0035]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(276610.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-169.0688, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-27.3629, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-325.0870, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9921],
        [0.9843],
        [0.9647],
        ...,
        [0.9421],
        [0.9408],
        [0.9396]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356397.5625, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9920],
        [0.9843],
        [0.9647],
        ...,
        [0.9420],
        [0.9407],
        [0.9395]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356381.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(213.9800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1186.6643, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.0719, device='cuda:0')



h[100].sum tensor(18.7487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0687, device='cuda:0')



h[200].sum tensor(22.3670, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.5094, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30936.0078, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(159683.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-66.3432, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.5262, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-169.1665, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9920],
        [0.9843],
        [0.9647],
        ...,
        [0.9420],
        [0.9407],
        [0.9395]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356381.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 360.0 event: 1800 loss: tensor(612.2153, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9920],
        [0.9842],
        [0.9646],
        ...,
        [0.9420],
        [0.9407],
        [0.9394]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356364.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.8734, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0012,  0.0068,  0.0040,  ...,  0.0056,  0.0058,  0.0061],
        [ 0.0012,  0.0068,  0.0040,  ...,  0.0056,  0.0058,  0.0061],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1168.3300, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8842, device='cuda:0')



h[100].sum tensor(18.7028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.0089, device='cuda:0')



h[200].sum tensor(22.2696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.2976, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0021, 0.0132, 0.0072,  ..., 0.0100, 0.0112, 0.0114],
        [0.0021, 0.0132, 0.0071,  ..., 0.0099, 0.0111, 0.0114],
        [0.0021, 0.0130, 0.0071,  ..., 0.0099, 0.0110, 0.0113],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31316.0352, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0416, 0.0000,  ..., 0.0227, 0.0000, 0.0082],
        [0.0000, 0.0355, 0.0000,  ..., 0.0203, 0.0000, 0.0060],
        [0.0000, 0.0332, 0.0000,  ..., 0.0193, 0.0000, 0.0055],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(164761.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-68.5684, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.9882, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-172.2159, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9920],
        [0.9842],
        [0.9646],
        ...,
        [0.9420],
        [0.9407],
        [0.9394]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356364.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9920],
        [0.9842],
        [0.9645],
        ...,
        [0.9419],
        [0.9406],
        [0.9393]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356348.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(272.3535, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1585.9399, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.0574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.2747, device='cuda:0')



h[100].sum tensor(19.6862, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.7242, device='cuda:0')



h[200].sum tensor(24.3583, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.3771, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38817.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0398, 0.0000,  ..., 0.0212, 0.0000, 0.0100],
        [0.0000, 0.0358, 0.0000,  ..., 0.0195, 0.0000, 0.0091],
        [0.0000, 0.0289, 0.0000,  ..., 0.0166, 0.0000, 0.0074],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(203107.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-104.8651, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.0499, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-226.6095, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9920],
        [0.9842],
        [0.9645],
        ...,
        [0.9419],
        [0.9406],
        [0.9393]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356348.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9920],
        [0.9842],
        [0.9645],
        ...,
        [0.9418],
        [0.9405],
        [0.9392]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356332.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(320.2350, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0011,  0.0065,  0.0038,  ...,  0.0053,  0.0056,  0.0058],
        [ 0.0019,  0.0094,  0.0059,  ...,  0.0081,  0.0081,  0.0086],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1924.7268, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4287, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.5424, device='cuda:0')



h[100].sum tensor(20.4822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0821, device='cuda:0')



h[200].sum tensor(26.0489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.1902, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0115, 0.0513, 0.0331,  ..., 0.0453, 0.0441, 0.0468],
        [0.0040, 0.0199, 0.0120,  ..., 0.0164, 0.0170, 0.0177],
        [0.0019, 0.0105, 0.0058,  ..., 0.0080, 0.0088, 0.0089],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(44506.9922, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.2133e-01, 0.0000e+00,  ..., 5.4603e-02, 0.0000e+00,
         3.2064e-02],
        [0.0000e+00, 7.8049e-02, 0.0000e+00,  ..., 3.7031e-02, 0.0000e+00,
         1.9751e-02],
        [0.0000e+00, 3.8424e-02, 0.0000e+00,  ..., 2.0830e-02, 0.0000e+00,
         8.7880e-03],
        ...,
        [0.0000e+00, 3.2944e-03, 0.0000e+00,  ..., 5.9657e-03, 0.0000e+00,
         1.2056e-04],
        [0.0000e+00, 4.7506e-03, 0.0000e+00,  ..., 6.5819e-03, 0.0000e+00,
         2.4113e-04],
        [0.0000e+00, 3.2949e-03, 0.0000e+00,  ..., 5.9650e-03, 0.0000e+00,
         1.2057e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(228812.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-131.6237, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-35.0378, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-267.5602, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9920],
        [0.9842],
        [0.9645],
        ...,
        [0.9418],
        [0.9405],
        [0.9392]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356332.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9920],
        [0.9841],
        [0.9644],
        ...,
        [0.9417],
        [0.9404],
        [0.9391]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356316., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.9907, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0025,  0.0116,  0.0074,  ...,  0.0101,  0.0100,  0.0105],
        [ 0.0049,  0.0201,  0.0134,  ...,  0.0183,  0.0174,  0.0185],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1364.8452, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3903, device='cuda:0')



h[100].sum tensor(19.1586, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.8063, device='cuda:0')



h[200].sum tensor(23.2377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.1240, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0045, 0.0218, 0.0133,  ..., 0.0182, 0.0186, 0.0194],
        [0.0081, 0.0370, 0.0235,  ..., 0.0322, 0.0317, 0.0335],
        [0.0145, 0.0621, 0.0408,  ..., 0.0556, 0.0535, 0.0568],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33307.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0762, 0.0000,  ..., 0.0360, 0.0000, 0.0198],
        [0.0000, 0.1325, 0.0000,  ..., 0.0585, 0.0000, 0.0364],
        [0.0000, 0.1896, 0.0000,  ..., 0.0810, 0.0000, 0.0538],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(166352.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-78.4770, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.8207, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-187.0998, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9920],
        [0.9841],
        [0.9644],
        ...,
        [0.9417],
        [0.9404],
        [0.9391]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356316., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9920],
        [0.9841],
        [0.9643],
        ...,
        [0.9416],
        [0.9403],
        [0.9390]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356299.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(175.1064, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0039,  0.0164,  0.0108,  ...,  0.0147,  0.0141,  0.0150],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0029,  0.0129,  0.0083,  ...,  0.0114,  0.0111,  0.0118],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(946.4843, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.1592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.6072, device='cuda:0')



h[100].sum tensor(18.1704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.9662, device='cuda:0')



h[200].sum tensor(21.1387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.6018, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0053, 0.0267, 0.0162,  ..., 0.0223, 0.0228, 0.0239],
        [0.0128, 0.0558, 0.0363,  ..., 0.0496, 0.0481, 0.0510],
        [0.0191, 0.0768, 0.0517,  ..., 0.0702, 0.0662, 0.0705],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27889.1523, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1467, 0.0000,  ..., 0.0647, 0.0000, 0.0393],
        [0.0000, 0.2219, 0.0000,  ..., 0.0936, 0.0000, 0.0639],
        [0.0000, 0.3045, 0.0000,  ..., 0.1246, 0.0000, 0.0922],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(146524.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-52.6288, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.9503, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-147.6023, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9920],
        [0.9841],
        [0.9643],
        ...,
        [0.9416],
        [0.9403],
        [0.9390]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356299.6875, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9919],
        [0.9840],
        [0.9643],
        ...,
        [0.9415],
        [0.9402],
        [0.9389]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356283.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.2598, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0026,  0.0119,  0.0076,  ...,  0.0105,  0.0103,  0.0109],
        [ 0.0016,  0.0082,  0.0050,  ...,  0.0069,  0.0070,  0.0074],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1128.3943, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2056, device='cuda:0')



h[100].sum tensor(18.5965, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7930, device='cuda:0')



h[200].sum tensor(22.0437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.5323, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0026, 0.0131, 0.0076,  ..., 0.0104, 0.0110, 0.0113],
        [0.0036, 0.0186, 0.0110,  ..., 0.0152, 0.0159, 0.0165],
        [0.0160, 0.0677, 0.0448,  ..., 0.0610, 0.0583, 0.0620],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(28731.8164, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0514, 0.0000,  ..., 0.0262, 0.0000, 0.0124],
        [0.0000, 0.0915, 0.0000,  ..., 0.0424, 0.0000, 0.0238],
        [0.0000, 0.1596, 0.0000,  ..., 0.0693, 0.0000, 0.0445],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(145739.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-56.1296, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.2035, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-153.6867, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9919],
        [0.9840],
        [0.9643],
        ...,
        [0.9415],
        [0.9402],
        [0.9389]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356283.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9919],
        [0.9840],
        [0.9642],
        ...,
        [0.9414],
        [0.9401],
        [0.9388]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356267.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(197.2151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1101.9932, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.4192, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.5777, device='cuda:0')



h[100].sum tensor(18.5320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.5932, device='cuda:0')



h[200].sum tensor(21.9068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.8242, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32165.2070, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.1796e-04, 0.0000e+00,  ..., 5.3371e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 2.1501e-03, 0.0000e+00,  ..., 6.0554e-03, 0.0000e+00,
         7.6893e-05],
        [0.0000e+00, 1.5123e-02, 0.0000e+00,  ..., 1.1172e-02, 0.0000e+00,
         3.4685e-03],
        ...,
        [0.0000e+00, 3.8338e-04, 0.0000e+00,  ..., 4.7291e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.8366e-04, 0.0000e+00,  ..., 4.7286e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.8366e-04, 0.0000e+00,  ..., 4.7283e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(174523.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-72.3130, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.5574, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-177.8347, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9919],
        [0.9840],
        [0.9642],
        ...,
        [0.9414],
        [0.9401],
        [0.9388]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356267.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9919],
        [0.9840],
        [0.9641],
        ...,
        [0.9413],
        [0.9400],
        [0.9387]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356250.6562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(239.0786, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1345.7192, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.3090, device='cuda:0')



h[100].sum tensor(19.1024, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.7805, device='cuda:0')



h[200].sum tensor(23.1184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.0323, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34309.6016, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(178827.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-83.0700, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.3970, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-193.6275, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9919],
        [0.9840],
        [0.9641],
        ...,
        [0.9413],
        [0.9400],
        [0.9387]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356250.6562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9919],
        [0.9839],
        [0.9640],
        ...,
        [0.9412],
        [0.9399],
        [0.9386]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356234.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.8557, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1145.4738, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.2198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2587, device='cuda:0')



h[100].sum tensor(18.6294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8099, device='cuda:0')



h[200].sum tensor(22.1137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.5922, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31189.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(165252.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-67.4457, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.5943, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-170.5724, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9919],
        [0.9839],
        [0.9640],
        ...,
        [0.9412],
        [0.9399],
        [0.9386]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356234.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9919],
        [0.9839],
        [0.9640],
        ...,
        [0.9411],
        [0.9398],
        [0.9385]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356217.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(219.3931, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1239.2461, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.7745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.5544, device='cuda:0')



h[100].sum tensor(18.8471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.2222, device='cuda:0')



h[200].sum tensor(22.5760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.0535, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(33230.8867, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(177905.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-77.1033, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.3264, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-185.9554, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9919],
        [0.9839],
        [0.9640],
        ...,
        [0.9411],
        [0.9398],
        [0.9385]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356217.9688, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 370.0 event: 1850 loss: tensor(592.2729, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9918],
        [0.9839],
        [0.9639],
        ...,
        [0.9410],
        [0.9397],
        [0.9385]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356201.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2864],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(179.0369, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0018,  0.0090,  0.0056,  ...,  0.0076,  0.0077,  0.0081],
        [ 0.0012,  0.0068,  0.0040,  ...,  0.0056,  0.0058,  0.0061],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(986.2920, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.9934, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9575, device='cuda:0')



h[100].sum tensor(18.2514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.0776, device='cuda:0')



h[200].sum tensor(21.3108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.9969, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0018, 0.0101, 0.0055,  ..., 0.0076, 0.0085, 0.0086],
        [0.0040, 0.0200, 0.0120,  ..., 0.0165, 0.0171, 0.0178],
        [0.0151, 0.0641, 0.0422,  ..., 0.0576, 0.0552, 0.0587],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29734.0801, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0436, 0.0000,  ..., 0.0231, 0.0000, 0.0099],
        [0.0000, 0.0877, 0.0000,  ..., 0.0408, 0.0000, 0.0226],
        [0.0000, 0.1603, 0.0000,  ..., 0.0695, 0.0000, 0.0448],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(160442.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-61.4559, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.2772, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-160.9011, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9918],
        [0.9839],
        [0.9639],
        ...,
        [0.9410],
        [0.9397],
        [0.9385]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356201.6250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9918],
        [0.9838],
        [0.9638],
        ...,
        [0.9409],
        [0.9396],
        [0.9384]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356185.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(289.9563, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0018,  0.0087,  0.0054,  ...,  0.0074,  0.0075,  0.0079],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1697.6975, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.5868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.8437, device='cuda:0')



h[100].sum tensor(19.9162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.2234, device='cuda:0')



h[200].sum tensor(24.8468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.1466, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0058, 0.0265, 0.0166,  ..., 0.0227, 0.0227, 0.0237],
        [0.0106, 0.0460, 0.0299,  ..., 0.0408, 0.0395, 0.0419],
        [0.0089, 0.0378, 0.0247,  ..., 0.0336, 0.0325, 0.0343],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38799.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1242, 0.0000,  ..., 0.0555, 0.0000, 0.0333],
        [0.0000, 0.1556, 0.0000,  ..., 0.0676, 0.0000, 0.0439],
        [0.0000, 0.1582, 0.0000,  ..., 0.0682, 0.0000, 0.0455],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(202672.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-104.1274, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.1755, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-226.2870, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9918],
        [0.9838],
        [0.9638],
        ...,
        [0.9409],
        [0.9396],
        [0.9384]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356185.2500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9918],
        [0.9838],
        [0.9638],
        ...,
        [0.9409],
        [0.9395],
        [0.9383]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356168.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3254],
        [0.3054],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(292.8219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0061,  0.0245,  0.0166,  ...,  0.0225,  0.0212,  0.0226],
        [ 0.0089,  0.0348,  0.0239,  ...,  0.0324,  0.0301,  0.0322],
        [ 0.0063,  0.0252,  0.0170,  ...,  0.0231,  0.0217,  0.0232],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1743.9719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0991, device='cuda:0')



h[100].sum tensor(20.0212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3047, device='cuda:0')



h[200].sum tensor(25.0698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.4346, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0328, 0.1287, 0.0880,  ..., 0.1193, 0.1113, 0.1188],
        [0.0282, 0.1120, 0.0762,  ..., 0.1033, 0.0967, 0.1033],
        [0.0228, 0.0922, 0.0622,  ..., 0.0845, 0.0796, 0.0849],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40970.8047, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3661, 0.0000,  ..., 0.1489, 0.0000, 0.1120],
        [0.0000, 0.3401, 0.0000,  ..., 0.1390, 0.0000, 0.1034],
        [0.0000, 0.3006, 0.0000,  ..., 0.1239, 0.0000, 0.0902],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(209420., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-114.5481, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-38.3849, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-241.8325, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9918],
        [0.9838],
        [0.9638],
        ...,
        [0.9409],
        [0.9395],
        [0.9383]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356168.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9918],
        [0.9838],
        [0.9637],
        ...,
        [0.9408],
        [0.9394],
        [0.9382]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356152.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(278.1749, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1651.6971, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.8207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.7936, device='cuda:0')



h[100].sum tensor(19.8019, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.8893, device='cuda:0')



h[200].sum tensor(24.6040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.9623, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(38507.0391, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0496, 0.0000,  ..., 0.0254, 0.0000, 0.0121],
        [0.0000, 0.0523, 0.0000,  ..., 0.0264, 0.0000, 0.0130],
        [0.0000, 0.0572, 0.0000,  ..., 0.0282, 0.0000, 0.0146],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(195773.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-101.9815, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.1195, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-223.2290, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9918],
        [0.9838],
        [0.9637],
        ...,
        [0.9408],
        [0.9394],
        [0.9382]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356152.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9918],
        [0.9837],
        [0.9636],
        ...,
        [0.9407],
        [0.9394],
        [0.9381]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356136.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(171.7983, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(927.7212, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.2908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.3123, device='cuda:0')



h[100].sum tensor(18.1061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.8723, device='cuda:0')



h[200].sum tensor(21.0021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.2693, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0021, 0.0111, 0.0062,  ..., 0.0085, 0.0093, 0.0094],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(26156.2891, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.9459e-03, 0.0000e+00,  ..., 6.0090e-03, 0.0000e+00,
         4.8793e-06],
        [0.0000e+00, 1.1483e-02, 0.0000e+00,  ..., 9.8771e-03, 0.0000e+00,
         2.1646e-03],
        [0.0000e+00, 3.8928e-02, 0.0000e+00,  ..., 2.1034e-02, 0.0000e+00,
         8.8198e-03],
        ...,
        [0.0000e+00, 3.8398e-04, 0.0000e+00,  ..., 4.7224e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.8426e-04, 0.0000e+00,  ..., 4.7219e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.8426e-04, 0.0000e+00,  ..., 4.7217e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(134669.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-44.0326, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-51.2788, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-135.4090, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9918],
        [0.9837],
        [0.9636],
        ...,
        [0.9407],
        [0.9394],
        [0.9381]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356136.1250, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9917],
        [0.9837],
        [0.9636],
        ...,
        [0.9406],
        [0.9393],
        [0.9380]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356119.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(230.6700, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0058,  0.0237,  0.0160,  ...,  0.0217,  0.0204,  0.0218],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1310.7570, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.4638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.5595, device='cuda:0')



h[100].sum tensor(18.9989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.5420, device='cuda:0')



h[200].sum tensor(22.8985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.1871, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0058, 0.0246, 0.0158,  ..., 0.0214, 0.0210, 0.0220],
        [0.0046, 0.0202, 0.0127,  ..., 0.0173, 0.0173, 0.0180],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32462.4473, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0170, 0.0000,  ..., 0.0120, 0.0000, 0.0042],
        [0.0000, 0.0535, 0.0000,  ..., 0.0264, 0.0000, 0.0144],
        [0.0000, 0.0687, 0.0000,  ..., 0.0325, 0.0000, 0.0187],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(165947.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-73.9423, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.0605, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-180.2840, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9917],
        [0.9837],
        [0.9636],
        ...,
        [0.9406],
        [0.9393],
        [0.9380]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356119.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9917],
        [0.9836],
        [0.9635],
        ...,
        [0.9405],
        [0.9392],
        [0.9379]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356103.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2664],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(203.7170, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0017,  0.0084,  0.0051,  ...,  0.0071,  0.0072,  0.0076],
        [ 0.0014,  0.0076,  0.0046,  ...,  0.0063,  0.0065,  0.0068],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1132.6832, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.1572, device='cuda:0')



h[100].sum tensor(18.5806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7776, device='cuda:0')



h[200].sum tensor(22.0100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.4777, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0016, 0.0095, 0.0051,  ..., 0.0070, 0.0080, 0.0080],
        [0.0027, 0.0152, 0.0086,  ..., 0.0119, 0.0129, 0.0132],
        [0.0073, 0.0357, 0.0222,  ..., 0.0305, 0.0307, 0.0323],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30730.6055, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0542, 0.0000,  ..., 0.0277, 0.0000, 0.0124],
        [0.0000, 0.0794, 0.0000,  ..., 0.0381, 0.0000, 0.0188],
        [0.0000, 0.1040, 0.0000,  ..., 0.0482, 0.0000, 0.0252],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(160805.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-65.6402, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.4981, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-167.9660, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9917],
        [0.9836],
        [0.9635],
        ...,
        [0.9405],
        [0.9392],
        [0.9379]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356103.3438, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9917],
        [0.9836],
        [0.9634],
        ...,
        [0.9404],
        [0.9391],
        [0.9378]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356086.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(266.2874, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0083,  0.0328,  0.0224,  ...,  0.0304,  0.0283,  0.0303],
        [ 0.0115,  0.0441,  0.0304,  ...,  0.0412,  0.0381,  0.0408],
        [ 0.0087,  0.0341,  0.0233,  ...,  0.0316,  0.0294,  0.0314],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1564.0513, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.2655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7341, device='cuda:0')



h[100].sum tensor(19.5845, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.5521, device='cuda:0')



h[200].sum tensor(24.1424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7673, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0379, 0.1474, 0.1012,  ..., 0.1372, 0.1274, 0.1362],
        [0.0427, 0.1649, 0.1137,  ..., 0.1539, 0.1426, 0.1525],
        [0.0414, 0.1603, 0.1104,  ..., 0.1496, 0.1386, 0.1482],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(37971.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 4.0113e-01, 0.0000e+00,  ..., 1.6188e-01, 0.0000e+00,
         1.2366e-01],
        [0.0000e+00, 4.4150e-01, 0.0000e+00,  ..., 1.7698e-01, 0.0000e+00,
         1.3722e-01],
        [0.0000e+00, 4.2735e-01, 0.0000e+00,  ..., 1.7143e-01, 0.0000e+00,
         1.3255e-01],
        ...,
        [0.0000e+00, 3.8420e-04, 0.0000e+00,  ..., 4.7199e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.8448e-04, 0.0000e+00,  ..., 4.7194e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.8448e-04, 0.0000e+00,  ..., 4.7192e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(197907.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-100.1872, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-40.8864, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-220.3960, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9917],
        [0.9836],
        [0.9634],
        ...,
        [0.9404],
        [0.9391],
        [0.9378]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356086.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9917],
        [0.9836],
        [0.9634],
        ...,
        [0.9404],
        [0.9391],
        [0.9378]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356086.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(215.2728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1205.7863, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.9756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.1872, device='cuda:0')



h[100].sum tensor(18.7488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1053, device='cuda:0')



h[200].sum tensor(22.3672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.6393, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0020, 0.0127, 0.0068,  ..., 0.0095, 0.0108, 0.0110],
        [0.0010, 0.0072, 0.0034,  ..., 0.0048, 0.0059, 0.0058],
        [0.0016, 0.0095, 0.0051,  ..., 0.0070, 0.0079, 0.0080],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31527.6465, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0387, 0.0000,  ..., 0.0218, 0.0000, 0.0066],
        [0.0000, 0.0327, 0.0000,  ..., 0.0191, 0.0000, 0.0053],
        [0.0000, 0.0379, 0.0000,  ..., 0.0212, 0.0000, 0.0066],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(163675.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-69.2716, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.0802, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-173.3264, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9917],
        [0.9836],
        [0.9634],
        ...,
        [0.9404],
        [0.9391],
        [0.9378]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356086.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9917],
        [0.9836],
        [0.9634],
        ...,
        [0.9403],
        [0.9390],
        [0.9377]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356070.5312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3291],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(234.5637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0033,  0.0144,  0.0094,  ...,  0.0129,  0.0124,  0.0132],
        [ 0.0022,  0.0102,  0.0065,  ...,  0.0089,  0.0088,  0.0093],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1352.4139, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.2812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.9065, device='cuda:0')



h[100].sum tensor(19.0881, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.6524, device='cuda:0')



h[200].sum tensor(23.0880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.5785, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0130, 0.0565, 0.0369,  ..., 0.0503, 0.0487, 0.0517],
        [0.0050, 0.0235, 0.0145,  ..., 0.0198, 0.0201, 0.0210],
        [0.0021, 0.0113, 0.0064,  ..., 0.0087, 0.0095, 0.0096],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32933.8398, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1310, 0.0000,  ..., 0.0582, 0.0000, 0.0356],
        [0.0000, 0.0898, 0.0000,  ..., 0.0418, 0.0000, 0.0229],
        [0.0000, 0.0573, 0.0000,  ..., 0.0288, 0.0000, 0.0133],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(167562.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-75.8882, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-45.6766, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-183.6136, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9917],
        [0.9836],
        [0.9634],
        ...,
        [0.9403],
        [0.9390],
        [0.9377]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356070.5312, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 380.0 event: 1900 loss: tensor(555.5647, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9917],
        [0.9835],
        [0.9633],
        ...,
        [0.9402],
        [0.9389],
        [0.9376]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356054.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(204.6884, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1162.5679, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.1916, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.2438, device='cuda:0')



h[100].sum tensor(18.6432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.8051, device='cuda:0')



h[200].sum tensor(22.1430, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.5754, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30300.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(155972.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-63.6976, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.9006, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.8214, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9917],
        [0.9835],
        [0.9633],
        ...,
        [0.9402],
        [0.9389],
        [0.9376]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356054.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9916],
        [0.9835],
        [0.9632],
        ...,
        [0.9401],
        [0.9388],
        [0.9375]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356037.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(295.9402, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1753.3936, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-8.3823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.3770, device='cuda:0')



h[100].sum tensor(20.0161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.3931, device='cuda:0')



h[200].sum tensor(25.0590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.7481, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(40017.7930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0310, 0.0000,  ..., 0.0181, 0.0000, 0.0060],
        [0.0000, 0.0169, 0.0000,  ..., 0.0123, 0.0000, 0.0020],
        [0.0000, 0.0116, 0.0000,  ..., 0.0100, 0.0000, 0.0011],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(204513.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-109.7813, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.4111, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-234.5920, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9916],
        [0.9835],
        [0.9632],
        ...,
        [0.9401],
        [0.9388],
        [0.9375]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356037.7500, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9916],
        [0.9835],
        [0.9631],
        ...,
        [0.9400],
        [0.9387],
        [0.9374]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356021.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(318.9587, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0039,  0.0166,  0.0109,  ...,  0.0149,  0.0143,  0.0152],
        [ 0.0021,  0.0101,  0.0063,  ...,  0.0087,  0.0087,  0.0092],
        [ 0.0054,  0.0220,  0.0148,  ...,  0.0201,  0.0190,  0.0202],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1948.5498, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.4603, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.4286, device='cuda:0')



h[100].sum tensor(20.4667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.0459, device='cuda:0')



h[200].sum tensor(26.0160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.0619, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0086, 0.0405, 0.0255,  ..., 0.0350, 0.0348, 0.0367],
        [0.0127, 0.0535, 0.0352,  ..., 0.0480, 0.0461, 0.0489],
        [0.0063, 0.0284, 0.0180,  ..., 0.0245, 0.0243, 0.0255],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(41888.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1238, 0.0000,  ..., 0.0560, 0.0000, 0.0316],
        [0.0000, 0.1348, 0.0000,  ..., 0.0599, 0.0000, 0.0363],
        [0.0000, 0.1115, 0.0000,  ..., 0.0503, 0.0000, 0.0298],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(208097.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-118.9908, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-37.4976, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-248.4875, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9916],
        [0.9835],
        [0.9631],
        ...,
        [0.9400],
        [0.9387],
        [0.9374]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356021.3125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9916],
        [0.9834],
        [0.9631],
        ...,
        [0.9399],
        [0.9386],
        [0.9373]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(356004.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(193.4448, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0016,  0.0081,  0.0049,  ...,  0.0068,  0.0069,  0.0073],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1080.5684, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.5960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(17.2416, device='cuda:0')



h[100].sum tensor(18.4456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.4863, device='cuda:0')



h[200].sum tensor(21.7232, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(19.4452, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0060, 0.0273, 0.0172,  ..., 0.0235, 0.0234, 0.0245],
        [0.0016, 0.0092, 0.0049,  ..., 0.0067, 0.0077, 0.0077],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29842.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.1086, 0.0000,  ..., 0.0492, 0.0000, 0.0292],
        [0.0000, 0.0583, 0.0000,  ..., 0.0290, 0.0000, 0.0143],
        [0.0000, 0.0231, 0.0000,  ..., 0.0147, 0.0000, 0.0047],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(156439.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-60.4813, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.8247, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-160.7842, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9916],
        [0.9834],
        [0.9631],
        ...,
        [0.9399],
        [0.9386],
        [0.9373]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(356004.9062, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9916],
        [0.9834],
        [0.9630],
        ...,
        [0.9398],
        [0.9385],
        [0.9372]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(355988.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(178.5778, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0036,  0.0153,  0.0100,  ...,  0.0137,  0.0132,  0.0140],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(978.4689, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.0858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(15.9165, device='cuda:0')



h[100].sum tensor(18.2062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.0646, device='cuda:0')



h[200].sum tensor(21.2149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(17.9507, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0047, 0.0224, 0.0137,  ..., 0.0188, 0.0191, 0.0200],
        [0.0037, 0.0209, 0.0121,  ..., 0.0168, 0.0178, 0.0185],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(27841.4004, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0354, 0.0000,  ..., 0.0196, 0.0000, 0.0080],
        [0.0000, 0.0578, 0.0000,  ..., 0.0289, 0.0000, 0.0136],
        [0.0000, 0.0747, 0.0000,  ..., 0.0360, 0.0000, 0.0177],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(146213.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-52.1424, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-49.9203, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-147.3013, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9916],
        [0.9834],
        [0.9630],
        ...,
        [0.9398],
        [0.9385],
        [0.9372]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(355988.5000, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9916],
        [0.9833],
        [0.9629],
        ...,
        [0.9398],
        [0.9384],
        [0.9371]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(355972.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(224.8168, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1278.3262, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.0378, device='cuda:0')



h[100].sum tensor(18.9002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.3760, device='cuda:0')



h[200].sum tensor(22.6888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.5987, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31603.6797, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0071, 0.0000, 0.0003],
        [0.0000, 0.0049, 0.0000,  ..., 0.0073, 0.0000, 0.0002],
        [0.0000, 0.0074, 0.0000,  ..., 0.0083, 0.0000, 0.0007],
        ...,
        [0.0000, 0.0027, 0.0000,  ..., 0.0056, 0.0000, 0.0003],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(161982.2344, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-70.3312, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.3500, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-174.7542, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9916],
        [0.9833],
        [0.9629],
        ...,
        [0.9398],
        [0.9384],
        [0.9371]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(355972.0312, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9915],
        [0.9833],
        [0.9629],
        ...,
        [0.9397],
        [0.9383],
        [0.9371]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(355955.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(248.9495, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1456.3250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.8263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.1887, device='cuda:0')



h[100].sum tensor(19.3105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.0604, device='cuda:0')



h[200].sum tensor(23.5602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.0245, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35022.8516, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0029, 0.0000,  ..., 0.0065, 0.0000, 0.0000],
        [0.0000, 0.0012, 0.0000,  ..., 0.0057, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(176468.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-86.2505, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.5110, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-199.0925, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9915],
        [0.9833],
        [0.9629],
        ...,
        [0.9397],
        [0.9383],
        [0.9371]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(355955.5938, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9915],
        [0.9833],
        [0.9628],
        ...,
        [0.9396],
        [0.9382],
        [0.9370]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(355939.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(211.5206, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1197.9169, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.0575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.8527, device='cuda:0')



h[100].sum tensor(18.7088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.9989, device='cuda:0')



h[200].sum tensor(22.2823, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.2622, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(30521.9551, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(158825.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-64.7317, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-47.8425, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-166.1938, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9915],
        [0.9833],
        [0.9628],
        ...,
        [0.9396],
        [0.9382],
        [0.9370]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(355939.1562, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9915],
        [0.9832],
        [0.9627],
        ...,
        [0.9395],
        [0.9382],
        [0.9369]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(355922.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5249],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(253.9485, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0038,  0.0161,  0.0106,  ...,  0.0145,  0.0139,  0.0147],
        [ 0.0040,  0.0171,  0.0113,  ...,  0.0154,  0.0147,  0.0157],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1483.5424, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.7088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.6343, device='cuda:0')



h[100].sum tensor(19.3679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(7.2022, device='cuda:0')



h[200].sum tensor(23.6822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.5270, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0037, 0.0172, 0.0105,  ..., 0.0143, 0.0146, 0.0151],
        [0.0070, 0.0308, 0.0197,  ..., 0.0268, 0.0264, 0.0278],
        [0.0214, 0.0873, 0.0587,  ..., 0.0798, 0.0754, 0.0803],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(35272.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0515, 0.0000,  ..., 0.0258, 0.0000, 0.0135],
        [0.0000, 0.0965, 0.0000,  ..., 0.0436, 0.0000, 0.0271],
        [0.0000, 0.1719, 0.0000,  ..., 0.0731, 0.0000, 0.0507],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(186101.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-87.5634, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-43.4520, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-200.6385, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9915],
        [0.9832],
        [0.9627],
        ...,
        [0.9395],
        [0.9382],
        [0.9369]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(355922.7188, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9915],
        [0.9832],
        [0.9627],
        ...,
        [0.9394],
        [0.9381],
        [0.9368]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(355906.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(315.1355, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1898.7244, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-7.7485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.0879, device='cuda:0')



h[100].sum tensor(20.3259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(8.9375, device='cuda:0')



h[200].sum tensor(25.7169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.6776, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(43310.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0019, 0.0000,  ..., 0.0060, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0087, 0.0000, 0.0009],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(221524.1719, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-125.2091, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-36.5234, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-258.2979, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9915],
        [0.9832],
        [0.9627],
        ...,
        [0.9394],
        [0.9381],
        [0.9368]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(355906.2500, device='cuda:0', grad_fn=<SumBackward0>)
epoch: 1 batch 390.0 event: 1950 loss: tensor(629.1576, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9914],
        [0.9832],
        [0.9626],
        ...,
        [0.9393],
        [0.9380],
        [0.9367]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(355889.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(240.6774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1397.5632, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.1273, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.4515, device='cuda:0')



h[100].sum tensor(19.1634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.8258, device='cuda:0')



h[200].sum tensor(23.2478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.1930, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(36349.6172, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(196644.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-93.3270, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-41.8698, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-209.2913, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9914],
        [0.9832],
        [0.9626],
        ...,
        [0.9393],
        [0.9380],
        [0.9367]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(355889.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9914],
        [0.9832],
        [0.9626],
        ...,
        [0.9393],
        [0.9380],
        [0.9367]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(355889.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(244.8800, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1444.2056, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-9.9066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(21.8260, device='cuda:0')



h[100].sum tensor(19.2712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.9450, device='cuda:0')



h[200].sum tensor(23.4769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(24.6155, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(34550.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0014, 0.0000,  ..., 0.0058, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0080, 0.0000, 0.0004],
        [0.0000, 0.0199, 0.0000,  ..., 0.0134, 0.0000, 0.0035],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(178114.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-83.2985, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-44.7591, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-194.4752, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9914],
        [0.9832],
        [0.9626],
        ...,
        [0.9393],
        [0.9380],
        [0.9367]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(355889.8125, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9914],
        [0.9831],
        [0.9625],
        ...,
        [0.9392],
        [0.9379],
        [0.9366]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(355873.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(216.7720, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0041,  0.0172,  0.0114,  ...,  0.0155,  0.0148,  0.0158],
        [ 0.0079,  0.0313,  0.0213,  ...,  0.0289,  0.0270,  0.0288],
        [ 0.0097,  0.0377,  0.0259,  ...,  0.0351,  0.0326,  0.0348],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1243.6719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.8605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(19.3208, device='cuda:0')



h[100].sum tensor(18.8050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.1478, device='cuda:0')



h[200].sum tensor(22.4867, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(21.7900, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0273, 0.1090, 0.0740,  ..., 0.1005, 0.0942, 0.1005],
        [0.0368, 0.1433, 0.0983,  ..., 0.1333, 0.1239, 0.1324],
        [0.0362, 0.1411, 0.0968,  ..., 0.1312, 0.1219, 0.1303],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(32389.6680, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.3635e-01, 0.0000e+00,  ..., 1.3742e-01, 0.0000e+00,
         1.0187e-01],
        [0.0000e+00, 3.8233e-01, 0.0000e+00,  ..., 1.5469e-01, 0.0000e+00,
         1.1736e-01],
        [0.0000e+00, 3.9181e-01, 0.0000e+00,  ..., 1.5823e-01, 0.0000e+00,
         1.2073e-01],
        ...,
        [0.0000e+00, 3.8517e-04, 0.0000e+00,  ..., 4.7091e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.8545e-04, 0.0000e+00,  ..., 4.7086e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.8545e-04, 0.0000e+00,  ..., 4.7084e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(172499.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-73.3565, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.3788, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-179.3683, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9914],
        [0.9831],
        [0.9625],
        ...,
        [0.9392],
        [0.9379],
        [0.9366]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(355873.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9914],
        [0.9831],
        [0.9624],
        ...,
        [0.9391],
        [0.9378],
        [0.9365]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(355856.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4875],
        [0.3958],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(142.4794, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0035,  0.0150,  0.0098,  ...,  0.0134,  0.0129,  0.0137],
        [ 0.0027,  0.0122,  0.0079,  ...,  0.0108,  0.0105,  0.0112],
        [ 0.0068,  0.0273,  0.0185,  ...,  0.0251,  0.0236,  0.0251],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(733.8666, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-13.2744, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(12.6991, device='cuda:0')



h[100].sum tensor(17.6253, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.0408, device='cuda:0')



h[200].sum tensor(19.9811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(14.3221, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0054, 0.0252, 0.0157,  ..., 0.0215, 0.0216, 0.0226],
        [0.0183, 0.0758, 0.0505,  ..., 0.0688, 0.0654, 0.0696],
        [0.0188, 0.0776, 0.0519,  ..., 0.0706, 0.0670, 0.0713],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(23573.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0824, 0.0000,  ..., 0.0383, 0.0000, 0.0223],
        [0.0000, 0.1486, 0.0000,  ..., 0.0644, 0.0000, 0.0427],
        [0.0000, 0.1710, 0.0000,  ..., 0.0732, 0.0000, 0.0497],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(126120.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-31.8390, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-53.6806, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-116.5862, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9914],
        [0.9831],
        [0.9624],
        ...,
        [0.9391],
        [0.9378],
        [0.9365]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(355856.8750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9914],
        [0.9831],
        [0.9624],
        ...,
        [0.9390],
        [0.9377],
        [0.9364]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(355840.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2546],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(202.2601, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0016,  0.0080,  0.0049,  ...,  0.0067,  0.0069,  0.0072],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0016,  0.0080,  0.0049,  ...,  0.0067,  0.0069,  0.0072],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1142.1244, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-11.3499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(18.0273, device='cuda:0')



h[100].sum tensor(18.5659, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(5.7363, device='cuda:0')



h[200].sum tensor(21.9787, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(20.3313, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0012, 0.0078, 0.0039,  ..., 0.0054, 0.0065, 0.0064],
        [0.0054, 0.0290, 0.0174,  ..., 0.0240, 0.0249, 0.0261],
        [0.0012, 0.0077, 0.0038,  ..., 0.0053, 0.0064, 0.0063],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(29404.2930, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0239, 0.0000,  ..., 0.0153, 0.0000, 0.0038],
        [0.0000, 0.0405, 0.0000,  ..., 0.0224, 0.0000, 0.0078],
        [0.0000, 0.0235, 0.0000,  ..., 0.0151, 0.0000, 0.0037],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(149940.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-59.4968, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-48.6098, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-158.4417, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9914],
        [0.9831],
        [0.9624],
        ...,
        [0.9390],
        [0.9377],
        [0.9364]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(355840.3750, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9914],
        [0.9830],
        [0.9623],
        ...,
        [0.9389],
        [0.9376],
        [0.9363]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(355823.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(501.3649, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [ 0.0023,  0.0109,  0.0069,  ...,  0.0095,  0.0094,  0.0099],
        [ 0.0102,  0.0394,  0.0271,  ...,  0.0367,  0.0341,  0.0364],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(3136.1064, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-1.9457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(44.6864, device='cuda:0')



h[100].sum tensor(23.1617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(14.2191, device='cuda:0')



h[200].sum tensor(31.7401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(50.3975, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0065, 0.0292, 0.0185,  ..., 0.0252, 0.0250, 0.0262],
        [0.0205, 0.0820, 0.0554,  ..., 0.0752, 0.0708, 0.0754],
        [0.0271, 0.1079, 0.0733,  ..., 0.0994, 0.0932, 0.0994],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(62387.1836, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.6416e-01, 0.0000e+00,  ..., 6.9984e-02, 0.0000e+00,
         4.8317e-02],
        [0.0000e+00, 2.9614e-01, 0.0000e+00,  ..., 1.2092e-01, 0.0000e+00,
         9.0641e-02],
        [0.0000e+00, 3.9809e-01, 0.0000e+00,  ..., 1.6013e-01, 0.0000e+00,
         1.2343e-01],
        ...,
        [0.0000e+00, 3.8539e-04, 0.0000e+00,  ..., 4.7066e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.8567e-04, 0.0000e+00,  ..., 4.7061e-03, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 3.8567e-04, 0.0000e+00,  ..., 4.7059e-03, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(313609.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-217.7029, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-17.9728, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-397.9831, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9914],
        [0.9830],
        [0.9623],
        ...,
        [0.9389],
        [0.9376],
        [0.9363]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(355823.9375, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9913],
        [0.9830],
        [0.9622],
        ...,
        [0.9388],
        [0.9375],
        [0.9362]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(355807.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(161.7711, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(868.6420, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-12.6486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(14.4186, device='cuda:0')



h[100].sum tensor(17.9312, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(4.5880, device='cuda:0')



h[200].sum tensor(20.6307, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(16.2613, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(25538.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0063, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(133163.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-41.8947, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-51.5318, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-131.3403, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9913],
        [0.9830],
        [0.9622],
        ...,
        [0.9388],
        [0.9375],
        [0.9362]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(355807.4688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9913],
        [0.9829],
        [0.9622],
        ...,
        [0.9387],
        [0.9374],
        [0.9361]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(355790.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(226.3286, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(1298.1685, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-10.6284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.1726, device='cuda:0')



h[100].sum tensor(18.9185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(6.4189, device='cuda:0')



h[200].sum tensor(22.7276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(22.7507, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(31603.6582, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(161031.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-69.8761, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-46.7109, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-174.2147, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9913],
        [0.9829],
        [0.9622],
        ...,
        [0.9387],
        [0.9374],
        [0.9361]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(355790.9688, device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')



input graph: 
g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet] tensor([[0.9913],
        [0.9829],
        [0.9621],
        ...,
        [0.9386],
        [0.9373],
        [0.9360]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(355774.4688, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([33980, 1]) 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(336.2469, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0106,  0.0411,  0.0283,  ...,  0.0384,  0.0356,  0.0380],
        [ 0.0068,  0.0273,  0.0185,  ...,  0.0252,  0.0236,  0.0252],
        [ 0.0033,  0.0144,  0.0094,  ...,  0.0128,  0.0124,  0.0132],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(2084.2988, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-6.9320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.9695, device='cuda:0')



h[100].sum tensor(20.7249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(9.5363, device='cuda:0')



h[200].sum tensor(26.5645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.7997, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0308, 0.1215, 0.0829,  ..., 0.1124, 0.1050, 0.1121],
        [0.0219, 0.0889, 0.0598,  ..., 0.0813, 0.0768, 0.0818],
        [0.0117, 0.0500, 0.0327,  ..., 0.0446, 0.0430, 0.0456],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([33980, 256]) 
h.sum tensor(45047.1953, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.3173, 0.0000,  ..., 0.1299, 0.0000, 0.0960],
        [0.0000, 0.2697, 0.0000,  ..., 0.1116, 0.0000, 0.0803],
        [0.0000, 0.2079, 0.0000,  ..., 0.0879, 0.0000, 0.0602],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([33980, 128]) 
h2.sum tensor(232149.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-134.1199, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-34.7118, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-271.2359, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=33980, num_edges=365930,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([33980, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9913],
        [0.9829],
        [0.9621],
        ...,
        [0.9386],
        [0.9373],
        [0.9360]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([365930, 1]) 
g.edata[efet].sum tensor(355774.4688, device='cuda:0', grad_fn=<SumBackward0>)
time passed so far:
 0:00:58.719300
evaluation loss: 602.9794311523438
epoch: 1 mean loss: 589.6746215820312
=> saveing checkpoint at epoch 1
checkpoint is saved at: /hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2



training loss:
 [590.00250244 589.67462158] 

\evaluation loss:
 [602.97943115 602.97943115]



eval_efficiency:
 [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan] 


eval_purity:
 [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan
 nan nan nan nan nan nan nan nan nan nan]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[0.9913],
        [0.9829],
        [0.9620],
        ...,
        [0.9386],
        [0.9372],
        [0.9359]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(71151.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(180.1173, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.5019, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(2.9581, device='cuda:0')



h[100].sum tensor(3.5999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(0.9412, device='cuda:0')



h[200].sum tensor(4.1551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(3.3361, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(4853.9785, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0053, 0.0000, 0.0000],
        [0.0000, 0.0018, 0.0000,  ..., 0.0059, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0084, 0.0000, 0.0010],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(24541.0664, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.1234, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-10.5358, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-24.4171, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9913],
        [0.9829],
        [0.9620],
        ...,
        [0.9386],
        [0.9372],
        [0.9359]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(71151.5938, device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network after training 
result1: tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[0.9913],
        [0.9829],
        [0.9620],
        ...,
        [0.9386],
        [0.9372],
        [0.9359]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(142303.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        ...,
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002],
        [-0.0005,  0.0004, -0.0005,  ..., -0.0005,  0.0003,  0.0002]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(839.8992, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-2.7468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(11.8082, device='cuda:0')



h[100].sum tensor(8.3027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(3.7573, device='cuda:0')



h[200].sum tensor(10.6528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(13.3173, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0007],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0011, 0.0006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(18781.4492, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0217, 0.0000,  ..., 0.0141, 0.0000, 0.0047],
        [0.0000, 0.0049, 0.0000,  ..., 0.0072, 0.0000, 0.0005],
        [0.0000, 0.0003, 0.0000,  ..., 0.0052, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0047, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(96028.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-57.7412, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-12.6283, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-114.7214, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[0.9913],
        [0.9829],
        [0.9620],
        ...,
        [0.9386],
        [0.9372],
        [0.9359]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(142303.1875, device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network after training 
result1: tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0', grad_fn=<ReluBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



total time: 0:01:02.332895 hpmesh elements: 44 to 45

real	1m41.254s
user	1m5.016s
sys	0m25.196s
