0: gpu031.ihep.ac.cn
GPU 0: Tesla V100-SXM2-32GB (UUID: GPU-c3328cc0-6d96-e175-21d8-3ddd5b25b878)
Allocate GPU cards : 0

modinfo:
filename:       /lib/modules/3.10.0-1127.8.2.el7.x86_64/extra/nvidia.ko.xz
alias:          char-major-195-*
version:        450.36.06
supported:      external
license:        NVIDIA
retpoline:      Y
rhelversion:    7.8
srcversion:     BB5CB243542347D4EB0C79C
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:        
vermagic:       3.10.0-1127.8.2.el7.x86_64 SMP mod_unload modversions 
parm:           NvSwitchRegDwords:NvSwitch regkey (charp)
parm:           NvSwitchBlacklist:NvSwitchBlacklist=uuid[,uuid...] (charp)
parm:           NVreg_ResmanDebugLevel:int
parm:           NVreg_RmLogonRC:int
parm:           NVreg_ModifyDeviceFiles:int
parm:           NVreg_DeviceFileUID:int
parm:           NVreg_DeviceFileGID:int
parm:           NVreg_DeviceFileMode:int
parm:           NVreg_InitializeSystemMemoryAllocations:int
parm:           NVreg_UsePageAttributeTable:int
parm:           NVreg_MapRegistersEarly:int
parm:           NVreg_RegisterForACPIEvents:int
parm:           NVreg_EnablePCIeGen3:int
parm:           NVreg_EnableMSI:int
parm:           NVreg_TCEBypassMode:int
parm:           NVreg_EnableStreamMemOPs:int
parm:           NVreg_EnableBacklightHandler:int
parm:           NVreg_RestrictProfilingToAdminUsers:int
parm:           NVreg_PreserveVideoMemoryAllocations:int
parm:           NVreg_DynamicPowerManagement:int
parm:           NVreg_DynamicPowerManagementVideoMemoryThreshold:int
parm:           NVreg_EnableUserNUMAManagement:int
parm:           NVreg_MemoryPoolSize:int
parm:           NVreg_KMallocHeapMaxSize:int
parm:           NVreg_VMallocHeapMaxSize:int
parm:           NVreg_IgnoreMMIOCheck:int
parm:           NVreg_NvLinkDisable:int
parm:           NVreg_EnablePCIERelaxedOrderingMode:int
parm:           NVreg_RegisterPCIDriver:int
parm:           NVreg_RegistryDwords:charp
parm:           NVreg_RegistryDwordsPerDevice:charp
parm:           NVreg_RmMsg:charp
parm:           NVreg_GpuBlacklist:charp
parm:           NVreg_TemporaryFilePath:charp
parm:           NVreg_AssignGpus:charp

nvidia-smi:
Fri Aug 12 16:45:09 2022       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.36.06    Driver Version: 450.36.06    CUDA Version: 11.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:B2:00.0 Off |                    0 |
| N/A   35C    P0    42W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

nvcc --version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

 torch version: 1.10.2

 cuda version: 11.3

 is cuda available: True

 CUDNN VERSION: 8200

 Number CUDA Devices: 1

 CUDA Device Name: Tesla V100-SXM2-32GB

 CUDA Device Total Memory [GB]: 34.089730048

 Device capability: (7, 0) 

 Cuda deviice: <torch.cuda.device object at 0x2aedfff438e0> 

 Is cuda initialized: True

 CUDA_HOME: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1

real	0m51.448s
user	0m3.023s
sys	0m2.089s
[16:46:01] /opt/dgl/src/runtime/tensordispatch.cc:43: TensorDispatcher: dlopen failed: /hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/tensoradapter/pytorch/libtensoradapter_pytorch_1.10.2.so: cannot open shared object file: No such file or directory
Using backend: pytorch
/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/cupy/_environment.py:438: UserWarning: 
--------------------------------------------------------------------------------

  CuPy may not function correctly because multiple CuPy packages are installed
  in your environment:

    cupy, cupy-cuda110

  Follow these steps to resolve this issue:

    1. For all packages listed above, run the following command to remove all
       existing CuPy installations:

         $ pip uninstall <package_name>

      If you previously installed CuPy via conda, also run the following:

         $ conda uninstall cupy

    2. Install the appropriate CuPy package.
       Refer to the Installation Guide for detailed instructions.

         https://docs.cupy.dev/en/stable/install.html

--------------------------------------------------------------------------------

  warnings.warn(f'''




 Training ... 






 The Network ... 






 The graph ... 



edge_index
 tensor([[   0,    1,    2,  ..., 4907, 4907, 4907],
        [   1,    2,    3,  ..., 4918, 4919, 4920]]) 

edge_index shape
 torch.Size([2, 36593])
graph: Graph(num_nodes=6796, num_edges=36593,
      ndata_schemes={}
      edata_schemes={}) 
nodes: tensor([   0,    1,    2,  ..., 6793, 6794, 6795], device='cuda:0') 
nodes shape: torch.Size([6796]) 
edges: (tensor([   0,    1,    2,  ..., 4907, 4907, 4907], device='cuda:0'), tensor([   1,    2,    3,  ..., 4918, 4919, 4920], device='cuda:0')) 
edges shae:

number of nodes: 6796

number of edges: 73186

node features (random input): tensor([[ 0.0291],
        [-0.2807],
        [-1.0716],
        ...,
        [-0.4523],
        [-1.4400],
        [-0.9797]], device='cuda:0', requires_grad=True) 
node features sum: tensor(-9.2806, device='cuda:0', grad_fn=<SumBackward0>)

edges features: tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
edges features sum: tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

example: 
Out degrees of node 234: 14

In degrees of node 234: 14





 Loading data ... 


shape (80000, 6796) (80000, 6796)
sum 5574226 8401300
shape torch.Size([80000, 6796]) torch.Size([80000, 6796])
Model name: DGLpppipiGcnReNewestweight7N2
net GCN(
  (conv1): GraphConv(in=1, out=256, normalization=both, activation=None)
  (conv2): GraphConv(in=256, out=128, normalization=both, activation=None)
  (conv3): GraphConv(in=128, out=64, normalization=both, activation=None)
  (conv4): GraphConv(in=64, out=32, normalization=both, activation=None)
  (conv5): GraphConv(in=32, out=1, normalization=both, activation=None)
)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[-0.0951,  0.1192, -0.0254,  0.0703, -0.0059,  0.0931,  0.0318,  0.0012,
         -0.1297,  0.0539,  0.1199,  0.0442, -0.1385,  0.0679, -0.0401, -0.0926,
         -0.0517,  0.0445,  0.1105,  0.1209,  0.0946, -0.0756, -0.1215, -0.0264,
          0.1254,  0.0304,  0.0773, -0.0764,  0.0644, -0.0550,  0.0349,  0.0970,
         -0.0639, -0.0428,  0.1178,  0.0828, -0.0184,  0.1011,  0.1050, -0.0803,
          0.1038,  0.1279,  0.0586,  0.0208, -0.0784, -0.0944,  0.0356, -0.1356,
          0.1164, -0.0024,  0.0032, -0.0808,  0.0241, -0.0747, -0.1282,  0.0029,
          0.1078, -0.1101, -0.0204, -0.0527, -0.0325, -0.1035,  0.1360,  0.0274,
          0.0161, -0.1371, -0.0397, -0.0195, -0.1477,  0.0743,  0.0544,  0.1172,
         -0.1173, -0.1318,  0.1307,  0.0427, -0.1389, -0.0390, -0.0667,  0.1187,
          0.0432,  0.0079, -0.0559,  0.0556,  0.0019, -0.0585, -0.1205, -0.1095,
          0.0559, -0.1114, -0.0120, -0.1019,  0.1074,  0.0239, -0.1020, -0.0446,
         -0.0669, -0.0385, -0.0453, -0.0808, -0.0504, -0.0944,  0.0100, -0.0857,
         -0.0163, -0.0496, -0.0114,  0.1420, -0.0151,  0.1224,  0.1034,  0.1205,
         -0.0600,  0.1342,  0.0253,  0.1461, -0.1085, -0.1240,  0.0532,  0.0101,
          0.0695,  0.0660,  0.0188,  0.0255, -0.1407,  0.1350, -0.0746, -0.1027,
          0.1380, -0.0573, -0.0778, -0.1306,  0.1334, -0.0483, -0.0249, -0.0578,
          0.0322,  0.0162,  0.1343,  0.0552,  0.0981,  0.1466,  0.1000, -0.0184,
          0.0333, -0.0081,  0.1039, -0.1156, -0.0993, -0.1203, -0.0456, -0.0996,
          0.0564, -0.1526,  0.0997,  0.0558,  0.0298,  0.0625, -0.0420, -0.0582,
          0.1039, -0.0893,  0.0509,  0.0248,  0.0353, -0.0756,  0.1347, -0.0058,
         -0.0977,  0.1467, -0.0100,  0.1351, -0.0558, -0.0680,  0.0221,  0.0038,
          0.0622, -0.1017,  0.1436, -0.0421, -0.0683,  0.1224,  0.0362, -0.0314,
          0.0428,  0.0730, -0.0224, -0.1220,  0.1377,  0.0803, -0.0901,  0.1048,
         -0.1363, -0.0489, -0.1509, -0.1312,  0.0411,  0.0217, -0.0670, -0.0478,
         -0.0299,  0.0471, -0.0720,  0.1434,  0.1387,  0.1127, -0.0056, -0.1020,
          0.1159, -0.0668, -0.0391, -0.0328,  0.0884, -0.0763,  0.1498, -0.1462,
         -0.1041, -0.0029,  0.0054,  0.1335,  0.0670, -0.1423, -0.1342,  0.0422,
          0.0681, -0.0030, -0.1197,  0.1235, -0.0696, -0.0045, -0.0960, -0.0487,
          0.0747, -0.0035,  0.0380,  0.1318,  0.1487,  0.0289, -0.0756,  0.1449,
         -0.0478,  0.0249, -0.0495, -0.0658,  0.0035, -0.0971, -0.0523,  0.0963,
         -0.0760,  0.1443,  0.0930,  0.0859, -0.0088,  0.1262, -0.0520, -0.0675]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0951,  0.1192, -0.0254,  0.0703, -0.0059,  0.0931,  0.0318,  0.0012,
         -0.1297,  0.0539,  0.1199,  0.0442, -0.1385,  0.0679, -0.0401, -0.0926,
         -0.0517,  0.0445,  0.1105,  0.1209,  0.0946, -0.0756, -0.1215, -0.0264,
          0.1254,  0.0304,  0.0773, -0.0764,  0.0644, -0.0550,  0.0349,  0.0970,
         -0.0639, -0.0428,  0.1178,  0.0828, -0.0184,  0.1011,  0.1050, -0.0803,
          0.1038,  0.1279,  0.0586,  0.0208, -0.0784, -0.0944,  0.0356, -0.1356,
          0.1164, -0.0024,  0.0032, -0.0808,  0.0241, -0.0747, -0.1282,  0.0029,
          0.1078, -0.1101, -0.0204, -0.0527, -0.0325, -0.1035,  0.1360,  0.0274,
          0.0161, -0.1371, -0.0397, -0.0195, -0.1477,  0.0743,  0.0544,  0.1172,
         -0.1173, -0.1318,  0.1307,  0.0427, -0.1389, -0.0390, -0.0667,  0.1187,
          0.0432,  0.0079, -0.0559,  0.0556,  0.0019, -0.0585, -0.1205, -0.1095,
          0.0559, -0.1114, -0.0120, -0.1019,  0.1074,  0.0239, -0.1020, -0.0446,
         -0.0669, -0.0385, -0.0453, -0.0808, -0.0504, -0.0944,  0.0100, -0.0857,
         -0.0163, -0.0496, -0.0114,  0.1420, -0.0151,  0.1224,  0.1034,  0.1205,
         -0.0600,  0.1342,  0.0253,  0.1461, -0.1085, -0.1240,  0.0532,  0.0101,
          0.0695,  0.0660,  0.0188,  0.0255, -0.1407,  0.1350, -0.0746, -0.1027,
          0.1380, -0.0573, -0.0778, -0.1306,  0.1334, -0.0483, -0.0249, -0.0578,
          0.0322,  0.0162,  0.1343,  0.0552,  0.0981,  0.1466,  0.1000, -0.0184,
          0.0333, -0.0081,  0.1039, -0.1156, -0.0993, -0.1203, -0.0456, -0.0996,
          0.0564, -0.1526,  0.0997,  0.0558,  0.0298,  0.0625, -0.0420, -0.0582,
          0.1039, -0.0893,  0.0509,  0.0248,  0.0353, -0.0756,  0.1347, -0.0058,
         -0.0977,  0.1467, -0.0100,  0.1351, -0.0558, -0.0680,  0.0221,  0.0038,
          0.0622, -0.1017,  0.1436, -0.0421, -0.0683,  0.1224,  0.0362, -0.0314,
          0.0428,  0.0730, -0.0224, -0.1220,  0.1377,  0.0803, -0.0901,  0.1048,
         -0.1363, -0.0489, -0.1509, -0.1312,  0.0411,  0.0217, -0.0670, -0.0478,
         -0.0299,  0.0471, -0.0720,  0.1434,  0.1387,  0.1127, -0.0056, -0.1020,
          0.1159, -0.0668, -0.0391, -0.0328,  0.0884, -0.0763,  0.1498, -0.1462,
         -0.1041, -0.0029,  0.0054,  0.1335,  0.0670, -0.1423, -0.1342,  0.0422,
          0.0681, -0.0030, -0.1197,  0.1235, -0.0696, -0.0045, -0.0960, -0.0487,
          0.0747, -0.0035,  0.0380,  0.1318,  0.1487,  0.0289, -0.0756,  0.1449,
         -0.0478,  0.0249, -0.0495, -0.0658,  0.0035, -0.0971, -0.0523,  0.0963,
         -0.0760,  0.1443,  0.0930,  0.0859, -0.0088,  0.1262, -0.0520, -0.0675]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[ 0.0408,  0.0128,  0.0853,  ..., -0.0736,  0.1142,  0.0674],
        [-0.0843, -0.0178, -0.0979,  ..., -0.0527, -0.0158, -0.0831],
        [-0.0681, -0.0400,  0.1187,  ...,  0.0650,  0.0265, -0.1106],
        ...,
        [ 0.1065,  0.0751, -0.0152,  ..., -0.0892, -0.1133, -0.0246],
        [ 0.0404, -0.0032,  0.0041,  ..., -0.0555,  0.0651,  0.0057],
        [-0.0750, -0.0537, -0.0424,  ..., -0.0542, -0.0265, -0.0622]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0408,  0.0128,  0.0853,  ..., -0.0736,  0.1142,  0.0674],
        [-0.0843, -0.0178, -0.0979,  ..., -0.0527, -0.0158, -0.0831],
        [-0.0681, -0.0400,  0.1187,  ...,  0.0650,  0.0265, -0.1106],
        ...,
        [ 0.1065,  0.0751, -0.0152,  ..., -0.0892, -0.1133, -0.0246],
        [ 0.0404, -0.0032,  0.0041,  ..., -0.0555,  0.0651,  0.0057],
        [-0.0750, -0.0537, -0.0424,  ..., -0.0542, -0.0265, -0.0622]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[ 0.1607,  0.1751,  0.0204,  ...,  0.0163, -0.0505, -0.0736],
        [ 0.1409,  0.0086, -0.0653,  ..., -0.1280,  0.0448, -0.0089],
        [ 0.0037, -0.0989, -0.1484,  ...,  0.1510, -0.0516,  0.0124],
        ...,
        [-0.1196,  0.0105,  0.1225,  ..., -0.1474, -0.0420, -0.1431],
        [ 0.0251, -0.1135, -0.1247,  ..., -0.0412, -0.0337, -0.0589],
        [ 0.1551, -0.0784, -0.0743,  ...,  0.0951,  0.0730, -0.1606]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1607,  0.1751,  0.0204,  ...,  0.0163, -0.0505, -0.0736],
        [ 0.1409,  0.0086, -0.0653,  ..., -0.1280,  0.0448, -0.0089],
        [ 0.0037, -0.0989, -0.1484,  ...,  0.1510, -0.0516,  0.0124],
        ...,
        [-0.1196,  0.0105,  0.1225,  ..., -0.1474, -0.0420, -0.1431],
        [ 0.0251, -0.1135, -0.1247,  ..., -0.0412, -0.0337, -0.0589],
        [ 0.1551, -0.0784, -0.0743,  ...,  0.0951,  0.0730, -0.1606]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[ 0.0795,  0.1066,  0.0831,  ..., -0.2216,  0.0680, -0.2274],
        [ 0.0813, -0.1359,  0.0118,  ...,  0.2407,  0.0589,  0.1448],
        [-0.1489,  0.1989, -0.0835,  ..., -0.1976, -0.0174, -0.0102],
        ...,
        [ 0.1022,  0.1691, -0.0549,  ...,  0.0054,  0.1470,  0.0030],
        [ 0.1872, -0.1003, -0.2381,  ...,  0.1036, -0.0097,  0.0146],
        [-0.2156,  0.0387,  0.0779,  ..., -0.0119,  0.0107, -0.2122]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.0795,  0.1066,  0.0831,  ..., -0.2216,  0.0680, -0.2274],
        [ 0.0813, -0.1359,  0.0118,  ...,  0.2407,  0.0589,  0.1448],
        [-0.1489,  0.1989, -0.0835,  ..., -0.1976, -0.0174, -0.0102],
        ...,
        [ 0.1022,  0.1691, -0.0549,  ...,  0.0054,  0.1470,  0.0030],
        [ 0.1872, -0.1003, -0.2381,  ...,  0.1036, -0.0097,  0.0146],
        [-0.2156,  0.0387,  0.0779,  ..., -0.0119,  0.0107, -0.2122]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[-0.3707],
        [-0.1655],
        [-0.3547],
        [ 0.1196],
        [-0.3502],
        [-0.0503],
        [ 0.3788],
        [-0.0501],
        [-0.3426],
        [-0.3257],
        [-0.1963],
        [ 0.3891],
        [ 0.2867],
        [ 0.3898],
        [ 0.1036],
        [ 0.2359],
        [-0.0145],
        [ 0.3866],
        [ 0.0757],
        [ 0.3240],
        [-0.0453],
        [ 0.1231],
        [ 0.0834],
        [-0.2711],
        [-0.2800],
        [-0.2659],
        [-0.2520],
        [ 0.0511],
        [-0.0607],
        [ 0.2577],
        [ 0.2927],
        [ 0.2376]], device='cuda:0') 
 Parameter containing:
tensor([[-0.3707],
        [-0.1655],
        [-0.3547],
        [ 0.1196],
        [-0.3502],
        [-0.0503],
        [ 0.3788],
        [-0.0501],
        [-0.3426],
        [-0.3257],
        [-0.1963],
        [ 0.3891],
        [ 0.2867],
        [ 0.3898],
        [ 0.1036],
        [ 0.2359],
        [-0.0145],
        [ 0.3866],
        [ 0.0757],
        [ 0.3240],
        [-0.0453],
        [ 0.1231],
        [ 0.0834],
        [-0.2711],
        [-0.2800],
        [-0.2659],
        [-0.2520],
        [ 0.0511],
        [-0.0607],
        [ 0.2577],
        [ 0.2927],
        [ 0.2376]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)
conv1.weight 
 torch.Size([1, 256]) 
 True 
 tensor([[ 0.1028, -0.0120,  0.1069, -0.0606,  0.0415,  0.1132,  0.1056,  0.0759,
          0.1423,  0.0432,  0.1209, -0.0925,  0.0074,  0.0148,  0.1069, -0.1142,
         -0.0756, -0.0676, -0.1111,  0.0884, -0.0065, -0.0749,  0.0006,  0.0370,
         -0.1415, -0.1033, -0.1467, -0.0990,  0.0788,  0.0483,  0.1291,  0.0247,
          0.0265, -0.1124,  0.0999,  0.0823, -0.0782,  0.0990, -0.0814,  0.0928,
         -0.0046,  0.1368,  0.1189, -0.0143,  0.1469, -0.0821,  0.0802, -0.0100,
          0.0336,  0.0233, -0.1199,  0.1097, -0.0706,  0.0769,  0.0327, -0.0082,
          0.1242,  0.0625,  0.0667, -0.1165, -0.0403, -0.0890, -0.0636,  0.0959,
          0.0814,  0.0072, -0.0603,  0.0100,  0.0600, -0.1100,  0.0303, -0.1501,
         -0.1429,  0.0681,  0.0465, -0.0317,  0.0623,  0.0226, -0.0767,  0.0830,
          0.1030,  0.0780, -0.0786, -0.1434,  0.0426, -0.0680,  0.1114,  0.0742,
         -0.0606, -0.0524, -0.0601,  0.0403, -0.0804, -0.1195, -0.1432, -0.0209,
         -0.0225,  0.0674,  0.1343, -0.1354, -0.1407,  0.1140,  0.0461, -0.1206,
         -0.0168,  0.1040,  0.0290,  0.0384,  0.0217,  0.0504,  0.0469, -0.0106,
         -0.0747,  0.1134, -0.1229, -0.1332,  0.1495, -0.0757,  0.0028,  0.0291,
         -0.0261, -0.0286, -0.0130, -0.1487,  0.0094,  0.1454, -0.0829, -0.0535,
          0.0870, -0.0960, -0.0481, -0.0903,  0.0987, -0.0800, -0.1062, -0.0954,
         -0.0138,  0.0428,  0.1511, -0.0120,  0.0981, -0.1322, -0.0748, -0.0594,
         -0.0298,  0.0243, -0.1495, -0.1156, -0.0207, -0.0477, -0.0973, -0.0015,
          0.0052,  0.0784, -0.0521,  0.0566, -0.0325,  0.0227, -0.0224,  0.0487,
          0.0939, -0.0341, -0.0894,  0.0017, -0.0465,  0.0004, -0.1036,  0.1058,
         -0.1234,  0.0965, -0.0273, -0.0693,  0.1096, -0.0493, -0.1142, -0.0550,
          0.0282,  0.1312,  0.1490, -0.0382, -0.1412, -0.1480, -0.0808,  0.0211,
          0.0642, -0.0060, -0.0538, -0.1183, -0.1457, -0.0244, -0.1134,  0.1007,
          0.0185,  0.0542,  0.1522,  0.1024,  0.1112, -0.0878, -0.1446,  0.1220,
         -0.0109,  0.0267,  0.0275, -0.0527,  0.1435, -0.0082,  0.0157,  0.1497,
          0.1445,  0.0668, -0.0621,  0.1243, -0.0122, -0.0676,  0.0358, -0.0311,
          0.0463, -0.0314, -0.0696,  0.0770,  0.0934, -0.1284,  0.0021,  0.1258,
         -0.0908,  0.0886,  0.1410, -0.1524,  0.0371,  0.0355,  0.0701,  0.1014,
          0.0397,  0.0295,  0.1093,  0.0159, -0.0700,  0.1424, -0.0626, -0.0191,
         -0.0214,  0.0043, -0.1251, -0.1027, -0.0146,  0.0590,  0.0806, -0.1525,
          0.0672,  0.0176,  0.1217,  0.0231, -0.1118,  0.1337, -0.0353, -0.0443]],
       device='cuda:0') 
 Parameter containing:
tensor([[ 0.1028, -0.0120,  0.1069, -0.0606,  0.0415,  0.1132,  0.1056,  0.0759,
          0.1423,  0.0432,  0.1209, -0.0925,  0.0074,  0.0148,  0.1069, -0.1142,
         -0.0756, -0.0676, -0.1111,  0.0884, -0.0065, -0.0749,  0.0006,  0.0370,
         -0.1415, -0.1033, -0.1467, -0.0990,  0.0788,  0.0483,  0.1291,  0.0247,
          0.0265, -0.1124,  0.0999,  0.0823, -0.0782,  0.0990, -0.0814,  0.0928,
         -0.0046,  0.1368,  0.1189, -0.0143,  0.1469, -0.0821,  0.0802, -0.0100,
          0.0336,  0.0233, -0.1199,  0.1097, -0.0706,  0.0769,  0.0327, -0.0082,
          0.1242,  0.0625,  0.0667, -0.1165, -0.0403, -0.0890, -0.0636,  0.0959,
          0.0814,  0.0072, -0.0603,  0.0100,  0.0600, -0.1100,  0.0303, -0.1501,
         -0.1429,  0.0681,  0.0465, -0.0317,  0.0623,  0.0226, -0.0767,  0.0830,
          0.1030,  0.0780, -0.0786, -0.1434,  0.0426, -0.0680,  0.1114,  0.0742,
         -0.0606, -0.0524, -0.0601,  0.0403, -0.0804, -0.1195, -0.1432, -0.0209,
         -0.0225,  0.0674,  0.1343, -0.1354, -0.1407,  0.1140,  0.0461, -0.1206,
         -0.0168,  0.1040,  0.0290,  0.0384,  0.0217,  0.0504,  0.0469, -0.0106,
         -0.0747,  0.1134, -0.1229, -0.1332,  0.1495, -0.0757,  0.0028,  0.0291,
         -0.0261, -0.0286, -0.0130, -0.1487,  0.0094,  0.1454, -0.0829, -0.0535,
          0.0870, -0.0960, -0.0481, -0.0903,  0.0987, -0.0800, -0.1062, -0.0954,
         -0.0138,  0.0428,  0.1511, -0.0120,  0.0981, -0.1322, -0.0748, -0.0594,
         -0.0298,  0.0243, -0.1495, -0.1156, -0.0207, -0.0477, -0.0973, -0.0015,
          0.0052,  0.0784, -0.0521,  0.0566, -0.0325,  0.0227, -0.0224,  0.0487,
          0.0939, -0.0341, -0.0894,  0.0017, -0.0465,  0.0004, -0.1036,  0.1058,
         -0.1234,  0.0965, -0.0273, -0.0693,  0.1096, -0.0493, -0.1142, -0.0550,
          0.0282,  0.1312,  0.1490, -0.0382, -0.1412, -0.1480, -0.0808,  0.0211,
          0.0642, -0.0060, -0.0538, -0.1183, -0.1457, -0.0244, -0.1134,  0.1007,
          0.0185,  0.0542,  0.1522,  0.1024,  0.1112, -0.0878, -0.1446,  0.1220,
         -0.0109,  0.0267,  0.0275, -0.0527,  0.1435, -0.0082,  0.0157,  0.1497,
          0.1445,  0.0668, -0.0621,  0.1243, -0.0122, -0.0676,  0.0358, -0.0311,
          0.0463, -0.0314, -0.0696,  0.0770,  0.0934, -0.1284,  0.0021,  0.1258,
         -0.0908,  0.0886,  0.1410, -0.1524,  0.0371,  0.0355,  0.0701,  0.1014,
          0.0397,  0.0295,  0.1093,  0.0159, -0.0700,  0.1424, -0.0626, -0.0191,
         -0.0214,  0.0043, -0.1251, -0.1027, -0.0146,  0.0590,  0.0806, -0.1525,
          0.0672,  0.0176,  0.1217,  0.0231, -0.1118,  0.1337, -0.0353, -0.0443]],
       device='cuda:0', requires_grad=True)
conv1.bias 
 torch.Size([256]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv2.weight 
 torch.Size([256, 128]) 
 True 
 tensor([[-0.0256,  0.1160,  0.1146,  ..., -0.0479,  0.0626, -0.0561],
        [-0.0329,  0.0010,  0.0108,  ..., -0.0668, -0.0988,  0.0366],
        [ 0.0709,  0.0022,  0.0243,  ..., -0.0324,  0.0450, -0.1217],
        ...,
        [-0.0228,  0.0107, -0.0361,  ...,  0.1098,  0.0469, -0.0788],
        [ 0.0062,  0.0748, -0.0297,  ...,  0.0586, -0.0175,  0.1118],
        [-0.0535, -0.0248,  0.0277,  ...,  0.0497,  0.0071,  0.0888]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0256,  0.1160,  0.1146,  ..., -0.0479,  0.0626, -0.0561],
        [-0.0329,  0.0010,  0.0108,  ..., -0.0668, -0.0988,  0.0366],
        [ 0.0709,  0.0022,  0.0243,  ..., -0.0324,  0.0450, -0.1217],
        ...,
        [-0.0228,  0.0107, -0.0361,  ...,  0.1098,  0.0469, -0.0788],
        [ 0.0062,  0.0748, -0.0297,  ...,  0.0586, -0.0175,  0.1118],
        [-0.0535, -0.0248,  0.0277,  ...,  0.0497,  0.0071,  0.0888]],
       device='cuda:0', requires_grad=True)
conv2.bias 
 torch.Size([128]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv3.weight 
 torch.Size([128, 64]) 
 True 
 tensor([[-0.0027, -0.1011,  0.1335,  ...,  0.1066,  0.0889, -0.0057],
        [-0.0596,  0.0342, -0.0259,  ...,  0.0992, -0.0881,  0.0771],
        [ 0.0922,  0.1342, -0.1258,  ..., -0.0626, -0.1288,  0.0043],
        ...,
        [ 0.1155, -0.0105,  0.1333,  ..., -0.1348,  0.1156, -0.0391],
        [-0.0438,  0.0536,  0.1389,  ..., -0.0632,  0.1767,  0.0732],
        [ 0.1758, -0.0874, -0.1185,  ..., -0.0732, -0.0132,  0.1682]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.0027, -0.1011,  0.1335,  ...,  0.1066,  0.0889, -0.0057],
        [-0.0596,  0.0342, -0.0259,  ...,  0.0992, -0.0881,  0.0771],
        [ 0.0922,  0.1342, -0.1258,  ..., -0.0626, -0.1288,  0.0043],
        ...,
        [ 0.1155, -0.0105,  0.1333,  ..., -0.1348,  0.1156, -0.0391],
        [-0.0438,  0.0536,  0.1389,  ..., -0.0632,  0.1767,  0.0732],
        [ 0.1758, -0.0874, -0.1185,  ..., -0.0732, -0.0132,  0.1682]],
       device='cuda:0', requires_grad=True)
conv3.bias 
 torch.Size([64]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True)
conv4.weight 
 torch.Size([64, 32]) 
 True 
 tensor([[-0.1563,  0.2447,  0.2188,  ...,  0.1009, -0.0742,  0.1276],
        [-0.1200,  0.0795,  0.2321,  ...,  0.1204, -0.0699, -0.0532],
        [-0.0133,  0.1043, -0.0225,  ..., -0.1308, -0.0137, -0.0726],
        ...,
        [ 0.0992,  0.1869, -0.1506,  ..., -0.1051,  0.0279,  0.0808],
        [-0.2383, -0.1013, -0.0086,  ...,  0.0665, -0.0418,  0.0515],
        [-0.1058, -0.1730,  0.0903,  ..., -0.1828, -0.2076,  0.1067]],
       device='cuda:0') 
 Parameter containing:
tensor([[-0.1563,  0.2447,  0.2188,  ...,  0.1009, -0.0742,  0.1276],
        [-0.1200,  0.0795,  0.2321,  ...,  0.1204, -0.0699, -0.0532],
        [-0.0133,  0.1043, -0.0225,  ..., -0.1308, -0.0137, -0.0726],
        ...,
        [ 0.0992,  0.1869, -0.1506,  ..., -0.1051,  0.0279,  0.0808],
        [-0.2383, -0.1013, -0.0086,  ...,  0.0665, -0.0418,  0.0515],
        [-0.1058, -0.1730,  0.0903,  ..., -0.1828, -0.2076,  0.1067]],
       device='cuda:0', requires_grad=True)
conv4.bias 
 torch.Size([32]) 
 True 
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0') 
 Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)
conv5.weight 
 torch.Size([32, 1]) 
 True 
 tensor([[ 0.3922],
        [ 0.2152],
        [-0.0708],
        [-0.1320],
        [ 0.3336],
        [ 0.1271],
        [-0.1854],
        [ 0.0584],
        [-0.1381],
        [-0.0893],
        [ 0.1930],
        [-0.4120],
        [ 0.0243],
        [ 0.0653],
        [ 0.3770],
        [-0.2362],
        [-0.4058],
        [ 0.1458],
        [ 0.0867],
        [-0.0089],
        [-0.0935],
        [-0.1946],
        [-0.3067],
        [ 0.2121],
        [-0.3575],
        [ 0.3732],
        [-0.3793],
        [ 0.0283],
        [-0.1156],
        [-0.4009],
        [ 0.0256],
        [ 0.1679]], device='cuda:0') 
 Parameter containing:
tensor([[ 0.3922],
        [ 0.2152],
        [-0.0708],
        [-0.1320],
        [ 0.3336],
        [ 0.1271],
        [-0.1854],
        [ 0.0584],
        [-0.1381],
        [-0.0893],
        [ 0.1930],
        [-0.4120],
        [ 0.0243],
        [ 0.0653],
        [ 0.3770],
        [-0.2362],
        [-0.4058],
        [ 0.1458],
        [ 0.0867],
        [-0.0089],
        [-0.0935],
        [-0.1946],
        [-0.3067],
        [ 0.2121],
        [-0.3575],
        [ 0.3732],
        [-0.3793],
        [ 0.0283],
        [-0.1156],
        [-0.4009],
        [ 0.0256],
        [ 0.1679]], device='cuda:0', requires_grad=True)
conv5.bias 
 torch.Size([1]) 
 True 
 tensor([0.], device='cuda:0') 
 Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')



input graph: 
g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([6796, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(33.1882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(75.2416, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1.4581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(1.4965, device='cuda:0')



h[100].sum tensor(1.5669, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(1.6081, device='cuda:0')



h[200].sum tensor(3.5602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(3.6539, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([6796, 256]) 
h.sum tensor(3137.2881, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0028, 0.0015,  ..., 0.0010, 0.0020, 0.0021],
        [0.0000, 0.0145, 0.0078,  ..., 0.0052, 0.0102, 0.0110],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([6796, 128]) 
h2.sum tensor(17050.3613, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-28.1677, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(263.7722, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(21.1011, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(125.0187, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(10.0012, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=6796, num_edges=73186,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0492],
        [0.0602],
        [0.0868],
        ...,
        [0.0139],
        [0.0139],
        [0.0110]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([6796, 1]) 
h5.sum tensor(1240.2255, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True) 
g.edata[efet].shape torch.Size([73186, 1]) 
g.edata[efet].sum tensor(73186., device='cuda:0', grad_fn=<SumBackward0>)

Passing event 20 from the network before training 
result1: tensor([[0.0492],
        [0.0602],
        [0.0868],
        ...,
        [0.0139],
        [0.0139],
        [0.0110]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')



input graph: 
g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([13592, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(132.4834, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<AddBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(268.7220, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(6.1968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(6.1466, device='cuda:0')



h[100].sum tensor(18.5955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(18.4448, device='cuda:0')



h[200].sum tensor(6.9884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(6.9318, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        ...,
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.],
        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',
       grad_fn=<ReluBackward0>) 
h.shape torch.Size([13592, 256]) 
h.sum tensor(15855.1025, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0087, 0.0000, 0.0282],
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.0000, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([13592, 128]) 
h2.sum tensor(84837.3281, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-26.1755, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-39.5444, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-75.3206, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=13592, num_edges=146372,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[0.0832],
        [0.0510],
        [0.0312],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([13592, 1]) 
h5.sum tensor(4887.1650, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([146372, 1]) 
g.edata[efet].sum tensor(146372., device='cuda:0', grad_fn=<SumBackward0>)

Passing two random events from the network before training 
result1: tensor([[0.0492],
        [0.0602],
        [0.0868],
        ...,
        [0.0139],
        [0.0139],
        [0.0110]], device='cuda:0', grad_fn=<AddBackward0>) 
result1.shape: torch.Size([6796, 1]) 
input: [0. 0. 0. ... 0. 0. 0.]



load_model False 
TraEvN 11001 
BatchSize 15 
EpochNum 200 
epoch_save 5 
LrVal 0.001 
weight_decay 1e-05 
startmesh 183 
endmesh 184 






optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.0022,  0.0067, -0.0796, -0.1388, -0.0670, -0.0547,  0.0772, -0.1215,
         -0.0201,  0.0202, -0.0893,  0.0775,  0.1358, -0.0804, -0.1097,  0.1047,
         -0.0576,  0.0831,  0.0309, -0.1125,  0.0267,  0.0628,  0.0032,  0.0378,
         -0.0949, -0.0654,  0.1356,  0.0242, -0.1425,  0.1455, -0.0243,  0.1068,
          0.0506, -0.1288, -0.0026,  0.0267,  0.1315, -0.0739, -0.1418, -0.0390,
         -0.1285, -0.1244,  0.1134, -0.0896, -0.1485, -0.1465, -0.1470,  0.0947,
          0.1305, -0.0161, -0.0002,  0.0234, -0.0398, -0.0934, -0.1249, -0.0027,
         -0.1394, -0.0078, -0.0700, -0.0091, -0.1134,  0.0036,  0.1288, -0.0828,
         -0.1445, -0.0551,  0.0269,  0.0733, -0.0634, -0.0710,  0.0227, -0.0302,
          0.0683, -0.1265,  0.1354, -0.0977, -0.0454,  0.0690,  0.0977, -0.1360,
          0.0143, -0.0692,  0.0415, -0.1230, -0.1056,  0.1243,  0.1120,  0.1261,
         -0.0239, -0.0591, -0.0645, -0.1227, -0.0461,  0.1420, -0.0156,  0.0256,
          0.0009,  0.1117,  0.0131, -0.0211, -0.1295, -0.1004,  0.0235, -0.1253,
          0.1151,  0.0286,  0.0489,  0.0674, -0.1122, -0.0090,  0.1090,  0.0622,
          0.0704,  0.1086,  0.1441, -0.0549, -0.0336, -0.0270, -0.0172,  0.0622,
         -0.0137,  0.0674,  0.0419, -0.0393,  0.0560, -0.1363,  0.1010,  0.0017,
         -0.1016, -0.1425, -0.0532, -0.0948,  0.1399, -0.0735, -0.0668, -0.0379,
          0.1410, -0.1281, -0.0668, -0.0757,  0.0555, -0.0100, -0.0804,  0.1066,
         -0.1131, -0.1323,  0.0254, -0.1028, -0.0148, -0.0080,  0.0326, -0.0802,
         -0.0279,  0.1286,  0.0408, -0.0582,  0.0552,  0.0315,  0.0414, -0.0341,
         -0.1019,  0.1246,  0.0051,  0.0804,  0.0640,  0.0535, -0.0488, -0.0595,
         -0.0689,  0.1253,  0.0143,  0.0402,  0.0805,  0.1156,  0.1306,  0.0198,
         -0.1027,  0.1315,  0.0270,  0.0045,  0.1138, -0.0555, -0.0467, -0.0012,
          0.0898,  0.1022,  0.0326, -0.0274,  0.0951, -0.0576,  0.1021,  0.0568,
          0.0651, -0.1033, -0.1121,  0.0506, -0.0123,  0.1450,  0.0067,  0.0013,
         -0.0130, -0.0163, -0.1072, -0.1191,  0.0734, -0.0850,  0.1176,  0.0998,
          0.0266,  0.1272,  0.0315,  0.0274, -0.0790,  0.1217, -0.0981,  0.0328,
          0.0033, -0.0906,  0.0984, -0.0448, -0.1314, -0.1025,  0.0251, -0.1157,
         -0.1095,  0.1152,  0.0014, -0.0247, -0.0163,  0.0444,  0.0068,  0.0152,
          0.1325,  0.0714, -0.1017,  0.0936,  0.1185,  0.1392, -0.0742, -0.1491,
         -0.0224,  0.0619,  0.1096,  0.0289, -0.0689, -0.0075,  0.1306, -0.1489,
         -0.0597,  0.0836, -0.0323,  0.0293,  0.0662, -0.0394, -0.1317,  0.0398]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0369, -0.0801, -0.0624,  ..., -0.0272,  0.0618, -0.0649],
        [ 0.1086,  0.0731, -0.1245,  ..., -0.0310, -0.0634, -0.0454],
        [ 0.0344, -0.0785,  0.0474,  ..., -0.1223, -0.0753, -0.0918],
        ...,
        [ 0.1198,  0.0131, -0.0658,  ..., -0.1165,  0.0086, -0.0768],
        [-0.1217,  0.0832,  0.0713,  ...,  0.0297, -0.0855,  0.1090],
        [ 0.0743,  0.1133,  0.0822,  ..., -0.0030,  0.0066,  0.0740]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-1.6791e-01,  2.5156e-02, -5.7330e-02,  ...,  1.7763e-02,
          5.8135e-03, -9.7493e-02],
        [-9.9585e-02,  7.8865e-02,  8.7555e-02,  ..., -1.5303e-01,
         -9.2100e-03, -3.5318e-02],
        [ 2.2207e-02,  6.8142e-02, -1.5057e-01,  ..., -4.6910e-03,
          4.6341e-02,  5.1000e-02],
        ...,
        [-1.4485e-01, -4.2984e-02, -2.4068e-03,  ...,  9.7951e-02,
          6.9469e-02, -5.1605e-02],
        [-6.2153e-02, -3.2917e-02,  9.4051e-02,  ..., -6.8511e-02,
          1.0511e-03, -8.1564e-02],
        [ 6.6629e-02,  3.8812e-02,  3.8849e-02,  ..., -4.7647e-05,
          1.2973e-01,  6.6415e-02]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0650, -0.0614, -0.1775,  ...,  0.1034,  0.0460, -0.1901],
        [-0.0305, -0.0274, -0.1389,  ...,  0.2364, -0.0751, -0.0078],
        [ 0.0203,  0.2036,  0.0168,  ..., -0.1454,  0.1808,  0.0729],
        ...,
        [-0.1751,  0.1306,  0.0365,  ..., -0.0765,  0.0607, -0.1068],
        [-0.0053, -0.2017,  0.0576,  ..., -0.1241, -0.1617,  0.1916],
        [-0.1413, -0.0682,  0.2216,  ..., -0.0703, -0.1943, -0.2169]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.3348],
        [ 0.1450],
        [ 0.2659],
        [ 0.2370],
        [ 0.3554],
        [ 0.3805],
        [-0.3647],
        [-0.0401],
        [ 0.1738],
        [ 0.1888],
        [-0.3589],
        [-0.0933],
        [ 0.3658],
        [-0.3128],
        [ 0.1231],
        [ 0.4011],
        [-0.0301],
        [ 0.0814],
        [ 0.2290],
        [ 0.4157],
        [ 0.4047],
        [-0.2345],
        [-0.1755],
        [ 0.2551],
        [-0.0596],
        [-0.3455],
        [ 0.0496],
        [-0.1747],
        [ 0.3618],
        [-0.3619],
        [-0.1235],
        [-0.3818]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 1e-05, 'amsgrad': False}]



optimizer.param_groups [{'params': [Parameter containing:
tensor([[ 0.0022,  0.0067, -0.0796, -0.1388, -0.0670, -0.0547,  0.0772, -0.1215,
         -0.0201,  0.0202, -0.0893,  0.0775,  0.1358, -0.0804, -0.1097,  0.1047,
         -0.0576,  0.0831,  0.0309, -0.1125,  0.0267,  0.0628,  0.0032,  0.0378,
         -0.0949, -0.0654,  0.1356,  0.0242, -0.1425,  0.1455, -0.0243,  0.1068,
          0.0506, -0.1288, -0.0026,  0.0267,  0.1315, -0.0739, -0.1418, -0.0390,
         -0.1285, -0.1244,  0.1134, -0.0896, -0.1485, -0.1465, -0.1470,  0.0947,
          0.1305, -0.0161, -0.0002,  0.0234, -0.0398, -0.0934, -0.1249, -0.0027,
         -0.1394, -0.0078, -0.0700, -0.0091, -0.1134,  0.0036,  0.1288, -0.0828,
         -0.1445, -0.0551,  0.0269,  0.0733, -0.0634, -0.0710,  0.0227, -0.0302,
          0.0683, -0.1265,  0.1354, -0.0977, -0.0454,  0.0690,  0.0977, -0.1360,
          0.0143, -0.0692,  0.0415, -0.1230, -0.1056,  0.1243,  0.1120,  0.1261,
         -0.0239, -0.0591, -0.0645, -0.1227, -0.0461,  0.1420, -0.0156,  0.0256,
          0.0009,  0.1117,  0.0131, -0.0211, -0.1295, -0.1004,  0.0235, -0.1253,
          0.1151,  0.0286,  0.0489,  0.0674, -0.1122, -0.0090,  0.1090,  0.0622,
          0.0704,  0.1086,  0.1441, -0.0549, -0.0336, -0.0270, -0.0172,  0.0622,
         -0.0137,  0.0674,  0.0419, -0.0393,  0.0560, -0.1363,  0.1010,  0.0017,
         -0.1016, -0.1425, -0.0532, -0.0948,  0.1399, -0.0735, -0.0668, -0.0379,
          0.1410, -0.1281, -0.0668, -0.0757,  0.0555, -0.0100, -0.0804,  0.1066,
         -0.1131, -0.1323,  0.0254, -0.1028, -0.0148, -0.0080,  0.0326, -0.0802,
         -0.0279,  0.1286,  0.0408, -0.0582,  0.0552,  0.0315,  0.0414, -0.0341,
         -0.1019,  0.1246,  0.0051,  0.0804,  0.0640,  0.0535, -0.0488, -0.0595,
         -0.0689,  0.1253,  0.0143,  0.0402,  0.0805,  0.1156,  0.1306,  0.0198,
         -0.1027,  0.1315,  0.0270,  0.0045,  0.1138, -0.0555, -0.0467, -0.0012,
          0.0898,  0.1022,  0.0326, -0.0274,  0.0951, -0.0576,  0.1021,  0.0568,
          0.0651, -0.1033, -0.1121,  0.0506, -0.0123,  0.1450,  0.0067,  0.0013,
         -0.0130, -0.0163, -0.1072, -0.1191,  0.0734, -0.0850,  0.1176,  0.0998,
          0.0266,  0.1272,  0.0315,  0.0274, -0.0790,  0.1217, -0.0981,  0.0328,
          0.0033, -0.0906,  0.0984, -0.0448, -0.1314, -0.1025,  0.0251, -0.1157,
         -0.1095,  0.1152,  0.0014, -0.0247, -0.0163,  0.0444,  0.0068,  0.0152,
          0.1325,  0.0714, -0.1017,  0.0936,  0.1185,  0.1392, -0.0742, -0.1491,
         -0.0224,  0.0619,  0.1096,  0.0289, -0.0689, -0.0075,  0.1306, -0.1489,
         -0.0597,  0.0836, -0.0323,  0.0293,  0.0662, -0.0394, -0.1317,  0.0398]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[ 0.0369, -0.0801, -0.0624,  ..., -0.0272,  0.0618, -0.0649],
        [ 0.1086,  0.0731, -0.1245,  ..., -0.0310, -0.0634, -0.0454],
        [ 0.0344, -0.0785,  0.0474,  ..., -0.1223, -0.0753, -0.0918],
        ...,
        [ 0.1198,  0.0131, -0.0658,  ..., -0.1165,  0.0086, -0.0768],
        [-0.1217,  0.0832,  0.0713,  ...,  0.0297, -0.0855,  0.1090],
        [ 0.0743,  0.1133,  0.0822,  ..., -0.0030,  0.0066,  0.0740]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-1.6791e-01,  2.5156e-02, -5.7330e-02,  ...,  1.7763e-02,
          5.8135e-03, -9.7493e-02],
        [-9.9585e-02,  7.8865e-02,  8.7555e-02,  ..., -1.5303e-01,
         -9.2100e-03, -3.5318e-02],
        [ 2.2207e-02,  6.8142e-02, -1.5057e-01,  ..., -4.6910e-03,
          4.6341e-02,  5.1000e-02],
        ...,
        [-1.4485e-01, -4.2984e-02, -2.4068e-03,  ...,  9.7951e-02,
          6.9469e-02, -5.1605e-02],
        [-6.2153e-02, -3.2917e-02,  9.4051e-02,  ..., -6.8511e-02,
          1.0511e-03, -8.1564e-02],
        [ 6.6629e-02,  3.8812e-02,  3.8849e-02,  ..., -4.7647e-05,
          1.2973e-01,  6.6415e-02]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.0650, -0.0614, -0.1775,  ...,  0.1034,  0.0460, -0.1901],
        [-0.0305, -0.0274, -0.1389,  ...,  0.2364, -0.0751, -0.0078],
        [ 0.0203,  0.2036,  0.0168,  ..., -0.1454,  0.1808,  0.0729],
        ...,
        [-0.1751,  0.1306,  0.0365,  ..., -0.0765,  0.0607, -0.1068],
        [-0.0053, -0.2017,  0.0576,  ..., -0.1241, -0.1617,  0.1916],
        [-0.1413, -0.0682,  0.2216,  ..., -0.0703, -0.1943, -0.2169]],
       device='cuda:0', requires_grad=True), Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True), Parameter containing:
tensor([[-0.3348],
        [ 0.1450],
        [ 0.2659],
        [ 0.2370],
        [ 0.3554],
        [ 0.3805],
        [-0.3647],
        [-0.0401],
        [ 0.1738],
        [ 0.1888],
        [-0.3589],
        [-0.0933],
        [ 0.3658],
        [-0.3128],
        [ 0.1231],
        [ 0.4011],
        [-0.0301],
        [ 0.0814],
        [ 0.2290],
        [ 0.4157],
        [ 0.4047],
        [-0.2345],
        [-0.1755],
        [ 0.2551],
        [-0.0596],
        [-0.3455],
        [ 0.0496],
        [-0.1747],
        [ 0.3618],
        [-0.3619],
        [-0.1235],
        [-0.3818]], device='cuda:0', requires_grad=True), Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)], 'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 1e-05, 'amsgrad': False}, {'params': [tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', requires_grad=True)], 'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 1e-05, 'amsgrad': False}]



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(773.1461, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(773.1461, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[ 0.0003,  0.0010, -0.0117,  ..., -0.0058, -0.0193,  0.0058],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        ...,
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-596.0553, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(1.6721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.8701, device='cuda:0')



h[100].sum tensor(-97.6995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(107.6403, device='cuda:0')



h[200].sum tensor(-9.7943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.4525, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0012, 0.0037, 0.0000,  ..., 0.0000, 0.0000, 0.0221],
        [0.0010, 0.0031, 0.0000,  ..., 0.0000, 0.0000, 0.0182],
        [0.0002, 0.0007, 0.0000,  ..., 0.0000, 0.0000, 0.0043],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(71960.3984, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0758, 0.1028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0649, 0.0881],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0521, 0.0707],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(311122.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-7.9772, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(80.1560, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4269.2998, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(239.5053, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[2.6773],
        [2.8969],
        [3.1967],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(145765.1719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 0.0 event: 0 loss: tensor(154.5596, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(704.9972, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3103],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(704.9972, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0009,  0.0014, -0.0043,  ..., -0.0021, -0.0071,  0.0011],
        [-0.0008,  0.0020, -0.0104,  ..., -0.0051, -0.0173,  0.0041],
        [-0.0009,  0.0017, -0.0076,  ..., -0.0037, -0.0126,  0.0028],
        ...,
        [-0.0010,  0.0010,  0.0000,  ...,  0.0000,  0.0000, -0.0010],
        [-0.0010,  0.0010,  0.0000,  ...,  0.0000,  0.0000, -0.0010],
        [-0.0010,  0.0010,  0.0000,  ...,  0.0000,  0.0000, -0.0010]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-1588.5454, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-101.1000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.7083, device='cuda:0')



h[100].sum tensor(-88.7080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(98.1524, device='cuda:0')



h[200].sum tensor(-8.3208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.8868, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0143],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0078],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0069],
        ...,
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(120287.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.1063e-01,
         2.5095e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.0394e-01,
         2.0595e-02],
        [1.1899e-05, 0.0000e+00, 1.2880e-04,  ..., 0.0000e+00, 9.3317e-02,
         1.7055e-02],
        ...,
        [2.1301e-03, 0.0000e+00, 1.6678e-02,  ..., 3.7676e-03, 2.6803e-02,
         0.0000e+00],
        [2.1301e-03, 0.0000e+00, 1.6678e-02,  ..., 3.7676e-03, 2.6803e-02,
         0.0000e+00],
        [2.1301e-03, 0.0000e+00, 1.6678e-02,  ..., 3.7676e-03, 2.6803e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(709061.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(1852.3965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-819.2863, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(358.7690, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(15793.6631, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(75.3426, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0284],
        [-0.0450],
        [-0.0770],
        ...,
        [-0.0920],
        [-0.0916],
        [-0.0915]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-221758.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(622.6307, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(622.6307, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0017,  0.0014,  0.0000,  ...,  0.0000,  0.0000, -0.0017],
        [-0.0017,  0.0014,  0.0000,  ...,  0.0000,  0.0000, -0.0017],
        [-0.0017,  0.0014,  0.0000,  ...,  0.0000,  0.0000, -0.0017],
        ...,
        [-0.0017,  0.0014,  0.0000,  ...,  0.0000,  0.0000, -0.0017],
        [-0.0017,  0.0014,  0.0000,  ...,  0.0000,  0.0000, -0.0017],
        [-0.0017,  0.0014,  0.0000,  ...,  0.0000,  0.0000, -0.0017]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-3284.2300, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-169.9110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.8869, device='cuda:0')



h[100].sum tensor(-78.0418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(86.6850, device='cuda:0')



h[200].sum tensor(-6.8121, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.5772, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(133866.2656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0133,  ..., 0.0092, 0.0547, 0.0000],
        [0.0000, 0.0000, 0.0185,  ..., 0.0124, 0.0413, 0.0000],
        [0.0000, 0.0000, 0.0196,  ..., 0.0133, 0.0373, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0193,  ..., 0.0132, 0.0369, 0.0000],
        [0.0000, 0.0000, 0.0193,  ..., 0.0132, 0.0369, 0.0000],
        [0.0000, 0.0000, 0.0193,  ..., 0.0132, 0.0369, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(913411.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(19.8125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-605.0071, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(486.6429, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4797.9121, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(278.8386, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0855],
        [-0.0549],
        [-0.0302],
        ...,
        [ 0.0008],
        [ 0.0007],
        [ 0.0006]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(11623.8516, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(667.8506, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2717],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(667.8506, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0022,  0.0011,  0.0000,  ...,  0.0000,  0.0000, -0.0022],
        [-0.0022,  0.0017, -0.0052,  ..., -0.0025, -0.0087,  0.0004],
        [-0.0022,  0.0011,  0.0000,  ...,  0.0000,  0.0000, -0.0022],
        ...,
        [-0.0022,  0.0011,  0.0000,  ...,  0.0000,  0.0000, -0.0022],
        [-0.0022,  0.0011,  0.0000,  ...,  0.0000,  0.0000, -0.0022],
        [-0.0022,  0.0011,  0.0000,  ...,  0.0000,  0.0000, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-4693.7451, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-223.0271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.9849, device='cuda:0')



h[100].sum tensor(-83.0156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.9807, device='cuda:0')



h[200].sum tensor(-6.7017, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.9432, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0014],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0037],
        ...,
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(151349.2969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0062,  ..., 0.0181, 0.0803, 0.0000],
        [0.0000, 0.0000, 0.0029,  ..., 0.0154, 0.0962, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.1231, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0130,  ..., 0.0236, 0.0463, 0.0000],
        [0.0000, 0.0000, 0.0130,  ..., 0.0236, 0.0463, 0.0000],
        [0.0000, 0.0000, 0.0130,  ..., 0.0236, 0.0463, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1148907.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-768.0131, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(446.9801, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6696.8330, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(286.9933, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1665],
        [-0.1817],
        [-0.1946],
        ...,
        [-0.1080],
        [-0.1077],
        [-0.1075]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-148119.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1023.0052, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1023.0052, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0022,  0.0011,  0.0000,  ...,  0.0000,  0.0000, -0.0022],
        [-0.0022,  0.0011,  0.0000,  ...,  0.0000,  0.0000, -0.0022],
        [-0.0022,  0.0011,  0.0000,  ...,  0.0000,  0.0000, -0.0022],
        ...,
        [-0.0022,  0.0011,  0.0000,  ...,  0.0000,  0.0000, -0.0022],
        [-0.0022,  0.0011,  0.0000,  ...,  0.0000,  0.0000, -0.0022],
        [-0.0022,  0.0011,  0.0000,  ...,  0.0000,  0.0000, -0.0022]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-4848.2129, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-223.0171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(47.4623, device='cuda:0')



h[100].sum tensor(-126.6595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(142.4267, device='cuda:0')



h[200].sum tensor(-10.2250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(53.5256, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        ...,
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(180234.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0123,  ..., 0.0234, 0.0487, 0.0000],
        [0.0000, 0.0000, 0.0095,  ..., 0.0219, 0.0571, 0.0000],
        [0.0000, 0.0000, 0.0046,  ..., 0.0178, 0.0809, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0130,  ..., 0.0236, 0.0463, 0.0000],
        [0.0000, 0.0000, 0.0130,  ..., 0.0236, 0.0463, 0.0000],
        [0.0000, 0.0000, 0.0130,  ..., 0.0236, 0.0463, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1257827.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-812.5790, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(517.2100, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7604.8906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(369.5384, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1421],
        [-0.1516],
        [-0.1676],
        ...,
        [-0.1082],
        [-0.1075],
        [-0.1078]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-154006.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(716.2418, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3965],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(716.2418, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0027,  0.0019, -0.0075,  ..., -0.0035, -0.0127,  0.0011],
        [-0.0026,  0.0010,  0.0000,  ...,  0.0000,  0.0000, -0.0026],
        [-0.0027,  0.0025, -0.0128,  ..., -0.0060, -0.0215,  0.0037],
        ...,
        [-0.0026,  0.0010,  0.0000,  ...,  0.0000,  0.0000, -0.0026],
        [-0.0026,  0.0010,  0.0000,  ...,  0.0000,  0.0000, -0.0026],
        [-0.0026,  0.0010,  0.0000,  ...,  0.0000,  0.0000, -0.0026]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-5909.5674, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-266.5734, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.2300, device='cuda:0')



h[100].sum tensor(-87.7638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(99.7179, device='cuda:0')



h[200].sum tensor(-6.5065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.4751, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0057],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0017],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(171342.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0020,  ..., 0.0311, 0.0991, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0260, 0.1479, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0260, 0.1538, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0111,  ..., 0.0356, 0.0511, 0.0000],
        [0.0000, 0.0000, 0.0111,  ..., 0.0356, 0.0511, 0.0000],
        [0.0000, 0.0000, 0.0111,  ..., 0.0356, 0.0511, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1364937.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-775.9899, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(495.2244, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2624.0120, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(342.1226, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0368],
        [-0.0256],
        [-0.0181],
        ...,
        [-0.0859],
        [-0.0856],
        [-0.0856]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-68010.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1014.7023, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2505],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1014.7023, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0030,  0.0016, -0.0050,  ..., -0.0023, -0.0085, -0.0004],
        [-0.0030,  0.0015, -0.0047,  ..., -0.0022, -0.0079, -0.0006],
        [-0.0030,  0.0009,  0.0000,  ...,  0.0000,  0.0000, -0.0029],
        ...,
        [-0.0030,  0.0009,  0.0000,  ...,  0.0000,  0.0000, -0.0029],
        [-0.0030,  0.0009,  0.0000,  ...,  0.0000,  0.0000, -0.0029],
        [-0.0030,  0.0009,  0.0000,  ...,  0.0000,  0.0000, -0.0029]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-6901.3823, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-303.6039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(47.0771, device='cuda:0')



h[100].sum tensor(-123.9131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(141.2707, device='cuda:0')



h[200].sum tensor(-8.3680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(53.0912, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0091],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0022],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(211096.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0339, 0.2101, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0394, 0.1532, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0392, 0.1645, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0105,  ..., 0.0458, 0.0548, 0.0000],
        [0.0000, 0.0000, 0.0105,  ..., 0.0458, 0.0548, 0.0000],
        [0.0000, 0.0000, 0.0105,  ..., 0.0458, 0.0548, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1659282.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-887.0684, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(662.9189, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(336.6629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(436.8341, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2380],
        [ 0.2407],
        [ 0.2513],
        ...,
        [-0.0274],
        [-0.0271],
        [-0.0275]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(113008.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(734.2592, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(734.2592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0033,  0.0013, -0.0047,  ..., -0.0021, -0.0080, -0.0009],
        [-0.0033,  0.0006,  0.0000,  ...,  0.0000,  0.0000, -0.0032],
        [-0.0033,  0.0006,  0.0000,  ...,  0.0000,  0.0000, -0.0032],
        ...,
        [-0.0033,  0.0006,  0.0000,  ...,  0.0000,  0.0000, -0.0032],
        [-0.0033,  0.0006,  0.0000,  ...,  0.0000,  0.0000, -0.0032],
        [-0.0033,  0.0006,  0.0000,  ...,  0.0000,  0.0000, -0.0032]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-8181.1299, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-335.1233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.0659, device='cuda:0')



h[100].sum tensor(-88.5743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(102.2263, device='cuda:0')



h[200].sum tensor(-5.3972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.4178, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0036],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(193691.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0420, 0.1707, 0.0000],
        [0.0000, 0.0000, 0.0032,  ..., 0.0455, 0.1153, 0.0000],
        [0.0000, 0.0000, 0.0069,  ..., 0.0477, 0.0753, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0109,  ..., 0.0483, 0.0560, 0.0000],
        [0.0000, 0.0000, 0.0109,  ..., 0.0483, 0.0560, 0.0000],
        [0.0000, 0.0000, 0.0109,  ..., 0.0483, 0.0560, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1605623.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-901.8109, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(694.3860, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(724.7599, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(533.0817, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1258],
        [-0.1535],
        [-0.1911],
        ...,
        [-0.2101],
        [-0.2093],
        [-0.2090]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-284840.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(731.1919, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(731.1919, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0036,  0.0011, -0.0040,  ..., -0.0018, -0.0068, -0.0015],
        [-0.0036,  0.0013, -0.0050,  ..., -0.0023, -0.0087, -0.0010],
        [-0.0035,  0.0005,  0.0000,  ...,  0.0000,  0.0000, -0.0035],
        ...,
        [-0.0035,  0.0005,  0.0000,  ...,  0.0000,  0.0000, -0.0035],
        [-0.0035,  0.0005,  0.0000,  ...,  0.0000,  0.0000, -0.0035],
        [-0.0035,  0.0005,  0.0000,  ...,  0.0000,  0.0000, -0.0035]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-9272.4639, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-362.8254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.9236, device='cuda:0')



h[100].sum tensor(-87.2334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(101.7993, device='cuda:0')



h[200].sum tensor(-4.7428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.2574, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0056],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0029, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(200838.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0400, 0.2170, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0434, 0.1644, 0.0000],
        [0.0000, 0.0000, 0.0026,  ..., 0.0470, 0.1125, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0085,  ..., 0.0498, 0.0566, 0.0000],
        [0.0000, 0.0000, 0.0085,  ..., 0.0498, 0.0566, 0.0000],
        [0.0000, 0.0000, 0.0085,  ..., 0.0498, 0.0566, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1659325.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-957.4204, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(38.4291, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(754.9951, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1886.5564, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(583.5199, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2408],
        [-0.2507],
        [-0.2719],
        ...,
        [-0.2629],
        [-0.2618],
        [-0.2614]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-402311.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(717.3862, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3613],
        [0.3167],
        [0.4028],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(717.3862, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0041,  0.0026, -0.0132,  ..., -0.0058, -0.0228,  0.0028],
        [-0.0041,  0.0027, -0.0137,  ..., -0.0061, -0.0236,  0.0031],
        [-0.0040,  0.0023, -0.0116,  ..., -0.0051, -0.0200,  0.0020],
        ...,
        [-0.0038,  0.0005,  0.0000,  ...,  0.0000,  0.0000, -0.0037],
        [-0.0038,  0.0005,  0.0000,  ...,  0.0000,  0.0000, -0.0037],
        [-0.0038,  0.0005,  0.0000,  ...,  0.0000,  0.0000, -0.0037]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-10180.3047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-387.2352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.2831, device='cuda:0')



h[100].sum tensor(-85.3026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(99.8772, device='cuda:0')



h[200].sum tensor(-4.0829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.5350, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0099],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0117],
        [0.0000, 0.0125, 0.0000,  ..., 0.0000, 0.0000, 0.0180],
        ...,
        [0.0000, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(205876.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.2941e-02, 2.9920e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.0318e-02, 3.2045e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.7263e-02, 3.4206e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 4.2197e-03,  ..., 5.0228e-02, 5.8719e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 2.3802e-03,  ..., 4.8467e-02, 7.9213e-02,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 1.7863e-04,  ..., 4.5123e-02, 1.2408e-01,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1696475.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1010.7080, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(1070.9423, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(778.6268, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1340.5942, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(559.9973, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3039],
        [-0.3131],
        [-0.3143],
        ...,
        [-0.2022],
        [-0.1836],
        [-0.1739]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-326719.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(693.4390, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(693.4390, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0040,  0.0007,  0.0000,  ...,  0.0000,  0.0000, -0.0040],
        [-0.0040,  0.0007,  0.0000,  ...,  0.0000,  0.0000, -0.0040],
        [-0.0040,  0.0007,  0.0000,  ...,  0.0000,  0.0000, -0.0040],
        ...,
        [-0.0040,  0.0007,  0.0000,  ...,  0.0000,  0.0000, -0.0040],
        [-0.0040,  0.0007,  0.0000,  ...,  0.0000,  0.0000, -0.0040],
        [-0.0040,  0.0007,  0.0000,  ...,  0.0000,  0.0000, -0.0040]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-10915.5176, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-408.8857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.1721, device='cuda:0')



h[100].sum tensor(-82.5118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.5432, device='cuda:0')



h[200].sum tensor(-3.4197, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.2821, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(212365.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0012,  ..., 0.0504, 0.0578, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0505, 0.0559, 0.0000],
        [0.0000, 0.0000, 0.0012,  ..., 0.0506, 0.0561, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0011,  ..., 0.0502, 0.0555, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0502, 0.0555, 0.0000],
        [0.0000, 0.0000, 0.0011,  ..., 0.0502, 0.0555, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1730910., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1009.6385, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(2015.3704, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(744.0911, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(441.6584, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(535.2632, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1095],
        [-0.1279],
        [-0.1476],
        ...,
        [-0.1277],
        [-0.1342],
        [-0.1359]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-125631.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 10.0 event: 150 loss: tensor(584.6758, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(718.2433, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(718.2433, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0042,  0.0008,  0.0000,  ...,  0.0000,  0.0000, -0.0041],
        [-0.0042,  0.0008,  0.0000,  ...,  0.0000,  0.0000, -0.0041],
        [-0.0042,  0.0008,  0.0000,  ...,  0.0000,  0.0000, -0.0041],
        ...,
        [-0.0042,  0.0008,  0.0000,  ...,  0.0000,  0.0000, -0.0041],
        [-0.0042,  0.0008,  0.0000,  ...,  0.0000,  0.0000, -0.0041],
        [-0.0042,  0.0008,  0.0000,  ...,  0.0000,  0.0000, -0.0041]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-11552.0527, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-428.2836, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.3229, device='cuda:0')



h[100].sum tensor(-84.9049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(99.9966, device='cuda:0')



h[200].sum tensor(-2.9841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.5799, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(222435.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0004,  ..., 0.0487, 0.0560, 0.0000],
        [0.0000, 0.0000, 0.0004,  ..., 0.0487, 0.0560, 0.0000],
        [0.0000, 0.0000, 0.0005,  ..., 0.0488, 0.0561, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0003,  ..., 0.0484, 0.0556, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0484, 0.0556, 0.0000],
        [0.0000, 0.0000, 0.0003,  ..., 0.0484, 0.0556, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1806596.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1065.4482, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3947.8713, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(697.7690, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(382.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(499.7466, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0117],
        [-0.0180],
        [-0.0217],
        ...,
        [-0.0378],
        [-0.0378],
        [-0.0378]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(59357.0898, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(765.3921, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(765.3921, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0047,  0.0027, -0.0095,  ..., -0.0040, -0.0167,  0.0005],
        [-0.0047,  0.0030, -0.0109,  ..., -0.0046, -0.0191,  0.0012],
        [-0.0053,  0.0065, -0.0305,  ..., -0.0128, -0.0536,  0.0112],
        ...,
        [-0.0044,  0.0010,  0.0000,  ...,  0.0000,  0.0000, -0.0043],
        [-0.0044,  0.0010,  0.0000,  ...,  0.0000,  0.0000, -0.0043],
        [-0.0044,  0.0010,  0.0000,  ...,  0.0000,  0.0000, -0.0043]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-12434.9912, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-445.7184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.5103, device='cuda:0')



h[100].sum tensor(-89.3117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(106.5608, device='cuda:0')



h[200].sum tensor(-2.5900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.0468, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0175, 0.0000,  ..., 0.0000, 0.0000, 0.0206],
        [0.0000, 0.0193, 0.0000,  ..., 0.0000, 0.0000, 0.0257],
        [0.0000, 0.0133, 0.0000,  ..., 0.0000, 0.0000, 0.0089],
        ...,
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(230528.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4665, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4919, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4253, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0517, 0.0543, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0517, 0.0543, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0517, 0.0543, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1860910., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1034.5305, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4031.5698, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(810.6763, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(379.7695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(531.9261, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2792],
        [-0.2924],
        [-0.2603],
        ...,
        [-0.0586],
        [-0.0524],
        [-0.0478]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-9319.7490, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(868.3438, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(868.3438, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0045,  0.0013,  0.0000,  ...,  0.0000,  0.0000, -0.0045],
        [-0.0045,  0.0013,  0.0000,  ...,  0.0000,  0.0000, -0.0045],
        [-0.0045,  0.0013,  0.0000,  ...,  0.0000,  0.0000, -0.0045],
        ...,
        [-0.0045,  0.0013,  0.0000,  ...,  0.0000,  0.0000, -0.0045],
        [-0.0045,  0.0013,  0.0000,  ...,  0.0000,  0.0000, -0.0045],
        [-0.0045,  0.0013,  0.0000,  ...,  0.0000,  0.0000, -0.0045]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-13244.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-461.5422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.2868, device='cuda:0')



h[100].sum tensor(-100.0475, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(120.8941, device='cuda:0')



h[200].sum tensor(-2.3053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(45.4334, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(238596.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0563, 0.0536, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0563, 0.0536, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0561, 0.0566, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0560, 0.0532, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0560, 0.0532, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0560, 0.0532, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1893788.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1020.0797, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3289.4902, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(930.7986, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(353.7562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(562.8418, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1503],
        [-0.1301],
        [-0.1030],
        ...,
        [-0.1325],
        [-0.1321],
        [-0.1320]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-133021.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(898.2412, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9526],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(898.2412, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0046,  0.0015,  0.0000,  ...,  0.0000,  0.0000, -0.0046],
        [-0.0052,  0.0046, -0.0159,  ..., -0.0064, -0.0283,  0.0037],
        [-0.0046,  0.0015,  0.0000,  ...,  0.0000,  0.0000, -0.0046],
        ...,
        [-0.0046,  0.0015,  0.0000,  ...,  0.0000,  0.0000, -0.0046],
        [-0.0046,  0.0015,  0.0000,  ...,  0.0000,  0.0000, -0.0046],
        [-0.0046,  0.0015,  0.0000,  ...,  0.0000,  0.0000, -0.0046]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-14059.2812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-475.6950, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(41.6739, device='cuda:0')



h[100].sum tensor(-102.8736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(125.0565, device='cuda:0')



h[200].sum tensor(-1.7807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(46.9977, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0070],
        [0.0000, 0.0190, 0.0000,  ..., 0.0000, 0.0000, 0.0163],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(244417.7031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.0264e-02, 1.9372e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.9622e-02, 2.3868e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 8.9012e-03, 2.7812e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.8370e-02, 5.2523e-02,
         2.2879e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.8370e-02, 5.2523e-02,
         2.2879e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.8370e-02, 5.2523e-02,
         2.2879e-04]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1947943.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1000.2461, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(3287.7903, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1068.5267, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(359.1802, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(615.2509, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1173],
        [-0.1236],
        [-0.1306],
        ...,
        [-0.1651],
        [-0.1646],
        [-0.1644]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-185119.6719, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(756.6324, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(756.6324, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0048,  0.0016,  0.0000,  ...,  0.0000,  0.0000, -0.0047],
        [-0.0048,  0.0016,  0.0000,  ...,  0.0000,  0.0000, -0.0047],
        [-0.0050,  0.0030, -0.0069,  ..., -0.0027, -0.0123, -0.0011],
        ...,
        [-0.0048,  0.0016,  0.0000,  ...,  0.0000,  0.0000, -0.0047],
        [-0.0048,  0.0016,  0.0000,  ...,  0.0000,  0.0000, -0.0047],
        [-0.0048,  0.0016,  0.0000,  ...,  0.0000,  0.0000, -0.0047]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-14921.1016, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-488.0643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.1039, device='cuda:0')



h[100].sum tensor(-86.0429, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(105.3412, device='cuda:0')



h[200].sum tensor(-1.0186, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.5885, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0015],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(235912.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0378, 0.1703, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0398, 0.1638, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0290, 0.2035, 0.0000],
        ...,
        [0.0000, 0.0043, 0.0000,  ..., 0.0593, 0.0521, 0.0008],
        [0.0000, 0.0043, 0.0000,  ..., 0.0593, 0.0521, 0.0008],
        [0.0000, 0.0043, 0.0000,  ..., 0.0593, 0.0521, 0.0008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1922721.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-980.4561, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4140.1621, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1091.5573, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(187.2748, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(527.8414, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0057],
        [ 0.0016],
        [-0.0113],
        ...,
        [-0.1555],
        [-0.1552],
        [-0.1551]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-153042.2188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(702.3387, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(702.3387, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0049,  0.0017,  0.0000,  ...,  0.0000,  0.0000, -0.0049],
        [-0.0049,  0.0017,  0.0000,  ...,  0.0000,  0.0000, -0.0049],
        [-0.0051,  0.0027, -0.0047,  ..., -0.0018, -0.0085, -0.0023],
        ...,
        [-0.0049,  0.0017,  0.0000,  ...,  0.0000,  0.0000, -0.0049],
        [-0.0049,  0.0017,  0.0000,  ...,  0.0000,  0.0000, -0.0049],
        [-0.0049,  0.0017,  0.0000,  ...,  0.0000,  0.0000, -0.0049]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-15598.5918, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-499.4019, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.5850, device='cuda:0')



h[100].sum tensor(-78.4459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(97.7823, device='cuda:0')



h[200].sum tensor(-0.5228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.7477, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 8.6188e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         1.2829e-05],
        [0.0000e+00, 7.6919e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.8893e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 6.6987e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.6987e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.6987e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(233982.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0032, 0.0000,  ..., 0.0504, 0.1190, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0509, 0.1233, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0464, 0.1564, 0.0000],
        ...,
        [0.0000, 0.0103, 0.0000,  ..., 0.0605, 0.0516, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0605, 0.0516, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0605, 0.0516, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1931814.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1032.3657, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5596.3579, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1117.0701, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(81.3133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(384.8393, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1355],
        [ 0.1548],
        [ 0.1762],
        ...,
        [-0.1205],
        [-0.1202],
        [-0.1202]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-58131.2734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1061.4436, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1061.4436, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0050,  0.0018,  0.0000,  ...,  0.0000,  0.0000, -0.0050],
        [-0.0050,  0.0018,  0.0000,  ...,  0.0000,  0.0000, -0.0050],
        [-0.0052,  0.0030, -0.0058,  ..., -0.0022, -0.0105, -0.0019],
        ...,
        [-0.0050,  0.0018,  0.0000,  ...,  0.0000,  0.0000, -0.0050],
        [-0.0050,  0.0018,  0.0000,  ...,  0.0000,  0.0000, -0.0050],
        [-0.0050,  0.0018,  0.0000,  ...,  0.0000,  0.0000, -0.0050]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-15865.0391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-510.8217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(49.2456, device='cuda:0')



h[100].sum tensor(-119.2100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(147.7782, device='cuda:0')



h[200].sum tensor(-0.2173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(55.5368, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0010],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(270729.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0576, 0.0887, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0481, 0.1447, 0.0000],
        [0.0000, 0.0003, 0.0000,  ..., 0.0337, 0.2173, 0.0000],
        ...,
        [0.0000, 0.0108, 0.0000,  ..., 0.0622, 0.0517, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0622, 0.0517, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0622, 0.0517, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2128096.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1168.6771, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6282.6631, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1282.6166, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(393.2514, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(427.4033, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1086],
        [ 0.1283],
        [ 0.1471],
        ...,
        [-0.1227],
        [-0.1227],
        [-0.1230]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-36316.4570, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(731.2302, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(731.2302, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0056,  0.0046, -0.0118,  ..., -0.0044, -0.0215,  0.0013],
        [-0.0054,  0.0038, -0.0082,  ..., -0.0030, -0.0149, -0.0006],
        [-0.0056,  0.0045, -0.0115,  ..., -0.0043, -0.0210,  0.0012],
        ...,
        [-0.0051,  0.0019,  0.0000,  ...,  0.0000,  0.0000, -0.0051],
        [-0.0051,  0.0019,  0.0000,  ...,  0.0000,  0.0000, -0.0051],
        [-0.0051,  0.0019,  0.0000,  ...,  0.0000,  0.0000, -0.0051]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-16836.4492, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-519.3481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.9254, device='cuda:0')



h[100].sum tensor(-80.5049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(101.8046, device='cuda:0')



h[200].sum tensor(0.2137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.2594, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.4271e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         9.4678e-04],
        [0.0000e+00, 1.5057e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.5342e-03],
        [0.0000e+00, 1.2315e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         5.4068e-05],
        ...,
        [0.0000e+00, 7.7704e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.7704e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.7704e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(239564.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0334, 0.2789, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0335, 0.2715, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0404, 0.2359, 0.0000],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0655, 0.0526, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0655, 0.0526, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0655, 0.0526, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1957847.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-966.4557, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5819.5723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1285.7126, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(126.2570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(547.8262, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0014],
        [ 0.0089],
        [ 0.0180],
        ...,
        [-0.2021],
        [-0.2015],
        [-0.2013]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-209966.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1144.1659, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1144.1659, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0052,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0051],
        [-0.0052,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0051],
        [-0.0052,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0051],
        ...,
        [-0.0052,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0051],
        [-0.0052,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0051],
        [-0.0052,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0051]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-16875.0527, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-529.1462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(53.0835, device='cuda:0')



h[100].sum tensor(-126.0279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(159.2951, device='cuda:0')



h[200].sum tensor(-50.8946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(59.8649, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(277229.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0027, 0.0000,  ..., 0.0659, 0.0882, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0667, 0.0770, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0673, 0.0693, 0.0000],
        ...,
        [0.0000, 0.0051, 0.0000,  ..., 0.0679, 0.0540, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0679, 0.0540, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0679, 0.0540, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2138909., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1125.1389, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(4616.0679, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1485.6472, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(541.4189, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(677.1390, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0571],
        [-0.0823],
        [-0.1146],
        ...,
        [-0.2234],
        [-0.2227],
        [-0.2225]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-244039.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(702.8621, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(702.8621, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0052,  0.0019,  0.0000,  ...,  0.0000,  0.0000, -0.0052],
        [-0.0052,  0.0019,  0.0000,  ...,  0.0000,  0.0000, -0.0052],
        [-0.0052,  0.0019,  0.0000,  ...,  0.0000,  0.0000, -0.0052],
        ...,
        [-0.0052,  0.0019,  0.0000,  ...,  0.0000,  0.0000, -0.0052],
        [-0.0052,  0.0019,  0.0000,  ...,  0.0000,  0.0000, -0.0052],
        [-0.0052,  0.0019,  0.0000,  ...,  0.0000,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-17771.0293, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-535.6526, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.6093, device='cuda:0')



h[100].sum tensor(-76.2505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(97.8551, device='cuda:0')



h[200].sum tensor(-97.1713, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.7751, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(243767.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0080, 0.0000,  ..., 0.0696, 0.0548, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0696, 0.0548, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0698, 0.0550, 0.0000],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0692, 0.0545, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0692, 0.0545, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0692, 0.0545, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1963436.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1120.4468, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5048.6348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1385.3042, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(168.9022, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(457.7277, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2515],
        [-0.2451],
        [-0.2331],
        ...,
        [-0.1975],
        [-0.1970],
        [-0.1969]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-181943.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 20.0 event: 300 loss: tensor(616.9868, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(859.1549, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3135],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.4685],
        [0.3289]], device='cuda:0') 
g.ndata[nfet].sum tensor(859.1549, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0055,  0.0026, -0.0032,  ..., -0.0011, -0.0060, -0.0035],
        [-0.0057,  0.0037, -0.0080,  ..., -0.0027, -0.0148, -0.0008],
        [-0.0053,  0.0018,  0.0000,  ...,  0.0000,  0.0000, -0.0053],
        ...,
        [-0.0057,  0.0035, -0.0071,  ..., -0.0024, -0.0131, -0.0013],
        [-0.0056,  0.0030, -0.0049,  ..., -0.0017, -0.0092, -0.0025],
        [-0.0057,  0.0035, -0.0071,  ..., -0.0024, -0.0131, -0.0013]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-17786.2168, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-543.1971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.8605, device='cuda:0')



h[100].sum tensor(-91.7013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(119.6148, device='cuda:0')



h[200].sum tensor(-139.4902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(44.9526, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0144, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(255926.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0540, 0.2137, 0.0000],
        [0.0000, 0.0017, 0.0000,  ..., 0.0587, 0.1774, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0617, 0.1453, 0.0000],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0610, 0.1426, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0543, 0.1957, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0512, 0.2088, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1988031., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1284.3184, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5409.0195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1360.1963, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(244.8568, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(275.5748, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1008],
        [ 0.0999],
        [ 0.0853],
        ...,
        [-0.0035],
        [ 0.0098],
        [ 0.0054]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-55292.6172, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(626.8560, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(626.8560, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0054,  0.0018,  0.0000,  ...,  0.0000,  0.0000, -0.0054],
        [-0.0054,  0.0018,  0.0000,  ...,  0.0000,  0.0000, -0.0054],
        [-0.0054,  0.0018,  0.0000,  ...,  0.0000,  0.0000, -0.0054],
        ...,
        [-0.0054,  0.0018,  0.0000,  ...,  0.0000,  0.0000, -0.0054],
        [-0.0054,  0.0018,  0.0000,  ...,  0.0000,  0.0000, -0.0054],
        [-0.0054,  0.0018,  0.0000,  ...,  0.0000,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-18404.4863, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-548.9640, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.0830, device='cuda:0')



h[100].sum tensor(-66.8624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.2733, device='cuda:0')



h[200].sum tensor(-177.5914, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.7983, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(243808., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0094, 0.0000,  ..., 0.0651, 0.1031, 0.0000],
        [0.0000, 0.0149, 0.0000,  ..., 0.0691, 0.0732, 0.0000],
        [0.0000, 0.0166, 0.0000,  ..., 0.0703, 0.0637, 0.0000],
        ...,
        [0.0000, 0.0168, 0.0000,  ..., 0.0699, 0.0561, 0.0000],
        [0.0000, 0.0168, 0.0000,  ..., 0.0699, 0.0561, 0.0000],
        [0.0000, 0.0168, 0.0000,  ..., 0.0699, 0.0561, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1960294.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1337.8177, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5642.5522, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1290.6208, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(79.2884, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(163.9108, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1085],
        [ 0.1041],
        [ 0.1080],
        ...,
        [-0.1070],
        [-0.1069],
        [-0.1069]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(18758.0156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(809.2412, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(809.2412, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0058,  0.0037, -0.0075,  ..., -0.0024, -0.0141, -0.0011],
        [-0.0056,  0.0026, -0.0031,  ..., -0.0010, -0.0058, -0.0037],
        [-0.0057,  0.0030, -0.0044,  ..., -0.0014, -0.0083, -0.0029],
        ...,
        [-0.0054,  0.0019,  0.0000,  ...,  0.0000,  0.0000, -0.0054],
        [-0.0054,  0.0019,  0.0000,  ...,  0.0000,  0.0000, -0.0054],
        [-0.0054,  0.0019,  0.0000,  ...,  0.0000,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-18738.9648, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-555.4072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.5447, device='cuda:0')



h[100].sum tensor(-85.6058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(112.6656, device='cuda:0')



h[200].sum tensor(-212.8031, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.3410, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0145, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(257012.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 8.7443e-04, 0.0000e+00,  ..., 6.0395e-02, 1.8471e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.5711e-02, 2.2123e-01,
         0.0000e+00],
        [0.0000e+00, 1.7766e-04, 0.0000e+00,  ..., 5.7871e-02, 1.8945e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 1.1883e-02, 0.0000e+00,  ..., 7.1379e-02, 5.9222e-02,
         0.0000e+00],
        [0.0000e+00, 1.1883e-02, 0.0000e+00,  ..., 7.1379e-02, 5.9222e-02,
         0.0000e+00],
        [0.0000e+00, 1.1883e-02, 0.0000e+00,  ..., 7.1379e-02, 5.9222e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2027738.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1359.7130, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5571.3755, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1470.9854, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(171.1083, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(294.5105, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1365],
        [ 0.1372],
        [ 0.1294],
        ...,
        [-0.1643],
        [-0.1640],
        [-0.1639]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-81319.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(783.1926, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(783.1926, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0055,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0055],
        [-0.0055,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0055],
        [-0.0055,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0055],
        ...,
        [-0.0055,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0055],
        [-0.0055,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0055],
        [-0.0055,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-19350.9043, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-560.6605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.3362, device='cuda:0')



h[100].sum tensor(-81.5796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(109.0390, device='cuda:0')



h[200].sum tensor(-244.5660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.9781, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0104, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(256240.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0663, 0.1546, 0.0000],
        [0.0000, 0.0025, 0.0000,  ..., 0.0704, 0.1046, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0725, 0.0805, 0.0000],
        ...,
        [0.0000, 0.0056, 0.0000,  ..., 0.0729, 0.0629, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0729, 0.0629, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0729, 0.0629, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2040846.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1277.5142, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5286.5557, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1606.3121, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(162.1261, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(429.8669, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0091],
        [-0.0589],
        [-0.1159],
        ...,
        [-0.2430],
        [-0.2424],
        [-0.2422]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-224020.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(676.4901, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(676.4901, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0055,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0055],
        [-0.0055,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0055],
        [-0.0055,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0055],
        ...,
        [-0.0055,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0055],
        [-0.0055,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0055],
        [-0.0055,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-19992.3945, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-565.1977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.3857, device='cuda:0')



h[100].sum tensor(-70.0527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(94.1835, device='cuda:0')



h[200].sum tensor(-273.3021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.3953, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(251318.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0045, 0.0000,  ..., 0.0732, 0.0645, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0732, 0.0645, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0734, 0.0647, 0.0000],
        ...,
        [0.0000, 0.0044, 0.0000,  ..., 0.0728, 0.0640, 0.0000],
        [0.0000, 0.0044, 0.0000,  ..., 0.0728, 0.0640, 0.0000],
        [0.0000, 0.0044, 0.0000,  ..., 0.0728, 0.0640, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2023229.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1271.3580, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(5788.2139, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1645.0372, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(210.1440, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(553.9181, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3396],
        [-0.3358],
        [-0.3292],
        ...,
        [-0.2614],
        [-0.2607],
        [-0.2605]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-272247.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(595.5216, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(595.5216, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056],
        [-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056],
        [-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056],
        ...,
        [-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056],
        [-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056],
        [-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-20615.5137, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-569.3673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.6292, device='cuda:0')



h[100].sum tensor(-61.0868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(82.9108, device='cuda:0')



h[200].sum tensor(-299.4628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.1588, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(243291.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 4.6495e-03, 0.0000e+00,  ..., 7.3653e-02, 7.3477e-02,
         0.0000e+00],
        [0.0000e+00, 2.9132e-03, 0.0000e+00,  ..., 7.0198e-02, 1.0461e-01,
         0.0000e+00],
        [0.0000e+00, 2.5815e-05, 0.0000e+00,  ..., 6.1622e-02, 1.8244e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 5.9748e-03, 0.0000e+00,  ..., 7.4192e-02, 6.3759e-02,
         0.0000e+00],
        [0.0000e+00, 5.9748e-03, 0.0000e+00,  ..., 7.4192e-02, 6.3759e-02,
         0.0000e+00],
        [0.0000e+00, 5.9748e-03, 0.0000e+00,  ..., 7.4192e-02, 6.3759e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1986828.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1279.6503, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(6537.4702, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1604.8845, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(84.3662, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(430.5219, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1368],
        [-0.0745],
        [-0.0222],
        ...,
        [-0.2392],
        [-0.2386],
        [-0.2384]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-205492.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(890.1510, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2673],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(890.1510, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056],
        [-0.0061,  0.0040, -0.0074,  ..., -0.0021, -0.0144, -0.0011],
        [-0.0058,  0.0030, -0.0037,  ..., -0.0011, -0.0073, -0.0034],
        ...,
        [-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056],
        [-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056],
        [-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-20437.0996, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-574.3948, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(41.2985, device='cuda:0')



h[100].sum tensor(-90.9761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(123.9302, device='cuda:0')



h[200].sum tensor(-324.3013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(46.5744, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0197, 0.0000,  ..., 0.0000, 0.0000, 0.0073],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264703.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.9448e-03, 0.0000e+00,  ..., 6.2971e-02, 1.6889e-01,
         0.0000e+00],
        [0.0000e+00, 2.2750e-04, 0.0000e+00,  ..., 5.1173e-02, 2.4195e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3710e-02, 3.7535e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 8.7892e-03, 0.0000e+00,  ..., 7.5119e-02, 6.3121e-02,
         0.0000e+00],
        [0.0000e+00, 8.7892e-03, 0.0000e+00,  ..., 7.5119e-02, 6.3121e-02,
         0.0000e+00],
        [0.0000e+00, 8.7892e-03, 0.0000e+00,  ..., 7.5119e-02, 6.3121e-02,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2082929.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1434.3806, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7448.5923, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1687.0402, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(257.1492, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(361.5439, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0850],
        [ 0.1160],
        [ 0.1271],
        ...,
        [-0.2000],
        [-0.1996],
        [-0.1995]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-92479.0391, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(684.0228, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(684.0228, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056],
        [-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056],
        [-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056],
        ...,
        [-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056],
        [-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056],
        [-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-20845.3867, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-573.6983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.7352, device='cuda:0')



h[100].sum tensor(-69.5630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(95.2322, device='cuda:0')



h[200].sum tensor(-323.7139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.7894, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(248408.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0019, 0.0000,  ..., 0.0665, 0.1414, 0.0000],
        [0.0000, 0.0020, 0.0000,  ..., 0.0707, 0.1115, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0722, 0.0974, 0.0000],
        ...,
        [0.0000, 0.0088, 0.0000,  ..., 0.0751, 0.0631, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0751, 0.0631, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0751, 0.0631, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2009322.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1374.1501, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7507.4629, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1621.7412, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(106.5442, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(329.7844, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0831],
        [ 0.0777],
        [ 0.0763],
        ...,
        [-0.2000],
        [-0.1996],
        [-0.1995]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-100047.7734, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1057.5356, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1057.5356, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056],
        [-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056],
        [-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056],
        ...,
        [-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056],
        [-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056],
        [-0.0056,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-20121.1680, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-574.9338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(49.0643, device='cuda:0')



h[100].sum tensor(-107.5454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(147.2341, device='cuda:0')



h[200].sum tensor(-324.7558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(55.3323, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(276235.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0089, 0.0000,  ..., 0.0756, 0.0636, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0756, 0.0636, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0758, 0.0638, 0.0000],
        ...,
        [0.0000, 0.0088, 0.0000,  ..., 0.0751, 0.0631, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0751, 0.0631, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0751, 0.0631, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2137571.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1494.8958, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7217.6064, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1742.3979, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(321.2686, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(379.0878, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1973],
        [-0.2075],
        [-0.2110],
        ...,
        [-0.2000],
        [-0.1996],
        [-0.1995]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-75203.4062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(842.9164, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(842.9164, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0056,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0057],
        [-0.0056,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0057],
        [-0.0059,  0.0030, -0.0035,  ..., -0.0010, -0.0068, -0.0035],
        ...,
        [-0.0056,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0057],
        [-0.0056,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0057],
        [-0.0056,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0057]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-20912.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-577.9308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.1071, device='cuda:0')



h[100].sum tensor(-85.5402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(117.3540, device='cuda:0')



h[200].sum tensor(-346.0958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(44.1030, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(262713.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0053, 0.0000,  ..., 0.0744, 0.0881, 0.0000],
        [0.0000, 0.0029, 0.0000,  ..., 0.0706, 0.1306, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0655, 0.1866, 0.0000],
        ...,
        [0.0000, 0.0076, 0.0000,  ..., 0.0761, 0.0644, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0761, 0.0644, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0761, 0.0644, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2077604.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1435.3116, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(7940.7173, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1759.0856, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(298.2543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(346.8815, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0212],
        [ 0.0413],
        [ 0.0886],
        ...,
        [-0.2138],
        [-0.2128],
        [-0.2086]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-107040.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 30.0 event: 450 loss: tensor(478.7623, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(799.4336, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2817],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(799.4336, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0059,  0.0032, -0.0037,  ..., -0.0010, -0.0074, -0.0034],
        [-0.0059,  0.0030, -0.0030,  ..., -0.0008, -0.0059, -0.0039],
        [-0.0061,  0.0040, -0.0067,  ..., -0.0018, -0.0132, -0.0015],
        ...,
        [-0.0057,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0057],
        [-0.0057,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0057],
        [-0.0057,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0057]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-21438.2031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-581.1127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.0897, device='cuda:0')



h[100].sum tensor(-79.5165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(111.3002, device='cuda:0')



h[200].sum tensor(-365.9020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.8279, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0153, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(256758.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0677, 0.1837, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0641, 0.2317, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0678, 0.1954, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0769, 0.0655, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0769, 0.0655, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0769, 0.0655, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2036733.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1377.5935, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(9026.6074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1800.3101, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(283.5447, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(358.4995, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0341],
        [ 0.0403],
        [ 0.0299],
        ...,
        [-0.2405],
        [-0.2400],
        [-0.2399]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-162529.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(898.5753, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(898.5753, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0057,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0058],
        [-0.0057,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0058],
        [-0.0057,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0058],
        ...,
        [-0.0057,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0058],
        [-0.0057,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0058],
        [-0.0057,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-21501.1328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-584.5486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(41.6894, device='cuda:0')



h[100].sum tensor(-89.7359, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(125.1031, device='cuda:0')



h[200].sum tensor(-384.4630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(47.0152, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264657.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0768, 0.0662, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0768, 0.0662, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0771, 0.0664, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0764, 0.0657, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0764, 0.0657, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0764, 0.0657, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2061897.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1377.2667, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(11821.9863, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1936.0801, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(392.9580, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(410.2360, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2934],
        [-0.2883],
        [-0.2765],
        ...,
        [-0.2511],
        [-0.2505],
        [-0.2503]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-172565.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(695.2383, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(695.2383, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0057,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0058],
        [-0.0057,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0058],
        [-0.0061,  0.0037, -0.0055,  ..., -0.0014, -0.0111, -0.0022],
        ...,
        [-0.0057,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0058],
        [-0.0057,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0058],
        [-0.0057,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-22197.1582, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-586.6370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.2556, device='cuda:0')



h[100].sum tensor(-68.8480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.7937, device='cuda:0')



h[200].sum tensor(-400.3334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.3762, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0141, 0.0000,  ..., 0.0000, 0.0000, 0.0014],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(252575.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0040, 0.0000,  ..., 0.0709, 0.1117, 0.0000],
        [0.0000, 0.0013, 0.0000,  ..., 0.0595, 0.1943, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0363, 0.3293, 0.0000],
        ...,
        [0.0000, 0.0087, 0.0000,  ..., 0.0747, 0.0652, 0.0006],
        [0.0000, 0.0087, 0.0000,  ..., 0.0747, 0.0652, 0.0006],
        [0.0000, 0.0088, 0.0000,  ..., 0.0746, 0.0661, 0.0003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2010839., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1328.3007, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15678.8691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1974.5410, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(257.7378, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(418.6378, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0669],
        [-0.0825],
        [-0.1087],
        ...,
        [-0.2386],
        [-0.2280],
        [-0.2142]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-140065.4531, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(827.3617, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2849],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(827.3617, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0032, -0.0036,  ..., -0.0009, -0.0072, -0.0035],
        [-0.0058,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0058],
        [-0.0060,  0.0032, -0.0036,  ..., -0.0009, -0.0072, -0.0035],
        ...,
        [-0.0058,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0058],
        [-0.0058,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0058],
        [-0.0058,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-22178.7930, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-589.6050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.3854, device='cuda:0')



h[100].sum tensor(-80.3281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(115.1884, device='cuda:0')



h[200].sum tensor(-415.8752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.2891, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(262153.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0629, 0.2033, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0630, 0.2050, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0674, 0.1495, 0.0000],
        ...,
        [0.0000, 0.0118, 0.0000,  ..., 0.0724, 0.0644, 0.0033],
        [0.0000, 0.0118, 0.0000,  ..., 0.0724, 0.0644, 0.0033],
        [0.0000, 0.0118, 0.0000,  ..., 0.0724, 0.0644, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2063619.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1421.3651, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19667.5664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2128.3748, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(361.8397, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(450.0813, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1422],
        [ 0.1326],
        [ 0.1030],
        ...,
        [-0.2379],
        [-0.2370],
        [-0.2342]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-76391.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(667.8674, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5122],
        [0.4766],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(667.8674, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062,  0.0040, -0.0064,  ..., -0.0016, -0.0130, -0.0016],
        [-0.0065,  0.0054, -0.0109,  ..., -0.0027, -0.0221,  0.0014],
        [-0.0066,  0.0055, -0.0114,  ..., -0.0028, -0.0230,  0.0017],
        ...,
        [-0.0058,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0058],
        [-0.0058,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0058],
        [-0.0058,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-22557.6016, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-589.0549, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.9857, device='cuda:0')



h[100].sum tensor(-64.8614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.9830, device='cuda:0')



h[200].sum tensor(-415.2921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.9441, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0185, 0.0000,  ..., 0.0000, 0.0000, 0.0043],
        [0.0000, 0.0219, 0.0000,  ..., 0.0000, 0.0000, 0.0081],
        [0.0000, 0.0203, 0.0000,  ..., 0.0000, 0.0000, 0.0049],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(250016.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0321, 0.4080, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0264, 0.4217, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0343, 0.3658, 0.0000],
        ...,
        [0.0000, 0.0118, 0.0000,  ..., 0.0724, 0.0644, 0.0033],
        [0.0000, 0.0118, 0.0000,  ..., 0.0724, 0.0644, 0.0033],
        [0.0000, 0.0118, 0.0000,  ..., 0.0724, 0.0644, 0.0033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2009005., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1356.0178, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19919.3262, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2066.5466, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(201.3770, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(429.6001, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0686],
        [ 0.0621],
        [ 0.0583],
        ...,
        [-0.2379],
        [-0.2375],
        [-0.2374]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-82463.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1044.4760, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1044.4760, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0030, -0.0027,  ..., -0.0006, -0.0054, -0.0041],
        [-0.0060,  0.0030, -0.0027,  ..., -0.0006, -0.0054, -0.0041],
        [-0.0058,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0058],
        ...,
        [-0.0063,  0.0043, -0.0072,  ..., -0.0017, -0.0147, -0.0010],
        [-0.0058,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0058],
        [-0.0058,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-21997.3516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-592.6981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(48.4584, device='cuda:0')



h[100].sum tensor(-101.1275, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(145.4159, device='cuda:0')



h[200].sum tensor(-430.4838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(54.6490, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0147, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(282223.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0589, 0.2476, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0601, 0.2300, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0584, 0.2548, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0598, 0.2145, 0.0000],
        [0.0000, 0.0015, 0.0000,  ..., 0.0628, 0.1725, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0687, 0.1006, 0.0036]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2167679.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1466.1409, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(22287.3301, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2266.0164, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(697.3915, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(580.4828, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1516],
        [ 0.1502],
        [ 0.1387],
        ...,
        [ 0.0557],
        [ 0.0136],
        [-0.0624]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-102246.5234, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(871.6460, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2766],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(871.6460, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0031, -0.0026,  ..., -0.0006, -0.0054, -0.0041],
        [-0.0060,  0.0033, -0.0034,  ..., -0.0008, -0.0069, -0.0036],
        [-0.0058,  0.0023,  0.0000,  ...,  0.0000,  0.0000, -0.0059],
        ...,
        [-0.0058,  0.0023,  0.0000,  ...,  0.0000,  0.0000, -0.0059],
        [-0.0058,  0.0023,  0.0000,  ...,  0.0000,  0.0000, -0.0059],
        [-0.0058,  0.0023,  0.0000,  ...,  0.0000,  0.0000, -0.0059]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-22897.4688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-594.1898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.4400, device='cuda:0')



h[100].sum tensor(-83.2495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(121.3539, device='cuda:0')



h[200].sum tensor(-442.3499, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(45.6062, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0171, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0124, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(263840.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0546, 0.2707, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0604, 0.2026, 0.0000],
        [0.0000, 0.0035, 0.0000,  ..., 0.0662, 0.1363, 0.0029],
        ...,
        [0.0000, 0.0097, 0.0000,  ..., 0.0717, 0.0646, 0.0133],
        [0.0000, 0.0097, 0.0000,  ..., 0.0717, 0.0646, 0.0133],
        [0.0000, 0.0097, 0.0000,  ..., 0.0717, 0.0646, 0.0133]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2095607.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1386.6611, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(24783.0723, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2284.5933, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(447.8291, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(648.0124, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0658],
        [ 0.0430],
        [-0.0062],
        ...,
        [-0.3105],
        [-0.3096],
        [-0.3093]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-184485.5156, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(948.4626, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(948.4626, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0058,  0.0023,  0.0000,  ...,  0.0000,  0.0000, -0.0059],
        [-0.0058,  0.0023,  0.0000,  ...,  0.0000,  0.0000, -0.0059],
        [-0.0061,  0.0033, -0.0031,  ..., -0.0007, -0.0065, -0.0037],
        ...,
        [-0.0058,  0.0023,  0.0000,  ...,  0.0000,  0.0000, -0.0059],
        [-0.0058,  0.0023,  0.0000,  ...,  0.0000,  0.0000, -0.0059],
        [-0.0058,  0.0023,  0.0000,  ...,  0.0000,  0.0000, -0.0059]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-23000.4941, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-596.3718, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(44.0039, device='cuda:0')



h[100].sum tensor(-89.4818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(132.0486, device='cuda:0')



h[200].sum tensor(-454.0703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(49.6254, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268879.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0058, 0.0000,  ..., 0.0695, 0.0815, 0.0116],
        [0.0000, 0.0036, 0.0000,  ..., 0.0669, 0.1166, 0.0054],
        [0.0000, 0.0011, 0.0000,  ..., 0.0650, 0.1492, 0.0006],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0701, 0.0660, 0.0166],
        [0.0000, 0.0073, 0.0000,  ..., 0.0701, 0.0660, 0.0166],
        [0.0000, 0.0073, 0.0000,  ..., 0.0701, 0.0660, 0.0166]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2118166.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1376.1213, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(26341.0762, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2324.2827, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(537.5783, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(736.9857, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2496],
        [-0.1484],
        [-0.0613],
        ...,
        [-0.3362],
        [-0.3330],
        [-0.3303]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-228915.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(696.0815, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8110],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(696.0815, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0058,  0.0023,  0.0000,  ...,  0.0000,  0.0000, -0.0059],
        [-0.0066,  0.0053, -0.0095,  ..., -0.0020, -0.0199,  0.0008],
        [-0.0058,  0.0023,  0.0000,  ...,  0.0000,  0.0000, -0.0059],
        ...,
        [-0.0058,  0.0023,  0.0000,  ...,  0.0000,  0.0000, -0.0059],
        [-0.0058,  0.0023,  0.0000,  ...,  0.0000,  0.0000, -0.0059],
        [-0.0058,  0.0023,  0.0000,  ...,  0.0000,  0.0000, -0.0059]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-23864.3477, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-597.2252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.2947, device='cuda:0')



h[100].sum tensor(-64.8689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.9111, device='cuda:0')



h[200].sum tensor(-463.4335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.4203, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0008],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0201, 0.0000,  ..., 0.0000, 0.0000, 0.0015],
        ...,
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(249761.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0583, 0.1433, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0565, 0.1652, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0486, 0.2340, 0.0000],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0667, 0.0675, 0.0172],
        [0.0000, 0.0073, 0.0000,  ..., 0.0667, 0.0675, 0.0172],
        [0.0000, 0.0073, 0.0000,  ..., 0.0667, 0.0675, 0.0172]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2013775., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1281.3562, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(28595.6641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2204.3999, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(282.2292, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(706.9542, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1539],
        [-0.0974],
        [-0.0585],
        ...,
        [-0.3339],
        [-0.3326],
        [-0.3323]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-226809.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(759.6832, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(759.6832, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0059,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0059],
        [-0.0059,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0059],
        [-0.0061,  0.0031, -0.0030,  ..., -0.0006, -0.0063, -0.0038],
        ...,
        [-0.0059,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0059],
        [-0.0059,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0059],
        [-0.0059,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0059]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-23748.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-599.0496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.2455, device='cuda:0')



h[100].sum tensor(-70.3510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(105.7660, device='cuda:0')



h[200].sum tensor(-473.1677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.7481, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0119, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(260707., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0021, 0.0000,  ..., 0.0592, 0.1128, 0.0028],
        [0.0000, 0.0021, 0.0000,  ..., 0.0566, 0.1447, 0.0026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0525, 0.1964, 0.0000],
        ...,
        [0.0000, 0.0087, 0.0000,  ..., 0.0624, 0.0692, 0.0161],
        [0.0000, 0.0087, 0.0000,  ..., 0.0624, 0.0692, 0.0161],
        [0.0000, 0.0087, 0.0000,  ..., 0.0624, 0.0692, 0.0161]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2068517.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1388.2529, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(30170.1133, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2214.1216, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(522.2092, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(664.5895, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1049],
        [ 0.1210],
        [ 0.1378],
        ...,
        [-0.3127],
        [-0.3118],
        [-0.3114]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-124724.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 40.0 event: 600 loss: tensor(503.8665, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(756.7803, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(756.7803, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0059,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        ...,
        [-0.0059,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-23882.9746, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-600.4948, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.1108, device='cuda:0')



h[100].sum tensor(-69.5808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(105.3618, device='cuda:0')



h[200].sum tensor(-481.7793, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.5962, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258222.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0094, 0.0000,  ..., 0.0597, 0.0735, 0.0166],
        [0.0000, 0.0096, 0.0000,  ..., 0.0597, 0.0751, 0.0162],
        [0.0000, 0.0094, 0.0000,  ..., 0.0591, 0.0862, 0.0126],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0596, 0.0706, 0.0172],
        [0.0000, 0.0094, 0.0000,  ..., 0.0596, 0.0706, 0.0172],
        [0.0000, 0.0094, 0.0000,  ..., 0.0596, 0.0706, 0.0172]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2049254.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1409.7205, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(31338.4453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2235.6182, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(454.4763, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(665.4915, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2010],
        [-0.1423],
        [-0.0530],
        ...,
        [-0.3129],
        [-0.3123],
        [-0.3121]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-96246.3672, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(718.3706, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(718.3706, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0059,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        ...,
        [-0.0059,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-24292.0938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-601.6837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.3288, device='cuda:0')



h[100].sum tensor(-65.5938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(100.0143, device='cuda:0')



h[200].sum tensor(-489.4644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.5865, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(253749.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0022, 0.0000,  ..., 0.0565, 0.1137, 0.0084],
        [0.0000, 0.0039, 0.0000,  ..., 0.0573, 0.1022, 0.0125],
        [0.0000, 0.0060, 0.0000,  ..., 0.0582, 0.0908, 0.0169],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0592, 0.0716, 0.0228],
        [0.0000, 0.0073, 0.0000,  ..., 0.0592, 0.0716, 0.0228],
        [0.0000, 0.0073, 0.0000,  ..., 0.0592, 0.0716, 0.0228]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2022086.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1327.7915, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(31091.7539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2323.1646, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(877.6980, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(821.6667, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1951e-04],
        [-7.6927e-03],
        [-1.9587e-02],
        ...,
        [-3.5821e-01],
        [-3.5695e-01],
        [-3.5656e-01]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-224986.8438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(976.5939, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(976.5939, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0059,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        ...,
        [-0.0059,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-23817.8516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-603.7888, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(45.3090, device='cuda:0')



h[100].sum tensor(-87.6778, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(135.9651, device='cuda:0')



h[200].sum tensor(-497.7215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(51.0973, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(274036.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0066, 0.0000,  ..., 0.0588, 0.0767, 0.0231],
        [0.0000, 0.0065, 0.0000,  ..., 0.0588, 0.0764, 0.0232],
        [0.0000, 0.0056, 0.0000,  ..., 0.0588, 0.0795, 0.0220],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0586, 0.0718, 0.0239],
        [0.0000, 0.0062, 0.0000,  ..., 0.0586, 0.0718, 0.0239],
        [0.0000, 0.0062, 0.0000,  ..., 0.0586, 0.0718, 0.0239]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2106410.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1385.2805, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(31459.9414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2447.1428, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1706.6577, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(921.1333, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3311],
        [-0.2969],
        [-0.2383],
        ...,
        [-0.3742],
        [-0.3728],
        [-0.3724]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-244917.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(893.9696, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5503],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(893.9696, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064,  0.0041, -0.0059,  ..., -0.0009, -0.0129, -0.0015],
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0073,  0.0076, -0.0161,  ..., -0.0026, -0.0352,  0.0064],
        ...,
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-24145.2461, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-604.6060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(41.4757, device='cuda:0')



h[100].sum tensor(-79.9254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(124.4618, device='cuda:0')



h[200].sum tensor(-503.8863, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(46.7742, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0203, 0.0000,  ..., 0.0000, 0.0000, 0.0069],
        [0.0000, 0.0195, 0.0000,  ..., 0.0000, 0.0000, 0.0073],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268849., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0435, 0.1969, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0221, 0.3476, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0112, 0.4156, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0576, 0.0714, 0.0213],
        [0.0000, 0.0059, 0.0000,  ..., 0.0576, 0.0714, 0.0213],
        [0.0000, 0.0059, 0.0000,  ..., 0.0576, 0.0714, 0.0213]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2057877.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1300.9982, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(34185.8555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2452.1169, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(950.3481, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(931.7767, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0005],
        [ 0.0184],
        [ 0.0196],
        ...,
        [-0.3628],
        [-0.3616],
        [-0.3613]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-188361.3594, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(668.5909, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2844],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(668.5909, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0061,  0.0027, -0.0022,  ..., -0.0003, -0.0048, -0.0043],
        [-0.0062,  0.0030, -0.0030,  ..., -0.0005, -0.0066, -0.0037],
        [-0.0059,  0.0019,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        ...,
        [-0.0059,  0.0019,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0019,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0019,  0.0000,  ...,  0.0000,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-24891.1016, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-604.7909, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.0192, device='cuda:0')



h[100].sum tensor(-59.2825, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.0838, device='cuda:0')



h[200].sum tensor(-508.7848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.9820, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.4621e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         3.5781e-05],
        [0.0000e+00, 1.0027e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.8329e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 7.7902e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.7902e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.7902e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(255736.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.8836e-02, 2.8633e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.7530e-02, 1.9167e-01,
         2.3319e-04],
        [0.0000e+00, 2.0746e-03, 0.0000e+00,  ..., 5.2821e-02, 1.2827e-01,
         6.0655e-03],
        ...,
        [0.0000e+00, 5.8580e-03, 0.0000e+00,  ..., 5.6541e-02, 7.0603e-02,
         1.8013e-02],
        [0.0000e+00, 5.8580e-03, 0.0000e+00,  ..., 5.6541e-02, 7.0603e-02,
         1.8013e-02],
        [0.0000e+00, 5.8580e-03, 0.0000e+00,  ..., 5.6541e-02, 7.0603e-02,
         1.8013e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2000693.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1201.1838, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(37826.4648, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2449.0984, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(432.8641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(912.8687, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1018],
        [ 0.0785],
        [ 0.0322],
        ...,
        [-0.3406],
        [-0.3417],
        [-0.3424]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-106490.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(801.7515, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3020],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(801.7515, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064,  0.0037, -0.0049,  ..., -0.0007, -0.0109, -0.0021],
        [-0.0064,  0.0039, -0.0055,  ..., -0.0008, -0.0123, -0.0016],
        [-0.0064,  0.0040, -0.0057,  ..., -0.0008, -0.0126, -0.0015],
        ...,
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-24790.7852, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-606.1703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.1972, device='cuda:0')



h[100].sum tensor(-70.1978, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(111.6229, device='cuda:0')



h[200].sum tensor(-514.7672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.9492, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0153, 0.0000,  ..., 0.0000, 0.0000, 0.0006],
        [0.0000, 0.0172, 0.0000,  ..., 0.0000, 0.0000, 0.0006],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(267337.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0228, 0.4007, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0273, 0.3754, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0331, 0.3441, 0.0000],
        ...,
        [0.0000, 0.0040, 0.0000,  ..., 0.0571, 0.0695, 0.0194],
        [0.0000, 0.0040, 0.0000,  ..., 0.0571, 0.0695, 0.0194],
        [0.0000, 0.0040, 0.0000,  ..., 0.0571, 0.0695, 0.0194]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2056390.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1248.8896, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39331.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2531.4270, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(806.9220, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(977.3307, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0886],
        [ 0.0859],
        [ 0.0804],
        ...,
        [-0.3757],
        [-0.3744],
        [-0.3740]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-169561.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(696.7634, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5815],
        [0.5928],
        [0.5962],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(696.7634, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0069,  0.0059, -0.0108,  ..., -0.0014, -0.0244,  0.0028],
        [-0.0079,  0.0098, -0.0216,  ..., -0.0029, -0.0487,  0.0115],
        [-0.0069,  0.0060, -0.0109,  ..., -0.0015, -0.0245,  0.0028],
        ...,
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-25477.9863, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-606.6159, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.3263, device='cuda:0')



h[100].sum tensor(-60.4591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(97.0060, device='cuda:0')



h[200].sum tensor(-519.1796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.4560, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0272, 0.0000,  ..., 0.0000, 0.0000, 0.0187],
        [0.0000, 0.0258, 0.0000,  ..., 0.0000, 0.0000, 0.0158],
        [0.0000, 0.0274, 0.0000,  ..., 0.0000, 0.0000, 0.0193],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(253557.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.2176e-03, 5.0982e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.3792e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0698e-04, 5.3840e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 1.7967e-03, 0.0000e+00,  ..., 5.7877e-02, 6.8655e-02,
         2.1909e-02],
        [0.0000e+00, 1.7967e-03, 0.0000e+00,  ..., 5.7877e-02, 6.8655e-02,
         2.1909e-02],
        [0.0000e+00, 1.7967e-03, 0.0000e+00,  ..., 5.7877e-02, 6.8655e-02,
         2.1909e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1982339., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1187.4733, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41916.3672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2525.2124, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(441.5700, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1004.2665, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0179],
        [ 0.0158],
        [ 0.0183],
        ...,
        [-0.4149],
        [-0.4134],
        [-0.4129]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-284375.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(567.5947, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(567.5947, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0062,  0.0031, -0.0028,  ..., -0.0003, -0.0064, -0.0037],
        ...,
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0060],
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-26167.9531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-606.8928, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.3335, device='cuda:0')



h[100].sum tensor(-48.7205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(79.0227, device='cuda:0')



h[200].sum tensor(-523.0210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.6976, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0033],
        ...,
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(245123.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0544, 0.1142, 0.0086],
        [0.0000, 0.0000, 0.0000,  ..., 0.0482, 0.1719, 0.0026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0343, 0.2719, 0.0000],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0576, 0.0687, 0.0215],
        [0.0000, 0.0009, 0.0000,  ..., 0.0576, 0.0687, 0.0215],
        [0.0000, 0.0009, 0.0000,  ..., 0.0576, 0.0687, 0.0215]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1953331.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1142.4529, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42517.6250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2468.6606, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(261.9523, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(992.1896, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0697],
        [-0.0314],
        [-0.0086],
        ...,
        [-0.4276],
        [-0.4260],
        [-0.4256]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-292623.7812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(730.3507, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(730.3507, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        ...,
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-25727.7930, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-608.1736, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.8846, device='cuda:0')



h[100].sum tensor(-62.2503, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(101.6822, device='cuda:0')



h[200].sum tensor(-527.8461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.2133, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(259367.9219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0539, 0.1088, 0.0074],
        [0.0000, 0.0000, 0.0000,  ..., 0.0548, 0.0969, 0.0082],
        [0.0000, 0.0000, 0.0000,  ..., 0.0528, 0.1269, 0.0047],
        ...,
        [0.0000, 0.0011, 0.0000,  ..., 0.0564, 0.0691, 0.0192],
        [0.0000, 0.0011, 0.0000,  ..., 0.0564, 0.0691, 0.0192],
        [0.0000, 0.0011, 0.0000,  ..., 0.0564, 0.0691, 0.0192]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2025622.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1231.0764, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41146.1484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2470.4817, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(621.2684, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(976.1310, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2382],
        [-0.1948],
        [-0.1112],
        ...,
        [-0.4255],
        [-0.4240],
        [-0.4236]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-216432.3438, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(672.5190, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2610],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(672.5190, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062,  0.0030, -0.0025,  ..., -0.0003, -0.0058, -0.0039],
        [-0.0062,  0.0030, -0.0024,  ..., -0.0003, -0.0056, -0.0040],
        [-0.0064,  0.0039, -0.0049,  ..., -0.0005, -0.0114, -0.0019],
        ...,
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0059,  0.0020,  0.0000,  ...,  0.0000,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-26037.7988, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-608.5641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.2015, device='cuda:0')



h[100].sum tensor(-56.3519, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.6306, device='cuda:0')



h[200].sum tensor(-531.2294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.1875, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0145, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(256005.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0442, 0.2101, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0392, 0.2697, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0408, 0.2502, 0.0000],
        ...,
        [0.0000, 0.0017, 0.0000,  ..., 0.0554, 0.0694, 0.0165],
        [0.0000, 0.0017, 0.0000,  ..., 0.0554, 0.0694, 0.0165],
        [0.0000, 0.0017, 0.0000,  ..., 0.0554, 0.0694, 0.0165]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2011313., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1200.2435, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41697.3047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2391.3755, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(469.6821, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(927.8531, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1159],
        [ 0.1256],
        [ 0.1309],
        ...,
        [-0.4186],
        [-0.4175],
        [-0.4172]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-160063.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 50.0 event: 750 loss: tensor(563.3039, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(682.6090, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(682.6090, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0065,  0.0041, -0.0051,  ..., -0.0005, -0.0119, -0.0016],
        ...,
        [-0.0060,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ...,  0.0000,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-26189.2578, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-609.1783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.6696, device='cuda:0')



h[100].sum tensor(-56.9590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(95.0354, device='cuda:0')



h[200].sum tensor(-534.6434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.7154, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0172, 0.0000,  ..., 0.0000, 0.0000, 0.0016],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(255886.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0419, 0.1909, 0.0010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0222, 0.3240, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.4321, 0.0000],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0546, 0.0694, 0.0170],
        [0.0000, 0.0015, 0.0000,  ..., 0.0546, 0.0694, 0.0170],
        [0.0000, 0.0015, 0.0000,  ..., 0.0546, 0.0694, 0.0170]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2011320.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1185.7804, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42539.2539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2398.7036, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(547.5310, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(949.7177, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0764],
        [ 0.1131],
        [ 0.1332],
        ...,
        [-0.4378],
        [-0.4366],
        [-0.4362]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-195954.0312, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(949.4442, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(949.4442, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-25465.4453, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-610.7003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(44.0494, device='cuda:0')



h[100].sum tensor(-79.1589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(132.1852, device='cuda:0')



h[200].sum tensor(-539.0217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(49.6767, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(275476.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0517, 0.1085, 0.0081],
        [0.0000, 0.0010, 0.0000,  ..., 0.0536, 0.0824, 0.0135],
        [0.0000, 0.0010, 0.0000,  ..., 0.0546, 0.0721, 0.0181],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0542, 0.0695, 0.0185],
        [0.0000, 0.0007, 0.0000,  ..., 0.0542, 0.0695, 0.0185],
        [0.0000, 0.0007, 0.0000,  ..., 0.0542, 0.0695, 0.0185]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2096585.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1210.7305, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42982.3086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2519.0342, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1273.2338, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1059.0425, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0445],
        [-0.1496],
        [-0.2546],
        ...,
        [-0.4675],
        [-0.4669],
        [-0.4666]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-257012.7969, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(729.1266, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6772],
        [0.4797],
        [0.4402],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(729.1266, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0068,  0.0053, -0.0076,  ..., -0.0006, -0.0181,  0.0008],
        [-0.0073,  0.0076, -0.0133,  ..., -0.0011, -0.0317,  0.0060],
        [-0.0067,  0.0051, -0.0072,  ..., -0.0006, -0.0172,  0.0005],
        ...,
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-26493.9766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-610.3173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.8278, device='cuda:0')



h[100].sum tensor(-59.4219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(101.5118, device='cuda:0')



h[200].sum tensor(-540.6675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.1493, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0239, 0.0000,  ..., 0.0000, 0.0000, 0.0101],
        [0.0000, 0.0244, 0.0000,  ..., 0.0000, 0.0000, 0.0106],
        [0.0000, 0.0253, 0.0000,  ..., 0.0000, 0.0000, 0.0158],
        ...,
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(257667.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.1372e-03, 4.2497e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.8476e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.4143e-03, 4.9475e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 3.9931e-04, 0.0000e+00,  ..., 5.3655e-02, 6.9507e-02,
         1.8295e-02],
        [0.0000e+00, 3.9931e-04, 0.0000e+00,  ..., 5.3655e-02, 6.9507e-02,
         1.8295e-02],
        [0.0000e+00, 3.9931e-04, 0.0000e+00,  ..., 5.3655e-02, 6.9507e-02,
         1.8295e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2024141.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1132.5745, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(44862.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2493.5234, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1193.6296, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1041.9534, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0272],
        [ 0.0546],
        [ 0.0703],
        ...,
        [-0.4857],
        [-0.4842],
        [-0.4837]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-288018.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(898.3650, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2461],
        [0.3000],
        [0.2915],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(898.3650, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0065,  0.0043, -0.0052,  ..., -0.0004, -0.0126, -0.0012],
        [-0.0065,  0.0041, -0.0047,  ..., -0.0004, -0.0114, -0.0017],
        [-0.0065,  0.0043, -0.0051,  ..., -0.0004, -0.0124, -0.0013],
        ...,
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-25889.1680, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-611.3623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(41.6796, device='cuda:0')



h[100].sum tensor(-72.5401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(125.0738, device='cuda:0')



h[200].sum tensor(-544.0117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(47.0042, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0162, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0163, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0161, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268225.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0347, 0.3094, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0344, 0.3058, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0346, 0.3014, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0518, 0.0837, 0.0105],
        [0.0000, 0.0000, 0.0000,  ..., 0.0497, 0.1125, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0446, 0.1788, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2065041.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1238.4446, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(46778.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2561.6235, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(999.6857, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1007.9233, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1224],
        [ 0.1215],
        [ 0.1219],
        ...,
        [-0.3114],
        [-0.1698],
        [-0.0445]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-211838.3906, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(755.4523, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.7568],
        [0.0000],
        [0.3125],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(755.4523, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064,  0.0039, -0.0042,  ..., -0.0003, -0.0101, -0.0021],
        [-0.0072,  0.0068, -0.0110,  ..., -0.0008, -0.0270,  0.0044],
        [-0.0062,  0.0029, -0.0019,  ..., -0.0001, -0.0046, -0.0043],
        ...,
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-26539.6992, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-611.2296, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.0492, device='cuda:0')



h[100].sum tensor(-60.5059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(105.1769, device='cuda:0')



h[200].sum tensor(-545.6078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.5267, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0250, 0.0000,  ..., 0.0000, 0.0000, 0.0129],
        [0.0000, 0.0169, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        [0.0000, 0.0178, 0.0000,  ..., 0.0000, 0.0000, 0.0044],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(261924.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4668, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0119, 0.3903, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0212, 0.3402, 0.0000],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0523, 0.0693, 0.0144],
        [0.0000, 0.0006, 0.0000,  ..., 0.0523, 0.0693, 0.0144],
        [0.0000, 0.0006, 0.0000,  ..., 0.0523, 0.0693, 0.0144]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2031644.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1186.6604, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(49061.5703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2536.1860, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(896.1324, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(992.6525, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0994],
        [ 0.0973],
        [ 0.0890],
        ...,
        [-0.4897],
        [-0.4881],
        [-0.4876]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-220461.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(997.9979, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(997.9979, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-25629.3711, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-612.4880, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(46.3021, device='cuda:0')



h[100].sum tensor(-79.4829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(138.9451, device='cuda:0')



h[200].sum tensor(-548.9039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(52.2172, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(288587.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0004, 0.0000,  ..., 0.0526, 0.0701, 0.0137],
        [0.0000, 0.0004, 0.0000,  ..., 0.0526, 0.0701, 0.0137],
        [0.0000, 0.0004, 0.0000,  ..., 0.0528, 0.0704, 0.0137],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0523, 0.0696, 0.0136],
        [0.0000, 0.0003, 0.0000,  ..., 0.0523, 0.0696, 0.0136],
        [0.0000, 0.0003, 0.0000,  ..., 0.0523, 0.0696, 0.0136]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2192434., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1417.9369, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(49301.9727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2706.5635, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1754.8735, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1095.5787, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5524],
        [-0.5776],
        [-0.5968],
        ...,
        [-0.5034],
        [-0.5018],
        [-0.5013]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-198493.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(821.8214, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4001],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(821.8214, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0063,  0.0036, -0.0032,  ..., -0.0002, -0.0081, -0.0029],
        [-0.0064,  0.0036, -0.0033,  ..., -0.0002, -0.0082, -0.0028],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-26417.9121, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-612.1423, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.1284, device='cuda:0')



h[100].sum tensor(-64.5174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(114.4171, device='cuda:0')



h[200].sum tensor(-549.9023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.9993, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0165, 0.0000,  ..., 0.0000, 0.0000, 0.0003],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(270504.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0348, 0.2656, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0437, 0.1822, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0486, 0.1300, 0.0020],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0526, 0.0700, 0.0135],
        [0.0000, 0.0000, 0.0000,  ..., 0.0526, 0.0700, 0.0135],
        [0.0000, 0.0000, 0.0000,  ..., 0.0526, 0.0700, 0.0135]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2079259., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1463.0452, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(50470.5859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2713.8320, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1511.1460, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1142.0713, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0465],
        [-0.1690],
        [-0.3165],
        ...,
        [-0.5264],
        [-0.5247],
        [-0.5242]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-248141.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(685.3923, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(685.3923, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064,  0.0038, -0.0038,  ..., -0.0002, -0.0096, -0.0023],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-27190.3027, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-611.9094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.7987, device='cuda:0')



h[100].sum tensor(-52.8558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(95.4229, device='cuda:0')



h[200].sum tensor(-550.9138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.8610, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0121, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0104, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(256314.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0380, 0.2070, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0450, 0.1485, 0.0000],
        [0.0000, 0.0004, 0.0000,  ..., 0.0500, 0.1029, 0.0036],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0516, 0.0691, 0.0137],
        [0.0000, 0.0004, 0.0000,  ..., 0.0516, 0.0691, 0.0137],
        [0.0000, 0.0004, 0.0000,  ..., 0.0516, 0.0691, 0.0137]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2007774.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1364.2495, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(52651.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2600.0408, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1252.0071, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1045.8186, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0339],
        [-0.1142],
        [-0.2049],
        ...,
        [-0.5297],
        [-0.5330],
        [-0.5336]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-256781.1094, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(674.5704, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(674.5704, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2102e-03,  3.0277e-03, -1.9248e-03,  ..., -7.8534e-05,
         -4.9134e-03, -4.1201e-03],
        [-5.9806e-03,  2.1682e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1117e-03],
        [-5.9806e-03,  2.1682e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1117e-03],
        ...,
        [-5.9806e-03,  2.1682e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1117e-03],
        [-5.9806e-03,  2.1682e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1117e-03],
        [-5.9806e-03,  2.1682e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1117e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-27436.9336, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-612.1620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.2967, device='cuda:0')



h[100].sum tensor(-52.1241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.9163, device='cuda:0')



h[200].sum tensor(-552.4580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.2948, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(257785.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0432, 0.1763, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0453, 0.1456, 0.0007],
        [0.0000, 0.0008, 0.0000,  ..., 0.0488, 0.0992, 0.0057],
        ...,
        [0.0000, 0.0010, 0.0000,  ..., 0.0506, 0.0679, 0.0148],
        [0.0000, 0.0010, 0.0000,  ..., 0.0506, 0.0679, 0.0148],
        [0.0000, 0.0010, 0.0000,  ..., 0.0506, 0.0679, 0.0148]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2037037.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1342.7852, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(54399.2539, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2546.5562, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1380.2206, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(957.1930, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0534],
        [-0.0181],
        [-0.1296],
        ...,
        [-0.5472],
        [-0.5455],
        [-0.5450]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-233672.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1020.5215, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1020.5215, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0068,  0.0051, -0.0064,  ..., -0.0002, -0.0166,  0.0007],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-26215.6055, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-613.6813, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(47.3471, device='cuda:0')



h[100].sum tensor(-78.0050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(142.0809, device='cuda:0')



h[200].sum tensor(-555.6062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(53.3956, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0196, 0.0000,  ..., 0.0000, 0.0000, 0.0066],
        [0.0000, 0.0161, 0.0000,  ..., 0.0000, 0.0000, 0.0007],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(288430.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.4656, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.4098, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.3564, 0.0000],
        ...,
        [0.0000, 0.0011, 0.0000,  ..., 0.0499, 0.0668, 0.0172],
        [0.0000, 0.0011, 0.0000,  ..., 0.0499, 0.0668, 0.0172],
        [0.0000, 0.0011, 0.0000,  ..., 0.0499, 0.0668, 0.0172]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2198134., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1409.9930, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(55198.5391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2615.3828, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2639.0215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1024.2266, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1391],
        [ 0.1424],
        [ 0.1439],
        ...,
        [-0.5703],
        [-0.5685],
        [-0.5680]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-262483.7188, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 60.0 event: 900 loss: tensor(567.6631, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(752.4667, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(752.4667, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2162e-03,  3.1353e-03, -1.8537e-03,  ..., -5.2189e-05,
         -4.8408e-03, -4.1120e-03],
        [-6.2162e-03,  3.1353e-03, -1.8537e-03,  ..., -5.2189e-05,
         -4.8408e-03, -4.1120e-03],
        [-5.9850e-03,  2.2753e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1152e-03],
        ...,
        [-5.9850e-03,  2.2753e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1152e-03],
        [-5.9850e-03,  2.2753e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1152e-03],
        [-5.9850e-03,  2.2753e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1152e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-27606.4121, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-612.8751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.9107, device='cuda:0')



h[100].sum tensor(-56.3361, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(104.7613, device='cuda:0')



h[200].sum tensor(-555.5051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.3705, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(256900.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0405, 0.2030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0420, 0.1825, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0442, 0.1531, 0.0000],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0497, 0.0662, 0.0200],
        [0.0000, 0.0009, 0.0000,  ..., 0.0497, 0.0662, 0.0200],
        [0.0000, 0.0009, 0.0000,  ..., 0.0497, 0.0662, 0.0200]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2050308., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1341.1724, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(56990.0312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2536.9043, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2538.6265, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(986.3895, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0961],
        [ 0.0474],
        [-0.0378],
        ...,
        [-0.6058],
        [-0.6038],
        [-0.6032]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-347784.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(582.9711, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3350],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(582.9711, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2062e-03,  3.0755e-03, -1.7192e-03,  ..., -3.8132e-05,
         -4.5418e-03, -4.2167e-03],
        [-6.5218e-03,  4.2458e-03, -4.1947e-03,  ..., -9.3038e-05,
         -1.1081e-02, -1.4818e-03],
        [-6.2752e-03,  3.3315e-03, -2.2608e-03,  ..., -5.0144e-05,
         -5.9725e-03, -3.6184e-03],
        ...,
        [-5.9869e-03,  2.2627e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1160e-03],
        [-5.9869e-03,  2.2627e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1160e-03],
        [-5.9869e-03,  2.2627e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1160e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-28389.2578, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-612.4633, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.0469, device='cuda:0')



h[100].sum tensor(-43.4878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(81.1634, device='cuda:0')



h[200].sum tensor(-555.8444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.5022, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0148, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0149, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(246511.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0390, 0.2230, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0386, 0.2309, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0362, 0.2652, 0.0000],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0502, 0.0663, 0.0192],
        [0.0000, 0.0007, 0.0000,  ..., 0.0502, 0.0663, 0.0192],
        [0.0000, 0.0007, 0.0000,  ..., 0.0502, 0.0663, 0.0192]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2015936.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1306.0256, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(56729.4062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2416.3137, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2840.5049, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(906.1495, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1801],
        [-0.0951],
        [-0.0474],
        ...,
        [-0.6179],
        [-0.6159],
        [-0.6154]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-327725., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(612.3512, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2952],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(612.3512, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.9887e-03,  2.2079e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1168e-03],
        [-6.4989e-03,  4.0906e-03, -3.9118e-03,  ..., -6.4330e-05,
         -1.0456e-02, -1.6979e-03],
        [-6.7496e-03,  5.0156e-03, -5.8337e-03,  ..., -9.5936e-05,
         -1.5593e-02,  4.7315e-04],
        ...,
        [-5.9887e-03,  2.2079e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1168e-03],
        [-5.9887e-03,  2.2079e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1168e-03],
        [-5.9887e-03,  2.2079e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1168e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-28247.0957, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-612.7607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.4100, device='cuda:0')



h[100].sum tensor(-45.4085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(85.2539, device='cuda:0')



h[200].sum tensor(-557.0629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.0394, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0141, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0005],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(252193.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0388, 0.2171, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0351, 0.2372, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0337, 0.2587, 0.0000],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0508, 0.0666, 0.0174],
        [0.0000, 0.0008, 0.0000,  ..., 0.0508, 0.0666, 0.0174],
        [0.0000, 0.0008, 0.0000,  ..., 0.0508, 0.0666, 0.0174]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2054355.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1389.7058, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(55210.0352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2340.9565, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3276.5952, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(835.0055, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1021],
        [ 0.1398],
        [ 0.1547],
        ...,
        [-0.6149],
        [-0.6135],
        [-0.6131]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-260722.9688, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(711.1062, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3845],
        [0.4937],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(711.1062, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.7976e-03,  5.1613e-03, -6.0510e-03,  ..., -6.6279e-05,
         -1.6366e-02,  8.7320e-04],
        [-6.3529e-03,  3.5278e-03, -2.7183e-03,  ..., -2.9775e-05,
         -7.3523e-03, -2.9770e-03],
        [-6.4559e-03,  3.9059e-03, -3.4898e-03,  ..., -3.8225e-05,
         -9.4389e-03, -2.0857e-03],
        ...,
        [-5.9902e-03,  2.1954e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1174e-03],
        [-5.9902e-03,  2.1954e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1174e-03],
        [-5.9902e-03,  2.1954e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1174e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-27818.5195, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-613.2661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.9917, device='cuda:0')



h[100].sum tensor(-51.7171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(99.0029, device='cuda:0')



h[200].sum tensor(-558.4907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.2064, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.7317e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.3799e-05],
        [0.0000e+00, 1.7339e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         8.7320e-04],
        [0.0000e+00, 1.1511e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 8.7816e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.7816e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.7816e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(257577.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0083, 0.4544, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0158, 0.3874, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0319, 0.2815, 0.0000],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0517, 0.0662, 0.0173],
        [0.0000, 0.0008, 0.0000,  ..., 0.0517, 0.0662, 0.0173],
        [0.0000, 0.0008, 0.0000,  ..., 0.0517, 0.0662, 0.0173]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2069821.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1521.6780, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(55178.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2261.3809, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3708.4893, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(743.4423, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1591],
        [ 0.1569],
        [ 0.1530],
        ...,
        [-0.6225],
        [-0.6212],
        [-0.6209]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-237554.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(700.8453, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(700.8453, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.9917e-03,  2.2623e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1175e-03],
        [-5.9917e-03,  2.2623e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1175e-03],
        [-6.6045e-03,  4.5067e-03, -4.4888e-03,  ..., -2.5623e-05,
         -1.2287e-02, -8.1287e-04],
        ...,
        [-5.9917e-03,  2.2623e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1175e-03],
        [-5.9917e-03,  2.2623e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1175e-03],
        [-5.9917e-03,  2.2623e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1175e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-27937.4531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-613.3777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.5157, device='cuda:0')



h[100].sum tensor(-50.4885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(97.5743, device='cuda:0')



h[200].sum tensor(-559.3147, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.6696, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0165, 0.0000,  ..., 0.0000, 0.0000, 0.0053],
        ...,
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258317.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 3.7942e-04, 0.0000e+00,  ..., 4.7189e-02, 1.0556e-01,
         7.7805e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.0595e-02, 2.0254e-01,
         1.6807e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.4234e-02, 3.6381e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 2.6196e-04, 0.0000e+00,  ..., 5.2798e-02, 6.5473e-02,
         1.9369e-02],
        [0.0000e+00, 2.6196e-04, 0.0000e+00,  ..., 5.2798e-02, 6.5473e-02,
         1.9369e-02],
        [0.0000e+00, 2.6196e-04, 0.0000e+00,  ..., 5.2798e-02, 6.5473e-02,
         1.9369e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2078535.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1561.0737, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(55708.2734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2198.1399, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4844.3730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(717.9214, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0546],
        [ 0.0038],
        [ 0.0576],
        ...,
        [-0.6639],
        [-0.6623],
        [-0.6618]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-300886.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1010.0837, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2603],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1010.0837, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.9917e-03,  2.2623e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1175e-03],
        [-6.2372e-03,  3.1618e-03, -1.7989e-03,  ..., -1.0269e-05,
         -4.9242e-03, -3.9916e-03],
        [-5.9917e-03,  2.2623e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1175e-03],
        ...,
        [-5.9917e-03,  2.2623e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1175e-03],
        [-5.9917e-03,  2.2623e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1175e-03],
        [-5.9917e-03,  2.2623e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1175e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-26560.4922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(46.8628, device='cuda:0')



h[100].sum tensor(-72.3985, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(140.6277, device='cuda:0')



h[200].sum tensor(-560.8484, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(52.8495, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0149, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0147, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        ...,
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(280472., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.1465e-02, 3.1619e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.5174e-02, 2.8671e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0960e-02, 3.7027e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 2.6196e-04, 0.0000e+00,  ..., 5.2798e-02, 6.5473e-02,
         1.9369e-02],
        [0.0000e+00, 2.6196e-04, 0.0000e+00,  ..., 5.2798e-02, 6.5473e-02,
         1.9369e-02],
        [0.0000e+00, 2.6196e-04, 0.0000e+00,  ..., 5.2798e-02, 6.5473e-02,
         1.9369e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2176405., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1694.7395, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(54310., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2299.9209, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5735.2471, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(758.2817, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0792],
        [ 0.0772],
        [ 0.0760],
        ...,
        [-0.6639],
        [-0.6623],
        [-0.6618]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-271108.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(743.8275, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(743.8275, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-5.9929e-03,  2.2962e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1176e-03],
        [-5.9929e-03,  2.2962e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1176e-03],
        [-6.2824e-03,  3.3523e-03, -2.0720e-03,  ..., -1.4760e-06,
         -5.7410e-03, -3.6125e-03],
        ...,
        [-5.9929e-03,  2.2962e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1176e-03],
        [-5.9929e-03,  2.2962e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1176e-03],
        [-5.9929e-03,  2.2962e-03,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00, -6.1176e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-27813.9688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-613.6614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.5098, device='cuda:0')



h[100].sum tensor(-52.8492, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(103.5585, device='cuda:0')



h[200].sum tensor(-560.3106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.9185, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(261087.7656, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0518, 0.0968, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0497, 0.1251, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0470, 0.1679, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0534, 0.0648, 0.0215],
        [0.0000, 0.0000, 0.0000,  ..., 0.0534, 0.0648, 0.0215],
        [0.0000, 0.0000, 0.0000,  ..., 0.0534, 0.0648, 0.0215]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2094306.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1642.8944, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(56160.5664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2183.3408, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5759.0635, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(696.8593, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1879],
        [-0.0834],
        [ 0.0025],
        ...,
        [-0.6960],
        [-0.6940],
        [-0.6934]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-347018.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(718.2053, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(718.2053, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0000,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-27960.4590, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-613.7020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.3211, device='cuda:0')



h[100].sum tensor(-50.7548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(99.9913, device='cuda:0')



h[200].sum tensor(-560.9182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.5779, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(257918.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.0466e-04, 0.0000e+00,  ..., 5.2716e-02, 6.6898e-02,
         2.1329e-02],
        [0.0000e+00, 2.7127e-04, 0.0000e+00,  ..., 5.2830e-02, 6.5635e-02,
         2.2061e-02],
        [0.0000e+00, 4.9634e-04, 0.0000e+00,  ..., 5.2851e-02, 6.8516e-02,
         2.0903e-02],
        ...,
        [0.0000e+00, 8.7570e-05, 0.0000e+00,  ..., 5.2600e-02, 6.4065e-02,
         2.2432e-02],
        [0.0000e+00, 8.7570e-05, 0.0000e+00,  ..., 5.2600e-02, 6.4065e-02,
         2.2432e-02],
        [0.0000e+00, 8.7571e-05, 0.0000e+00,  ..., 5.2600e-02, 6.4065e-02,
         2.2432e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2073318.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1607.0326, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(57728.6953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2088.6511, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4547.4248, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(723.4800, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5293],
        [-0.6012],
        [-0.6242],
        ...,
        [-0.7023],
        [-0.7007],
        [-0.7003]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-329957.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(787.2802, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(787.2802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0022,  0.0000,  ...,  0.0008,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0008,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0008,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0022,  0.0000,  ...,  0.0008,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0008,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ...,  0.0008,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-27665.9023, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.0364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.5258, device='cuda:0')



h[100].sum tensor(-54.4527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(109.6081, device='cuda:0')



h[200].sum tensor(-561.8773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.1920, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0087, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0087, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0032, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0032, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(266844.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0002, 0.0000,  ..., 0.0499, 0.0849, 0.0128],
        [0.0000, 0.0002, 0.0000,  ..., 0.0500, 0.0820, 0.0142],
        [0.0000, 0.0005, 0.0000,  ..., 0.0498, 0.0836, 0.0131],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0505, 0.0632, 0.0229],
        [0.0000, 0.0003, 0.0000,  ..., 0.0505, 0.0632, 0.0229],
        [0.0000, 0.0003, 0.0000,  ..., 0.0505, 0.0632, 0.0229]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2113853.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1664.9454, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(58914.3828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2018.4304, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3578.4465, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(742.2067, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1481],
        [-0.1642],
        [-0.1476],
        ...,
        [-0.7052],
        [-0.7032],
        [-0.7027]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-293398.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(639.0419, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5508],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(639.0419, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0021,  0.0000,  ...,  0.0002,  0.0000, -0.0061],
        [-0.0065,  0.0040, -0.0035,  ...,  0.0002, -0.0100, -0.0016],
        [-0.0064,  0.0038, -0.0030,  ...,  0.0002, -0.0087, -0.0022],
        ...,
        [-0.0060,  0.0021,  0.0000,  ...,  0.0002,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ...,  0.0002,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ...,  0.0002,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-28612.6992, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-613.6061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.6483, device='cuda:0')



h[100].sum tensor(-43.9763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(88.9698, device='cuda:0')



h[200].sum tensor(-561.7416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.4359, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0104, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0117, 0.0000,  ..., 0.0007, 0.0000, 0.0000],
        [0.0000, 0.0184, 0.0000,  ..., 0.0008, 0.0000, 0.0033],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0006, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0006, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(253294.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0432, 0.1417, 0.0042],
        [0.0000, 0.0000, 0.0000,  ..., 0.0347, 0.2051, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.3182, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0514, 0.0630, 0.0250],
        [0.0000, 0.0000, 0.0000,  ..., 0.0514, 0.0630, 0.0250],
        [0.0000, 0.0000, 0.0000,  ..., 0.0514, 0.0630, 0.0250]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2056609., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1537.5859, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(61011.2422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2048.8801, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3265.8264, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(782.3632, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0057],
        [ 0.0705],
        [ 0.1256],
        ...,
        [-0.7346],
        [-0.7326],
        [-0.7320]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-329308.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 70.0 event: 1050 loss: tensor(488.0063, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(738.7397, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(738.7397, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0021,  0.0000,  ..., -0.0006,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0006,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0006,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0021,  0.0000,  ..., -0.0006,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0006,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0006,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-28420.6992, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.0629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.2738, device='cuda:0')



h[100].sum tensor(-50.2622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(102.8501, device='cuda:0')



h[200].sum tensor(-562.7792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.6523, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(261080.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0520, 0.0656, 0.0261],
        [0.0000, 0.0000, 0.0000,  ..., 0.0522, 0.0631, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0523, 0.0634, 0.0275],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0519, 0.0626, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.0519, 0.0626, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.0519, 0.0626, 0.0273]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2108342., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1568.8894, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(61915.7695, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2147.9089, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4079.9873, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(818.7651, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6269],
        [-0.7595],
        [-0.8452],
        ...,
        [-0.7624],
        [-0.7599],
        [-0.7591]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-368400.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1028.7395, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5215],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1028.7395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0073,  0.0067, -0.0082,  ..., -0.0016, -0.0243,  0.0051],
        [-0.0069,  0.0054, -0.0059,  ..., -0.0015, -0.0175,  0.0019],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0013,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0021,  0.0000,  ..., -0.0013,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0013,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0013,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-27194.7949, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(47.7283, device='cuda:0')



h[100].sum tensor(-69.8083, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(143.2250, device='cuda:0')



h[200].sum tensor(-564.7780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(53.8256, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0259, 0.0000,  ..., 0.0000, 0.0000, 0.0179],
        [0.0000, 0.0169, 0.0000,  ..., 0.0000, 0.0000, 0.0052],
        [0.0000, 0.0131, 0.0000,  ..., 0.0000, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(285084.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5898, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.3963, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0266, 0.2444, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0522, 0.0626, 0.0285],
        [0.0000, 0.0000, 0.0000,  ..., 0.0522, 0.0626, 0.0285],
        [0.0000, 0.0000, 0.0000,  ..., 0.0522, 0.0626, 0.0285]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2218839.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1632.6494, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(61794.4844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2283.5903, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5252.3745, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(893.2361, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0488],
        [ 0.0082],
        [-0.1203],
        ...,
        [-0.7817],
        [-0.7791],
        [-0.7783]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-367700.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(708.4672, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5762],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(708.4672, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0063,  0.0030, -0.0017,  ..., -0.0020, -0.0051, -0.0037],
        [-0.0065,  0.0040, -0.0034,  ..., -0.0021, -0.0101, -0.0014],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0019,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0021,  0.0000,  ..., -0.0019,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0019,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0019,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-28866.3477, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.1240, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.8693, device='cuda:0')



h[100].sum tensor(-47.4533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(98.6355, device='cuda:0')



h[200].sum tensor(-563.5753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.0684, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0152, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258938.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0243, 0.3057, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0360, 0.2292, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0386, 0.2230, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0522, 0.0632, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0522, 0.0632, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0522, 0.0632, 0.0278]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2102803., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1499.0508, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(63262.8633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2202.9517, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3499.5461, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(840.4496, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1343],
        [ 0.1142],
        [ 0.0779],
        ...,
        [-0.7869],
        [-0.7843],
        [-0.7835]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-328145.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(756.4738, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(756.4738, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0069,  0.0050, -0.0052,  ..., -0.0030, -0.0158,  0.0013],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0025,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0025,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0020,  0.0000,  ..., -0.0025,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0025,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0025,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-28758.1211, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3619, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.0966, device='cuda:0')



h[100].sum tensor(-49.9712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(105.3191, device='cuda:0')



h[200].sum tensor(-564.2090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.5802, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0193, 0.0000,  ..., 0.0000, 0.0000, 0.0091],
        [0.0000, 0.0153, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(261396.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5441, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.4425, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.3764, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0521, 0.0632, 0.0283],
        [0.0000, 0.0000, 0.0000,  ..., 0.0521, 0.0632, 0.0283],
        [0.0000, 0.0000, 0.0000,  ..., 0.0521, 0.0632, 0.0283]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2111999.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1461.1613, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(64105.8594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2219.7690, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2729.5884, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(860.0107, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1277],
        [ 0.1337],
        [ 0.1377],
        ...,
        [-0.7988],
        [-0.7961],
        [-0.7953]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-319551.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(657.7744, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6680],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(657.7744, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064,  0.0036, -0.0026,  ..., -0.0033, -0.0081, -0.0023],
        [-0.0068,  0.0049, -0.0049,  ..., -0.0036, -0.0151,  0.0011],
        [-0.0062,  0.0028, -0.0012,  ..., -0.0031, -0.0038, -0.0043],
        ...,
        [-0.0060,  0.0020,  0.0000,  ..., -0.0030,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0030,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0030,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-29451.7969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.0490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.5174, device='cuda:0')



h[100].sum tensor(-42.8229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.5778, device='cuda:0')



h[200].sum tensor(-564.0566, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.4160, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0200, 0.0000,  ..., 0.0000, 0.0000, 0.0048],
        [0.0000, 0.0146, 0.0000,  ..., 0.0000, 0.0000, 0.0013],
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(255046.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.3944, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.3504, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0272, 0.3110, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0522, 0.0629, 0.0302],
        [0.0000, 0.0000, 0.0000,  ..., 0.0522, 0.0629, 0.0302],
        [0.0000, 0.0000, 0.0000,  ..., 0.0522, 0.0629, 0.0302]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2098411., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1390.4417, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(65748.4922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2219.9883, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2165.4727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(863.1771, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1671],
        [ 0.1676],
        [ 0.1642],
        ...,
        [-0.8216],
        [-0.8188],
        [-0.8180]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-354577.9062, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(736.8851, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(736.8851, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0021,  0.0000,  ..., -0.0035,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0035,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0035,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0021,  0.0000,  ..., -0.0035,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0035,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0035,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-29274.5859, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.1878, device='cuda:0')



h[100].sum tensor(-47.2896, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(102.5919, device='cuda:0')



h[200].sum tensor(-564.7751, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.5552, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(262491.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0523, 0.0718, 0.0280],
        [0.0000, 0.0000, 0.0000,  ..., 0.0521, 0.0743, 0.0267],
        [0.0000, 0.0000, 0.0000,  ..., 0.0522, 0.0767, 0.0256],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0525, 0.0627, 0.0325],
        [0.0000, 0.0000, 0.0000,  ..., 0.0525, 0.0627, 0.0325],
        [0.0000, 0.0000, 0.0000,  ..., 0.0525, 0.0627, 0.0325]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2155842.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1409.1031, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(66127.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2284.5930, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2701.5020, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(892.6292, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6080],
        [-0.5427],
        [-0.4845],
        ...,
        [-0.8518],
        [-0.8489],
        [-0.8480]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-412467., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(840.4587, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6602],
        [0.2854],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(840.4587, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0065,  0.0037, -0.0028,  ..., -0.0038, -0.0086, -0.0020],
        [-0.0068,  0.0050, -0.0048,  ..., -0.0041, -0.0150,  0.0011],
        [-0.0063,  0.0030, -0.0016,  ..., -0.0037, -0.0048, -0.0038],
        ...,
        [-0.0060,  0.0021,  0.0000,  ..., -0.0035,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0035,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0035,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-28723.1641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.9930, device='cuda:0')



h[100].sum tensor(-54.3066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(117.0118, device='cuda:0')



h[200].sum tensor(-565.3291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.9744, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0215, 0.0000,  ..., 0.0000, 0.0000, 0.0091],
        [0.0000, 0.0174, 0.0000,  ..., 0.0000, 0.0000, 0.0050],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(269514.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4528, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0082, 0.3926, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0226, 0.2843, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0525, 0.0627, 0.0325],
        [0.0000, 0.0000, 0.0000,  ..., 0.0525, 0.0627, 0.0325],
        [0.0000, 0.0000, 0.0000,  ..., 0.0525, 0.0627, 0.0325]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2186645.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1471.6577, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(65567.0156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2323.4126, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2974.4495, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(896.9341, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1172],
        [ 0.1039],
        [ 0.0621],
        ...,
        [-0.8518],
        [-0.8489],
        [-0.8480]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-374852.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(619.7634, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(619.7634, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0021,  0.0000,  ..., -0.0039,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0039,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0039,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0021,  0.0000,  ..., -0.0039,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0039,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0039,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-30028.0762, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.0148, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.7539, device='cuda:0')



h[100].sum tensor(-39.4780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(86.2858, device='cuda:0')



h[200].sum tensor(-564.4908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.4272, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(250215.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0530, 0.0632, 0.0345],
        [0.0000, 0.0000, 0.0000,  ..., 0.0530, 0.0632, 0.0345],
        [0.0000, 0.0000, 0.0000,  ..., 0.0532, 0.0634, 0.0346],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0528, 0.0626, 0.0344],
        [0.0000, 0.0000, 0.0000,  ..., 0.0528, 0.0626, 0.0344],
        [0.0000, 0.0000, 0.0000,  ..., 0.0528, 0.0626, 0.0344]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2104968., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1337.2871, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(67558.4297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2262.5676, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2195.1040, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(867.8766, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1014],
        [-1.1076],
        [-1.1080],
        ...,
        [-0.8796],
        [-0.8766],
        [-0.8758]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-459474., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(706.6313, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(706.6313, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062,  0.0028, -0.0012,  ..., -0.0045, -0.0037, -0.0043],
        [-0.0062,  0.0028, -0.0012,  ..., -0.0045, -0.0037, -0.0043],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0043,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0021,  0.0000,  ..., -0.0043,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0043,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0043,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-29552.2930, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3747, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.7841, device='cuda:0')



h[100].sum tensor(-44.3870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(98.3799, device='cuda:0')



h[200].sum tensor(-565.1932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.9723, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(262293.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0450, 0.1891, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0461, 0.1749, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0482, 0.1438, 0.0055],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0527, 0.0634, 0.0325],
        [0.0000, 0.0000, 0.0000,  ..., 0.0527, 0.0634, 0.0325],
        [0.0000, 0.0000, 0.0000,  ..., 0.0527, 0.0634, 0.0325]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2174242., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1364.6256, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(66232.4844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2240.3914, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2109.9919, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(855.7956, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0224],
        [-0.0133],
        [-0.1463],
        ...,
        [-0.8828],
        [-0.8799],
        [-0.8790]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-348312.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1078.0719, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1078.0719, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0020,  0.0000,  ..., -0.0046,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0046,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0046,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0020,  0.0000,  ..., -0.0046,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0046,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0046,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-27523.7793, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.8354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(50.0171, device='cuda:0')



h[100].sum tensor(-67.7906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(150.0932, device='cuda:0')



h[200].sum tensor(-567.3925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(56.4068, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(296477.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0525, 0.0772, 0.0232],
        [0.0000, 0.0000, 0.0000,  ..., 0.0533, 0.0674, 0.0292],
        [0.0000, 0.0000, 0.0000,  ..., 0.0531, 0.0740, 0.0257],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0532, 0.0643, 0.0306],
        [0.0000, 0.0000, 0.0000,  ..., 0.0532, 0.0643, 0.0306],
        [0.0000, 0.0000, 0.0000,  ..., 0.0532, 0.0643, 0.0306]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2334300.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1512.7916, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(63687.6211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2298.8296, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3475.9570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(889.0461, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3100],
        [-0.4865],
        [-0.5755],
        ...,
        [-0.8897],
        [-0.8870],
        [-0.8862]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-277754.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 80.0 event: 1200 loss: tensor(479.9787, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(737.7015, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4673],
        [0.4421],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(737.7015, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064,  0.0036, -0.0023,  ..., -0.0054, -0.0076, -0.0023],
        [-0.0068,  0.0048, -0.0041,  ..., -0.0058, -0.0134,  0.0006],
        [-0.0074,  0.0070, -0.0074,  ..., -0.0065, -0.0243,  0.0061],
        ...,
        [-0.0060,  0.0021,  0.0000,  ..., -0.0049,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0049,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0049,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-29587.1465, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.2256, device='cuda:0')



h[100].sum tensor(-45.2927, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(102.7056, device='cuda:0')



h[200].sum tensor(-565.8167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.5980, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0006],
        [0.0000, 0.0201, 0.0000,  ..., 0.0000, 0.0000, 0.0075],
        [0.0000, 0.0228, 0.0000,  ..., 0.0000, 0.0000, 0.0114],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(263365.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0229, 0.2943, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.4404, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5443, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0538, 0.0648, 0.0321],
        [0.0000, 0.0000, 0.0000,  ..., 0.0538, 0.0648, 0.0321],
        [0.0000, 0.0000, 0.0000,  ..., 0.0538, 0.0648, 0.0321]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2185314.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1393.1730, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(64626.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2229.9683, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3311.0955, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(813.0325, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1037],
        [ 0.1283],
        [ 0.1336],
        ...,
        [-0.9282],
        [-0.9252],
        [-0.9243]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-357369.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(974.2247, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(974.2247, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0022,  0.0000,  ..., -0.0052,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ..., -0.0052,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ..., -0.0052,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0022,  0.0000,  ..., -0.0052,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ..., -0.0052,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ..., -0.0052,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-28508.7852, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.4911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(45.1991, device='cuda:0')



h[100].sum tensor(-59.4144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(135.6353, device='cuda:0')



h[200].sum tensor(-567.2432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(50.9733, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(279623.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0542, 0.0650, 0.0344],
        [0.0000, 0.0000, 0.0000,  ..., 0.0542, 0.0650, 0.0344],
        [0.0000, 0.0000, 0.0000,  ..., 0.0543, 0.0653, 0.0344],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0539, 0.0645, 0.0343],
        [0.0000, 0.0000, 0.0000,  ..., 0.0539, 0.0645, 0.0343],
        [0.0000, 0.0000, 0.0000,  ..., 0.0535, 0.0716, 0.0303]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2270630.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1457.4834, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(64075.0312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2309.6201, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4317.8330, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(853.2200, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9278],
        [-1.0408],
        [-1.1162],
        ...,
        [-0.9537],
        [-0.9187],
        [-0.8398]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-394629.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(691.6102, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(691.6102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0022,  0.0000,  ..., -0.0055,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ..., -0.0055,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ..., -0.0055,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0022,  0.0000,  ..., -0.0055,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ..., -0.0055,  0.0000, -0.0061],
        [-0.0060,  0.0022,  0.0000,  ..., -0.0055,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-30112.1641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.0872, device='cuda:0')



h[100].sum tensor(-41.6399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.2886, device='cuda:0')



h[200].sum tensor(-565.9770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.1864, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(256469.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0530, 0.0760, 0.0291],
        [0.0000, 0.0000, 0.0000,  ..., 0.0538, 0.0666, 0.0347],
        [0.0000, 0.0000, 0.0000,  ..., 0.0541, 0.0647, 0.0359],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0537, 0.0639, 0.0358],
        [0.0000, 0.0000, 0.0000,  ..., 0.0537, 0.0639, 0.0358],
        [0.0000, 0.0000, 0.0000,  ..., 0.0537, 0.0639, 0.0358]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2176767.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1365.7847, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(66419.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2234.0161, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2809.6580, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(779.4927, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5320],
        [-0.7495],
        [-0.9124],
        ...,
        [-0.9810],
        [-0.9777],
        [-0.9767]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-404947.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(688.0817, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(688.0817, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0021,  0.0000,  ..., -0.0057,  0.0000, -0.0061],
        [-0.0065,  0.0036, -0.0022,  ..., -0.0063, -0.0075, -0.0022],
        [-0.0069,  0.0050, -0.0042,  ..., -0.0068, -0.0143,  0.0013],
        ...,
        [-0.0060,  0.0021,  0.0000,  ..., -0.0057,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0057,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0057,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-30198.5566, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.9235, device='cuda:0')



h[100].sum tensor(-40.8777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(95.7973, device='cuda:0')



h[200].sum tensor(-566.1176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.0017, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0013],
        [0.0000, 0.0215, 0.0000,  ..., 0.0000, 0.0000, 0.0111],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(257098.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0374, 0.1631, 0.0102],
        [0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.2886, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0023, 0.4603, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0531, 0.0631, 0.0363],
        [0.0000, 0.0000, 0.0000,  ..., 0.0531, 0.0631, 0.0363],
        [0.0000, 0.0000, 0.0000,  ..., 0.0531, 0.0631, 0.0363]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2183274., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1357.1201, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(67909.2656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2217.9263, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1721.4858, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(749.3842, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1594],
        [ 0.0672],
        [ 0.2090],
        ...,
        [-0.9612],
        [-0.9246],
        [-0.8972]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-370642.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(705.7184, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2520],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(705.7184, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062,  0.0029, -0.0011,  ..., -0.0062, -0.0038, -0.0041],
        [-0.0062,  0.0029, -0.0011,  ..., -0.0063, -0.0039, -0.0041],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0059,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0021,  0.0000,  ..., -0.0059,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0059,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0059,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-30134.7695, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5834, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.7418, device='cuda:0')



h[100].sum tensor(-41.6309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(98.2528, device='cuda:0')



h[200].sum tensor(-566.3729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.9245, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0139, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(260571.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0398, 0.2434, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0435, 0.1947, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0483, 0.1413, 0.0069],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0531, 0.0630, 0.0378],
        [0.0000, 0.0000, 0.0000,  ..., 0.0531, 0.0630, 0.0378],
        [0.0000, 0.0000, 0.0000,  ..., 0.0531, 0.0630, 0.0378]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2192365., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1267.6631, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(69095.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2353.8826, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1914.5947, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(827.3380, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1314],
        [ 0.1164],
        [ 0.0808],
        ...,
        [-1.0222],
        [-1.0184],
        [-1.0171]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-426550.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(682.1505, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(682.1505, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0021,  0.0000,  ..., -0.0061,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0061,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0061,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0021,  0.0000,  ..., -0.0061,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0061,  0.0000, -0.0061],
        [-0.0060,  0.0021,  0.0000,  ..., -0.0061,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-30283.3555, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.6483, device='cuda:0')



h[100].sum tensor(-39.5276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(94.9716, device='cuda:0')



h[200].sum tensor(-566.3661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.6914, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258501.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0465, 0.1367, 0.0139],
        [0.0000, 0.0000, 0.0000,  ..., 0.0513, 0.0914, 0.0228],
        [0.0000, 0.0000, 0.0000,  ..., 0.0522, 0.0837, 0.0275],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0530, 0.0631, 0.0388],
        [0.0000, 0.0000, 0.0000,  ..., 0.0530, 0.0631, 0.0388],
        [0.0000, 0.0000, 0.0000,  ..., 0.0530, 0.0631, 0.0388]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2165036., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1149.4961, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(70649.3828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2469.0027, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1813.7961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(891.1133, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1967],
        [-0.3767],
        [-0.4724],
        ...,
        [-1.0466],
        [-1.0428],
        [-1.0416]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-472464.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(839.8027, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2754],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(839.8027, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062,  0.0027, -0.0010,  ..., -0.0066, -0.0034, -0.0043],
        [-0.0063,  0.0029, -0.0012,  ..., -0.0067, -0.0042, -0.0039],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0063,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0020,  0.0000,  ..., -0.0063,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0063,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0063,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-29464.3633, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.9626, device='cuda:0')



h[100].sum tensor(-48.3957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(116.9205, device='cuda:0')



h[200].sum tensor(-567.3179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.9401, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(278693.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.9999e-02, 2.5351e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.3313e-02, 2.0604e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.5532e-02, 1.7227e-01,
         1.7026e-04],
        ...,
        [0.0000e+00, 1.6365e-04, 0.0000e+00,  ..., 5.2625e-02, 6.2857e-02,
         3.8687e-02],
        [0.0000e+00, 1.6365e-04, 0.0000e+00,  ..., 5.2625e-02, 6.2857e-02,
         3.8687e-02],
        [0.0000e+00, 1.6365e-04, 0.0000e+00,  ..., 5.2625e-02, 6.2857e-02,
         3.8687e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2277232.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1248.7198, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(70339., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2513.2483, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2080.9456, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(891.6666, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1163],
        [ 0.0991],
        [ 0.0600],
        ...,
        [-1.0597],
        [-1.0559],
        [-1.0548]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-420510.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(795.9210, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(795.9210, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0019,  0.0000,  ..., -0.0065,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0065,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0065,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0019,  0.0000,  ..., -0.0065,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0065,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0065,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-29811.6836, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.9267, device='cuda:0')



h[100].sum tensor(-45.2206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(110.8111, device='cuda:0')



h[200].sum tensor(-567.1942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.6441, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(272280.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0523, 0.0628, 0.0393],
        [0.0000, 0.0017, 0.0000,  ..., 0.0523, 0.0628, 0.0393],
        [0.0000, 0.0018, 0.0000,  ..., 0.0524, 0.0631, 0.0393],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0520, 0.0623, 0.0392],
        [0.0000, 0.0015, 0.0000,  ..., 0.0520, 0.0623, 0.0392],
        [0.0000, 0.0015, 0.0000,  ..., 0.0520, 0.0623, 0.0392]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2247889.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1304.9216, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(71904.6797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2459.8447, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1275.8080, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(795.1523, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1571],
        [-1.1913],
        [-1.2048],
        ...,
        [-1.0727],
        [-1.0691],
        [-1.0680]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-333461.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(792.6016, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(792.6016, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0020,  0.0000,  ..., -0.0066,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0066,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0066,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0020,  0.0000,  ..., -0.0066,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0066,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0066,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-30053.9238, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.7727, device='cuda:0')



h[100].sum tensor(-44.2890, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(110.3490, device='cuda:0')



h[200].sum tensor(-567.2587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.4704, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268887.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0516, 0.0616, 0.0435],
        [0.0000, 0.0017, 0.0000,  ..., 0.0516, 0.0616, 0.0435],
        [0.0000, 0.0018, 0.0000,  ..., 0.0517, 0.0619, 0.0436],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0513, 0.0611, 0.0434],
        [0.0000, 0.0015, 0.0000,  ..., 0.0513, 0.0611, 0.0434],
        [0.0000, 0.0015, 0.0000,  ..., 0.0513, 0.0611, 0.0434]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2237085.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1182.4934, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(74251.4531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2413.5732, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1034.2157, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(831.3932, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.2172],
        [-1.1439],
        [-1.0263],
        ...,
        [-1.1126],
        [-1.1088],
        [-1.1076]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-452969.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(642.5922, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4746],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(642.5922, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0068,  0.0045, -0.0032,  ..., -0.0079, -0.0119,  0.0005],
        [-0.0072,  0.0060, -0.0050,  ..., -0.0086, -0.0187,  0.0043],
        [-0.0074,  0.0066, -0.0057,  ..., -0.0088, -0.0213,  0.0057],
        ...,
        [-0.0060,  0.0020,  0.0000,  ..., -0.0067,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0067,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0067,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31085.5488, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.8130, device='cuda:0')



h[100].sum tensor(-35.6210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.4641, device='cuda:0')



h[200].sum tensor(-566.5986, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.6217, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0201, 0.0000,  ..., 0.0000, 0.0000, 0.0086],
        [0.0000, 0.0206, 0.0000,  ..., 0.0000, 0.0000, 0.0093],
        [0.0000, 0.0222, 0.0000,  ..., 0.0000, 0.0000, 0.0125],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(257670.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5531, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5420, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5153, 0.0000],
        ...,
        [0.0000, 0.0010, 0.0000,  ..., 0.0507, 0.0609, 0.0466],
        [0.0000, 0.0010, 0.0000,  ..., 0.0507, 0.0609, 0.0466],
        [0.0000, 0.0010, 0.0000,  ..., 0.0507, 0.0609, 0.0466]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2197483.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1062.4092, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(76024.8125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2381.8013, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(735.6603, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(852.0334, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0986],
        [ 0.1099],
        [ 0.1083],
        ...,
        [-1.1478],
        [-1.1437],
        [-1.1425]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-568206.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 90.0 event: 1350 loss: tensor(531.4315, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(868.9813, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(868.9813, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0019,  0.0000,  ..., -0.0069,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0069,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0069,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0019,  0.0000,  ..., -0.0069,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0069,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0069,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-29911.6289, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2649, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.3164, device='cuda:0')



h[100].sum tensor(-47.2704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(120.9829, device='cuda:0')



h[200].sum tensor(-567.8074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(45.4668, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(274745.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0513, 0.0646, 0.0449],
        [0.0000, 0.0016, 0.0000,  ..., 0.0513, 0.0665, 0.0440],
        [0.0000, 0.0016, 0.0000,  ..., 0.0510, 0.0750, 0.0393],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0510, 0.0621, 0.0458],
        [0.0000, 0.0014, 0.0000,  ..., 0.0510, 0.0621, 0.0458],
        [0.0000, 0.0014, 0.0000,  ..., 0.0510, 0.0621, 0.0458]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2287424.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1274.9413, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(73793.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2507.4102, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1370.9199, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(836.1224, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.0454],
        [-0.9240],
        [-0.7693],
        ...,
        [-1.1590],
        [-1.1551],
        [-1.1539]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-430336.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(850.4086, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3420],
        [0.3516],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(850.4086, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0069,  0.0049, -0.0035,  ..., -0.0084, -0.0134,  0.0015],
        [-0.0066,  0.0039, -0.0023,  ..., -0.0079, -0.0089, -0.0011],
        [-0.0067,  0.0042, -0.0027,  ..., -0.0080, -0.0104, -0.0002],
        ...,
        [-0.0060,  0.0019,  0.0000,  ..., -0.0070,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0070,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0070,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-30123.3164, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.4547, device='cuda:0')



h[100].sum tensor(-45.5740, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(118.3971, device='cuda:0')



h[200].sum tensor(-567.7755, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(44.4950, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0159, 0.0000,  ..., 0.0000, 0.0000, 0.0019],
        [0.0000, 0.0171, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0162, 0.0000,  ..., 0.0000, 0.0000, 0.0003],
        ...,
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(275776.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.4051, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0092, 0.4025, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.3787, 0.0000],
        ...,
        [0.0000, 0.0012, 0.0000,  ..., 0.0512, 0.0634, 0.0458],
        [0.0000, 0.0012, 0.0000,  ..., 0.0512, 0.0634, 0.0458],
        [0.0000, 0.0012, 0.0000,  ..., 0.0512, 0.0634, 0.0458]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2296745.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1287.7622, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(73371.0703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2527.7690, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1422.3599, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(836.9613, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2626],
        [ 0.2653],
        [ 0.2704],
        ...,
        [-1.1772],
        [-1.1732],
        [-1.1720]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-405925.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(666.9403, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3867],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(666.9403, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0063,  0.0029, -0.0011,  ..., -0.0075, -0.0042, -0.0037],
        [-0.0067,  0.0041, -0.0025,  ..., -0.0081, -0.0097, -0.0005],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0071,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0020,  0.0000,  ..., -0.0071,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0071,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0071,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31306.8359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.9427, device='cuda:0')



h[100].sum tensor(-35.4968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.8540, device='cuda:0')



h[200].sum tensor(-566.9319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.8956, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0178, 0.0000,  ..., 0.0000, 0.0000, 0.0037],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0003],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0003],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(260892.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.4288, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0211, 0.3381, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0283, 0.2811, 0.0000],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0512, 0.0645, 0.0476],
        [0.0000, 0.0005, 0.0000,  ..., 0.0512, 0.0645, 0.0476],
        [0.0000, 0.0005, 0.0000,  ..., 0.0512, 0.0645, 0.0476]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2233079.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1205.1572, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(73951.7500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2507.9670, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(986.5977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(835.6829, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2265],
        [ 0.1935],
        [ 0.0835],
        ...,
        [-1.2092],
        [-1.2059],
        [-1.2048]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-458474.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(671.6265, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(671.6265, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0020,  0.0000,  ..., -0.0071,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0071,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0071,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0020,  0.0000,  ..., -0.0071,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0071,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0071,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31385.9648, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.1601, device='cuda:0')



h[100].sum tensor(-35.5559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.5064, device='cuda:0')



h[200].sum tensor(-567.0402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.1408, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(261664.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0455, 0.1212, 0.0160],
        [0.0000, 0.0000, 0.0000,  ..., 0.0449, 0.1297, 0.0116],
        [0.0000, 0.0000, 0.0000,  ..., 0.0451, 0.1295, 0.0115],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0510, 0.0652, 0.0492],
        [0.0000, 0.0002, 0.0000,  ..., 0.0510, 0.0652, 0.0492],
        [0.0000, 0.0002, 0.0000,  ..., 0.0510, 0.0652, 0.0492]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2244668.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1199.4882, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(74217.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2514.5811, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1084.7506, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(843.5691, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0572],
        [ 0.0428],
        [ 0.0983],
        ...,
        [-1.2384],
        [-1.2343],
        [-1.2330]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-479188.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(814.8634, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5649],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(814.8634, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064,  0.0032, -0.0014,  ..., -0.0079, -0.0058, -0.0027],
        [-0.0065,  0.0037, -0.0019,  ..., -0.0081, -0.0078, -0.0015],
        [-0.0065,  0.0037, -0.0019,  ..., -0.0081, -0.0077, -0.0016],
        ...,
        [-0.0060,  0.0019,  0.0000,  ..., -0.0072,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0072,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0072,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-30741.6289, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1259, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.8056, device='cuda:0')



h[100].sum tensor(-42.4623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(113.4484, device='cuda:0')



h[200].sum tensor(-567.8140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.6352, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0175, 0.0000,  ..., 0.0000, 0.0000, 0.0050],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(273102.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0339, 0.2537, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0216, 0.3158, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0083, 0.4070, 0.0000],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0508, 0.0654, 0.0501],
        [0.0000, 0.0006, 0.0000,  ..., 0.0508, 0.0654, 0.0501],
        [0.0000, 0.0006, 0.0000,  ..., 0.0508, 0.0654, 0.0501]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2308816.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1266.4141, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(74274.3594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2522.5554, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1523.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(837.5927, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2324],
        [ 0.2567],
        [ 0.2535],
        ...,
        [-1.2608],
        [-1.2566],
        [-1.2553]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-469190.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(660.7474, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3130],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(660.7474, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0065,  0.0036, -0.0018,  ..., -0.0081, -0.0074, -0.0017],
        [-0.0063,  0.0029, -0.0010,  ..., -0.0078, -0.0043, -0.0036],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0073,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0019,  0.0000,  ..., -0.0073,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0073,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0073,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31827.6055, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.6553, device='cuda:0')



h[100].sum tensor(-33.9191, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.9918, device='cuda:0')



h[200].sum tensor(-567.0681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.5716, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0133, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(260744.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0354, 0.2595, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0399, 0.2029, 0.0044],
        [0.0000, 0.0000, 0.0000,  ..., 0.0456, 0.1374, 0.0178],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0502, 0.0707, 0.0484],
        [0.0000, 0.0000, 0.0000,  ..., 0.0497, 0.0809, 0.0425],
        [0.0000, 0.0000, 0.0000,  ..., 0.0495, 0.0860, 0.0395]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2266090.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1232.8973, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(75708.2109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2484.4570, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(969.6612, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(790.8427, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1846],
        [ 0.0697],
        [-0.1259],
        ...,
        [-1.2222],
        [-1.1358],
        [-1.0775]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-453768.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(634.9830, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(634.9830, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0019,  0.0000,  ..., -0.0074,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0074,  0.0000, -0.0061],
        [-0.0064,  0.0031, -0.0012,  ..., -0.0080, -0.0052, -0.0029],
        ...,
        [-0.0060,  0.0019,  0.0000,  ..., -0.0074,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0074,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0074,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-32154.6953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.4600, device='cuda:0')



h[100].sum tensor(-32.3355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(88.4047, device='cuda:0')



h[200].sum tensor(-566.9962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.2235, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(259525.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0469, 0.1107, 0.0260],
        [0.0000, 0.0000, 0.0000,  ..., 0.0392, 0.1560, 0.0171],
        [0.0000, 0.0000, 0.0000,  ..., 0.0225, 0.2635, 0.0028],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0501, 0.0657, 0.0532],
        [0.0000, 0.0005, 0.0000,  ..., 0.0501, 0.0657, 0.0532],
        [0.0000, 0.0005, 0.0000,  ..., 0.0501, 0.0657, 0.0532]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2279788., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1215.5847, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(76682.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2482.2207, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1128.9333, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(798.6909, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1170],
        [-0.0143],
        [ 0.1109],
        ...,
        [-1.3248],
        [-1.3205],
        [-1.3192]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-519116.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(703.0192, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3762],
        [0.4150],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(703.0192, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0068,  0.0045, -0.0026,  ..., -0.0087, -0.0111,  0.0007],
        [-0.0064,  0.0031, -0.0012,  ..., -0.0080, -0.0050, -0.0030],
        [-0.0064,  0.0032, -0.0013,  ..., -0.0081, -0.0055, -0.0027],
        ...,
        [-0.0060,  0.0020,  0.0000,  ..., -0.0074,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0074,  0.0000, -0.0061],
        [-0.0060,  0.0020,  0.0000,  ..., -0.0074,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31921.5703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.6165, device='cuda:0')



h[100].sum tensor(-35.2026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(97.8770, device='cuda:0')



h[200].sum tensor(-567.3691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.7833, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0141, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0147, 0.0000,  ..., 0.0000, 0.0000, 0.0007],
        [0.0000, 0.0119, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(263440.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0274, 0.2822, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0265, 0.2815, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0324, 0.2461, 0.0000],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0497, 0.0658, 0.0548],
        [0.0000, 0.0003, 0.0000,  ..., 0.0497, 0.0658, 0.0548],
        [0.0000, 0.0003, 0.0000,  ..., 0.0497, 0.0658, 0.0548]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2302415.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1225.4614, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(77668.8047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2505.2295, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1319.4834, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(822.3750, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2562],
        [ 0.2063],
        [ 0.0961],
        ...,
        [-1.3611],
        [-1.3567],
        [-1.3554]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-559861.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(860.3190, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(860.3190, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0019,  0.0000,  ..., -0.0075,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0075,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0075,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0019,  0.0000,  ..., -0.0075,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0075,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0075,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-30925.8984, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.3229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.9145, device='cuda:0')



h[100].sum tensor(-42.6712, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(119.7769, device='cuda:0')



h[200].sum tensor(-568.2196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(45.0135, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(279017.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.3512e-04, 0.0000e+00,  ..., 4.7962e-02, 8.8789e-02,
         3.9523e-02],
        [0.0000e+00, 1.3512e-04, 0.0000e+00,  ..., 4.5706e-02, 1.0890e-01,
         2.8550e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.8198e-02, 1.8855e-01,
         1.0658e-02],
        ...,
        [0.0000e+00, 5.1693e-04, 0.0000e+00,  ..., 4.9436e-02, 6.6199e-02,
         5.3378e-02],
        [0.0000e+00, 5.1693e-04, 0.0000e+00,  ..., 4.9436e-02, 6.6199e-02,
         5.3378e-02],
        [0.0000e+00, 5.1693e-04, 0.0000e+00,  ..., 4.9436e-02, 6.6199e-02,
         5.3378e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2381636.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1325.0184, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(77054.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2541.1875, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2139.0889, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(826.4111, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6768],
        [-0.5288],
        [-0.2259],
        ...,
        [-1.3777],
        [-1.3732],
        [-1.3719]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-440823.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(768.4202, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4836],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(768.4202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0019,  0.0000,  ..., -0.0075,  0.0000, -0.0061],
        [-0.0065,  0.0034, -0.0014,  ..., -0.0083, -0.0063, -0.0021],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0075,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0019,  0.0000,  ..., -0.0075,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0075,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0075,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31489.4336, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.6508, device='cuda:0')



h[100].sum tensor(-37.5785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(106.9824, device='cuda:0')



h[200].sum tensor(-567.7775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.2052, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(271267.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0373, 0.2185, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0428, 0.1610, 0.0035],
        [0.0000, 0.0000, 0.0000,  ..., 0.0387, 0.1856, 0.0009],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0492, 0.0666, 0.0531],
        [0.0000, 0.0004, 0.0000,  ..., 0.0492, 0.0666, 0.0531],
        [0.0000, 0.0004, 0.0000,  ..., 0.0492, 0.0666, 0.0531]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2335446., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1270.7200, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(78810.6562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2517.0520, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1964.7867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(826.8555, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1896],
        [ 0.1922],
        [ 0.1987],
        ...,
        [-1.4037],
        [-1.3992],
        [-1.3979]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-473162.1562, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 100.0 event: 1500 loss: tensor(514.5978, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(761.2621, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(761.2621, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6703e-03,  4.0292e-03, -2.0075e-03,  ..., -8.6963e-03,
         -9.0354e-03, -3.4719e-04],
        [-6.7042e-03,  4.1388e-03, -2.1100e-03,  ..., -8.7544e-03,
         -9.4969e-03, -5.3457e-05],
        [-6.9565e-03,  4.9527e-03, -2.8712e-03,  ..., -9.1857e-03,
         -1.2923e-02,  2.1270e-03],
        ...,
        [-6.0050e-03,  1.8827e-03,  0.0000e+00,  ..., -7.5589e-03,
          0.0000e+00, -6.0979e-03],
        [-6.0050e-03,  1.8827e-03,  0.0000e+00,  ..., -7.5589e-03,
          0.0000e+00, -6.0979e-03],
        [-6.0050e-03,  1.8827e-03,  0.0000e+00,  ..., -7.5589e-03,
          0.0000e+00, -6.0979e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31543.4941, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.3187, device='cuda:0')



h[100].sum tensor(-36.8848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(105.9858, device='cuda:0')



h[200].sum tensor(-567.7838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.8307, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0194, 0.0000,  ..., 0.0000, 0.0000, 0.0088],
        [0.0000, 0.0163, 0.0000,  ..., 0.0000, 0.0000, 0.0057],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0006],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(271377.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.6006e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.7722e-03, 4.4968e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1158e-02, 3.2972e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 4.7705e-04, 0.0000e+00,  ..., 4.8876e-02, 6.6602e-02,
         5.3375e-02],
        [0.0000e+00, 4.7705e-04, 0.0000e+00,  ..., 4.8876e-02, 6.6602e-02,
         5.3375e-02],
        [0.0000e+00, 4.7705e-04, 0.0000e+00,  ..., 4.8876e-02, 6.6602e-02,
         5.3375e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2344509.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1286.8875, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(80143.8359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2528.2036, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2037.9856, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(819.5305, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1845],
        [ 0.2101],
        [ 0.2351],
        ...,
        [-1.4317],
        [-1.4271],
        [-1.4258]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-449508.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(793.1372, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.2448],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(793.1372, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0066,  0.0037, -0.0016,  ..., -0.0086, -0.0075, -0.0013],
        [-0.0065,  0.0035, -0.0015,  ..., -0.0085, -0.0067, -0.0018],
        [-0.0065,  0.0035, -0.0015,  ..., -0.0085, -0.0068, -0.0017],
        ...,
        [-0.0060,  0.0019,  0.0000,  ..., -0.0076,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0076,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0076,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31442.7227, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.7976, device='cuda:0')



h[100].sum tensor(-37.9776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(110.4236, device='cuda:0')



h[200].sum tensor(-567.9772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.4985, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0146, 0.0000,  ..., 0.0000, 0.0000, 0.0008],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0124, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(276626.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0117, 0.3471, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0237, 0.2849, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0301, 0.2572, 0.0000],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0483, 0.0661, 0.0552],
        [0.0000, 0.0006, 0.0000,  ..., 0.0483, 0.0661, 0.0552],
        [0.0000, 0.0006, 0.0000,  ..., 0.0483, 0.0661, 0.0552]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2389469., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1284.6609, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(81810.9062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2548.2578, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2341.0337, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(845.3555, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1050],
        [ 0.0569],
        [-0.0207],
        ...,
        [-1.4689],
        [-1.4641],
        [-1.4627]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-526258.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(719.9349, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(719.9349, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0019,  0.0000,  ..., -0.0076,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0076,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0076,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0019,  0.0000,  ..., -0.0076,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0076,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0076,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31899.3340, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.4014, device='cuda:0')



h[100].sum tensor(-34.4182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(100.2321, device='cuda:0')



h[200].sum tensor(-567.5968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.6684, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(269417.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0007, 0.0000,  ..., 0.0484, 0.0689, 0.0539],
        [0.0000, 0.0004, 0.0000,  ..., 0.0476, 0.0818, 0.0459],
        [0.0000, 0.0000, 0.0000,  ..., 0.0453, 0.1151, 0.0283],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0483, 0.0661, 0.0552],
        [0.0000, 0.0006, 0.0000,  ..., 0.0483, 0.0661, 0.0552],
        [0.0000, 0.0006, 0.0000,  ..., 0.0483, 0.0661, 0.0552]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2354517.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1284.4368, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(82062.2422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2535.1741, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2036.9907, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(821.4274, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4718],
        [-1.2086],
        [-0.8368],
        ...,
        [-1.4689],
        [-1.4641],
        [-1.4627]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-502438.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(820.8998, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(820.8998, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0019,  0.0000,  ..., -0.0076,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0076,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0076,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0019,  0.0000,  ..., -0.0076,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0076,  0.0000, -0.0061],
        [-0.0060,  0.0019,  0.0000,  ..., -0.0076,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31424.1641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.0856, device='cuda:0')



h[100].sum tensor(-38.3576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(114.2888, device='cuda:0')



h[200].sum tensor(-568.0949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.9510, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(276347.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0008, 0.0000,  ..., 0.0479, 0.0664, 0.0574],
        [0.0000, 0.0008, 0.0000,  ..., 0.0479, 0.0664, 0.0574],
        [0.0000, 0.0008, 0.0000,  ..., 0.0480, 0.0667, 0.0574],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0476, 0.0658, 0.0573],
        [0.0000, 0.0006, 0.0000,  ..., 0.0476, 0.0658, 0.0573],
        [0.0000, 0.0006, 0.0000,  ..., 0.0476, 0.0658, 0.0573]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2390199.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1254.1077, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(84361.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2559.8887, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2171.5776, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(864.9130, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6340],
        [-1.6778],
        [-1.6765],
        ...,
        [-1.5084],
        [-1.5035],
        [-1.5021]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-591777.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(714.6083, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(714.6083, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-32024.4980, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.1542, device='cuda:0')



h[100].sum tensor(-33.4996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(99.4905, device='cuda:0')



h[200].sum tensor(-567.6378, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.3897, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(269523., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0463, 0.0904, 0.0442],
        [0.0000, 0.0000, 0.0000,  ..., 0.0435, 0.1253, 0.0230],
        [0.0000, 0.0000, 0.0000,  ..., 0.0383, 0.1919, 0.0089],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0472, 0.0661, 0.0584],
        [0.0000, 0.0005, 0.0000,  ..., 0.0472, 0.0661, 0.0584],
        [0.0000, 0.0005, 0.0000,  ..., 0.0472, 0.0661, 0.0584]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2371110., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1254.5775, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(85850.7266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2564.8184, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2059.7444, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(846.5394, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5496],
        [-0.2388],
        [ 0.0151],
        ...,
        [-1.5406],
        [-1.5357],
        [-1.5342]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-580496.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(801.6138, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(801.6138, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061],
        [-0.0066,  0.0036, -0.0015,  ..., -0.0087, -0.0074, -0.0011]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31370.7656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.1908, device='cuda:0')



h[100].sum tensor(-36.8592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(111.6037, device='cuda:0')



h[200].sum tensor(-568.0781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.9420, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(280121.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.2369e-04, 0.0000e+00,  ..., 4.4535e-02, 1.0340e-01,
         3.4481e-02],
        [0.0000e+00, 4.7030e-04, 0.0000e+00,  ..., 4.6977e-02, 7.5562e-02,
         5.2285e-02],
        [0.0000e+00, 7.7390e-04, 0.0000e+00,  ..., 4.7524e-02, 6.8529e-02,
         5.6838e-02],
        ...,
        [0.0000e+00, 2.6593e-04, 0.0000e+00,  ..., 4.4543e-02, 9.2748e-02,
         4.0019e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.5108e-02, 1.5495e-01,
         1.8914e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.3296e-02, 2.3022e-01,
         3.5772e-03]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2422573.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1321.5027, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(84678.0156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2595.1255, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3206.0706, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(852.5536, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4977],
        [-0.9061],
        [-1.2612],
        ...,
        [-1.1583],
        [-0.7474],
        [-0.3248]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-504911.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(886.3563, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(886.3563, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-30744.4141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.4614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(41.1225, device='cuda:0')



h[100].sum tensor(-40.4001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(123.4019, device='cuda:0')



h[200].sum tensor(-568.5474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(46.3759, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(288149.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0446, 0.1289, 0.0210],
        [0.0000, 0.0000, 0.0000,  ..., 0.0405, 0.1529, 0.0142],
        [0.0000, 0.0000, 0.0000,  ..., 0.0248, 0.2479, 0.0010],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0469, 0.0687, 0.0565],
        [0.0000, 0.0004, 0.0000,  ..., 0.0469, 0.0687, 0.0565],
        [0.0000, 0.0004, 0.0000,  ..., 0.0469, 0.0687, 0.0565]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2459317.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1335.5551, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(84657.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2624.3521, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4172.3936, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(879.2245, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5981],
        [-0.2240],
        [ 0.0715],
        ...,
        [-1.5856],
        [-1.5806],
        [-1.5792]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-496171.2812, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(824.5493, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(824.5493, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0065,  0.0032, -0.0012,  ..., -0.0086, -0.0058, -0.0021],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0077,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31133.6016, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.2549, device='cuda:0')



h[100].sum tensor(-37.0900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(114.7969, device='cuda:0')



h[200].sum tensor(-568.2455, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.1420, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0059],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(284938.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.8295e-03, 4.0057e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5263e-02, 3.0655e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.4255e-02, 2.3722e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 1.2652e-05, 0.0000e+00,  ..., 4.6525e-02, 6.9469e-02,
         5.7556e-02],
        [0.0000e+00, 1.2652e-05, 0.0000e+00,  ..., 4.6525e-02, 6.9469e-02,
         5.7556e-02],
        [0.0000e+00, 1.2651e-05, 0.0000e+00,  ..., 4.6525e-02, 6.9469e-02,
         5.7556e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2462207., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1350.1614, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(85344.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2648.0598, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4895.2627, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(882.3423, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2425],
        [ 0.2457],
        [ 0.2494],
        ...,
        [-1.6201],
        [-1.6150],
        [-1.6135]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-499091.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(781.0431, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3311],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(781.0431, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        [-0.0063,  0.0028, -0.0008,  ..., -0.0083, -0.0039, -0.0034],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0018,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0078,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31493.8867, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0560, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.2365, device='cuda:0')



h[100].sum tensor(-34.4590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(108.7398, device='cuda:0')



h[200].sum tensor(-568.0117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.8657, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0104, 0.0000,  ..., 0.0000, 0.0000, 0.0003],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(277374.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.9945e-02, 1.6410e-01,
         4.0905e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.4252e-02, 1.6465e-01,
         7.9555e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.1810e-02, 2.4038e-01,
         4.7458e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.6028e-02, 6.9830e-02,
         5.9835e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.6028e-02, 6.9830e-02,
         5.9835e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.6028e-02, 6.9830e-02,
         5.9835e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2420453.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1315.5604, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(87011.4844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2652.9526, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4872.2900, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(890.7531, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7863],
        [-0.4544],
        [-0.1005],
        ...,
        [-1.6620],
        [-1.6568],
        [-1.6553]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-559521.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(817.5811, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(817.5811, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        [-0.0062,  0.0024, -0.0005,  ..., -0.0082, -0.0026, -0.0043],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31213.2168, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.9316, device='cuda:0')



h[100].sum tensor(-35.7946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(113.8267, device='cuda:0')



h[200].sum tensor(-568.2335, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.7774, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(281290.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0366, 0.1703, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0386, 0.1912, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0365, 0.2224, 0.0032],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0456, 0.0701, 0.0609],
        [0.0000, 0.0000, 0.0000,  ..., 0.0456, 0.0701, 0.0609],
        [0.0000, 0.0000, 0.0000,  ..., 0.0456, 0.0701, 0.0609]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2451237.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1341.3240, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(87586.4297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2667.9507, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4988.5498, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(889.1302, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2800],
        [ 0.2790],
        [ 0.2948],
        ...,
        [-1.6950],
        [-1.6898],
        [-1.6882]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-567799.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 110.0 event: 1650 loss: tensor(469.4566, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(748.4974, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(748.4974, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31394.6484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.7265, device='cuda:0')



h[100].sum tensor(-32.5479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(104.2086, device='cuda:0')



h[200].sum tensor(-567.9161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.1628, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(280931.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0440, 0.0944, 0.0453],
        [0.0000, 0.0003, 0.0000,  ..., 0.0459, 0.0741, 0.0586],
        [0.0000, 0.0006, 0.0000,  ..., 0.0461, 0.0721, 0.0601],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0457, 0.0711, 0.0600],
        [0.0000, 0.0004, 0.0000,  ..., 0.0457, 0.0711, 0.0600],
        [0.0000, 0.0004, 0.0000,  ..., 0.0457, 0.0711, 0.0600]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2449924.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1349.9517, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(87624.0469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2543.2913, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4524.6191, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(875.8197, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8963],
        [-1.2336],
        [-1.4277],
        ...,
        [-1.7166],
        [-1.7113],
        [-1.7098]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-493836.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(821.4076, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(821.4076, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        [-0.0068,  0.0041, -0.0017,  ..., -0.0092, -0.0090,  0.0004],
        ...,
        [-0.0066,  0.0037, -0.0014,  ..., -0.0090, -0.0077, -0.0005],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-30919.8906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.1092, device='cuda:0')



h[100].sum tensor(-35.2891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(114.3595, device='cuda:0')



h[200].sum tensor(-568.3069, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.9776, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(284054.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.7034e-04, 0.0000e+00,  ..., 4.2577e-02, 9.7945e-02,
         4.4057e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.2766e-02, 1.5869e-01,
         2.1501e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.7246e-02, 1.9419e-01,
         8.6674e-03],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.5849e-02, 1.6533e-01,
         1.1113e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.7603e-02, 1.4395e-01,
         2.4162e-02],
        [0.0000e+00, 1.8727e-04, 0.0000e+00,  ..., 4.3328e-02, 9.4135e-02,
         4.6030e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2456049.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1330.5902, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(87978.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2607.8950, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4755.7056, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(889.7874, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4456],
        [-1.0612],
        [-0.6815],
        ...,
        [-0.8223],
        [-1.0908],
        [-1.4063]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-544796.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(927.7004, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(927.7004, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        ...,
        [-0.0063,  0.0028, -0.0007,  ..., -0.0084, -0.0040, -0.0032],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-30291.2207, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.6309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(43.0406, device='cuda:0')



h[100].sum tensor(-39.1581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(129.1580, device='cuda:0')



h[200].sum tensor(-568.8431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(48.5391, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0000, 0.0009],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(295610.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 5.1662e-04, 0.0000e+00,  ..., 4.6016e-02, 7.2184e-02,
         6.2774e-02],
        [0.0000e+00, 5.1662e-04, 0.0000e+00,  ..., 4.6016e-02, 7.2184e-02,
         6.2774e-02],
        [0.0000e+00, 6.0020e-04, 0.0000e+00,  ..., 4.6145e-02, 7.2512e-02,
         6.2796e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.0882e-02, 2.6796e-01,
         3.5793e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.4145e-02, 1.7350e-01,
         2.1076e-02],
        [0.0000e+00, 1.7473e-04, 0.0000e+00,  ..., 4.2282e-02, 1.0308e-01,
         4.2395e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2537673.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1399.7622, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(86720.0469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2735.5679, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5999.6074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(907.3917, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0792],
        [-2.0837],
        [-2.0725],
        ...,
        [-0.0195],
        [-0.4836],
        [-1.0603]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-538965., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(686.4061, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(686.4061, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0078,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31987.5352, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.8458, device='cuda:0')



h[100].sum tensor(-28.4103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(95.5641, device='cuda:0')



h[200].sum tensor(-567.5912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.9141, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(273967.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0426, 0.1292, 0.0309],
        [0.0000, 0.0004, 0.0000,  ..., 0.0448, 0.0887, 0.0543],
        [0.0000, 0.0008, 0.0000,  ..., 0.0455, 0.0776, 0.0616],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0454, 0.0713, 0.0647],
        [0.0000, 0.0006, 0.0000,  ..., 0.0454, 0.0713, 0.0647],
        [0.0000, 0.0006, 0.0000,  ..., 0.0454, 0.0713, 0.0647]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2441601.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1316.1013, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(89460.3281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2681.0273, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4188.6592, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(843.3701, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6611],
        [-1.0145],
        [-1.1862],
        ...,
        [-1.8247],
        [-1.8192],
        [-1.8176]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-577780.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(740.0468, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(740.0468, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31639.3086, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9308, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.3344, device='cuda:0')



h[100].sum tensor(-30.3579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(103.0321, device='cuda:0')



h[200].sum tensor(-567.8893, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.7207, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(281723.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0442, 0.0862, 0.0563],
        [0.0000, 0.0000, 0.0000,  ..., 0.0402, 0.1252, 0.0331],
        [0.0000, 0.0000, 0.0000,  ..., 0.0249, 0.2418, 0.0105],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0451, 0.0714, 0.0656],
        [0.0000, 0.0009, 0.0000,  ..., 0.0451, 0.0714, 0.0656],
        [0.0000, 0.0009, 0.0000,  ..., 0.0451, 0.0714, 0.0656]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2487897.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1343.2588, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(89355.5859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2698.7261, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4152.0127, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(849.8260, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-9.1612e-01],
        [-4.1271e-01],
        [-4.1297e-04],
        ...,
        [-1.8556e+00],
        [-1.8501e+00],
        [-1.8485e+00]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-578538.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(646.9058, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(646.9058, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0063,  0.0026, -0.0006,  ..., -0.0084, -0.0033, -0.0036],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0069,  0.0045, -0.0017,  ..., -0.0095, -0.0102,  0.0016],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-32145.3047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5938, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.0132, device='cuda:0')



h[100].sum tensor(-26.2591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(90.0647, device='cuda:0')



h[200].sum tensor(-567.4291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.8473, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0016],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(274674.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0372, 0.1756, 0.0051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0274, 0.2157, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0258, 0.2092, 0.0020],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0450, 0.0710, 0.0666],
        [0.0000, 0.0005, 0.0000,  ..., 0.0450, 0.0710, 0.0666],
        [0.0000, 0.0005, 0.0000,  ..., 0.0450, 0.0710, 0.0666]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2449242.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1195.0564, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(90125.6250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2784.9414, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3486.7776, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(922.9613, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0664],
        [ 0.1073],
        [ 0.0215],
        ...,
        [-1.8885],
        [-1.8830],
        [-1.8814]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-577540.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(739.4287, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6763],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(739.4287, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0070,  0.0049, -0.0020,  ..., -0.0097, -0.0117,  0.0028],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0066,  0.0037, -0.0012,  ..., -0.0090, -0.0073, -0.0006],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31574.6133, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9154, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.3058, device='cuda:0')



h[100].sum tensor(-29.7720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(102.9461, device='cuda:0')



h[200].sum tensor(-567.8735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.6883, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0156, 0.0000,  ..., 0.0000, 0.0000, 0.0060],
        [0.0000, 0.0172, 0.0000,  ..., 0.0000, 0.0000, 0.0066],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(280459.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.4693e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.0446e-03, 4.5968e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6809e-02, 3.0610e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 5.3614e-04, 0.0000e+00,  ..., 4.5026e-02, 7.1031e-02,
         6.6564e-02],
        [0.0000e+00, 5.3614e-04, 0.0000e+00,  ..., 4.5026e-02, 7.1031e-02,
         6.6564e-02],
        [0.0000e+00, 5.3614e-04, 0.0000e+00,  ..., 4.5026e-02, 7.1031e-02,
         6.6564e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2473244., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1209.8452, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(89617.7344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2799.2446, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3814.4373, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(942.0706, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2528],
        [ 0.2885],
        [ 0.3172],
        ...,
        [-1.8886],
        [-1.8830],
        [-1.8814]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-584475.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(879.5100, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(879.5100, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-30565.4336, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.4276, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.8048, device='cuda:0')



h[100].sum tensor(-34.8860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(122.4487, device='cuda:0')



h[200].sum tensor(-568.5861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(46.0176, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(292549.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0384, 0.1769, 0.0068],
        [0.0000, 0.0000, 0.0000,  ..., 0.0313, 0.1944, 0.0093],
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.3283, 0.0042],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0449, 0.0766, 0.0628],
        [0.0000, 0.0000, 0.0000,  ..., 0.0448, 0.0795, 0.0610],
        [0.0000, 0.0000, 0.0000,  ..., 0.0449, 0.0766, 0.0628]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2508086.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1076.3680, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(88240.9297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2954.2522, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4444.6440, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1104.7148, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2581],
        [ 0.2216],
        [ 0.1927],
        ...,
        [-1.7938],
        [-1.7555],
        [-1.7531]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-588019.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(836.2919, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4211],
        [0.5288],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(836.2919, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0073,  0.0057, -0.0024,  ..., -0.0102, -0.0144,  0.0051],
        [-0.0065,  0.0032, -0.0009,  ..., -0.0088, -0.0056, -0.0018],
        [-0.0067,  0.0038, -0.0013,  ..., -0.0091, -0.0077, -0.0001],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31029.4258, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2662, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.7997, device='cuda:0')



h[100].sum tensor(-32.7007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(116.4317, device='cuda:0')



h[200].sum tensor(-568.3685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.7564, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0145, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0162, 0.0000,  ..., 0.0000, 0.0000, 0.0053],
        [0.0000, 0.0154, 0.0000,  ..., 0.0000, 0.0000, 0.0012],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(294739.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.8632e-04, 4.1967e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.9346e-03, 4.1731e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.5489e-03, 3.9712e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 6.4157e-04, 0.0000e+00,  ..., 4.5288e-02, 7.1843e-02,
         6.5904e-02],
        [0.0000e+00, 6.4157e-04, 0.0000e+00,  ..., 4.4557e-02, 7.7496e-02,
         6.2147e-02],
        [0.0000e+00, 3.2079e-04, 0.0000e+00,  ..., 4.2508e-02, 9.6702e-02,
         4.9328e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2533190.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1157.3081, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(88148.1094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2892.8167, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4255.8828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1026.1544, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3085],
        [ 0.3202],
        [ 0.3257],
        ...,
        [-1.9016],
        [-1.7756],
        [-1.5065]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-511156.5938, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(698.2069, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(698.2069, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31914.2930, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.3933, device='cuda:0')



h[100].sum tensor(-27.4449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(97.2070, device='cuda:0')



h[200].sum tensor(-567.6857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.5315, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(282155.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0008, 0.0000,  ..., 0.0455, 0.0725, 0.0659],
        [0.0000, 0.0008, 0.0000,  ..., 0.0455, 0.0725, 0.0659],
        [0.0000, 0.0009, 0.0000,  ..., 0.0457, 0.0728, 0.0659],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0453, 0.0718, 0.0659],
        [0.0000, 0.0006, 0.0000,  ..., 0.0453, 0.0718, 0.0659],
        [0.0000, 0.0006, 0.0000,  ..., 0.0453, 0.0718, 0.0659]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2473469.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1124.2600, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(88466.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2858.1099, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3743.5469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(994.4852, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1672],
        [-2.1851],
        [-2.1868],
        ...,
        [-1.7924],
        [-1.7761],
        [-1.7047]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-536399.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 120.0 event: 1800 loss: tensor(402.4156, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(735.3322, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3020],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(735.3322, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062,  0.0023, -0.0003,  ..., -0.0082, -0.0021, -0.0044],
        [-0.0063,  0.0026, -0.0005,  ..., -0.0084, -0.0031, -0.0036],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31816.7754, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.1157, device='cuda:0')



h[100].sum tensor(-28.7510, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(102.3757, device='cuda:0')



h[200].sum tensor(-567.9116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.4740, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(286384.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0337, 0.2703, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0379, 0.2003, 0.0129],
        [0.0000, 0.0000, 0.0000,  ..., 0.0419, 0.1369, 0.0309],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0452, 0.0723, 0.0677],
        [0.0000, 0.0016, 0.0000,  ..., 0.0452, 0.0723, 0.0677],
        [0.0000, 0.0016, 0.0000,  ..., 0.0452, 0.0723, 0.0677]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2501436., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1262.6078, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(89051.4766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2888.8745, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2963.9653, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(955.6813, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2267],
        [-0.0313],
        [-0.4445],
        ...,
        [-1.9733],
        [-1.9676],
        [-1.9660]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-558567.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(640.5535, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3850],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(640.5535, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0064,  0.0028, -0.0006,  ..., -0.0086, -0.0039, -0.0030],
        [-0.0063,  0.0025, -0.0004,  ..., -0.0083, -0.0027, -0.0040],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-32672.9336, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5597, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.7185, device='cuda:0')



h[100].sum tensor(-24.4946, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.1803, device='cuda:0')



h[200].sum tensor(-567.3997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.5150, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0014],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(278835.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0390, 0.1566, 0.0252],
        [0.0000, 0.0000, 0.0000,  ..., 0.0336, 0.2226, 0.0114],
        [0.0000, 0.0000, 0.0000,  ..., 0.0211, 0.3311, 0.0000],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0448, 0.0726, 0.0715],
        [0.0000, 0.0016, 0.0000,  ..., 0.0448, 0.0726, 0.0715],
        [0.0000, 0.0016, 0.0000,  ..., 0.0448, 0.0726, 0.0715]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2490176., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1299.8806, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(90681.9141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2997.8894, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2192.3022, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(984.4070, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0297],
        [ 0.1687],
        [ 0.2803],
        ...,
        [-2.0267],
        [-2.0208],
        [-2.0191]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-626078.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(603.2856, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(603.2856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-33136.1172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.9894, device='cuda:0')



h[100].sum tensor(-22.9496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(83.9917, device='cuda:0')



h[200].sum tensor(-567.2399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.5651, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(277774.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0015, 0.0000,  ..., 0.0449, 0.0735, 0.0741],
        [0.0000, 0.0015, 0.0000,  ..., 0.0449, 0.0735, 0.0741],
        [0.0000, 0.0016, 0.0000,  ..., 0.0450, 0.0738, 0.0741],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0446, 0.0728, 0.0741],
        [0.0000, 0.0013, 0.0000,  ..., 0.0446, 0.0728, 0.0741],
        [0.0000, 0.0013, 0.0000,  ..., 0.0446, 0.0728, 0.0741]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2499703.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1308.7297, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(91126.9844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3012.3135, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2466.8984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(993.3260, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1680],
        [-2.2771],
        [-2.3699],
        ...,
        [-2.0733],
        [-2.0673],
        [-2.0656]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-692460.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(860.9531, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(860.9531, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0063,  0.0026, -0.0004,  ..., -0.0084, -0.0028, -0.0038],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0079,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-31605.3594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.3912, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.9439, device='cuda:0')



h[100].sum tensor(-32.2035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(119.8652, device='cuda:0')



h[200].sum tensor(-568.5551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(45.0467, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(295256.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0293, 0.2725, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0220, 0.3193, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.3248, 0.0000],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0445, 0.0734, 0.0735],
        [0.0000, 0.0013, 0.0000,  ..., 0.0445, 0.0734, 0.0735],
        [0.0000, 0.0013, 0.0000,  ..., 0.0445, 0.0734, 0.0735]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2575575.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1403.7577, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(89282.8906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2965.6021, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3764.3501, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(966.2031, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3335],
        [ 0.3207],
        [ 0.3076],
        ...,
        [-2.0987],
        [-2.0928],
        [-2.0911]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-600528.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(606.1854, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(606.1854, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-33518.4336, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4324, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.1239, device='cuda:0')



h[100].sum tensor(-22.2331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(84.3954, device='cuda:0')



h[200].sum tensor(-567.2336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.7168, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(275022.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0020, 0.0000,  ..., 0.0444, 0.0772, 0.0704],
        [0.0000, 0.0010, 0.0000,  ..., 0.0436, 0.0934, 0.0599],
        [0.0000, 0.0000, 0.0000,  ..., 0.0411, 0.1410, 0.0328],
        ...,
        [0.0000, 0.0018, 0.0000,  ..., 0.0443, 0.0737, 0.0722],
        [0.0000, 0.0018, 0.0000,  ..., 0.0443, 0.0737, 0.0722],
        [0.0000, 0.0018, 0.0000,  ..., 0.0443, 0.0737, 0.0722]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2484501.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1306.3024, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(90916.7500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2751.9163, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2628.1985, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(818.7688, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7802],
        [-1.3371],
        [-0.7901],
        ...,
        [-2.1177],
        [-2.1117],
        [-2.1101]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-619029.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(716.1239, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4924],
        [0.5430],
        [0.6982],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(716.1239, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0070,  0.0049, -0.0015,  ..., -0.0098, -0.0105,  0.0027],
        [-0.0074,  0.0061, -0.0021,  ..., -0.0104, -0.0142,  0.0058],
        [-0.0071,  0.0051, -0.0016,  ..., -0.0098, -0.0109,  0.0031],
        ...,
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-32951.0469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.2245, device='cuda:0')



h[100].sum tensor(-25.9573, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(99.7015, device='cuda:0')



h[200].sum tensor(-567.8046, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.4690, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0215, 0.0000,  ..., 0.0000, 0.0000, 0.0157],
        [0.0000, 0.0232, 0.0000,  ..., 0.0000, 0.0000, 0.0206],
        [0.0000, 0.0229, 0.0000,  ..., 0.0000, 0.0000, 0.0202],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(283790.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6482, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7012, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6912, 0.0000],
        ...,
        [0.0000, 0.0018, 0.0000,  ..., 0.0438, 0.0737, 0.0736],
        [0.0000, 0.0018, 0.0000,  ..., 0.0438, 0.0737, 0.0736],
        [0.0000, 0.0018, 0.0000,  ..., 0.0438, 0.0737, 0.0736]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2538191., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1356.3459, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(90460.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2762.2031, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3386.7095, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(822.3306, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1683],
        [ 0.1512],
        [ 0.1359],
        ...,
        [-2.1596],
        [-2.1535],
        [-2.1518]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-609677.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(819.5873, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(819.5873, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-32389.4180, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.0247, device='cuda:0')



h[100].sum tensor(-29.5026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(114.1060, device='cuda:0')



h[200].sum tensor(-568.3650, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.8824, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(290277.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0436, 0.0743, 0.0761],
        [0.0000, 0.0017, 0.0000,  ..., 0.0436, 0.0743, 0.0761],
        [0.0000, 0.0017, 0.0000,  ..., 0.0436, 0.0779, 0.0741],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0433, 0.0737, 0.0761],
        [0.0000, 0.0015, 0.0000,  ..., 0.0433, 0.0737, 0.0761],
        [0.0000, 0.0015, 0.0000,  ..., 0.0433, 0.0737, 0.0761]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2575045.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1378.1611, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(90689.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2803.3833, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4093.4668, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(865.1455, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4528],
        [-2.3049],
        [-2.0556],
        ...,
        [-2.2066],
        [-2.1989],
        [-2.1965]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-688730.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(596.8394, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(596.8394, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062,  0.0025, -0.0003,  ..., -0.0084, -0.0024, -0.0040],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-34034.3672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.6903, device='cuda:0')



h[100].sum tensor(-21.0321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(83.0942, device='cuda:0')



h[200].sum tensor(-567.1991, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.2278, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(274824.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0295, 0.2490, 0.0091],
        [0.0000, 0.0000, 0.0000,  ..., 0.0363, 0.1699, 0.0262],
        [0.0000, 0.0000, 0.0000,  ..., 0.0407, 0.1135, 0.0519],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0423, 0.0781, 0.0740],
        [0.0000, 0.0015, 0.0000,  ..., 0.0426, 0.0739, 0.0768],
        [0.0000, 0.0015, 0.0000,  ..., 0.0426, 0.0739, 0.0768]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2516354., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1293.4591, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(93047.3438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2757.8733, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3004.1414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(820.5443, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1908],
        [-0.0612],
        [-0.4400],
        ...,
        [-2.1582],
        [-2.2240],
        [-2.2341]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-726674.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(714.4186, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(714.4186, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-33241.4453, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.1454, device='cuda:0')



h[100].sum tensor(-24.8263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(99.4641, device='cuda:0')



h[200].sum tensor(-567.8013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.3797, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(284943.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0000,  ..., 0.0421, 0.0800, 0.0740],
        [0.0000, 0.0015, 0.0000,  ..., 0.0421, 0.0769, 0.0759],
        [0.0000, 0.0018, 0.0000,  ..., 0.0423, 0.0758, 0.0769],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0419, 0.0748, 0.0768],
        [0.0000, 0.0015, 0.0000,  ..., 0.0419, 0.0748, 0.0768],
        [0.0000, 0.0015, 0.0000,  ..., 0.0419, 0.0748, 0.0768]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2563716.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1213.1500, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(92804.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2868.9375, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3360.1196, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(745.0648, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4987],
        [-1.8628],
        [-2.1122],
        ...,
        [-2.2721],
        [-2.2657],
        [-2.2639]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-647347.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(800.6929, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(800.6929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-32972.0039, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.1481, device='cuda:0')



h[100].sum tensor(-27.4968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(111.4755, device='cuda:0')



h[200].sum tensor(-568.2529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.8938, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(293350.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 2.6150e-04, 0.0000e+00,  ..., 4.1782e-02, 7.5692e-02,
         7.9299e-02],
        [0.0000e+00, 2.0332e-04, 0.0000e+00,  ..., 4.1678e-02, 7.8521e-02,
         7.7498e-02],
        [0.0000e+00, 7.2569e-05, 0.0000e+00,  ..., 4.1352e-02, 9.0802e-02,
         6.9896e-02],
        ...,
        [0.0000e+00, 7.9402e-05, 0.0000e+00,  ..., 4.1533e-02, 7.4992e-02,
         7.9254e-02],
        [0.0000e+00, 7.9402e-05, 0.0000e+00,  ..., 4.1533e-02, 7.4992e-02,
         7.9254e-02],
        [0.0000e+00, 7.9402e-05, 0.0000e+00,  ..., 4.1533e-02, 7.4992e-02,
         7.9254e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2617764.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1377.0581, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(91719.2031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2867.6016, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5565.4170, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(916.7269, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6745],
        [-1.5301],
        [-1.1945],
        ...,
        [-2.3227],
        [-2.3162],
        [-2.3144]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-720473.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(561.5115, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3306],
        [0.5620],
        [0.3574],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(561.5115, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0065,  0.0034, -0.0007,  ..., -0.0089, -0.0052, -0.0015],
        [-0.0070,  0.0048, -0.0013,  ..., -0.0097, -0.0095,  0.0023],
        [-0.0072,  0.0054, -0.0015,  ..., -0.0101, -0.0115,  0.0041],
        ...,
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-34633.0469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2720, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0513, device='cuda:0')



h[100].sum tensor(-19.2353, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(78.1758, device='cuda:0')



h[200].sum tensor(-567.0222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.3794, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0177, 0.0000,  ..., 0.0000, 0.0000, 0.0085],
        [0.0000, 0.0211, 0.0000,  ..., 0.0000, 0.0000, 0.0161],
        [0.0000, 0.0210, 0.0000,  ..., 0.0000, 0.0000, 0.0144],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(272381.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.7062e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 6.3153e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 6.5334e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 7.9402e-05, 0.0000e+00,  ..., 4.1533e-02, 7.4992e-02,
         7.9254e-02],
        [0.0000e+00, 7.9402e-05, 0.0000e+00,  ..., 4.1533e-02, 7.4992e-02,
         7.9254e-02],
        [0.0000e+00, 7.9402e-05, 0.0000e+00,  ..., 4.1533e-02, 7.4992e-02,
         7.9254e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2514507.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1269.3219, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(93733.8984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2797.9707, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4181.6943, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(868.4859, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3714],
        [ 0.3592],
        [ 0.3554],
        ...,
        [-2.3227],
        [-2.3162],
        [-2.3144]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-789660.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(889.4646, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(889.4646, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0063,  0.0028, -0.0004,  ..., -0.0085, -0.0032, -0.0032],
        [-0.0072,  0.0056, -0.0015,  ..., -0.0102, -0.0118,  0.0045],
        ...,
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-32516.9844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.4589, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(41.2667, device='cuda:0')



h[100].sum tensor(-29.8218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(123.8346, device='cuda:0')



h[200].sum tensor(-568.6638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(46.5385, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0151, 0.0000,  ..., 0.0000, 0.0000, 0.0053],
        [0.0000, 0.0178, 0.0000,  ..., 0.0000, 0.0000, 0.0085],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(300071.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.3222, 0.0099],
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.5084, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6575, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0416, 0.0763, 0.0785],
        [0.0000, 0.0000, 0.0000,  ..., 0.0416, 0.0763, 0.0785],
        [0.0000, 0.0000, 0.0000,  ..., 0.0416, 0.0763, 0.0785]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2629178.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1369.1953, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(90902.5781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2883.3567, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6917.3301, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(990.3504, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1415],
        [ 0.2006],
        [ 0.2078],
        ...,
        [-2.3442],
        [-2.3377],
        [-2.3359]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-758224.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(895.5227, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4541],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(895.5227, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0073,  0.0058, -0.0016,  ..., -0.0103, -0.0124,  0.0050],
        [-0.0069,  0.0044, -0.0011,  ..., -0.0095, -0.0082,  0.0012],
        [-0.0064,  0.0032, -0.0006,  ..., -0.0087, -0.0043, -0.0023],
        ...,
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-32399.4160, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.5217, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(41.5477, device='cuda:0')



h[100].sum tensor(-30.3963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(124.6781, device='cuda:0')



h[200].sum tensor(-568.7506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(46.8555, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0189, 0.0000,  ..., 0.0000, 0.0000, 0.0085],
        [0.0000, 0.0158, 0.0000,  ..., 0.0000, 0.0000, 0.0051],
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0012],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(299716.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5422, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0008, 0.4808, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.4030, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0416, 0.0763, 0.0785],
        [0.0000, 0.0000, 0.0000,  ..., 0.0416, 0.0763, 0.0785],
        [0.0000, 0.0000, 0.0000,  ..., 0.0416, 0.0763, 0.0785]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2630067., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1409.7717, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(90173.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2897.9192, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7125.3516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(977.7704, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3692],
        [ 0.3823],
        [ 0.3983],
        ...,
        [-2.3441],
        [-2.3377],
        [-2.3359]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-682615.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3276],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(829.1929, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3276],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(829.1929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0063,  0.0027, -0.0004,  ..., -0.0085, -0.0029, -0.0034],
        [-0.0063,  0.0026, -0.0003,  ..., -0.0084, -0.0025, -0.0039],
        [-0.0066,  0.0035, -0.0007,  ..., -0.0090, -0.0054, -0.0012],
        ...,
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-32893.1992, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.4704, device='cuda:0')



h[100].sum tensor(-27.8354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(115.4434, device='cuda:0')



h[200].sum tensor(-568.4248, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.3850, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0127, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(296815.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0305, 0.2342, 0.0066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0228, 0.3156, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.3296, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0416, 0.0778, 0.0750],
        [0.0000, 0.0000, 0.0000,  ..., 0.0416, 0.0778, 0.0750],
        [0.0000, 0.0000, 0.0000,  ..., 0.0416, 0.0778, 0.0750]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2601068., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1400.9858, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(90370.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2875.1028, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6452.0869, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(914.2081, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3845],
        [ 0.4305],
        [ 0.4355],
        ...,
        [-2.3407],
        [-2.3344],
        [-2.3326]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-524846.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3127],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(812.6552, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3127],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(812.6552, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0063,  0.0027, -0.0003,  ..., -0.0085, -0.0028, -0.0036],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0079,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-33199.6328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.7031, device='cuda:0')



h[100].sum tensor(-26.8404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(113.1409, device='cuda:0')



h[200].sum tensor(-568.3331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.5197, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(298021.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0393, 0.1203, 0.0477],
        [0.0000, 0.0000, 0.0000,  ..., 0.0388, 0.1291, 0.0419],
        [0.0000, 0.0000, 0.0000,  ..., 0.0370, 0.1647, 0.0188],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0413, 0.0786, 0.0745],
        [0.0000, 0.0000, 0.0000,  ..., 0.0413, 0.0786, 0.0745],
        [0.0000, 0.0000, 0.0000,  ..., 0.0413, 0.0786, 0.0745]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2607464., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1400.6493, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(90287.2656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2880.2617, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6609.6738, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(911.5509, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5136],
        [-1.6201],
        [-1.6939],
        ...,
        [-2.3607],
        [-2.3544],
        [-2.3527]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-489926., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(683.2072, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(683.2072, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0018,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0080,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-34532.9531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.6974, device='cuda:0')



h[100].sum tensor(-22.2340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(95.1187, device='cuda:0')



h[200].sum tensor(-567.6675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.7467, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(287303.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0407, 0.0855, 0.0746],
        [0.0000, 0.0000, 0.0000,  ..., 0.0399, 0.1017, 0.0640],
        [0.0000, 0.0000, 0.0000,  ..., 0.0361, 0.1519, 0.0350],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0406, 0.0783, 0.0787],
        [0.0000, 0.0000, 0.0000,  ..., 0.0406, 0.0783, 0.0787],
        [0.0000, 0.0000, 0.0000,  ..., 0.0406, 0.0783, 0.0787]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2577214., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1340.5496, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(92316.7500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2889.6079, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6512.4595, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(938.4743, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5695],
        [-1.0785],
        [-0.5346],
        ...,
        [-2.4221],
        [-2.4155],
        [-2.4137]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-629253.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(734.4700, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(734.4700, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0066,  0.0035, -0.0006,  ..., -0.0090, -0.0052, -0.0012],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0018,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0080,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-34454.4570, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9262, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.0757, device='cuda:0')



h[100].sum tensor(-23.5697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(102.2557, device='cuda:0')



h[200].sum tensor(-567.9327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.4289, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0153, 0.0000,  ..., 0.0000, 0.0000, 0.0067],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0008],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(290205.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.4842, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0086, 0.3696, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0214, 0.2249, 0.0191],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0392, 0.0776, 0.0823],
        [0.0000, 0.0000, 0.0000,  ..., 0.0392, 0.0776, 0.0823],
        [0.0000, 0.0000, 0.0000,  ..., 0.0392, 0.0776, 0.0823]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2601312.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1217.3235, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(93838.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2908.7979, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6353.2471, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(945.7435, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3130],
        [ 0.2184],
        [-0.0944],
        ...,
        [-2.4827],
        [-2.4759],
        [-2.4739]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-734906.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4324],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(818.9298, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4324],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(818.9298, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064,  0.0030, -0.0004,  ..., -0.0087, -0.0037, -0.0026],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0064,  0.0030, -0.0004,  ..., -0.0087, -0.0037, -0.0026],
        ...,
        [-0.0060,  0.0018,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0080,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-33856.5391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2485, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.9942, device='cuda:0')



h[100].sum tensor(-25.9767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(114.0145, device='cuda:0')



h[200].sum tensor(-568.3785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.8480, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(301277.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0345, 0.1438, 0.0384],
        [0.0000, 0.0000, 0.0000,  ..., 0.0317, 0.1905, 0.0077],
        [0.0000, 0.0000, 0.0000,  ..., 0.0346, 0.1442, 0.0384],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0280, 0.1892, 0.0212],
        [0.0000, 0.0000, 0.0000,  ..., 0.0343, 0.1337, 0.0442],
        [0.0000, 0.0000, 0.0000,  ..., 0.0365, 0.1033, 0.0643]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2657010.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1106.7371, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(94276.9922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2896.7544, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5568.9014, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(891.9998, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.7208],
        [-1.6740],
        [-1.8810],
        ...,
        [-0.0848],
        [-0.6792],
        [-1.3589]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-640093.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8413],
        [0.5146],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(866.4119, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.8413],
        [0.5146],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(866.4119, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0074,  0.0061, -0.0014,  ..., -0.0105, -0.0125,  0.0061],
        [-0.0076,  0.0065, -0.0016,  ..., -0.0108, -0.0137,  0.0074],
        [-0.0071,  0.0050, -0.0011,  ..., -0.0099, -0.0093,  0.0030],
        ...,
        [-0.0060,  0.0018,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0080,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-33580.2891, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.4169, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.1971, device='cuda:0')



h[100].sum tensor(-27.0205, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(120.6251, device='cuda:0')



h[200].sum tensor(-568.6123, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(45.3323, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0293, 0.0000,  ..., 0.0000, 0.0000, 0.0387],
        [0.0000, 0.0248, 0.0000,  ..., 0.0000, 0.0000, 0.0258],
        [0.0000, 0.0202, 0.0000,  ..., 0.0000, 0.0000, 0.0130],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(303618.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.9322, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8557, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7812, 0.0000],
        ...,
        [0.0000, 0.0019, 0.0000,  ..., 0.0377, 0.0778, 0.0822],
        [0.0000, 0.0019, 0.0000,  ..., 0.0377, 0.0778, 0.0822],
        [0.0000, 0.0019, 0.0000,  ..., 0.0377, 0.0778, 0.0822]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2651302.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1189.7361, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(95594.6406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2917.5945, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4905.0854, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(832.9900, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1682],
        [ 0.1755],
        [ 0.1821],
        ...,
        [-2.5384],
        [-2.5316],
        [-2.5297]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-690813.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3062],
        [0.0000],
        [0.6787],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(684.8071, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3062],
        [0.0000],
        [0.6787],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(684.8071, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062,  0.0024, -0.0002,  ..., -0.0083, -0.0017, -0.0044],
        [-0.0074,  0.0061, -0.0014,  ..., -0.0105, -0.0124,  0.0062],
        [-0.0071,  0.0050, -0.0010,  ..., -0.0099, -0.0094,  0.0032],
        ...,
        [-0.0060,  0.0018,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0018,  0.0000,  ..., -0.0080,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-34803.3750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.7716, device='cuda:0')



h[100].sum tensor(-21.0429, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(95.3414, device='cuda:0')



h[200].sum tensor(-567.6804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.8304, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0218, 0.0000,  ..., 0.0000, 0.0000, 0.0173],
        [0.0000, 0.0162, 0.0000,  ..., 0.0000, 0.0000, 0.0075],
        [0.0000, 0.0211, 0.0000,  ..., 0.0000, 0.0000, 0.0171],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(294534.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6835, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6457, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6704, 0.0000],
        ...,
        [0.0000, 0.0022, 0.0000,  ..., 0.0379, 0.0785, 0.0832],
        [0.0000, 0.0022, 0.0000,  ..., 0.0379, 0.0785, 0.0832],
        [0.0000, 0.0022, 0.0000,  ..., 0.0379, 0.0785, 0.0832]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2617753., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1366.1299, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(95873.1484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3078.3169, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4656.2422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(893.1440, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1596],
        [ 0.1481],
        [ 0.1499],
        ...,
        [-2.5583],
        [-2.5486],
        [-2.5420]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-651229.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 140.0 event: 2100 loss: tensor(899.1452, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5986],
        [0.6084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(830.1096, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5986],
        [0.6084],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(830.1096, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0066,  0.0035, -0.0005,  ..., -0.0090, -0.0049, -0.0012],
        [-0.0066,  0.0035, -0.0005,  ..., -0.0090, -0.0049, -0.0011],
        [-0.0071,  0.0052, -0.0011,  ..., -0.0100, -0.0098,  0.0038],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-34019.0391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.5129, device='cuda:0')



h[100].sum tensor(-25.0034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(115.5710, device='cuda:0')



h[200].sum tensor(-568.4027, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.4329, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0165, 0.0000,  ..., 0.0000, 0.0000, 0.0058],
        [0.0000, 0.0181, 0.0000,  ..., 0.0000, 0.0000, 0.0082],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(305903.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.2763, 0.0101],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.4390, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5427, 0.0000],
        ...,
        [0.0000, 0.0027, 0.0000,  ..., 0.0373, 0.0787, 0.0836],
        [0.0000, 0.0027, 0.0000,  ..., 0.0373, 0.0787, 0.0836],
        [0.0000, 0.0027, 0.0000,  ..., 0.0373, 0.0787, 0.0836]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2677565., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1322.1350, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(96636.8516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3006.4868, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4783.1543, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(820.1081, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2108],
        [ 0.3092],
        [ 0.3265],
        ...,
        [-2.6042],
        [-2.5973],
        [-2.5954]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-689889.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(668.7966, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(668.7966, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0063,  0.0026, -0.0002,  ..., -0.0084, -0.0023, -0.0037]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35415.2812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.0288, device='cuda:0')



h[100].sum tensor(-20.0906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.1124, device='cuda:0')



h[200].sum tensor(-567.6216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.9927, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(291742.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0030, 0.0000,  ..., 0.0368, 0.0797, 0.0851],
        [0.0000, 0.0029, 0.0000,  ..., 0.0368, 0.0801, 0.0848],
        [0.0000, 0.0029, 0.0000,  ..., 0.0369, 0.0819, 0.0839],
        ...,
        [0.0000, 0.0011, 0.0000,  ..., 0.0354, 0.1005, 0.0711],
        [0.0000, 0.0000, 0.0000,  ..., 0.0330, 0.1437, 0.0430],
        [0.0000, 0.0000, 0.0000,  ..., 0.0323, 0.1574, 0.0341]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2627292., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1212.4077, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(99216.3047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2956.7690, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3710.7837, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(774.8688, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8954],
        [-2.7169],
        [-2.4325],
        ...,
        [-2.3602],
        [-2.0707],
        [-1.8732]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-753028.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(950.9102, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(950.9102, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062,  0.0024, -0.0002,  ..., -0.0084, -0.0020, -0.0040],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-33580.5703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.7418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(44.1174, device='cuda:0')



h[100].sum tensor(-28.0713, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(132.3893, device='cuda:0')



h[200].sum tensor(-569.0642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(49.7534, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(317056., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0277, 0.2156, 0.0097],
        [0.0000, 0.0000, 0.0000,  ..., 0.0303, 0.1836, 0.0237],
        [0.0000, 0.0000, 0.0000,  ..., 0.0335, 0.1307, 0.0527],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0359, 0.0792, 0.0855],
        [0.0000, 0.0032, 0.0000,  ..., 0.0359, 0.0792, 0.0855],
        [0.0000, 0.0032, 0.0000,  ..., 0.0359, 0.0792, 0.0855]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2765532., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1299.4624, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(97521.1484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3043.5891, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5126.9688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(829.6836, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2235],
        [ 0.0718],
        [-0.0397],
        ...,
        [-2.6813],
        [-2.6742],
        [-2.6723]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-664077.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9517],
        [0.2546],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(778.1165, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9517],
        [0.2546],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(778.1165, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0076,  0.0067, -0.0013,  ..., -0.0109, -0.0135,  0.0081],
        [-0.0072,  0.0053, -0.0010,  ..., -0.0101, -0.0096,  0.0040],
        [-0.0074,  0.0058, -0.0011,  ..., -0.0104, -0.0112,  0.0057],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35061.3672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0885, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.1007, device='cuda:0')



h[100].sum tensor(-22.5605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(108.3323, device='cuda:0')



h[200].sum tensor(-568.1624, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.7125, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0150, 0.0000,  ..., 0.0000, 0.0000, 0.0057],
        [0.0000, 0.0234, 0.0000,  ..., 0.0000, 0.0000, 0.0226],
        [0.0000, 0.0184, 0.0000,  ..., 0.0000, 0.0000, 0.0112],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(299736.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4695, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5733, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5187, 0.0000],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0353, 0.0795, 0.0868],
        [0.0000, 0.0031, 0.0000,  ..., 0.0353, 0.0795, 0.0868],
        [0.0000, 0.0031, 0.0000,  ..., 0.0353, 0.0795, 0.0868]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2682532.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1183.0032, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(100555.8906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3014.4424, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3791.2749, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(810.2148, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4108],
        [ 0.3797],
        [ 0.3333],
        ...,
        [-2.7223],
        [-2.7152],
        [-2.7132]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-728814.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(709.1974, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(709.1974, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35679.6641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.9032, device='cuda:0')



h[100].sum tensor(-20.2600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(98.7371, device='cuda:0')



h[200].sum tensor(-567.8088, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.1066, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(296326.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0254, 0.2022, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.0319, 0.1307, 0.0554],
        [0.0000, 0.0000, 0.0000,  ..., 0.0334, 0.1174, 0.0643],
        ...,
        [0.0000, 0.0029, 0.0000,  ..., 0.0349, 0.0801, 0.0876],
        [0.0000, 0.0029, 0.0000,  ..., 0.0349, 0.0801, 0.0876],
        [0.0000, 0.0029, 0.0000,  ..., 0.0349, 0.0801, 0.0876]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2673216.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1170.5195, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(101643.7500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3028.0400, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3872.0649, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(820.5389, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0509],
        [-0.3618],
        [-0.5310],
        ...,
        [-2.7565],
        [-2.7493],
        [-2.7474]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-792723.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(592.2180, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(592.2180, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0069,  0.0044, -0.0007,  ..., -0.0096, -0.0071,  0.0016],
        [-0.0069,  0.0044, -0.0007,  ..., -0.0096, -0.0072,  0.0017],
        [-0.0069,  0.0045, -0.0007,  ..., -0.0096, -0.0074,  0.0020],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-36423.7344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3965, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.4759, device='cuda:0')



h[100].sum tensor(-16.6557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(82.4508, device='cuda:0')



h[200].sum tensor(-567.2066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.9860, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0166, 0.0000,  ..., 0.0000, 0.0000, 0.0039],
        [0.0000, 0.0167, 0.0000,  ..., 0.0000, 0.0000, 0.0043],
        [0.0000, 0.0143, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(289725.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5092, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5116, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.4392, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0347, 0.0811, 0.0863],
        [0.0000, 0.0033, 0.0000,  ..., 0.0347, 0.0811, 0.0863],
        [0.0000, 0.0033, 0.0000,  ..., 0.0347, 0.0811, 0.0863]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2634110.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1200.8613, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(102931.2344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3020.7249, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3174.0696, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(740.2335, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4119],
        [ 0.3910],
        [ 0.2715],
        ...,
        [-2.7651],
        [-2.7580],
        [-2.7560]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-689907.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4397],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(736.9994, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4397],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(736.9994, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0070,  0.0046, -0.0007,  ..., -0.0097, -0.0076,  0.0023],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0064,  0.0029, -0.0003,  ..., -0.0087, -0.0033, -0.0025],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35596.3359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.1931, device='cuda:0')



h[100].sum tensor(-20.3409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(102.6078, device='cuda:0')



h[200].sum tensor(-567.9340, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.5612, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0135, 0.0000,  ..., 0.0000, 0.0000, 0.0023],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(297824.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.3807, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.3486, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.2256, 0.0170],
        ...,
        [0.0000, 0.0022, 0.0000,  ..., 0.0340, 0.0818, 0.0883],
        [0.0000, 0.0022, 0.0000,  ..., 0.0340, 0.0818, 0.0883],
        [0.0000, 0.0022, 0.0000,  ..., 0.0340, 0.0818, 0.0883]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2671171.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1241.0312, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(103043.4531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3096.9690, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4484.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(831.9135, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3746],
        [ 0.3421],
        [ 0.2451],
        ...,
        [-2.8073],
        [-2.8002],
        [-2.7984]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-730197.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4617],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(596.1998, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4617],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(596.1998, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0071,  0.0050, -0.0008,  ..., -0.0100, -0.0086,  0.0035],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0064,  0.0029, -0.0003,  ..., -0.0088, -0.0034, -0.0023],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-36731.8281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.6607, device='cuda:0')



h[100].sum tensor(-16.2342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(83.0052, device='cuda:0')



h[200].sum tensor(-567.2199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.1943, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.0000, 0.0017],
        [0.0000, 0.0149, 0.0000,  ..., 0.0000, 0.0000, 0.0052],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(287624.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0026, 0.3125, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0021, 0.3501, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.2689, 0.0000],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0332, 0.0826, 0.0896],
        [0.0000, 0.0016, 0.0000,  ..., 0.0332, 0.0826, 0.0896],
        [0.0000, 0.0016, 0.0000,  ..., 0.0332, 0.0826, 0.0896]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2626502., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1170.5479, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(105353.8906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3099.8625, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4234.6001, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(855.6023, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1885],
        [ 0.4035],
        [ 0.5010],
        ...,
        [-2.8438],
        [-2.8365],
        [-2.8345]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-820559.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(629.3951, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(629.3951, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-36485.8047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5311, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.2008, device='cuda:0')



h[100].sum tensor(-16.9047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.6268, device='cuda:0')



h[200].sum tensor(-567.3939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.9312, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(293039.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0330, 0.0844, 0.0895],
        [0.0000, 0.0016, 0.0000,  ..., 0.0330, 0.0844, 0.0895],
        [0.0000, 0.0017, 0.0000,  ..., 0.0331, 0.0847, 0.0895],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0327, 0.0836, 0.0895],
        [0.0000, 0.0013, 0.0000,  ..., 0.0327, 0.0836, 0.0895],
        [0.0000, 0.0013, 0.0000,  ..., 0.0327, 0.0836, 0.0895]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2660649.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1237.5906, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(105263.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3147.8774, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5043.6675, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(863.9746, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1988],
        [-3.2516],
        [-3.2818],
        ...,
        [-2.8642],
        [-2.8570],
        [-2.8550]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-701142.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(751.1064, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(751.1064, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35682.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.8476, device='cuda:0')



h[100].sum tensor(-19.8500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(104.5719, device='cuda:0')



h[200].sum tensor(-568.0124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.2993, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(301566.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0164, 0.2593, 0.0104],
        [0.0000, 0.0000, 0.0000,  ..., 0.0248, 0.1761, 0.0349],
        [0.0000, 0.0000, 0.0000,  ..., 0.0300, 0.1240, 0.0655],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0325, 0.0842, 0.0906],
        [0.0000, 0.0008, 0.0000,  ..., 0.0325, 0.0842, 0.0906],
        [0.0000, 0.0008, 0.0000,  ..., 0.0325, 0.0842, 0.0906]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2698218., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1301.2764, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(105075.9688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3209.8728, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6124.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(912.2526, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1277],
        [-0.2540],
        [-0.8116],
        ...,
        [-2.8993],
        [-2.8900],
        [-2.8775]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-694699.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2620],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(744.5989, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2620],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(744.5989, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062,  0.0023, -0.0002,  ..., -0.0084, -0.0018, -0.0040],
        [-0.0063,  0.0023, -0.0002,  ..., -0.0084, -0.0019, -0.0039],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35738.0273, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9503, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.5456, device='cuda:0')



h[100].sum tensor(-19.6491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(103.6659, device='cuda:0')



h[200].sum tensor(-567.9735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.9588, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(299749.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0069, 0.3339, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.2514, 0.0064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.2263, 0.0068],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0325, 0.0842, 0.0906],
        [0.0000, 0.0008, 0.0000,  ..., 0.0325, 0.0842, 0.0906],
        [0.0000, 0.0008, 0.0000,  ..., 0.0325, 0.0842, 0.0906]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2684460.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1286.2915, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(105500.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3202.5093, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5885.5586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(910.3962, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4589],
        [ 0.3892],
        [ 0.2796],
        ...,
        [-2.8999],
        [-2.8926],
        [-2.8906]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-714477.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(837.0669, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(837.0669, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064,  0.0029, -0.0003,  ..., -0.0088, -0.0033, -0.0022],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0065,  0.0030, -0.0003,  ..., -0.0088, -0.0036, -0.0019],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35038.9297, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.3054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.8357, device='cuda:0')



h[100].sum tensor(-21.8547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(116.5396, device='cuda:0')



h[200].sum tensor(-568.4648, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.7969, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0139, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        [0.0000, 0.0145, 0.0000,  ..., 0.0000, 0.0000, 0.0052],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(308267.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.6639e-03, 3.6698e-01,
         5.8659e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.1736e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 6.0365e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 7.7229e-04, 0.0000e+00,  ..., 3.2261e-02, 8.4837e-02,
         9.0739e-02],
        [0.0000e+00, 7.7229e-04, 0.0000e+00,  ..., 3.2261e-02, 8.4837e-02,
         9.0739e-02],
        [0.0000e+00, 7.7229e-04, 0.0000e+00,  ..., 3.2261e-02, 8.4837e-02,
         9.0739e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2733766., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1354.6953, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(104825.9688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3247.2590, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6632.0342, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(924.7657, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3576],
        [ 0.2961],
        [ 0.2464],
        ...,
        [-2.9245],
        [-2.9172],
        [-2.9152]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-676407.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(833.3936, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(833.3936, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0071,  0.0050, -0.0007,  ..., -0.0100, -0.0082,  0.0037],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0061]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35108.4062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2882, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.6653, device='cuda:0')



h[100].sum tensor(-21.4037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(116.0282, device='cuda:0')



h[200].sum tensor(-568.4404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.6047, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0119, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(310938.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0086, 0.2635, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0023, 0.3621, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4241, 0.0000],
        ...,
        [0.0000, 0.0010, 0.0000,  ..., 0.0318, 0.0851, 0.0915],
        [0.0000, 0.0010, 0.0000,  ..., 0.0318, 0.0851, 0.0915],
        [0.0000, 0.0010, 0.0000,  ..., 0.0318, 0.0851, 0.0915]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2753121.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1363.4565, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(105669.9688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3268.8535, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6512.9175, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(931.7007, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5445],
        [ 0.5124],
        [ 0.4825],
        ...,
        [-2.9528],
        [-2.9456],
        [-2.9437]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-684055.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3877],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(667.8540, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3877],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(667.8540, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0068,  0.0039, -0.0004,  ..., -0.0093, -0.0055,  0.0006],
        [-0.0064,  0.0028, -0.0002,  ..., -0.0087, -0.0029, -0.0026],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-36338.4922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.9851, device='cuda:0')



h[100].sum tensor(-16.9498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.9812, device='cuda:0')



h[200].sum tensor(-567.6045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.9434, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0006],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0006],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(298428.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.2729, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0126, 0.2548, 0.0044],
        [0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.2616, 0.0045],
        ...,
        [0.0000, 0.0010, 0.0000,  ..., 0.0317, 0.0854, 0.0925],
        [0.0000, 0.0010, 0.0000,  ..., 0.0317, 0.0854, 0.0925],
        [0.0000, 0.0010, 0.0000,  ..., 0.0317, 0.0854, 0.0925]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2702062., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1342.5060, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(107096.9141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3264.1113, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5832.4180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(900.3448, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4899],
        [ 0.5142],
        [ 0.5494],
        ...,
        [-2.9867],
        [-2.9796],
        [-2.9777]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-710746., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2700],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(795.8710, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2700],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(795.8710, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0063,  0.0024, -0.0001,  ..., -0.0084, -0.0018, -0.0038],
        [-0.0062,  0.0023, -0.0001,  ..., -0.0084, -0.0016, -0.0041],
        [-0.0065,  0.0031, -0.0003,  ..., -0.0088, -0.0034, -0.0018],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35474.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.9244, device='cuda:0')



h[100].sum tensor(-19.6134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(110.8042, device='cuda:0')



h[200].sum tensor(-568.2045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.6415, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(305438.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.3238e-02, 2.6444e-01,
         2.8308e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.1848e-02, 2.8774e-01,
         1.3109e-05],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.7188e-02, 2.4381e-01,
         7.8736e-03],
        ...,
        [0.0000e+00, 1.2911e-03, 0.0000e+00,  ..., 3.1451e-02, 8.5737e-02,
         9.2724e-02],
        [0.0000e+00, 1.2911e-03, 0.0000e+00,  ..., 3.1451e-02, 8.5737e-02,
         9.2724e-02],
        [0.0000e+00, 1.2911e-03, 0.0000e+00,  ..., 3.1451e-02, 8.5737e-02,
         9.2724e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2726110.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1393.4235, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(107102.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3293.5820, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5680.6274, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(901.2997, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3894],
        [ 0.3830],
        [ 0.2894],
        ...,
        [-3.0127],
        [-3.0055],
        [-3.0037]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-682609.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(775.7432, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(775.7432, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35646.3594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.9906, device='cuda:0')



h[100].sum tensor(-19.0444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(108.0019, device='cuda:0')



h[200].sum tensor(-568.1481, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.5884, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0001],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306781.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.1713, 0.0379],
        [0.0000, 0.0000, 0.0000,  ..., 0.0180, 0.1846, 0.0315],
        [0.0000, 0.0000, 0.0000,  ..., 0.0079, 0.2682, 0.0120],
        ...,
        [0.0000, 0.0019, 0.0000,  ..., 0.0306, 0.0856, 0.0937],
        [0.0000, 0.0019, 0.0000,  ..., 0.0306, 0.0856, 0.0937],
        [0.0000, 0.0019, 0.0000,  ..., 0.0306, 0.0856, 0.0937]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2755871., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1393.5847, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(107981.8359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3310.1641, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5080.7104, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(887.8761, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2802],
        [ 0.3619],
        [ 0.4200],
        ...,
        [-3.0485],
        [-3.0413],
        [-3.0394]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-648989.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6108],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(892.3154, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6108],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(892.3154, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0066,  0.0034, -0.0003,  ..., -0.0090, -0.0040, -0.0010],
        [-0.0069,  0.0042, -0.0004,  ..., -0.0095, -0.0060,  0.0015],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-34819.6328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.4824, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(41.3989, device='cuda:0')



h[100].sum tensor(-21.4032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(124.2315, device='cuda:0')



h[200].sum tensor(-568.7093, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(46.6876, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0023],
        [0.0000, 0.0198, 0.0000,  ..., 0.0000, 0.0000, 0.0152],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(316285.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.6695e-04, 3.3003e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 9.8281e-04, 4.2928e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 6.2616e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 1.7221e-03, 0.0000e+00,  ..., 3.0306e-02, 8.5670e-02,
         9.5600e-02],
        [0.0000e+00, 1.7221e-03, 0.0000e+00,  ..., 3.0306e-02, 8.5670e-02,
         9.5600e-02],
        [0.0000e+00, 1.7221e-03, 0.0000e+00,  ..., 3.0306e-02, 8.5670e-02,
         9.5600e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2800923.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1509.3040, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(107929.1875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3338.3562, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5691.6484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1072.8022, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3009],
        [ 0.2680],
        [ 0.2298],
        ...,
        [-3.1020],
        [-3.0947],
        [-3.0927]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-721613.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(772.9529, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(772.9529, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35781.8359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.8611, device='cuda:0')



h[100].sum tensor(-18.5676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(107.6134, device='cuda:0')



h[200].sum tensor(-568.1703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.4424, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(307875.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0016, 0.0000,  ..., 0.0293, 0.0912, 0.0939],
        [0.0000, 0.0004, 0.0000,  ..., 0.0274, 0.1139, 0.0792],
        [0.0000, 0.0000, 0.0000,  ..., 0.0245, 0.1472, 0.0578],
        ...,
        [0.0000, 0.0021, 0.0000,  ..., 0.0295, 0.0854, 0.0971],
        [0.0000, 0.0021, 0.0000,  ..., 0.0295, 0.0854, 0.0971],
        [0.0000, 0.0021, 0.0000,  ..., 0.0295, 0.0854, 0.0971]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2779123.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1351.4282, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110275.3828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3325.7661, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4384.6509, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(956.8663, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9314],
        [-1.3319],
        [-0.6822],
        ...,
        [-3.1467],
        [-3.1393],
        [-3.1373]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-797881.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(685.4463, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(685.4463, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-36544.1562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.8013, device='cuda:0')



h[100].sum tensor(-16.1165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(95.4304, device='cuda:0')



h[200].sum tensor(-567.6971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.8639, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(299205.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.2671, 0.0153],
        [0.0000, 0.0000, 0.0000,  ..., 0.0192, 0.1661, 0.0466],
        [0.0000, 0.0000, 0.0000,  ..., 0.0260, 0.1228, 0.0731],
        ...,
        [0.0000, 0.0028, 0.0000,  ..., 0.0292, 0.0855, 0.0967],
        [0.0000, 0.0028, 0.0000,  ..., 0.0292, 0.0855, 0.0967],
        [0.0000, 0.0028, 0.0000,  ..., 0.0292, 0.0855, 0.0967]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2739003., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1274.1877, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(111447.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3302.3008, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3072.5854, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(840.0571, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1782],
        [-0.0815],
        [-0.3857],
        ...,
        [-3.1649],
        [-3.1575],
        [-3.1556]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-727397.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(668.5398, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(668.5398, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-36745.7305, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.0169, device='cuda:0')



h[100].sum tensor(-15.5309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.0766, device='cuda:0')



h[200].sum tensor(-567.6224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.9793, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(299355.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0025, 0.0000,  ..., 0.0291, 0.0894, 0.0966],
        [0.0000, 0.0013, 0.0000,  ..., 0.0278, 0.1041, 0.0870],
        [0.0000, 0.0000, 0.0000,  ..., 0.0240, 0.1485, 0.0585],
        ...,
        [0.0000, 0.0023, 0.0000,  ..., 0.0291, 0.0860, 0.0983],
        [0.0000, 0.0023, 0.0000,  ..., 0.0291, 0.0860, 0.0983],
        [0.0000, 0.0023, 0.0000,  ..., 0.0291, 0.0860, 0.0983]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2740236.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1286.3231, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(111350.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3332.5845, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3811.3457, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(859.8752, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8232],
        [-2.2469],
        [-1.4928],
        ...,
        [-3.2107],
        [-3.2032],
        [-3.2012]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-803817., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 160.0 event: 2400 loss: tensor(925.3587, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2483],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(721.0981, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2483],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(721.0981, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0065,  0.0030, -0.0002,  ..., -0.0088, -0.0029, -0.0021],
        [-0.0067,  0.0036, -0.0003,  ..., -0.0091, -0.0042, -0.0004],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-36281.8672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8890, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.4553, device='cuda:0')



h[100].sum tensor(-16.4775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(100.3940, device='cuda:0')



h[200].sum tensor(-567.8900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.7292, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0143, 0.0000,  ..., 0.0000, 0.0000, 0.0035],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(304756.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0159, 0.2050, 0.0336],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.3067, 0.0115],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4301, 0.0000],
        ...,
        [0.0000, 0.0029, 0.0000,  ..., 0.0289, 0.0867, 0.0958],
        [0.0000, 0.0029, 0.0000,  ..., 0.0289, 0.0867, 0.0958],
        [0.0000, 0.0029, 0.0000,  ..., 0.0289, 0.0867, 0.0958]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2756948.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1338.7794, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109944.0547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3309.8342, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3772.9087, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(819.2644, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1275],
        [ 0.1877],
        [ 0.3826],
        ...,
        [-3.2043],
        [-3.1969],
        [-3.1949]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-659453.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5166],
        [0.5562],
        [0.3340],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(919.1187, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5166],
        [0.5562],
        [0.3340],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(919.1187, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0071,  0.0048, -0.0004,  ..., -0.0099, -0.0067,  0.0031],
        [-0.0070,  0.0047, -0.0004,  ..., -0.0098, -0.0066,  0.0030],
        [-0.0071,  0.0049, -0.0004,  ..., -0.0099, -0.0070,  0.0035],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-34837.2109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.5809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(42.6425, device='cuda:0')



h[100].sum tensor(-20.3976, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(127.9632, device='cuda:0')



h[200].sum tensor(-568.8461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(48.0900, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0202, 0.0000,  ..., 0.0000, 0.0000, 0.0156],
        [0.0000, 0.0196, 0.0000,  ..., 0.0000, 0.0000, 0.0140],
        [0.0000, 0.0169, 0.0000,  ..., 0.0000, 0.0000, 0.0060],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(320830.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6955, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6676, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6054, 0.0000],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0286, 0.0868, 0.0960],
        [0.0000, 0.0032, 0.0000,  ..., 0.0286, 0.0868, 0.0960],
        [0.0000, 0.0032, 0.0000,  ..., 0.0286, 0.0868, 0.0960]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2827310.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1264.2604, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109640.3438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3371.9639, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4205.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(831.8072, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1836],
        [ 0.1861],
        [ 0.1899],
        ...,
        [-3.2346],
        [-3.2273],
        [-3.2254]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-661615.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2778],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(592.2856, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2778],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(592.2856, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.4865e-03,  3.0553e-03, -1.8448e-04,  ..., -8.8364e-03,
         -3.0300e-03, -1.8432e-03],
        [-6.2234e-03,  2.2684e-03, -8.3561e-05,  ..., -8.3616e-03,
         -1.3725e-03, -4.1280e-03],
        [-6.2686e-03,  2.4038e-03, -1.0092e-04,  ..., -8.4433e-03,
         -1.6576e-03, -3.7349e-03],
        ...,
        [-6.0055e-03,  1.6169e-03,  0.0000e+00,  ..., -7.9686e-03,
          0.0000e+00, -6.0198e-03],
        [-6.0055e-03,  1.6169e-03,  0.0000e+00,  ..., -7.9686e-03,
          0.0000e+00, -6.0198e-03],
        [-6.0055e-03,  1.6169e-03,  0.0000e+00,  ..., -7.9686e-03,
          0.0000e+00, -6.0198e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-37521.1641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3820, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.4791, device='cuda:0')



h[100].sum tensor(-12.9562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(82.4603, device='cuda:0')



h[200].sum tensor(-567.1893, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.9895, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(296964.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0119, 0.2555, 0.0124],
        [0.0000, 0.0000, 0.0000,  ..., 0.0065, 0.2980, 0.0013],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.2422, 0.0180],
        ...,
        [0.0000, 0.0026, 0.0000,  ..., 0.0279, 0.0870, 0.0984],
        [0.0000, 0.0026, 0.0000,  ..., 0.0279, 0.0870, 0.0984],
        [0.0000, 0.0026, 0.0000,  ..., 0.0279, 0.0870, 0.0984]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2736009.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1234.8669, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(112512.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3328.9004, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3414.5376, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(818.4762, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2367],
        [ 0.2928],
        [ 0.0544],
        ...,
        [-3.2939],
        [-3.2863],
        [-3.2844]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-787644.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4675],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(855.4351, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4675],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(855.4351, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064,  0.0029, -0.0002,  ..., -0.0088, -0.0028, -0.0022],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0064,  0.0029, -0.0002,  ..., -0.0088, -0.0028, -0.0022],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-35554.3008, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.3528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.6879, device='cuda:0')



h[100].sum tensor(-18.7246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(119.0969, device='cuda:0')



h[200].sum tensor(-568.5309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(44.7580, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(315260.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0217, 0.1591, 0.0519],
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.2098, 0.0187],
        [0.0000, 0.0000, 0.0000,  ..., 0.0218, 0.1595, 0.0519],
        ...,
        [0.0000, 0.0026, 0.0000,  ..., 0.0279, 0.0870, 0.0984],
        [0.0000, 0.0026, 0.0000,  ..., 0.0279, 0.0870, 0.0984],
        [0.0000, 0.0026, 0.0000,  ..., 0.0279, 0.0870, 0.0984]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2817491.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1328.4028, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110637.3438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3381.9854, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4593.8291, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(864.5576, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9690],
        [-1.8401],
        [-2.0334],
        ...,
        [-3.2939],
        [-3.2863],
        [-3.2844]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-746983.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(765.2953, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(765.2953, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-36228.1055, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0746, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.5058, device='cuda:0')



h[100].sum tensor(-16.8047, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(106.5473, device='cuda:0')



h[200].sum tensor(-568.1468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.0417, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(313672.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0011, 0.0000,  ..., 0.0260, 0.1090, 0.0855],
        [0.0000, 0.0023, 0.0000,  ..., 0.0272, 0.0944, 0.0950],
        [0.0000, 0.0026, 0.0000,  ..., 0.0276, 0.0904, 0.0979],
        ...,
        [0.0000, 0.0025, 0.0000,  ..., 0.0274, 0.0873, 0.0991],
        [0.0000, 0.0025, 0.0000,  ..., 0.0274, 0.0873, 0.0991],
        [0.0000, 0.0025, 0.0000,  ..., 0.0274, 0.0873, 0.0991]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2825287., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1308.3497, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(111769.4844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3375.6272, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4417.1074, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(869.1671, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5721],
        [-1.9411],
        [-2.0692],
        ...,
        [-3.3302],
        [-3.3226],
        [-3.3206]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-771828.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(748.3770, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(748.3770, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-36524.3828, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.7209, device='cuda:0')



h[100].sum tensor(-15.8911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(104.1919, device='cuda:0')



h[200].sum tensor(-567.9909, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.1565, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(308888.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0014, 0.0000,  ..., 0.0256, 0.1060, 0.0878],
        [0.0000, 0.0028, 0.0000,  ..., 0.0270, 0.0911, 0.0975],
        [0.0000, 0.0029, 0.0000,  ..., 0.0273, 0.0888, 0.0993],
        ...,
        [0.0000, 0.0025, 0.0000,  ..., 0.0270, 0.0876, 0.0993],
        [0.0000, 0.0025, 0.0000,  ..., 0.0270, 0.0876, 0.0993],
        [0.0000, 0.0025, 0.0000,  ..., 0.0270, 0.0876, 0.0993]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2800931.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1317.9625, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(112048.8438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3370.8762, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3970.5225, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(842.8336, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2041],
        [-2.6787],
        [-2.8224],
        ...,
        [-3.3580],
        [-3.3504],
        [-3.3484]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-694041.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(717.0449, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(717.0449, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-36972.3164, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.2673, device='cuda:0')



h[100].sum tensor(-15.0595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(99.8297, device='cuda:0')



h[200].sum tensor(-567.8492, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.5172, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306496.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0011, 0.0000,  ..., 0.0243, 0.1073, 0.0889],
        [0.0000, 0.0000, 0.0000,  ..., 0.0167, 0.1571, 0.0568],
        [0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.2959, 0.0238],
        ...,
        [0.0000, 0.0019, 0.0000,  ..., 0.0268, 0.0880, 0.1009],
        [0.0000, 0.0019, 0.0000,  ..., 0.0268, 0.0880, 0.1009],
        [0.0000, 0.0019, 0.0000,  ..., 0.0264, 0.0924, 0.0980]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2780641.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1274.8796, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(113104.2734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3389.5818, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3985.6509, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(866.0348, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1747],
        [-1.1351],
        [-0.2733],
        ...,
        [-3.3906],
        [-3.3342],
        [-3.1026]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-778626.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(772.0468, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(772.0468, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2444e-03,  2.2552e-03, -7.6195e-05,  ..., -8.4000e-03,
         -1.4147e-03, -3.9529e-03],
        [-6.7433e-03,  3.7295e-03, -2.3536e-04,  ..., -9.3002e-03,
         -4.3701e-03,  3.7558e-04],
        [-6.7433e-03,  3.7295e-03, -2.3536e-04,  ..., -9.3002e-03,
         -4.3701e-03,  3.7558e-04],
        ...,
        [-6.0055e-03,  1.5494e-03,  0.0000e+00,  ..., -7.9690e-03,
          0.0000e+00, -6.0250e-03],
        [-6.0055e-03,  1.5494e-03,  0.0000e+00,  ..., -7.9690e-03,
          0.0000e+00, -6.0250e-03],
        [-6.0055e-03,  1.5494e-03,  0.0000e+00,  ..., -7.9690e-03,
          0.0000e+00, -6.0250e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-36687.3477, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0610, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.8191, device='cuda:0')



h[100].sum tensor(-15.9465, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(107.4873, device='cuda:0')



h[200].sum tensor(-568.1281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.3950, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0125, 0.0000,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(309171.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3994, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3705, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3573, 0.0000],
        ...,
        [0.0000, 0.0018, 0.0000,  ..., 0.0269, 0.0886, 0.1004],
        [0.0000, 0.0018, 0.0000,  ..., 0.0269, 0.0886, 0.1004],
        [0.0000, 0.0018, 0.0000,  ..., 0.0269, 0.0886, 0.1004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2788739.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1310.8550, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(111944.2969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3389.8936, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4393.2900, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(863.0050, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6513],
        [ 0.6517],
        [ 0.6537],
        ...,
        [-3.4302],
        [-3.4226],
        [-3.4206]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-731624.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(600.6669, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(600.6669, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        ...,
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-38081.0156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.8679, device='cuda:0')



h[100].sum tensor(-12.2388, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(83.6271, device='cuda:0')



h[200].sum tensor(-567.2582, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.4280, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(299988.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0235, 0.1228, 0.0778],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.1473, 0.0616],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.1967, 0.0364],
        ...,
        [0.0000, 0.0022, 0.0000,  ..., 0.0266, 0.0890, 0.0994],
        [0.0000, 0.0022, 0.0000,  ..., 0.0266, 0.0890, 0.0994],
        [0.0000, 0.0022, 0.0000,  ..., 0.0266, 0.0890, 0.0994]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2751613.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1256.9800, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(112904.4062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3331.2705, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3319.8984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(807.4350, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7678],
        [-0.5500],
        [-0.3076],
        ...,
        [-3.4462],
        [-3.4386],
        [-3.4367]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-704308.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(679.1522, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(679.1522, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6904e-03,  3.5651e-03, -1.9878e-04,  ..., -9.2049e-03,
         -3.9320e-03, -8.0168e-05],
        [-6.1977e-03,  2.1116e-03, -5.5785e-05,  ..., -8.3160e-03,
         -1.1034e-03, -4.3543e-03],
        [-6.0055e-03,  1.5445e-03,  0.0000e+00,  ..., -7.9692e-03,
          0.0000e+00, -6.0217e-03],
        ...,
        [-6.0055e-03,  1.5445e-03,  0.0000e+00,  ..., -7.9692e-03,
          0.0000e+00, -6.0217e-03],
        [-6.0055e-03,  1.5445e-03,  0.0000e+00,  ..., -7.9692e-03,
          0.0000e+00, -6.0217e-03],
        [-6.0055e-03,  1.5445e-03,  0.0000e+00,  ..., -7.9692e-03,
          0.0000e+00, -6.0217e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-37676.0195, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.5092, device='cuda:0')



h[100].sum tensor(-13.5861, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(94.5541, device='cuda:0')



h[200].sum tensor(-567.6530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.5345, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(304147.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0048, 0.3060, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.2822, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.2963, 0.0000],
        ...,
        [0.0000, 0.0019, 0.0000,  ..., 0.0264, 0.0893, 0.1004],
        [0.0000, 0.0019, 0.0000,  ..., 0.0264, 0.0893, 0.1004],
        [0.0000, 0.0019, 0.0000,  ..., 0.0264, 0.0893, 0.1004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2767834.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1275.2358, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(112537.4531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3350.4722, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3866.7476, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(831.2516, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6357],
        [ 0.6393],
        [ 0.6273],
        ...,
        [-3.4867],
        [-3.4787],
        [-3.4765]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-759694.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 170.0 event: 2550 loss: tensor(429.2160, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3137],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(676.3348, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3137],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(676.3348, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.5539e-03,  0.0000e+00,  ..., -7.9693e-03,
          0.0000e+00, -6.0183e-03],
        [-6.3026e-03,  2.4312e-03, -8.2215e-05,  ..., -8.5054e-03,
         -1.6794e-03, -3.4401e-03],
        [-6.2049e-03,  2.1425e-03, -5.5165e-05,  ..., -8.3290e-03,
         -1.1268e-03, -4.2883e-03],
        ...,
        [-6.0055e-03,  1.5539e-03,  0.0000e+00,  ..., -7.9693e-03,
          0.0000e+00, -6.0183e-03],
        [-6.0055e-03,  1.5539e-03,  0.0000e+00,  ..., -7.9693e-03,
          0.0000e+00, -6.0183e-03],
        [-6.0055e-03,  1.5539e-03,  0.0000e+00,  ..., -7.9693e-03,
          0.0000e+00, -6.0183e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-37846.2891, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.3785, device='cuda:0')



h[100].sum tensor(-13.3671, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(94.1619, device='cuda:0')



h[200].sum tensor(-567.6526, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.3871, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306107.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.2885, 0.0117],
        [0.0000, 0.0000, 0.0000,  ..., 0.0044, 0.3184, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.3454, 0.0000],
        ...,
        [0.0000, 0.0020, 0.0000,  ..., 0.0260, 0.0893, 0.1012],
        [0.0000, 0.0020, 0.0000,  ..., 0.0260, 0.0893, 0.1012],
        [0.0000, 0.0020, 0.0000,  ..., 0.0260, 0.0893, 0.1012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2787472.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1269.1545, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(112795.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3346.7708, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3778.0371, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(830.7509, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4108],
        [ 0.4489],
        [ 0.4177],
        ...,
        [-3.5426],
        [-3.5349],
        [-3.5329]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-836855.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2839],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(665.7146, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2839],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(665.7146, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.5539e-03,  0.0000e+00,  ..., -7.9693e-03,
          0.0000e+00, -6.0183e-03],
        [-6.2744e-03,  2.3479e-03, -7.4409e-05,  ..., -8.4545e-03,
         -1.5199e-03, -3.6849e-03],
        [-6.5241e-03,  3.0851e-03, -1.4350e-04,  ..., -8.9050e-03,
         -2.9313e-03, -1.5180e-03],
        ...,
        [-6.0055e-03,  1.5539e-03,  0.0000e+00,  ..., -7.9693e-03,
          0.0000e+00, -6.0183e-03],
        [-6.0055e-03,  1.5539e-03,  0.0000e+00,  ..., -7.9693e-03,
          0.0000e+00, -6.0183e-03],
        [-6.0055e-03,  1.5539e-03,  0.0000e+00,  ..., -7.9693e-03,
          0.0000e+00, -6.0183e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-37961.5078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.8858, device='cuda:0')



h[100].sum tensor(-13.0692, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.6833, device='cuda:0')



h[200].sum tensor(-567.5752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.8315, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(303220.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0189, 0.1623, 0.0543],
        [0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.2438, 0.0249],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.3133, 0.0015],
        ...,
        [0.0000, 0.0020, 0.0000,  ..., 0.0260, 0.0893, 0.1012],
        [0.0000, 0.0020, 0.0000,  ..., 0.0260, 0.0893, 0.1012],
        [0.0000, 0.0020, 0.0000,  ..., 0.0260, 0.0893, 0.1012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2773219.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1270.3868, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(113206.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3346.1199, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3510.0742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(813.3252, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3295],
        [-0.4160],
        [ 0.1724],
        ...,
        [-3.5428],
        [-3.5350],
        [-3.5330]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-778853.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(743.8781, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3022],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(743.8781, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.5504e-03,  0.0000e+00,  ..., -7.9693e-03,
          0.0000e+00, -6.0140e-03],
        [-6.2918e-03,  2.3963e-03, -7.5479e-05,  ..., -8.4859e-03,
         -1.5927e-03, -3.5293e-03],
        [-6.2136e-03,  2.1652e-03, -5.4858e-05,  ..., -8.3447e-03,
         -1.1576e-03, -4.2081e-03],
        ...,
        [-6.0055e-03,  1.5504e-03,  0.0000e+00,  ..., -7.9693e-03,
          0.0000e+00, -6.0140e-03],
        [-6.0055e-03,  1.5504e-03,  0.0000e+00,  ..., -7.9693e-03,
          0.0000e+00, -6.0140e-03],
        [-6.0055e-03,  1.5504e-03,  0.0000e+00,  ..., -7.9693e-03,
          0.0000e+00, -6.0140e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-37496.7578, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.5122, device='cuda:0')



h[100].sum tensor(-14.3404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(103.5655, device='cuda:0')



h[200].sum tensor(-567.9667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.9211, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(310022.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0194, 0.1588, 0.0562],
        [0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.2175, 0.0279],
        [0.0000, 0.0000, 0.0000,  ..., 0.0076, 0.2824, 0.0038],
        ...,
        [0.0000, 0.0028, 0.0000,  ..., 0.0254, 0.0892, 0.1010],
        [0.0000, 0.0028, 0.0000,  ..., 0.0254, 0.0892, 0.1010],
        [0.0000, 0.0028, 0.0000,  ..., 0.0254, 0.0892, 0.1010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2819866.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1342.3203, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(112632.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3351.0366, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3429.1086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(779.0367, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6507],
        [-0.1233],
        [ 0.1846],
        ...,
        [-3.5760],
        [-3.5683],
        [-3.5663]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-689167.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(617.3267, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(617.3267, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0065,  0.0029, -0.0001,  ..., -0.0088, -0.0026, -0.0019],
        [-0.0065,  0.0029, -0.0001,  ..., -0.0088, -0.0026, -0.0019],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-38520.6758, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.6408, device='cuda:0')



h[100].sum tensor(-11.7378, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(85.9466, device='cuda:0')



h[200].sum tensor(-567.3297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.2997, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0006],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0104, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(302107., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3659, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.3348, 0.0032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.3253, 0.0000],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0247, 0.0889, 0.1027],
        [0.0000, 0.0030, 0.0000,  ..., 0.0247, 0.0889, 0.1027],
        [0.0000, 0.0030, 0.0000,  ..., 0.0247, 0.0889, 0.1027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2781975.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1347.2766, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(115244.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3408.2629, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2667.5554, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(622.3776, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5403],
        [ 0.5631],
        [ 0.5854],
        ...,
        [-3.6268],
        [-3.6275],
        [-3.6271]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-819191.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(752.8810, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(752.8810, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-37249.4453, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.9299, device='cuda:0')



h[100].sum tensor(-14.1400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(104.8189, device='cuda:0')



h[200].sum tensor(-568.0374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.3922, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(314568.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0030, 0.0000,  ..., 0.0246, 0.0898, 0.1046],
        [0.0000, 0.0030, 0.0000,  ..., 0.0243, 0.0930, 0.1026],
        [0.0000, 0.0015, 0.0000,  ..., 0.0233, 0.1057, 0.0947],
        ...,
        [0.0000, 0.0027, 0.0000,  ..., 0.0244, 0.0890, 0.1046],
        [0.0000, 0.0027, 0.0000,  ..., 0.0244, 0.0890, 0.1046],
        [0.0000, 0.0027, 0.0000,  ..., 0.0244, 0.0890, 0.1046]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2817373.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1485.3057, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(115773.8438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3557.8188, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3027.8384, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(247.7505, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.7331],
        [-3.2663],
        [-2.5779],
        ...,
        [-3.6998],
        [-3.6917],
        [-3.6896]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-874016.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2744],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(692.5428, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2744],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(692.5428, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.5505e-03,  0.0000e+00,  ..., -7.9695e-03,
          0.0000e+00, -5.9984e-03],
        [-6.2654e-03,  2.3212e-03, -5.9150e-05,  ..., -8.4385e-03,
         -1.3789e-03, -3.7399e-03],
        [-6.0055e-03,  1.5505e-03,  0.0000e+00,  ..., -7.9695e-03,
          0.0000e+00, -5.9984e-03],
        ...,
        [-6.0055e-03,  1.5505e-03,  0.0000e+00,  ..., -7.9695e-03,
          0.0000e+00, -5.9984e-03],
        [-6.0055e-03,  1.5505e-03,  0.0000e+00,  ..., -7.9695e-03,
          0.0000e+00, -5.9984e-03],
        [-6.0055e-03,  1.5505e-03,  0.0000e+00,  ..., -7.9695e-03,
          0.0000e+00, -5.9984e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-38035.4062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7676, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.1305, device='cuda:0')



h[100].sum tensor(-12.7770, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.4184, device='cuda:0')



h[200].sum tensor(-567.7229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.2352, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(308998.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.1487, 0.0647],
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.1794, 0.0450],
        [0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.2275, 0.0148],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0240, 0.0891, 0.1026],
        [0.0000, 0.0034, 0.0000,  ..., 0.0240, 0.0891, 0.1026],
        [0.0000, 0.0034, 0.0000,  ..., 0.0240, 0.0891, 0.1026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2812953.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1429.2439, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(115526.4062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3455.4424, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2620.1348, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(539.7217, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4942],
        [-0.9601],
        [-0.6653],
        ...,
        [-3.6983],
        [-3.6903],
        [-3.6883]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-723198., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(844.3544, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(844.3544, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-37015.2266, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.3500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.1738, device='cuda:0')



h[100].sum tensor(-15.4216, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(117.5542, device='cuda:0')



h[200].sum tensor(-568.5278, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(44.1782, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(321495.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0013, 0.0000,  ..., 0.0229, 0.1039, 0.0952],
        [0.0000, 0.0027, 0.0000,  ..., 0.0238, 0.0936, 0.1017],
        [0.0000, 0.0028, 0.0000,  ..., 0.0241, 0.0910, 0.1037],
        ...,
        [0.0000, 0.0024, 0.0000,  ..., 0.0239, 0.0898, 0.1037],
        [0.0000, 0.0024, 0.0000,  ..., 0.0239, 0.0898, 0.1037],
        [0.0000, 0.0024, 0.0000,  ..., 0.0239, 0.0898, 0.1037]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2864784.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1484.1562, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(114248.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3528.8892, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4507.3335, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(585.0304, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.7684],
        [-3.9854],
        [-4.0098],
        ...,
        [-3.7481],
        [-3.7401],
        [-3.7380]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-819719.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(695.6584, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(695.6584, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        ...,
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-38387.4766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.2750, device='cuda:0')



h[100].sum tensor(-12.4142, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.8522, device='cuda:0')



h[200].sum tensor(-567.7375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.3982, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(311023.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0023, 0.0000,  ..., 0.0233, 0.0952, 0.0990],
        [0.0000, 0.0020, 0.0000,  ..., 0.0232, 0.0965, 0.0982],
        [0.0000, 0.0020, 0.0000,  ..., 0.0232, 0.0980, 0.0975],
        ...,
        [0.0000, 0.0027, 0.0000,  ..., 0.0233, 0.0926, 0.1002],
        [0.0000, 0.0027, 0.0000,  ..., 0.0235, 0.0900, 0.1019],
        [0.0000, 0.0027, 0.0000,  ..., 0.0235, 0.0900, 0.1019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2819800.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1411.6099, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(115465., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3434.3848, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3274.4585, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(592.0656, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0440],
        [-3.1221],
        [-3.0956],
        ...,
        [-3.0444],
        [-3.5572],
        [-3.6912]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-744429.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(588.2256, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(588.2256, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        ...,
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-39452.9766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3746, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.2907, device='cuda:0')



h[100].sum tensor(-10.2981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(81.8950, device='cuda:0')



h[200].sum tensor(-567.1805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.7771, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(301110.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0230, 0.0909, 0.1011],
        [0.0000, 0.0033, 0.0000,  ..., 0.0230, 0.0909, 0.1011],
        [0.0000, 0.0034, 0.0000,  ..., 0.0231, 0.0913, 0.1011],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0229, 0.0900, 0.1012],
        [0.0000, 0.0030, 0.0000,  ..., 0.0229, 0.0900, 0.1012],
        [0.0000, 0.0030, 0.0000,  ..., 0.0229, 0.0900, 0.1012]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2773275., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1348.9089, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(117113.4219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3364.7114, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2236.5024, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(614.4714, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.0894],
        [-3.9377],
        [-3.6236],
        ...,
        [-3.7680],
        [-3.7602],
        [-3.7582]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-708893., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2742]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(767.4426, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2742]], device='cuda:0') 
g.ndata[nfet].sum tensor(767.4426, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.5502e-03,  0.0000e+00,  ..., -7.9697e-03,
          0.0000e+00, -5.9851e-03],
        [-6.0055e-03,  1.5502e-03,  0.0000e+00,  ..., -7.9697e-03,
          0.0000e+00, -5.9851e-03],
        [-6.0055e-03,  1.5502e-03,  0.0000e+00,  ..., -7.9697e-03,
          0.0000e+00, -5.9851e-03],
        ...,
        [-6.0055e-03,  1.5502e-03,  0.0000e+00,  ..., -7.9697e-03,
          0.0000e+00, -5.9851e-03],
        [-6.5444e-03,  3.1395e-03, -1.0018e-04,  ..., -8.9421e-03,
         -2.6815e-03, -1.2975e-03],
        [-6.2847e-03,  2.3737e-03, -5.1907e-05,  ..., -8.4735e-03,
         -1.3894e-03, -3.5562e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-38420.1602, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0427, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.6055, device='cuda:0')



h[100].sum tensor(-13.2419, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(106.8463, device='cuda:0')



h[200].sum tensor(-568.1036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.1541, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(312418.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.1403, 0.0728],
        [0.0000, 0.0007, 0.0000,  ..., 0.0213, 0.1048, 0.0957],
        [0.0000, 0.0016, 0.0000,  ..., 0.0221, 0.0977, 0.1004],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0087, 0.2240, 0.0323],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.2755, 0.0122],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.3163, 0.0011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2828561.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1381.7388, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(117083.8750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3469.4951, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4264.8110, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(740.6053, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5540],
        [-3.0968],
        [-3.1237],
        ...,
        [-0.9104],
        [ 0.0152],
        [ 0.5205]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-863054.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 180.0 event: 2700 loss: tensor(450.3270, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4451],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(686.5952, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4451],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(686.5952, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.4270e-03,  2.7829e-03, -7.4409e-05,  ..., -8.7303e-03,
         -2.0638e-03, -2.3139e-03],
        [-6.0055e-03,  1.5428e-03,  0.0000e+00,  ..., -7.9697e-03,
          0.0000e+00, -5.9816e-03],
        [-6.4270e-03,  2.7829e-03, -7.4409e-05,  ..., -8.7303e-03,
         -2.0638e-03, -2.3139e-03],
        ...,
        [-6.4665e-03,  2.8992e-03, -8.1383e-05,  ..., -8.8016e-03,
         -2.2572e-03, -1.9701e-03],
        [-6.4665e-03,  2.8992e-03, -8.1383e-05,  ..., -8.8016e-03,
         -2.2572e-03, -1.9701e-03],
        [-6.0055e-03,  1.5428e-03,  0.0000e+00,  ..., -7.9697e-03,
          0.0000e+00, -5.9816e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-39164.8203, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.8546, device='cuda:0')



h[100].sum tensor(-11.6329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(95.5904, device='cuda:0')



h[200].sum tensor(-567.6846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.9240, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(308307.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.1903, 0.0406],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.2811, 0.0034],
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.2865, 0.0058],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.2182, 0.0196],
        [0.0000, 0.0000, 0.0000,  ..., 0.0101, 0.2182, 0.0196],
        [0.0000, 0.0000, 0.0000,  ..., 0.0123, 0.1959, 0.0341]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2805096., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1356.9423, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(116676.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3411.2715, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4131.9316, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(754.1122, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1663],
        [ 0.2383],
        [ 0.4407],
        ...,
        [-1.5906],
        [-1.5857],
        [-2.1077]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-795819.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(656.0895, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(656.0895, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.5245e-03,  0.0000e+00,  ..., -7.9697e-03,
          0.0000e+00, -5.9763e-03],
        [-6.2143e-03,  2.1376e-03, -3.4984e-05,  ..., -8.3465e-03,
         -1.0058e-03, -4.1587e-03],
        [-6.6393e-03,  3.3854e-03, -1.0619e-04,  ..., -9.1134e-03,
         -3.0531e-03, -4.5925e-04],
        ...,
        [-6.0055e-03,  1.5245e-03,  0.0000e+00,  ..., -7.9697e-03,
          0.0000e+00, -5.9763e-03],
        [-6.0055e-03,  1.5245e-03,  0.0000e+00,  ..., -7.9697e-03,
          0.0000e+00, -5.9763e-03],
        [-6.0055e-03,  1.5245e-03,  0.0000e+00,  ..., -7.9697e-03,
          0.0000e+00, -5.9763e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-39484.7578, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.4392, device='cuda:0')



h[100].sum tensor(-10.9297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.3433, device='cuda:0')



h[200].sum tensor(-567.5282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.3279, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(305730.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3887e-03, 2.5707e-01,
         1.2508e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6741e-03, 3.0347e-01,
         5.6847e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3973e-05, 3.3149e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.9108e-02, 1.2251e-01,
         7.9700e-02],
        [0.0000e+00, 6.4581e-04, 0.0000e+00,  ..., 2.1059e-02, 1.0188e-01,
         9.3278e-02],
        [0.0000e+00, 1.2916e-03, 0.0000e+00,  ..., 2.2035e-02, 9.1569e-02,
         1.0007e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2792188., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1367.3293, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(116217.8281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3353.9468, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3460.8955, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(729.1936, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4217],
        [ 0.5903],
        [ 0.6706],
        ...,
        [-2.6579],
        [-3.2463],
        [-3.6268]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-664717.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(736.2992, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(736.2992, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        ...,
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-39115.8672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9359, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.1606, device='cuda:0')



h[100].sum tensor(-12.1199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(102.5104, device='cuda:0')



h[200].sum tensor(-567.9561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.5246, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(313614.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.1232, 0.0826],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.1098, 0.0911],
        [0.0000, 0.0000, 0.0000,  ..., 0.0208, 0.1044, 0.0948],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0215, 0.0915, 0.1023],
        [0.0000, 0.0000, 0.0000,  ..., 0.0215, 0.0915, 0.1023],
        [0.0000, 0.0000, 0.0000,  ..., 0.0215, 0.0915, 0.1023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2839456.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1393.5343, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(116201.3516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3421.5422, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5252.5903, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(810.4340, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0887],
        [-2.4104],
        [-2.6899],
        ...,
        [-3.9259],
        [-3.9178],
        [-3.9158]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-745585.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(606.7572, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(606.7572, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0060]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-40406.3672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.1505, device='cuda:0')



h[100].sum tensor(-9.7839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(84.4750, device='cuda:0')



h[200].sum tensor(-567.2796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.7467, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(299752.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0090, 0.1974, 0.0372],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.1665, 0.0571],
        [0.0000, 0.0000, 0.0000,  ..., 0.0102, 0.2072, 0.0333],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0208, 0.0912, 0.1047],
        [0.0000, 0.0000, 0.0000,  ..., 0.0208, 0.0912, 0.1047],
        [0.0000, 0.0000, 0.0000,  ..., 0.0208, 0.0912, 0.1047]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2767444.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1294.1841, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(119895.8594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3420.7749, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4492.4326, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(813.2126, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0298],
        [-0.1598],
        [-0.1702],
        ...,
        [-3.9935],
        [-3.9853],
        [-3.9833]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-913681.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5459],
        [0.4412],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(626.6273, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5459],
        [0.4412],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(626.6273, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.3459e-03,  5.4437e-03, -1.9144e-04,  ..., -1.0389e-02,
         -6.1476e-03,  5.7332e-03],
        [-6.4233e-03,  2.7424e-03, -5.9674e-05,  ..., -8.7237e-03,
         -1.9162e-03, -2.3119e-03],
        [-6.9503e-03,  4.2853e-03, -1.3494e-04,  ..., -9.6746e-03,
         -4.3330e-03,  2.2831e-03],
        ...,
        [-6.0055e-03,  1.5191e-03,  0.0000e+00,  ..., -7.9698e-03,
          0.0000e+00, -5.9553e-03],
        [-6.0055e-03,  1.5191e-03,  0.0000e+00,  ..., -7.9698e-03,
          0.0000e+00, -5.9553e-03],
        [-6.0055e-03,  1.5191e-03,  0.0000e+00,  ..., -7.9698e-03,
          0.0000e+00, -5.9553e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-40524.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.0723, device='cuda:0')



h[100].sum tensor(-9.8958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.2414, device='cuda:0')



h[200].sum tensor(-567.3687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.7863, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0171, 0.0000,  ..., 0.0000, 0.0000, 0.0089],
        [0.0000, 0.0141, 0.0000,  ..., 0.0000, 0.0000, 0.0023],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(303827.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4294, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5083, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4679, 0.0000],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0207, 0.0917, 0.1018],
        [0.0000, 0.0009, 0.0000,  ..., 0.0207, 0.0917, 0.1018],
        [0.0000, 0.0009, 0.0000,  ..., 0.0207, 0.0917, 0.1018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2796497.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1358.6772, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(118211.9766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3364.0259, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3640.8665, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(733.2253, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7054],
        [ 0.7108],
        [ 0.7106],
        ...,
        [-3.9703],
        [-3.9622],
        [-3.9602]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-712805.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3396],
        [0.2639],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(658.0156, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3396],
        [0.2639],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(658.0156, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.4677e-03,  2.9043e-03, -6.2522e-05,  ..., -8.8037e-03,
         -2.0849e-03, -1.9132e-03],
        [-6.5393e-03,  3.1147e-03, -7.2219e-05,  ..., -8.9331e-03,
         -2.4083e-03, -1.2876e-03],
        [-6.5154e-03,  3.0443e-03, -6.8974e-05,  ..., -8.8898e-03,
         -2.3001e-03, -1.4970e-03],
        ...,
        [-6.0055e-03,  1.5476e-03,  0.0000e+00,  ..., -7.9698e-03,
          0.0000e+00, -5.9462e-03],
        [-6.0055e-03,  1.5476e-03,  0.0000e+00,  ..., -7.9698e-03,
          0.0000e+00, -5.9462e-03],
        [-6.0055e-03,  1.5476e-03,  0.0000e+00,  ..., -7.9698e-03,
          0.0000e+00, -5.9462e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-40475.7891, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6382, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.5286, device='cuda:0')



h[100].sum tensor(-10.2657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.6114, device='cuda:0')



h[200].sum tensor(-567.5447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.4286, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(310342.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.4855e-03, 2.9148e-01,
         2.5833e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0427e-04, 3.0613e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.0937e-03, 2.7079e-01,
         6.8854e-03],
        ...,
        [0.0000e+00, 5.2110e-04, 0.0000e+00,  ..., 2.0491e-02, 9.1891e-02,
         1.0188e-01],
        [0.0000e+00, 5.2110e-04, 0.0000e+00,  ..., 2.0491e-02, 9.1891e-02,
         1.0188e-01],
        [0.0000e+00, 5.2110e-04, 0.0000e+00,  ..., 2.0491e-02, 9.1891e-02,
         1.0188e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2829707.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1438.0227, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(116761.6484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3323.3369, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4405.0225, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(849.9244, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1400],
        [ 0.3247],
        [ 0.5027],
        ...,
        [-3.9275],
        [-3.9785],
        [-3.9934]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-717149.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(577.7657, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(577.7657, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.5942e-03,  0.0000e+00,  ..., -7.9698e-03,
          0.0000e+00, -5.9373e-03],
        [-6.0055e-03,  1.5942e-03,  0.0000e+00,  ..., -7.9698e-03,
          0.0000e+00, -5.9373e-03],
        [-6.2655e-03,  2.3598e-03, -3.3295e-05,  ..., -8.4389e-03,
         -1.1536e-03, -3.6671e-03],
        ...,
        [-6.0055e-03,  1.5942e-03,  0.0000e+00,  ..., -7.9698e-03,
          0.0000e+00, -5.9373e-03],
        [-6.0055e-03,  1.5942e-03,  0.0000e+00,  ..., -7.9698e-03,
          0.0000e+00, -5.9373e-03],
        [-6.0055e-03,  1.5942e-03,  0.0000e+00,  ..., -7.9698e-03,
          0.0000e+00, -5.9373e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-41326.4102, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.8054, device='cuda:0')



h[100].sum tensor(-8.8457, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(80.4387, device='cuda:0')



h[200].sum tensor(-567.1288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.2298, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0000, 0.0012],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(303454.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.1398, 0.0734],
        [0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.1884, 0.0422],
        [0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.2633, 0.0165],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0918, 0.1035],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0918, 0.1035],
        [0.0000, 0.0000, 0.0000,  ..., 0.0203, 0.0918, 0.1035]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2779865., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1436.5596, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(117606.5469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3250.1733, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4461.8433, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(994.4220, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1432],
        [-0.9649],
        [-0.6211],
        ...,
        [-4.0684],
        [-4.0596],
        [-4.0571]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-889889.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1021.8862, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1021.8862, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-38093.5195, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-616.0345, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(47.4104, device='cuda:0')



h[100].sum tensor(-15.6061, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(142.2709, device='cuda:0')



h[200].sum tensor(-569.4742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(53.4670, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(341794.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0192, 0.1046, 0.0948],
        [0.0000, 0.0007, 0.0000,  ..., 0.0201, 0.0951, 0.1010],
        [0.0000, 0.0010, 0.0000,  ..., 0.0203, 0.0936, 0.1022],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0202, 0.0918, 0.1026],
        [0.0000, 0.0007, 0.0000,  ..., 0.0202, 0.0918, 0.1026],
        [0.0000, 0.0007, 0.0000,  ..., 0.0202, 0.0918, 0.1026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2979322.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1601.7771, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(113309., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3356.3887, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(6273.8389, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(953.3644, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.4979],
        [-3.1249],
        [-3.4551],
        ...,
        [-4.0822],
        [-4.0740],
        [-4.0720]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-749685.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(671.1691, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(671.1691, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        ...,
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-41346.5234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.1389, device='cuda:0')



h[100].sum tensor(-9.9026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.4427, device='cuda:0')



h[200].sum tensor(-567.5966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.1168, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(307940.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0011, 0.0000,  ..., 0.0180, 0.1115, 0.0891],
        [0.0000, 0.0014, 0.0000,  ..., 0.0199, 0.0954, 0.0998],
        [0.0000, 0.0003, 0.0000,  ..., 0.0190, 0.1047, 0.0940],
        ...,
        [0.0000, 0.0019, 0.0000,  ..., 0.0200, 0.0915, 0.1019],
        [0.0000, 0.0019, 0.0000,  ..., 0.0200, 0.0915, 0.1019],
        [0.0000, 0.0019, 0.0000,  ..., 0.0200, 0.0915, 0.1019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2821622.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1436.8514, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(117238.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3293.9568, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2551.5215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(680.1427, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1392],
        [-1.3274],
        [-1.1130],
        ...,
        [-4.0892],
        [-4.0811],
        [-4.0791]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-671836.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3145],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(939.4324, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3145],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(939.4324, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.5999e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.9223e-03],
        [-6.9524e-03,  4.3783e-03, -1.0255e-04,  ..., -9.6785e-03,
         -3.9967e-03,  2.3556e-03],
        [-6.6546e-03,  3.5044e-03, -7.0298e-05,  ..., -9.1411e-03,
         -2.7397e-03, -2.4795e-04],
        ...,
        [-6.0055e-03,  1.5999e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.9223e-03],
        [-6.0055e-03,  1.5999e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.9223e-03],
        [-6.0055e-03,  1.5999e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.9223e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-39765.9688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.6615, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(43.5849, device='cuda:0')



h[100].sum tensor(-13.6108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(130.7913, device='cuda:0')



h[200].sum tensor(-568.9587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(49.1529, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0172, 0.0000,  ..., 0.0000, 0.0000, 0.0086],
        [0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0008],
        [0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.0000, 0.0024],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(327639., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.0286e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.6501e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.6782e-04, 3.0853e-01,
         1.1240e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.9740e-02, 9.0433e-02,
         1.0723e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.9740e-02, 9.0433e-02,
         1.0723e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.9740e-02, 9.0433e-02,
         1.0723e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2914250.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1584.3127, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(115824.0938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3280.2070, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5324.1128, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(807.1150, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5071],
        [ 0.3861],
        [-0.1313],
        ...,
        [-4.2164],
        [-4.2077],
        [-4.2056]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-954013.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 190.0 event: 2850 loss: tensor(486.5877, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3274],
        [0.3391],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(682.7941, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3274],
        [0.3391],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(682.7941, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6430e-03,  3.4300e-03, -6.5215e-05,  ..., -9.1202e-03,
         -2.6460e-03, -3.4988e-04],
        [-6.3156e-03,  2.4752e-03, -3.1721e-05,  ..., -8.5294e-03,
         -1.2870e-03, -3.2119e-03],
        [-6.3267e-03,  2.5076e-03, -3.2856e-05,  ..., -8.5494e-03,
         -1.3331e-03, -3.1149e-03],
        ...,
        [-6.0055e-03,  1.5710e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.9223e-03],
        [-6.0055e-03,  1.5710e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.9223e-03],
        [-6.0055e-03,  1.5710e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.9223e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-42138.5742, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.6782, device='cuda:0')



h[100].sum tensor(-9.7822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(95.0612, device='cuda:0')



h[200].sum tensor(-567.6737, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.7251, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(305017.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3594, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.2989, 0.0058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.2172, 0.0280],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0910, 0.1064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0910, 0.1064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0910, 0.1064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2798440.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1426.3997, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(117298.1484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3321.4341, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3403.3364, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(741.7003, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6323],
        [ 0.4731],
        [ 0.2409],
        ...,
        [-4.2208],
        [-4.2122],
        [-4.2101]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-914247.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(806.7513, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(806.7513, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        ...,
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-41387.9961, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.4292, device='cuda:0')



h[100].sum tensor(-11.3112, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(112.3190, device='cuda:0')



h[200].sum tensor(-568.2910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.2108, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(317584.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0116, 0.1677, 0.0545],
        [0.0000, 0.0006, 0.0000,  ..., 0.0182, 0.1110, 0.0904],
        [0.0000, 0.0012, 0.0000,  ..., 0.0198, 0.0964, 0.1003],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0198, 0.0927, 0.1020],
        [0.0000, 0.0008, 0.0000,  ..., 0.0198, 0.0927, 0.1020],
        [0.0000, 0.0008, 0.0000,  ..., 0.0198, 0.0927, 0.1020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2852680., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1528.2255, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(112834.2266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3313.3643, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3499.1455, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(680.6806, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.9976],
        [-1.9902],
        [-2.7408],
        ...,
        [-4.1557],
        [-4.1477],
        [-4.1457]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-576134., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.7583],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(749.2865, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.7583],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(749.2865, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.5690e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.9198e-03],
        [-6.7237e-03,  3.6497e-03, -6.5430e-05,  ..., -9.2658e-03,
         -2.8820e-03,  3.5844e-04],
        [-6.0055e-03,  1.5690e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.9198e-03],
        ...,
        [-6.0055e-03,  1.5690e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.9198e-03],
        [-6.0055e-03,  1.5690e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.9198e-03],
        [-6.0055e-03,  1.5690e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.9198e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-42316.7734, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.7631, device='cuda:0')



h[100].sum tensor(-10.3812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(104.3185, device='cuda:0')



h[200].sum tensor(-568.0190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.2041, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0024],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0164, 0.0000,  ..., 0.0000, 0.0000, 0.0077],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(312678.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2999, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2881, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4155, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0923, 0.1048],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0923, 0.1048],
        [0.0000, 0.0000, 0.0000,  ..., 0.0201, 0.0923, 0.1048]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2824272.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1493.8970, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(112033.8281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3329.8687, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4600.7866, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(751.9939, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6608],
        [ 0.7095],
        [ 0.7223],
        ...,
        [-4.2341],
        [-4.2258],
        [-4.2238]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-772044., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5645],
        [0.3079],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(864.6859, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5645],
        [0.3079],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(864.6859, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.9872e-03,  4.3895e-03, -8.4325e-05,  ..., -9.7414e-03,
         -3.8734e-03,  2.6649e-03],
        [-6.7368e-03,  3.6668e-03, -6.2816e-05,  ..., -9.2895e-03,
         -2.8854e-03,  4.7560e-04],
        [-6.2971e-03,  2.3976e-03, -2.5045e-05,  ..., -8.4960e-03,
         -1.1504e-03, -3.3689e-03],
        ...,
        [-6.0055e-03,  1.5561e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.9180e-03],
        [-6.0055e-03,  1.5561e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.9180e-03],
        [-6.0055e-03,  1.5561e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.9180e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-41903.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.4135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.1171, device='cuda:0')



h[100].sum tensor(-11.7863, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(120.3848, device='cuda:0')



h[200].sum tensor(-568.6160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(45.2420, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0179, 0.0000,  ..., 0.0000, 0.0000, 0.0116],
        [0.0000, 0.0148, 0.0000,  ..., 0.0000, 0.0000, 0.0062],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0000, 0.0005],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(322234.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5709, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4741, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.3292, 0.0168],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0912, 0.1076],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0912, 0.1076],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0912, 0.1076]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2880562.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1541.1621, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(111954.8984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3385.6367, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(5234.6865, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(801.8173, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5740],
        [ 0.4479],
        [-0.0177],
        ...,
        [-4.3021],
        [-4.2935],
        [-4.2914]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-849555.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(639.9918, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(639.9918, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0014,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0014,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        ...,
        [-0.0060,  0.0014,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0014,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0014,  0.0000,  ..., -0.0080,  0.0000, -0.0059]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-44013.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.6924, device='cuda:0')



h[100].sum tensor(-8.5820, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.1021, device='cuda:0')



h[200].sum tensor(-567.4665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.4856, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(304199.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5376e-02, 1.2342e-01,
         8.6595e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.7460e-02, 1.0439e-01,
         9.9072e-02],
        [0.0000e+00, 9.8047e-05, 0.0000e+00,  ..., 1.8496e-02, 9.6166e-02,
         1.0469e-01],
        ...,
        [0.0000e+00, 4.5281e-05, 0.0000e+00,  ..., 1.8762e-02, 9.0541e-02,
         1.0763e-01],
        [0.0000e+00, 4.5281e-05, 0.0000e+00,  ..., 1.8762e-02, 9.0541e-02,
         1.0763e-01],
        [0.0000e+00, 4.5281e-05, 0.0000e+00,  ..., 1.8762e-02, 9.0541e-02,
         1.0763e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2798449.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1477.1957, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(115850.5859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3317.9612, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2469.3606, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(690.6844, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.6368],
        [-2.5348],
        [-3.2597],
        ...,
        [-4.3103],
        [-4.3017],
        [-4.2997]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-801845.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5654],
        [0.6240],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(734.3351, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5654],
        [0.6240],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(734.3351, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.6658e-03,  6.2600e-03, -1.2651e-04,  ..., -1.0966e-02,
         -6.3311e-03,  8.6120e-03],
        [-7.5768e-03,  6.0052e-03, -1.1973e-04,  ..., -1.0805e-02,
         -5.9915e-03,  7.8329e-03],
        [-7.0957e-03,  4.6285e-03, -8.3070e-05,  ..., -9.9371e-03,
         -4.1571e-03,  3.6250e-03],
        ...,
        [-6.5216e-03,  2.9856e-03, -3.9325e-05,  ..., -8.9012e-03,
         -1.9679e-03, -1.3969e-03],
        [-6.0055e-03,  1.5087e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.9112e-03],
        [-6.0055e-03,  1.5087e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.9112e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-43825.9766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.0694, device='cuda:0')



h[100].sum tensor(-9.6383, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(102.2369, device='cuda:0')



h[200].sum tensor(-567.9369, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.4218, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0247, 0.0000,  ..., 0.0000, 0.0000, 0.0333],
        [0.0000, 0.0225, 0.0000,  ..., 0.0000, 0.0000, 0.0265],
        [0.0000, 0.0165, 0.0000,  ..., 0.0000, 0.0000, 0.0132],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306911.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8847, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7945, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6435, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.2717, 0.0138],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.2193, 0.0358],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.1334, 0.0815]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2793045.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1488.1006, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(114752.2188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3331.5984, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2888.5486, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(751.0394, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5436],
        [ 0.5593],
        [ 0.5878],
        ...,
        [-0.0979],
        [-1.1003],
        [-2.4956]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-964138.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(723.8710, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(723.8710, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        ...,
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-44102.4062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8764, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.5840, device='cuda:0')



h[100].sum tensor(-9.3106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(100.7801, device='cuda:0')



h[200].sum tensor(-567.8738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.8743, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(308334.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0960, 0.1041],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0961, 0.1041],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0969, 0.1038],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0903, 0.1074],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0903, 0.1074],
        [0.0000, 0.0000, 0.0000,  ..., 0.0198, 0.0903, 0.1074]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2788116.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1536.1022, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110771.7734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3257.4421, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3066.7163, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(698.8849, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4278],
        [-3.6264],
        [-3.8233],
        ...,
        [-4.3567],
        [-4.3483],
        [-4.3462]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-837660.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2944],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(756.7474, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2944],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(756.7474, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.4962e-03,  2.9271e-03, -3.3076e-05,  ..., -8.8553e-03,
         -1.8079e-03, -1.5926e-03],
        [-6.2173e-03,  2.1225e-03, -1.4279e-05,  ..., -8.3522e-03,
         -7.8048e-04, -4.0357e-03],
        [-6.2844e-03,  2.3159e-03, -1.8797e-05,  ..., -8.4731e-03,
         -1.0275e-03, -3.4485e-03],
        ...,
        [-6.0055e-03,  1.5113e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.8915e-03],
        [-6.0055e-03,  1.5113e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.8915e-03],
        [-6.0055e-03,  1.5113e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.8915e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-43894.1328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0136, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.1093, device='cuda:0')



h[100].sum tensor(-9.6163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(105.3572, device='cuda:0')



h[200].sum tensor(-568.0635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.5945, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(316339.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.2625, 0.0035],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.2548, 0.0076],
        [0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.1988, 0.0352],
        ...,
        [0.0000, 0.0010, 0.0000,  ..., 0.0202, 0.0912, 0.1029],
        [0.0000, 0.0010, 0.0000,  ..., 0.0202, 0.0912, 0.1029],
        [0.0000, 0.0010, 0.0000,  ..., 0.0202, 0.0912, 0.1029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2823676., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1625.2922, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(107171.9922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3181.4092, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2781.9180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(598.9055, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3994],
        [-0.1124],
        [-0.8598],
        ...,
        [-4.2946],
        [-4.2865],
        [-4.2846]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-589661.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(673.5757, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(673.5757, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2434e-03,  2.2006e-03, -1.5065e-05,  ..., -8.3992e-03,
         -8.6148e-04, -3.7981e-03],
        [-6.2434e-03,  2.2006e-03, -1.5065e-05,  ..., -8.3992e-03,
         -8.6148e-04, -3.7981e-03],
        [-6.0055e-03,  1.5134e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.8835e-03],
        ...,
        [-6.0055e-03,  1.5134e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.8835e-03],
        [-6.0055e-03,  1.5134e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.8835e-03],
        [-6.0055e-03,  1.5134e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.8835e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-44894.2734, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.2505, device='cuda:0')



h[100].sum tensor(-8.4105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.7778, device='cuda:0')



h[200].sum tensor(-567.6372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.2428, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(310532.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0090, 0.2102, 0.0269],
        [0.0000, 0.0000, 0.0000,  ..., 0.0079, 0.2228, 0.0203],
        [0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.1949, 0.0386],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0199, 0.0903, 0.1041],
        [0.0000, 0.0015, 0.0000,  ..., 0.0199, 0.0903, 0.1041],
        [0.0000, 0.0015, 0.0000,  ..., 0.0199, 0.0903, 0.1041]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2802505.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1596.7886, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(108026.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3148.5754, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2243.0730, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(569.8622, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6957],
        [-0.6218],
        [-1.0943],
        ...,
        [-4.3335],
        [-4.3255],
        [-4.3236]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-631741.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(720.0071, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(720.0071, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-45050.5547, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.4047, device='cuda:0')



h[100].sum tensor(-8.8390, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(100.2421, device='cuda:0')



h[200].sum tensor(-567.8790, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.6721, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(310565.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0011, 0.0000,  ..., 0.0191, 0.0919, 0.1077],
        [0.0000, 0.0006, 0.0000,  ..., 0.0144, 0.1228, 0.0878],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.2121, 0.0435],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0192, 0.0881, 0.1096],
        [0.0000, 0.0008, 0.0000,  ..., 0.0192, 0.0881, 0.1096],
        [0.0000, 0.0008, 0.0000,  ..., 0.0192, 0.0881, 0.1096]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2802630., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1573.3254, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109595.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3201.0713, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2411.8208, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(649.9462, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0900],
        [-1.1691],
        [-0.4376],
        ...,
        [-4.4535],
        [-4.4449],
        [-4.4429]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-902615.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 200.0 event: 3000 loss: tensor(496.1729, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(715.7893, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(715.7893, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-45565.1172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.2090, device='cuda:0')



h[100].sum tensor(-8.5761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(99.6549, device='cuda:0')



h[200].sum tensor(-567.8338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.4515, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(307950.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 1.2558e-04, 0.0000e+00,  ..., 1.2949e-02, 1.3107e-01,
         8.5419e-02],
        [0.0000e+00, 5.7400e-04, 0.0000e+00,  ..., 1.6726e-02, 1.0331e-01,
         1.0336e-01],
        [0.0000e+00, 4.4842e-04, 0.0000e+00,  ..., 1.7343e-02, 1.0017e-01,
         1.0571e-01],
        ...,
        [0.0000e+00, 9.2755e-04, 0.0000e+00,  ..., 1.8240e-02, 8.6238e-02,
         1.1378e-01],
        [0.0000e+00, 9.2755e-04, 0.0000e+00,  ..., 1.8240e-02, 8.6238e-02,
         1.1378e-01],
        [0.0000e+00, 9.2755e-04, 0.0000e+00,  ..., 1.8240e-02, 8.6238e-02,
         1.1378e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2800886., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1549.6875, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(112047.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3233.0933, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2224.1025, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(677.7036, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3730],
        [-2.3344],
        [-2.9824],
        ...,
        [-4.5412],
        [-4.5325],
        [-4.5308]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1073350.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(742.8186, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(742.8186, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        ...,
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0059]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-45437.7578, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.4630, device='cuda:0')



h[100].sum tensor(-8.7328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(103.4180, device='cuda:0')



h[200].sum tensor(-567.9681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.8657, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(308821.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0027, 0.0000,  ..., 0.0179, 0.0911, 0.1088],
        [0.0000, 0.0014, 0.0000,  ..., 0.0166, 0.1042, 0.1004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0135, 0.1388, 0.0786],
        ...,
        [0.0000, 0.0024, 0.0000,  ..., 0.0180, 0.0871, 0.1108],
        [0.0000, 0.0024, 0.0000,  ..., 0.0180, 0.0871, 0.1108],
        [0.0000, 0.0024, 0.0000,  ..., 0.0180, 0.0871, 0.1108]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2794568., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1603.5376, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(111762.9297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3192.6284, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1744.2108, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(584.9357, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.7871],
        [-4.3814],
        [-3.7701],
        ...,
        [-4.5026],
        [-4.4939],
        [-4.4919]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-840111.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2808],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(773.8542, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2808],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(773.8542, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.5199e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.8664e-03],
        [-6.2714e-03,  2.2782e-03, -1.3017e-05,  ..., -8.4498e-03,
         -8.9821e-04, -3.5322e-03],
        [-6.2117e-03,  2.1078e-03, -1.0093e-05,  ..., -8.3420e-03,
         -6.9640e-04, -4.0567e-03],
        ...,
        [-6.0055e-03,  1.5199e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.8664e-03],
        [-6.0055e-03,  1.5199e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.8664e-03],
        [-6.0055e-03,  1.5199e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.8664e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-45304.6016, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0714, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.9029, device='cuda:0')



h[100].sum tensor(-8.9731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(107.7389, device='cuda:0')



h[200].sum tensor(-568.1434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.4895, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0010],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(313494.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3212, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2785, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3036, 0.0000],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0181, 0.0878, 0.1104],
        [0.0000, 0.0016, 0.0000,  ..., 0.0181, 0.0878, 0.1104],
        [0.0000, 0.0016, 0.0000,  ..., 0.0181, 0.0878, 0.1104]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2809237.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1618.1282, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109717., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3205.9077, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2269.3689, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(634.3882, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7911],
        [ 0.7899],
        [ 0.7589],
        ...,
        [-4.5155],
        [-4.5069],
        [-4.5048]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-828845.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(698.3163, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(698.3163, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0059]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-46055.5000, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.3984, device='cuda:0')



h[100].sum tensor(-7.9297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(97.2222, device='cuda:0')



h[200].sum tensor(-567.7463, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.5372, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(308051.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0007, 0.0000,  ..., 0.0184, 0.0893, 0.1102],
        [0.0000, 0.0004, 0.0000,  ..., 0.0178, 0.0952, 0.1063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0166, 0.1090, 0.0977],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0182, 0.0885, 0.1102],
        [0.0000, 0.0004, 0.0000,  ..., 0.0182, 0.0885, 0.1102],
        [0.0000, 0.0004, 0.0000,  ..., 0.0182, 0.0885, 0.1102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2765077.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1578.1735, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109127.5859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3200.4167, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2249.1768, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(674.8992, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5819],
        [-3.0461],
        [-2.3955],
        ...,
        [-4.5361],
        [-4.5274],
        [-4.5253]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-905691.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4607],
        [0.4805],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(725.4158, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4607],
        [0.4805],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(725.4158, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.1100e-03,  4.6664e-03, -4.7290e-05,  ..., -9.9629e-03,
         -3.6018e-03,  3.8410e-03],
        [-6.8081e-03,  3.8144e-03, -3.4366e-05,  ..., -9.4183e-03,
         -2.6175e-03,  1.1898e-03],
        [-6.4418e-03,  2.7804e-03, -1.8682e-05,  ..., -8.7573e-03,
         -1.4229e-03, -2.0275e-03],
        ...,
        [-6.0055e-03,  1.5489e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.8598e-03],
        [-6.0055e-03,  1.5489e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.8598e-03],
        [-6.0055e-03,  1.5489e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.8598e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-45902.0117, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.6556, device='cuda:0')



h[100].sum tensor(-8.1600, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(100.9951, device='cuda:0')



h[200].sum tensor(-567.9172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.9551, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0149, 0.0000,  ..., 0.0000, 0.0000, 0.0048],
        [0.0000, 0.0151, 0.0000,  ..., 0.0000, 0.0000, 0.0069],
        [0.0000, 0.0136, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(311782.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.0164e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.7122e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.1909e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 3.7411e-05, 0.0000e+00,  ..., 1.8189e-02, 8.9284e-02,
         1.0913e-01],
        [0.0000e+00, 3.7411e-05, 0.0000e+00,  ..., 1.8189e-02, 8.9284e-02,
         1.0913e-01],
        [0.0000e+00, 3.7411e-05, 0.0000e+00,  ..., 1.8189e-02, 8.9284e-02,
         1.0913e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2777692.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1609.7323, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(107992.2344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3215.8542, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2521.0571, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(692.9728, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6367],
        [ 0.6114],
        [ 0.5855],
        ...,
        [-4.5329],
        [-4.5245],
        [-4.5225]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-809212.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3125],
        [0.4851],
        [0.2457],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(639.6056, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3125],
        [0.4851],
        [0.2457],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(639.6056, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.0391e-03,  4.4251e-03, -4.1328e-05,  ..., -9.8350e-03,
         -3.3115e-03,  3.2266e-03],
        [-6.9958e-03,  4.3038e-03, -3.9599e-05,  ..., -9.7569e-03,
         -3.1729e-03,  2.8466e-03],
        [-7.2769e-03,  5.0922e-03, -5.0841e-05,  ..., -1.0264e-02,
         -4.0737e-03,  5.3169e-03],
        ...,
        [-6.0055e-03,  1.5264e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.8551e-03],
        [-6.0055e-03,  1.5264e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.8551e-03],
        [-6.0055e-03,  1.5264e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.8551e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-46762.7773, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.6745, device='cuda:0')



h[100].sum tensor(-6.9732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.0483, device='cuda:0')



h[200].sum tensor(-567.4321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.4654, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0140, 0.0000,  ..., 0.0000, 0.0000, 0.0045],
        [0.0000, 0.0176, 0.0000,  ..., 0.0000, 0.0000, 0.0125],
        [0.0000, 0.0161, 0.0000,  ..., 0.0000, 0.0000, 0.0078],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(305060.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4582, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5559, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5648, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0898, 0.1086],
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0898, 0.1086],
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0898, 0.1086]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2747951.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1555.4081, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109506.9297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3203.0317, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2097.6782, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(686.5157, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5383],
        [ 0.6676],
        [ 0.6830],
        ...,
        [-4.5375],
        [-4.5291],
        [-4.5272]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-812728.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2710],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(875.0629, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2710],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(875.0629, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2622e-03,  2.2462e-03, -1.0263e-05,  ..., -8.4331e-03,
         -8.2234e-04, -3.5998e-03],
        [-6.2489e-03,  2.2089e-03, -9.7310e-06,  ..., -8.4091e-03,
         -7.7972e-04, -3.7167e-03],
        [-6.5055e-03,  2.9287e-03, -1.9994e-05,  ..., -8.8722e-03,
         -1.6021e-03, -1.4615e-03],
        ...,
        [-6.0055e-03,  1.5264e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.8551e-03],
        [-6.0055e-03,  1.5264e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.8551e-03],
        [-6.0055e-03,  1.5264e-03,  0.0000e+00,  ..., -7.9699e-03,
          0.0000e+00, -5.8551e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-44845.4570, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.4501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.5985, device='cuda:0')



h[100].sum tensor(-9.6184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(121.8296, device='cuda:0')



h[200].sum tensor(-568.6667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(45.7850, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(323890.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0041, 0.2294, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2811, 0.0038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0034, 0.2373, 0.0194],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0898, 0.1086],
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0898, 0.1086],
        [0.0000, 0.0000, 0.0000,  ..., 0.0178, 0.0898, 0.1086]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2830300., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1652.3271, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(107732., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3258.6152, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2930.6245, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(726.7590, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3925],
        [-0.1457],
        [-0.5716],
        ...,
        [-4.5375],
        [-4.5291],
        [-4.5272]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-719752.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(732.1022, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(732.1022, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        ...,
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-46165.4609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.9659, device='cuda:0')



h[100].sum tensor(-7.8864, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(101.9260, device='cuda:0')



h[200].sum tensor(-567.9262, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.3050, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(309691.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.1032, 0.1016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0160, 0.1059, 0.0999],
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.1066, 0.0997],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0173, 0.0898, 0.1100],
        [0.0000, 0.0000, 0.0000,  ..., 0.0173, 0.0898, 0.1100],
        [0.0000, 0.0000, 0.0000,  ..., 0.0173, 0.0898, 0.1100]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2751440., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1582.1344, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109365.6172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3249.4209, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2435.5503, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(739.5203, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5553],
        [-3.3497],
        [-3.2337],
        ...,
        [-4.5809],
        [-4.5724],
        [-4.5704]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-871523.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5527],
        [0.3008],
        [0.4023],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(671.0349, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5527],
        [0.3008],
        [0.4023],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(671.0349, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2904e-03,  2.3265e-03, -9.9043e-06,  ..., -8.4840e-03,
         -8.8087e-04, -3.3374e-03],
        [-7.1136e-03,  4.6156e-03, -3.8525e-05,  ..., -9.9694e-03,
         -3.4264e-03,  3.9028e-03],
        [-6.9018e-03,  4.0266e-03, -3.1161e-05,  ..., -9.5873e-03,
         -2.7714e-03,  2.0400e-03],
        ...,
        [-6.0055e-03,  1.5343e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.8429e-03],
        [-6.0055e-03,  1.5343e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.8429e-03],
        [-6.0055e-03,  1.5343e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.8429e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-46729.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6863, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.1326, device='cuda:0')



h[100].sum tensor(-7.0923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.4240, device='cuda:0')



h[200].sum tensor(-567.6112, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.1098, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0147, 0.0000,  ..., 0.0000, 0.0000, 0.0072],
        [0.0000, 0.0144, 0.0000,  ..., 0.0000, 0.0000, 0.0061],
        [0.0000, 0.0168, 0.0000,  ..., 0.0000, 0.0000, 0.0105],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(304010.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4427, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5590, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6939, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0901, 0.1098],
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0901, 0.1098],
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0901, 0.1098]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2718399.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1557.9150, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110590.1875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3242.7031, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1895.1001, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(720.4547, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3459],
        [ 0.4500],
        [ 0.4361],
        ...,
        [-4.5948],
        [-4.5863],
        [-4.5843]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-833812.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(639.6882, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(639.6882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-47176.0859, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5797, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.6783, device='cuda:0')



h[100].sum tensor(-6.6641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.0598, device='cuda:0')



h[200].sum tensor(-567.4639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.4697, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(303274.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0166, 0.0909, 0.1105],
        [0.0000, 0.0000, 0.0000,  ..., 0.0158, 0.0988, 0.1053],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.1108, 0.0977],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0165, 0.0900, 0.1105],
        [0.0000, 0.0000, 0.0000,  ..., 0.0165, 0.0900, 0.1105],
        [0.0000, 0.0000, 0.0000,  ..., 0.0165, 0.0900, 0.1105]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2721773.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1558.3707, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110898.5859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3255.8965, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2024.4175, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(724.6275, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.1444],
        [-3.4482],
        [-2.5661],
        ...,
        [-4.6286],
        [-4.6200],
        [-4.6180]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-858945.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4165],
        [0.3311],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(833.5092, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4165],
        [0.3311],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(833.5092, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3190e-03,  2.4285e-03, -1.0148e-05,  ..., -8.5357e-03,
         -9.5237e-04, -3.0730e-03],
        [-6.4000e-03,  2.6535e-03, -1.2768e-05,  ..., -8.6818e-03,
         -1.1982e-03, -2.3607e-03],
        [-6.6304e-03,  3.2941e-03, -2.0225e-05,  ..., -9.0975e-03,
         -1.8980e-03, -3.3283e-04],
        ...,
        [-6.0055e-03,  1.5568e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.8326e-03],
        [-6.0055e-03,  1.5568e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.8326e-03],
        [-6.0055e-03,  1.5568e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.8326e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-45652.2812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.6706, device='cuda:0')



h[100].sum tensor(-8.6474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(116.0443, device='cuda:0')



h[200].sum tensor(-568.4420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.6108, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0133, 0.0000,  ..., 0.0000, 0.0000, 0.0043],
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0000, 0.0054],
        [0.0000, 0.0141, 0.0000,  ..., 0.0000, 0.0000, 0.0049],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(317877.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5154, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5743, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6284, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0165, 0.0900, 0.1105],
        [0.0000, 0.0000, 0.0000,  ..., 0.0165, 0.0900, 0.1105],
        [0.0000, 0.0000, 0.0000,  ..., 0.0165, 0.0900, 0.1105]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2791606.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1642.8525, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(108812.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3298.3145, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2930.7925, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(758.5369, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4557],
        [ 0.4308],
        [ 0.4054],
        ...,
        [-4.6286],
        [-4.6200],
        [-4.6180]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-784411.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(627.4034, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(627.4034, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-47524.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.1084, device='cuda:0')



h[100].sum tensor(-6.4078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.3495, device='cuda:0')



h[200].sum tensor(-567.3964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.8269, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(302031., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0104, 0.1356, 0.0826],
        [0.0000, 0.0000, 0.0000,  ..., 0.0125, 0.1197, 0.0930],
        [0.0000, 0.0000, 0.0000,  ..., 0.0092, 0.1588, 0.0678],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0158, 0.0894, 0.1123],
        [0.0000, 0.0000, 0.0000,  ..., 0.0158, 0.0894, 0.1123],
        [0.0000, 0.0000, 0.0000,  ..., 0.0158, 0.0894, 0.1123]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2718660.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1549.2897, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(111884.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3269.2686, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2002.1261, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(726.0275, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8987],
        [-2.1377],
        [-1.6565],
        ...,
        [-4.6798],
        [-4.6710],
        [-4.6690]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-953776.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(661.1234, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(661.1234, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.5738e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.8136e-03],
        [-6.5681e-03,  3.1373e-03, -1.5726e-05,  ..., -8.9852e-03,
         -1.6486e-03, -8.5526e-04],
        [-6.5681e-03,  3.1373e-03, -1.5726e-05,  ..., -8.9852e-03,
         -1.6486e-03, -8.5526e-04],
        ...,
        [-6.0055e-03,  1.5738e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.8136e-03],
        [-6.0055e-03,  1.5738e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.8136e-03],
        [-6.0055e-03,  1.5738e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.8136e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-47238.2812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.6728, device='cuda:0')



h[100].sum tensor(-6.6333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.0441, device='cuda:0')



h[200].sum tensor(-567.5721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.5912, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306584.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2659, 0.0045],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2609, 0.0076],
        [0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.2429, 0.0174],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0157, 0.0896, 0.1111],
        [0.0000, 0.0005, 0.0000,  ..., 0.0157, 0.0896, 0.1111],
        [0.0000, 0.0005, 0.0000,  ..., 0.0157, 0.0896, 0.1111]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2736954., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1709.3715, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(111611.4219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3187.4009, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2081.2966, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(607.8383, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6281],
        [ 0.3766],
        [-0.0152],
        ...,
        [-4.6864],
        [-4.6775],
        [-4.6749]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-876230.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(690.2186, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(690.2186, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-47043.4219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.0227, device='cuda:0')



h[100].sum tensor(-6.8062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.0948, device='cuda:0')



h[200].sum tensor(-567.7260, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.1136, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(310175.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0139, 0.1167, 0.0921],
        [0.0000, 0.0000, 0.0000,  ..., 0.0115, 0.1392, 0.0774],
        [0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.1562, 0.0666],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0163, 0.0902, 0.1089],
        [0.0000, 0.0007, 0.0000,  ..., 0.0163, 0.0902, 0.1089],
        [0.0000, 0.0007, 0.0000,  ..., 0.0163, 0.0902, 0.1089]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2737400., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1901.3521, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110403.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3077.0815, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2174.1577, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(476.9011, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7340],
        [-2.3483],
        [-2.1701],
        ...,
        [-4.6831],
        [-4.6746],
        [-4.6727]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-796822.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4353],
        [0.0000],
        [0.2944],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(664.4467, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4353],
        [0.0000],
        [0.2944],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(664.4467, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.7269e-03,  3.6628e-03, -1.7326e-05,  ..., -9.2717e-03,
         -2.0389e-03,  5.6953e-04],
        [-7.3325e-03,  5.3485e-03, -3.1871e-05,  ..., -1.0364e-02,
         -3.7506e-03,  5.9134e-03],
        [-6.5218e-03,  3.0918e-03, -1.2399e-05,  ..., -8.9015e-03,
         -1.4591e-03, -1.2405e-03],
        ...,
        [-6.0055e-03,  1.6547e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7959e-03],
        [-6.0055e-03,  1.6547e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7959e-03],
        [-6.0055e-03,  1.6547e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7959e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-47646.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.8270, device='cuda:0')



h[100].sum tensor(-6.4064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.5068, device='cuda:0')



h[200].sum tensor(-567.5804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.7651, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0219, 0.0000,  ..., 0.0000, 0.0000, 0.0251],
        [0.0000, 0.0167, 0.0000,  ..., 0.0000, 0.0000, 0.0099],
        [0.0000, 0.0184, 0.0000,  ..., 0.0000, 0.0000, 0.0143],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(305678.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 7.7201e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 6.9951e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 6.9814e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 4.9428e-04, 0.0000e+00,  ..., 1.6395e-02, 8.9906e-02,
         1.1032e-01],
        [0.0000e+00, 4.9428e-04, 0.0000e+00,  ..., 1.6395e-02, 8.9906e-02,
         1.1032e-01],
        [0.0000e+00, 4.9428e-04, 0.0000e+00,  ..., 1.6395e-02, 8.9906e-02,
         1.1032e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2725240.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1765.6659, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110735.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3134.3826, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1929.4346, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(553.3042, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3668],
        [ 0.3691],
        [ 0.3785],
        ...,
        [-4.7334],
        [-4.7247],
        [-4.7227]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-816096.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(754.8121, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(754.8121, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-47456.3516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0116, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.0195, device='cuda:0')



h[100].sum tensor(-7.1749, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(105.0878, device='cuda:0')



h[200].sum tensor(-568.0607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.4932, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(312448.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0004, 0.0000,  ..., 0.0135, 0.1046, 0.1038],
        [0.0000, 0.0004, 0.0000,  ..., 0.0092, 0.1223, 0.0926],
        [0.0000, 0.0000, 0.0000,  ..., 0.0031, 0.1932, 0.0555],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0158, 0.0889, 0.1133],
        [0.0000, 0.0006, 0.0000,  ..., 0.0158, 0.0889, 0.1133],
        [0.0000, 0.0006, 0.0000,  ..., 0.0158, 0.0889, 0.1133]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2773927., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1596.5861, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(111562.9219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3261.9023, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2523.6448, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(716.6019, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7490],
        [-2.0857],
        [-1.0620],
        ...,
        [-4.8073],
        [-4.7983],
        [-4.7963]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-951691.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(713.0103, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(713.0103, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-48258.0234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8364, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.0801, device='cuda:0')



h[100].sum tensor(-6.6022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(99.2680, device='cuda:0')



h[200].sum tensor(-567.8185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.3061, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(307419.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0014, 0.0000,  ..., 0.0155, 0.0959, 0.1076],
        [0.0000, 0.0007, 0.0000,  ..., 0.0139, 0.1113, 0.0976],
        [0.0000, 0.0000, 0.0000,  ..., 0.0092, 0.1562, 0.0686],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0159, 0.0892, 0.1115],
        [0.0000, 0.0013, 0.0000,  ..., 0.0159, 0.0892, 0.1115],
        [0.0000, 0.0013, 0.0000,  ..., 0.0156, 0.0924, 0.1095]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2759542., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1629.6411, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(111501.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3230.6494, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2183.6138, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(653.5641, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1622],
        [-2.3088],
        [-1.2932],
        ...,
        [-4.7791],
        [-4.7158],
        [-4.5666]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-826357.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(605.3301, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(605.3301, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-49091.8711, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.0843, device='cuda:0')



h[100].sum tensor(-5.6375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(84.2763, device='cuda:0')



h[200].sum tensor(-567.2865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.6720, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(299184.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0008, 0.0000,  ..., 0.0139, 0.1106, 0.0979],
        [0.0000, 0.0016, 0.0000,  ..., 0.0155, 0.0949, 0.1082],
        [0.0000, 0.0018, 0.0000,  ..., 0.0161, 0.0905, 0.1114],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0159, 0.0892, 0.1115],
        [0.0000, 0.0013, 0.0000,  ..., 0.0159, 0.0892, 0.1115],
        [0.0000, 0.0013, 0.0000,  ..., 0.0159, 0.0892, 0.1115]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2709385.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1556.8092, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(112991.4141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3198.0244, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1545.8768, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(642.9860, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1228],
        [-4.2065],
        [-4.8912],
        ...,
        [-4.7912],
        [-4.7834],
        [-4.7826]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-895294.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(746.4790, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(746.4790, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.9736e-03,  4.3470e-03, -1.9865e-05,  ..., -9.7169e-03,
         -2.6386e-03,  2.7705e-03],
        [-6.4888e-03,  3.0016e-03, -9.9169e-06,  ..., -8.8421e-03,
         -1.3172e-03, -1.5117e-03],
        [-6.6127e-03,  3.3454e-03, -1.2459e-05,  ..., -9.0656e-03,
         -1.6549e-03, -4.1753e-04],
        ...,
        [-6.0055e-03,  1.6604e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7806e-03],
        [-6.0055e-03,  1.6604e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7806e-03],
        [-6.0055e-03,  1.6604e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7806e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-47966.6328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.6329, device='cuda:0')



h[100].sum tensor(-6.9393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(103.9276, device='cuda:0')



h[200].sum tensor(-568.0045, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.0572, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0159, 0.0000,  ..., 0.0000, 0.0000, 0.0077],
        [0.0000, 0.0153, 0.0000,  ..., 0.0000, 0.0000, 0.0048],
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(314567.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6202, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6170, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5756, 0.0000],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0159, 0.0892, 0.1115],
        [0.0000, 0.0013, 0.0000,  ..., 0.0159, 0.0892, 0.1115],
        [0.0000, 0.0013, 0.0000,  ..., 0.0159, 0.0892, 0.1115]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2802028.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1636.4713, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110736.3281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3239.8335, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2803.7041, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(680.0576, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4957],
        [ 0.4770],
        [ 0.4795],
        ...,
        [-4.7964],
        [-4.7876],
        [-4.7856]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-844658.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6421],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(663.0317, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6421],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(663.0317, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.6363e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7753e-03],
        [-6.6136e-03,  3.3189e-03, -1.1506e-05,  ..., -9.0673e-03,
         -1.6274e-03, -4.0264e-04],
        [-6.0055e-03,  1.6363e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7753e-03],
        ...,
        [-6.0055e-03,  1.6363e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7753e-03],
        [-6.0055e-03,  1.6363e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7753e-03],
        [-6.0055e-03,  1.6363e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7753e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-49005.7461, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6594, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.7613, device='cuda:0')



h[100].sum tensor(-6.0436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.3098, device='cuda:0')



h[200].sum tensor(-567.5739, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.6911, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0127, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(307109.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3182, 0.0005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.2039, 0.0371],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.1663, 0.0601],
        ...,
        [0.0000, 0.0018, 0.0000,  ..., 0.0161, 0.0897, 0.1098],
        [0.0000, 0.0018, 0.0000,  ..., 0.0161, 0.0897, 0.1098],
        [0.0000, 0.0018, 0.0000,  ..., 0.0161, 0.0897, 0.1098]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2764135., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1625.4095, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(111433.1250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3200.0420, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2063.9441, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(622.1047, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1240],
        [-0.8843],
        [-2.2585],
        ...,
        [-4.7867],
        [-4.7780],
        [-4.7760]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-733366.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 220.0 event: 3300 loss: tensor(750.7819, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2524],
        [0.3745],
        [0.2993],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(786.6696, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2524],
        [0.3745],
        [0.2993],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(786.6696, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6090e-03,  3.4106e-03, -1.0512e-05,  ..., -9.0590e-03,
         -1.5857e-03, -4.3950e-04],
        [-6.5281e-03,  3.1868e-03, -9.1015e-06,  ..., -8.9129e-03,
         -1.3730e-03, -1.1549e-03],
        [-6.8603e-03,  4.1049e-03, -1.4887e-05,  ..., -9.5124e-03,
         -2.2458e-03,  1.7804e-03],
        ...,
        [-6.0055e-03,  1.7426e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7721e-03],
        [-6.0055e-03,  1.7426e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7721e-03],
        [-6.0055e-03,  1.7426e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7721e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-48523.1719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1135, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.4975, device='cuda:0')



h[100].sum tensor(-7.0261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(109.5231, device='cuda:0')



h[200].sum tensor(-568.2015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.1601, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0009],
        [0.0000, 0.0149, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        [0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(314302.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.3534e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.6471e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.6578e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 8.3528e-05, 0.0000e+00,  ..., 1.5948e-02, 8.9435e-02,
         1.1341e-01],
        [0.0000e+00, 8.3528e-05, 0.0000e+00,  ..., 1.5948e-02, 8.9435e-02,
         1.1341e-01],
        [0.0000e+00, 8.3528e-05, 0.0000e+00,  ..., 1.5948e-02, 8.9435e-02,
         1.1341e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2779157.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1636.3320, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110221.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3277.3843, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3203.4883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(749.4581, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6418],
        [ 0.6099],
        [ 0.5873],
        ...,
        [-4.8737],
        [-4.8647],
        [-4.8626]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-968112.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2460],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(668.4948, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2460],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(668.4948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5415e-03,  3.1768e-03, -8.5774e-06,  ..., -8.9371e-03,
         -1.3826e-03, -1.0335e-03],
        [-6.4996e-03,  3.0616e-03, -7.9067e-06,  ..., -8.8615e-03,
         -1.2745e-03, -1.4038e-03],
        [-6.7842e-03,  3.8439e-03, -1.2462e-05,  ..., -9.3751e-03,
         -2.0087e-03,  1.1114e-03],
        ...,
        [-6.0055e-03,  1.7038e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7694e-03],
        [-6.0055e-03,  1.7038e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7694e-03],
        [-6.0055e-03,  1.7038e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7694e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-49775.1680, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6661, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.0148, device='cuda:0')



h[100].sum tensor(-5.8351, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.0704, device='cuda:0')



h[200].sum tensor(-567.5834, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.9769, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0141, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306227.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.1247e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.6469e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.0557e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 2.9600e-04, 0.0000e+00,  ..., 1.6196e-02, 8.9856e-02,
         1.1138e-01],
        [0.0000e+00, 2.9600e-04, 0.0000e+00,  ..., 1.6196e-02, 8.9856e-02,
         1.1138e-01],
        [0.0000e+00, 2.9600e-04, 0.0000e+00,  ..., 1.6196e-02, 8.9856e-02,
         1.1138e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2733109.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1592.1619, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110621.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3215.4453, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2398.9099, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(707.2350, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6442],
        [ 0.6912],
        [ 0.7326],
        ...,
        [-4.8469],
        [-4.8381],
        [-4.8361]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-910760., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2832],
        [0.0000],
        [0.5044],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(730.8582, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2832],
        [0.0000],
        [0.5044],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(730.8582, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5011e-03,  2.9521e-03, -7.2725e-06,  ..., -8.8642e-03,
         -1.2549e-03, -1.3892e-03],
        [-6.9818e-03,  4.2634e-03, -1.4327e-05,  ..., -9.7316e-03,
         -2.4722e-03,  2.8582e-03],
        [-6.4378e-03,  2.7797e-03, -6.3449e-06,  ..., -8.7501e-03,
         -1.0948e-03, -1.9477e-03],
        ...,
        [-6.8226e-03,  3.8292e-03, -1.1992e-05,  ..., -9.4444e-03,
         -2.0692e-03,  1.4519e-03],
        [-6.4046e-03,  2.6891e-03, -5.8573e-06,  ..., -8.6902e-03,
         -1.0107e-03, -2.2413e-03],
        [-6.0055e-03,  1.6004e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7677e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-49458.3359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.9081, device='cuda:0')



h[100].sum tensor(-6.2901, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(101.7528, device='cuda:0')



h[200].sum tensor(-567.9196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.2399, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0141, 0.0000,  ..., 0.0000, 0.0000, 0.0057],
        [0.0000, 0.0125, 0.0000,  ..., 0.0000, 0.0000, 0.0025],
        [0.0000, 0.0185, 0.0000,  ..., 0.0000, 0.0000, 0.0161],
        ...,
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.0000, 0.0015],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(309493.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5246, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5183, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6155, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3803, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3391, 0.0082],
        [0.0000, 0.0000, 0.0000,  ..., 0.0047, 0.2369, 0.0344]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2739647.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1657.7609, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109800.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3173.4800, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1962.0969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(629.7362, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7210],
        [ 0.7124],
        [ 0.6909],
        ...,
        [ 0.4470],
        [-0.0451],
        [-1.2829]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-659731.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(974.8064, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(974.8064, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-47898.6953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.8441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(45.2261, device='cuda:0')



h[100].sum tensor(-8.3007, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(135.7162, device='cuda:0')



h[200].sum tensor(-569.2111, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(51.0037, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(329790.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.1123, 0.0971],
        [0.0000, 0.0002, 0.0000,  ..., 0.0151, 0.0953, 0.1083],
        [0.0000, 0.0007, 0.0000,  ..., 0.0157, 0.0908, 0.1115],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0155, 0.0889, 0.1120],
        [0.0000, 0.0003, 0.0000,  ..., 0.0155, 0.0889, 0.1120],
        [0.0000, 0.0003, 0.0000,  ..., 0.0155, 0.0889, 0.1120]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2827411., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1705.4814, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(108482.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3274.9355, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3342.0261, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(770.1748, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8325],
        [-3.8032],
        [-4.4572],
        ...,
        [-4.8739],
        [-4.8652],
        [-4.8632]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-845955.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(749.3197, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(749.3197, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-50017.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9898, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.7647, device='cuda:0')



h[100].sum tensor(-6.2333, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(104.3231, device='cuda:0')



h[200].sum tensor(-568.0305, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.2058, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(310145.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 5.4114e-05, 0.0000e+00,  ..., 1.4100e-02, 1.0044e-01,
         1.0678e-01],
        [0.0000e+00, 6.9723e-05, 0.0000e+00,  ..., 1.5178e-02, 8.9031e-02,
         1.1410e-01],
        [0.0000e+00, 1.0335e-04, 0.0000e+00,  ..., 1.5116e-02, 9.0792e-02,
         1.1316e-01],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5105e-02, 8.7757e-02,
         1.1445e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5105e-02, 8.7757e-02,
         1.1445e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5105e-02, 8.7757e-02,
         1.1445e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2720364.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1607.0563, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110581.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3082.2319, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2087.0669, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(775.5860, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.3624],
        [-4.7207],
        [-4.6268],
        ...,
        [-4.9333],
        [-4.9243],
        [-4.9223]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1004568.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6152],
        [0.6953],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(682.3420, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6152],
        [0.6953],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(682.3420, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.9358e-03,  6.8322e-03, -2.1555e-05,  ..., -1.1453e-02,
         -4.6231e-03,  1.1291e-02],
        [-6.5882e-03,  3.2023e-03, -6.5068e-06,  ..., -9.0214e-03,
         -1.3956e-03, -6.1388e-04],
        [-6.6640e-03,  3.4066e-03, -7.3537e-06,  ..., -9.1583e-03,
         -1.5772e-03,  5.6136e-05],
        ...,
        [-6.0055e-03,  1.6327e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7615e-03],
        [-6.0055e-03,  1.6327e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7615e-03],
        [-6.0055e-03,  1.6327e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7615e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-50531.1328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.6572, device='cuda:0')



h[100].sum tensor(-5.5373, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(94.9982, device='cuda:0')



h[200].sum tensor(-567.6667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.7014, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0218, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0188, 0.0000,  ..., 0.0000, 0.0000, 0.0180],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306832.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7828, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5920, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3723, 0.0011],
        ...,
        [0.0000, 0.0010, 0.0000,  ..., 0.0159, 0.0882, 0.1096],
        [0.0000, 0.0010, 0.0000,  ..., 0.0159, 0.0882, 0.1096],
        [0.0000, 0.0010, 0.0000,  ..., 0.0159, 0.0882, 0.1096]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2689116., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1610.6135, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109285.9375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2808.1514, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1236.4640, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(694.8022, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6440],
        [ 0.7178],
        [ 0.7960],
        ...,
        [-4.8594],
        [-4.8509],
        [-4.8491]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-740186., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(752.5095, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(752.5095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-50268.4453, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.9126, device='cuda:0')



h[100].sum tensor(-5.9835, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(104.7672, device='cuda:0')



h[200].sum tensor(-568.0214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.3727, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(313567.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0012, 0.0000,  ..., 0.0162, 0.0884, 0.1095],
        [0.0000, 0.0012, 0.0000,  ..., 0.0162, 0.0884, 0.1095],
        [0.0000, 0.0013, 0.0000,  ..., 0.0162, 0.0888, 0.1095],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0161, 0.0876, 0.1096],
        [0.0000, 0.0009, 0.0000,  ..., 0.0161, 0.0876, 0.1096],
        [0.0000, 0.0009, 0.0000,  ..., 0.0161, 0.0876, 0.1096]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2725538.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1663.4778, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(107084.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2843.3555, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1686.4524, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(701.1741, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.9138],
        [-5.2008],
        [-5.3540],
        ...,
        [-4.8811],
        [-4.8726],
        [-4.8706]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-724593.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3611],
        [0.3528],
        [0.3372],
        ...,
        [0.0000],
        [0.0000],
        [0.5405]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(760.6605, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3611],
        [0.3528],
        [0.3372],
        ...,
        [0.0000],
        [0.0000],
        [0.5405]], device='cuda:0') 
g.ndata[nfet].sum tensor(760.6605, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.1170e-03,  4.7047e-03, -1.0208e-05,  ..., -9.9756e-03,
         -2.5641e-03,  4.0657e-03],
        [-7.2373e-03,  5.0276e-03, -1.1314e-05,  ..., -1.0193e-02,
         -2.8419e-03,  5.1295e-03],
        [-6.9022e-03,  4.1286e-03, -8.2352e-06,  ..., -9.5879e-03,
         -2.0686e-03,  2.1677e-03],
        ...,
        [-6.0055e-03,  1.7239e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7549e-03],
        [-6.5174e-03,  3.0969e-03, -4.7018e-06,  ..., -8.8937e-03,
         -1.1810e-03, -1.2316e-03],
        [-6.4488e-03,  2.9128e-03, -4.0714e-06,  ..., -8.7699e-03,
         -1.0227e-03, -1.8381e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-50665.4922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.2908, device='cuda:0')



h[100].sum tensor(-5.9348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(105.9020, device='cuda:0')



h[200].sum tensor(-568.0647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.7992, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0185, 0.0000,  ..., 0.0000, 0.0000, 0.0153],
        [0.0000, 0.0170, 0.0000,  ..., 0.0000, 0.0000, 0.0104],
        [0.0000, 0.0160, 0.0000,  ..., 0.0000, 0.0000, 0.0070],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0140, 0.0000,  ..., 0.0000, 0.0000, 0.0038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(311537.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 6.4035e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.9983e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.5874e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.9228e-03, 1.8196e-01,
         5.4132e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.8219e-04, 2.6498e-01,
         2.3641e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.4160e-01,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2709735.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1643.8933, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(107293.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2984.6768, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1698.1772, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(745.1707, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6046],
        [ 0.6131],
        [ 0.6261],
        ...,
        [-2.5700],
        [-1.2067],
        [-0.4522]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-935301.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3193],
        [0.2859],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(731.9716, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3193],
        [0.2859],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(731.9716, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3079e-03,  2.5209e-03, -2.5071e-06,  ..., -8.5157e-03,
         -6.8473e-04, -3.0747e-03],
        [-6.4804e-03,  2.9825e-03, -3.9363e-06,  ..., -8.8268e-03,
         -1.0751e-03, -1.5507e-03],
        [-6.9528e-03,  4.2471e-03, -7.8524e-06,  ..., -9.6793e-03,
         -2.1446e-03,  2.6252e-03],
        ...,
        [-6.0055e-03,  1.7113e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7482e-03],
        [-6.0055e-03,  1.7113e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7482e-03],
        [-6.0055e-03,  1.7113e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7482e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-51187.1367, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8893, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.9598, device='cuda:0')



h[100].sum tensor(-5.5616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(101.9079, device='cuda:0')



h[200].sum tensor(-567.8917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.2981, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0039],
        [0.0000, 0.0156, 0.0000,  ..., 0.0000, 0.0000, 0.0073],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(307276.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.5889e-03, 2.6782e-01,
         3.0607e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.2626e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.5847e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 9.8037e-05, 0.0000e+00,  ..., 1.5971e-02, 8.5533e-02,
         1.1241e-01],
        [0.0000e+00, 9.8037e-05, 0.0000e+00,  ..., 1.5971e-02, 8.5533e-02,
         1.1241e-01],
        [0.0000e+00, 9.8037e-05, 0.0000e+00,  ..., 1.5971e-02, 8.5533e-02,
         1.1241e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2695898., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1633.5375, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(107287., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2986.3188, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1390.5999, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(699.0886, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0451],
        [ 0.4097],
        [ 0.5674],
        ...,
        [-4.9794],
        [-4.9706],
        [-4.9686]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-941985.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5166],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(776.4883, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5166],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(776.4883, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.6372e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7370e-03],
        [-6.4948e-03,  2.9457e-03, -3.6474e-06,  ..., -8.8528e-03,
         -1.0870e-03, -1.4086e-03],
        [-6.4259e-03,  2.7614e-03, -3.1337e-06,  ..., -8.7285e-03,
         -9.3391e-04, -2.0181e-03],
        ...,
        [-6.0055e-03,  1.6372e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7370e-03],
        [-6.0055e-03,  1.6372e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7370e-03],
        [-6.0055e-03,  1.6372e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7370e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-50866.3906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.0251, device='cuda:0')



h[100].sum tensor(-5.8221, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(108.1056, device='cuda:0')



h[200].sum tensor(-568.1422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.6273, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0134, 0.0000,  ..., 0.0000, 0.0000, 0.0032],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(314038.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0070, 0.1796, 0.0490],
        [0.0000, 0.0000, 0.0000,  ..., 0.0036, 0.2357, 0.0282],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3358, 0.0000],
        ...,
        [0.0000, 0.0012, 0.0000,  ..., 0.0160, 0.0876, 0.1076],
        [0.0000, 0.0017, 0.0000,  ..., 0.0162, 0.0857, 0.1088],
        [0.0000, 0.0017, 0.0000,  ..., 0.0162, 0.0857, 0.1088]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2740155.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1738.1178, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(105391.9062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2946.1084, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1656.1340, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(597.7225, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4453],
        [-0.0133],
        [ 0.3001],
        ...,
        [-4.6081],
        [-4.7525],
        [-4.8554]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-700443.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 230.0 event: 3450 loss: tensor(488.4577, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(704.8528, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(704.8528, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0057],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0057],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0057],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0057],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0057],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0057]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-51747.2109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7878, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.7016, device='cuda:0')



h[100].sum tensor(-5.1468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(98.1323, device='cuda:0')



h[200].sum tensor(-567.7515, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.8792, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0015],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(307531.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.1816, 0.0532],
        [0.0000, 0.0009, 0.0000,  ..., 0.0102, 0.1163, 0.0893],
        [0.0000, 0.0010, 0.0000,  ..., 0.0135, 0.1047, 0.0970],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0162, 0.0849, 0.1096],
        [0.0000, 0.0015, 0.0000,  ..., 0.0162, 0.0849, 0.1096],
        [0.0000, 0.0015, 0.0000,  ..., 0.0162, 0.0849, 0.1096]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2711049.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1689.0469, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(106762.6484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2971.2666, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1203.4156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(587.1667, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8205],
        [-2.4179],
        [-2.5084],
        ...,
        [-4.9727],
        [-4.9641],
        [-4.9622]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-771444.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(846.5963, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(846.5963, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2575e-03,  2.3634e-03, -1.5002e-06,  ..., -8.4248e-03,
         -5.3908e-04, -3.4827e-03],
        [-6.0055e-03,  1.6900e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7157e-03],
        [-7.0144e-03,  4.3856e-03, -6.0054e-06,  ..., -9.7906e-03,
         -2.1580e-03,  3.2234e-03],
        ...,
        [-6.0055e-03,  1.6900e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7157e-03],
        [-6.0055e-03,  1.6900e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7157e-03],
        [-6.0055e-03,  1.6900e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.7157e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-50893.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.3590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.2778, device='cuda:0')



h[100].sum tensor(-6.1613, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(117.8663, device='cuda:0')



h[200].sum tensor(-568.5408, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(44.2955, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.0000, 0.0032],
        [0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.0000, 0.0015],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(317799.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2898, 0.0021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3893, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4585, 0.0000],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0155, 0.0831, 0.1135],
        [0.0000, 0.0007, 0.0000,  ..., 0.0155, 0.0831, 0.1135],
        [0.0000, 0.0007, 0.0000,  ..., 0.0155, 0.0831, 0.1135]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2756862.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1680.2733, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(108202.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3085.1899, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1896.9329, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(692.5899, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6795],
        [ 0.6346],
        [ 0.5932],
        ...,
        [-5.0716],
        [-5.0623],
        [-5.0601]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1019302., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1173.6125, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1173.6125, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0057],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0057],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0057],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0057],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0057],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0057]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-48335.7266, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-616.5719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(54.4497, device='cuda:0')



h[100].sum tensor(-8.3617, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(163.3948, device='cuda:0')



h[200].sum tensor(-570.2168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(61.4057, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(349257.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0011, 0.0000,  ..., 0.0157, 0.0843, 0.1117],
        [0.0000, 0.0011, 0.0000,  ..., 0.0157, 0.0843, 0.1117],
        [0.0000, 0.0012, 0.0000,  ..., 0.0158, 0.0847, 0.1116],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0156, 0.0835, 0.1117],
        [0.0000, 0.0008, 0.0000,  ..., 0.0156, 0.0835, 0.1117],
        [0.0000, 0.0008, 0.0000,  ..., 0.0156, 0.0835, 0.1117]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2939652., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1846.7715, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(104149.4531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3131.5596, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(3938.4912, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(752.6471, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.5122],
        [-5.5655],
        [-5.5779],
        ...,
        [-4.8920],
        [-4.9845],
        [-5.0295]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-898432.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(589.2750, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(589.2750, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0057],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0057],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0057],
        ...,
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0057],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0057],
        [-0.0060,  0.0017,  0.0000,  ..., -0.0080,  0.0000, -0.0057]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-53118.1016, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.3394, device='cuda:0')



h[100].sum tensor(-4.1756, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(82.0411, device='cuda:0')



h[200].sum tensor(-567.1933, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.8320, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(297235.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0011, 0.0000,  ..., 0.0157, 0.0843, 0.1117],
        [0.0000, 0.0011, 0.0000,  ..., 0.0148, 0.0896, 0.1081],
        [0.0000, 0.0005, 0.0000,  ..., 0.0138, 0.0970, 0.1034],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0156, 0.0835, 0.1117],
        [0.0000, 0.0008, 0.0000,  ..., 0.0156, 0.0835, 0.1117],
        [0.0000, 0.0008, 0.0000,  ..., 0.0156, 0.0835, 0.1117]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2653517.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1604.6104, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109623.6562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3008.6064, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(908.0198, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(616.0944, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.4398],
        [-5.2320],
        [-4.7433],
        ...,
        [-5.0558],
        [-5.0469],
        [-5.0449]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-998966.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.7803],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(700.7450, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.7803],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(700.7450, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.5855e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6946e-03],
        [-6.7445e-03,  3.5467e-03, -3.4388e-06,  ..., -9.3035e-03,
         -1.5214e-03,  8.6208e-04],
        [-6.0055e-03,  1.5855e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6946e-03],
        ...,
        [-6.0055e-03,  1.5855e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6946e-03],
        [-6.0055e-03,  1.5855e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6946e-03],
        [-6.0055e-03,  1.5855e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6946e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-52131.9609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7918, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.5110, device='cuda:0')



h[100].sum tensor(-4.8591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(97.5604, device='cuda:0')



h[200].sum tensor(-567.7569, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.6643, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0009],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0150, 0.0000,  ..., 0.0000, 0.0000, 0.0069],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306891.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.1788, 0.0524],
        [0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.2231, 0.0291],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3584, 0.0000],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0164, 0.0854, 0.1064],
        [0.0000, 0.0013, 0.0000,  ..., 0.0164, 0.0854, 0.1064],
        [0.0000, 0.0013, 0.0000,  ..., 0.0164, 0.0854, 0.1064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2693151., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1708.1858, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(105807.6641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2963.4746, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1094.6321, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(551.9966, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4701],
        [-0.3258],
        [ 0.4659],
        ...,
        [-4.9723],
        [-4.9641],
        [-4.9624]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-678519.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1005.0828, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1005.0828, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.9042e-03,  3.9822e-03, -3.6606e-06,  ..., -9.5916e-03,
         -1.8150e-03,  2.2946e-03],
        [-6.4560e-03,  2.7954e-03, -1.8349e-06,  ..., -8.7828e-03,
         -9.0978e-04, -1.6848e-03],
        [-6.0055e-03,  1.6027e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6844e-03],
        ...,
        [-6.0055e-03,  1.6027e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6844e-03],
        [-6.0055e-03,  1.6027e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6844e-03],
        [-6.0055e-03,  1.6027e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6844e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-49717.5664, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.9357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(46.6308, device='cuda:0')



h[100].sum tensor(-6.8674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(139.9314, device='cuda:0')



h[200].sum tensor(-569.3376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(52.5879, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0041],
        [0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.0000, 0.0023],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(333005.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.3025e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.3856e-01,
         9.2626e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.3805e-03, 2.3384e-01,
         3.3417e-02],
        ...,
        [0.0000e+00, 3.0565e-04, 0.0000e+00,  ..., 1.6754e-02, 8.5875e-02,
         1.0553e-01],
        [0.0000e+00, 3.0565e-04, 0.0000e+00,  ..., 1.6754e-02, 8.5875e-02,
         1.0553e-01],
        [0.0000e+00, 3.0565e-04, 0.0000e+00,  ..., 1.6754e-02, 8.5875e-02,
         1.0553e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2806831., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1798.8573, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(102531.2734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3017.6501, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2331.9622, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(661.2662, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4989],
        [-0.0701],
        [-1.0367],
        ...,
        [-4.9528],
        [-4.9095],
        [-4.8653]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-663629.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(873.2468, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(873.2468, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.6110e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6781e-03],
        [-6.0055e-03,  1.6110e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6781e-03],
        [-6.6361e-03,  3.2691e-03, -2.2300e-06,  ..., -9.1078e-03,
         -1.2493e-03, -7.8056e-05],
        ...,
        [-6.0055e-03,  1.6110e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6781e-03],
        [-6.0055e-03,  1.6110e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6781e-03],
        [-6.0055e-03,  1.6110e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6781e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-51053.0156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.4419, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.5143, device='cuda:0')



h[100].sum tensor(-5.8422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(121.5767, device='cuda:0')



h[200].sum tensor(-568.6553, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(45.6899, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(320120.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2470, 0.0090],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2758, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2797, 0.0049],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0164, 0.0852, 0.1080],
        [0.0000, 0.0000, 0.0000,  ..., 0.0164, 0.0852, 0.1080],
        [0.0000, 0.0000, 0.0000,  ..., 0.0164, 0.0852, 0.1080]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2743706.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1730.6787, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(104728.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3051.4941, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1637.6042, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(697.1398, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.8207],
        [ 0.9240],
        [ 0.9388],
        ...,
        [-5.0482],
        [-5.0397],
        [-5.0379]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-770112.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5698],
        [0.6216],
        [0.5776],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(592.9977, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5698],
        [0.6216],
        [0.5776],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(592.9977, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.4955e-03,  5.4631e-03, -4.5297e-06,  ..., -1.0659e-02,
         -2.8955e-03,  7.5680e-03],
        [-7.9928e-03,  6.7620e-03, -6.0415e-06,  ..., -1.1556e-02,
         -3.8618e-03,  1.1986e-02],
        [-7.4366e-03,  5.3093e-03, -4.3506e-06,  ..., -1.0552e-02,
         -2.7810e-03,  7.0447e-03],
        ...,
        [-6.0055e-03,  1.5714e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6700e-03],
        [-6.0055e-03,  1.5714e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6700e-03],
        [-6.0055e-03,  1.5714e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6700e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-53555.0430, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5121, device='cuda:0')



h[100].sum tensor(-3.9059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(82.5594, device='cuda:0')



h[200].sum tensor(-567.2314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0268, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0230, 0.0000,  ..., 0.0000, 0.0000, 0.0343],
        [0.0000, 0.0261, 0.0000,  ..., 0.0000, 0.0000, 0.0447],
        [0.0000, 0.0264, 0.0000,  ..., 0.0000, 0.0000, 0.0457],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(297633.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8947, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0446, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.0462, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0841, 0.1104],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0841, 0.1104],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0841, 0.1104]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2640773., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1579.4224, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110375.8438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3035.6919, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(769.1900, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(674.7122, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5006],
        [ 0.5080],
        [ 0.5182],
        ...,
        [-4.6771],
        [-4.9608],
        [-5.0651]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-970253., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(811.9741, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(811.9741, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.7510e-03,  3.4752e-03, -1.9247e-06,  ..., -9.3152e-03,
         -1.4208e-03,  9.6663e-04],
        [-6.0055e-03,  1.5352e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6603e-03],
        [-6.0055e-03,  1.5352e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6603e-03],
        ...,
        [-6.0055e-03,  1.5352e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6603e-03],
        [-6.0055e-03,  1.5352e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6603e-03],
        [-6.0055e-03,  1.5352e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6603e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-51836.1719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.6715, device='cuda:0')



h[100].sum tensor(-5.2424, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(113.0461, device='cuda:0')



h[200].sum tensor(-568.3594, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.4840, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0008],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0010],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(313646.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3265, 0.0135],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2443, 0.0217],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2109, 0.0254],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0842, 0.1098],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0842, 0.1098],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0842, 0.1098]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2711537.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1670.4434, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109250.3594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3071.1816, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1373.6216, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(697.2979, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7429],
        [ 0.7556],
        [ 0.7618],
        ...,
        [-5.1175],
        [-5.1086],
        [-5.1067]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-883254.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2764],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(809.3580, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2764],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(809.3580, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2170e-03,  2.1114e-03, -4.5668e-07,  ..., -8.3515e-03,
         -3.9526e-04, -3.7704e-03],
        [-6.2673e-03,  2.2424e-03, -5.6532e-07,  ..., -8.4423e-03,
         -4.8928e-04, -3.3230e-03],
        [-6.0055e-03,  1.5605e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6510e-03],
        ...,
        [-6.0055e-03,  1.5605e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6510e-03],
        [-6.0055e-03,  1.5605e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6510e-03],
        [-6.0055e-03,  1.5605e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6510e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-51894.5469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1824, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.5501, device='cuda:0')



h[100].sum tensor(-5.0614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(112.6819, device='cuda:0')



h[200].sum tensor(-568.2967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.3472, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(314005.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3111, 0.0022],
        [0.0000, 0.0000, 0.0000,  ..., 0.0023, 0.2346, 0.0258],
        [0.0000, 0.0000, 0.0000,  ..., 0.0075, 0.1624, 0.0560],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0851, 0.1076],
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0851, 0.1076],
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0851, 0.1076]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2711008.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1679.9276, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(106964.6953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3022.5437, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1435.7272, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(677.8054, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5302],
        [-0.3089],
        [-1.6242],
        ...,
        [-5.1019],
        [-5.0933],
        [-5.0914]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-820059.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 240.0 event: 3600 loss: tensor(431.5301, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2485],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(693.7061, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2485],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(693.7061, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6320e-03,  3.2264e-03, -1.1099e-06,  ..., -9.1005e-03,
         -1.1485e-03, -7.1228e-05],
        [-6.5411e-03,  2.9894e-03, -9.4881e-07,  ..., -8.9364e-03,
         -9.8182e-04, -8.8030e-04],
        [-6.3057e-03,  2.3760e-03, -5.3180e-07,  ..., -8.5116e-03,
         -5.5030e-04, -2.9742e-03],
        ...,
        [-6.0055e-03,  1.5938e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6446e-03],
        [-6.0055e-03,  1.5938e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6446e-03],
        [-6.0055e-03,  1.5938e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6446e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-52748.4883, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.1845, device='cuda:0')



h[100].sum tensor(-4.2629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.5804, device='cuda:0')



h[200].sum tensor(-567.7174, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.2960, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0036],
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0010],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(304719.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4767, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4290, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3674, 0.0029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0891, 0.1029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.1144, 0.0859],
        [0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.1735, 0.0480]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2660070.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1660.6848, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(105270.8672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2952.8237, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(810.7098, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(614.1823, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7468],
        [ 0.7318],
        [ 0.7327],
        ...,
        [-4.5018],
        [-3.4664],
        [-1.9748]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-723871.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(676.9705, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(676.9705, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0056],
        ...,
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0016,  0.0000,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-52827.4492, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.4080, device='cuda:0')



h[100].sum tensor(-4.1514, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(94.2504, device='cuda:0')



h[200].sum tensor(-567.6951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.4204, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306418.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 6.4974e-03, 1.9306e-01,
         3.4321e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.3865e-02, 1.2221e-01,
         8.0667e-02],
        [0.0000e+00, 4.2236e-05, 0.0000e+00,  ..., 1.6449e-02, 1.0038e-01,
         9.5595e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.7717e-02, 8.6155e-02,
         1.0455e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.7717e-02, 8.6155e-02,
         1.0455e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.7717e-02, 8.6155e-02,
         1.0455e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2671056.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1656.7020, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(104449.6406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2930.6018, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1031.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(612.2054, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.2041],
        [-3.5733],
        [-4.5812],
        ...,
        [-5.1017],
        [-5.0933],
        [-5.0907]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-751708.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(678.8495, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(678.8495, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.5952e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6308e-03],
        [-6.0055e-03,  1.5952e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6308e-03],
        [-6.8597e-03,  3.8165e-03, -9.3021e-07,  ..., -9.5113e-03,
         -1.5057e-03,  1.9721e-03],
        ...,
        [-6.0055e-03,  1.5952e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6308e-03],
        [-6.0055e-03,  1.5952e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6308e-03],
        [-6.0055e-03,  1.5952e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6308e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-53077.8906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.4952, device='cuda:0')



h[100].sum tensor(-4.0199, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(94.5120, device='cuda:0')



h[200].sum tensor(-567.6533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.5187, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0006],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(303490.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 8.3267e-03, 1.3614e-01,
         7.2931e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.0413e-03, 2.2160e-01,
         3.5466e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.2571e-01,
         1.3460e-02],
        ...,
        [0.0000e+00, 1.4763e-04, 0.0000e+00,  ..., 1.7358e-02, 8.5144e-02,
         1.0648e-01],
        [0.0000e+00, 1.4763e-04, 0.0000e+00,  ..., 1.7358e-02, 8.5144e-02,
         1.0648e-01],
        [0.0000e+00, 1.4763e-04, 0.0000e+00,  ..., 1.7358e-02, 8.5144e-02,
         1.0648e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2668937.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1642.2756, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(106815.4531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2945.3792, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(773.0959, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(588.6313, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7103],
        [-0.1279],
        [ 0.3339],
        ...,
        [-5.1640],
        [-5.1554],
        [-5.1535]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-803545.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(656.4587, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(656.4587, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6470e-03,  3.2476e-03, -5.0743e-07,  ..., -9.1276e-03,
         -1.1088e-03,  8.7351e-05],
        [-6.0055e-03,  1.5835e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6246e-03],
        [-7.4858e-03,  5.4232e-03, -1.1708e-06,  ..., -1.0641e-02,
         -2.5586e-03,  7.5552e-03],
        ...,
        [-6.0055e-03,  1.5835e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6246e-03],
        [-6.0055e-03,  1.5835e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6246e-03],
        [-6.0055e-03,  1.5835e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6246e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-53493.9805, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6321, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.4564, device='cuda:0')



h[100].sum tensor(-3.8064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.3947, device='cuda:0')



h[200].sum tensor(-567.5363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.3472, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0129, 0.0000,  ..., 0.0000, 0.0000, 0.0067],
        [0.0000, 0.0146, 0.0000,  ..., 0.0000, 0.0000, 0.0076],
        [0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0000, 0.0110],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(300277., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.4762e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.8937e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 6.0595e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 5.4014e-04, 0.0000e+00,  ..., 1.6639e-02, 8.3779e-02,
         1.0959e-01],
        [0.0000e+00, 5.4014e-04, 0.0000e+00,  ..., 1.6639e-02, 8.3779e-02,
         1.0959e-01],
        [0.0000e+00, 5.4014e-04, 0.0000e+00,  ..., 1.6639e-02, 8.3779e-02,
         1.0959e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2669665.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1621.2798, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110176.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2992.3914, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(561.9689, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(592.3315, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6107],
        [ 0.5962],
        [ 0.5820],
        ...,
        [-5.2414],
        [-5.2357],
        [-5.2371]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-938514.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2423],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(665.2990, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2423],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(665.2990, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2350e-03,  2.1759e-03, -1.1918e-07,  ..., -8.3841e-03,
         -3.8891e-04, -3.5718e-03],
        [-6.0055e-03,  1.5804e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6161e-03],
        [-6.2350e-03,  2.1759e-03, -1.1918e-07,  ..., -8.3841e-03,
         -3.8891e-04, -3.5718e-03],
        ...,
        [-6.0055e-03,  1.5804e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6161e-03],
        [-6.0055e-03,  1.5804e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6161e-03],
        [-6.0055e-03,  1.5804e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6161e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-53325.0938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6534, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.8665, device='cuda:0')



h[100].sum tensor(-3.7623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.6254, device='cuda:0')



h[200].sum tensor(-567.5658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.8097, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(302699.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.2207e-03, 1.9448e-01,
         4.0825e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 2.5715e-01,
         9.8944e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 5.4734e-06, 2.4040e-01,
         1.9633e-02],
        ...,
        [0.0000e+00, 8.7550e-04, 0.0000e+00,  ..., 1.6537e-02, 8.3625e-02,
         1.0966e-01],
        [0.0000e+00, 8.7550e-04, 0.0000e+00,  ..., 1.6537e-02, 8.3625e-02,
         1.0966e-01],
        [0.0000e+00, 8.7550e-04, 0.0000e+00,  ..., 1.6537e-02, 8.3625e-02,
         1.0966e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2670815., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1488.8269, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110451.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2963.7871, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(801.7274, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(453.4886, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0184],
        [ 0.4132],
        [ 0.5758],
        ...,
        [-5.2623],
        [-5.2556],
        [-5.2566]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-984013.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5176],
        [0.3240],
        [0.4539],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(806.5464, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5176],
        [0.3240],
        [0.4539],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(806.5464, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.2021e-03,  4.6771e-03, -3.2606e-07,  ..., -1.0129e-02,
         -1.9881e-03,  5.0589e-03],
        [-6.9256e-03,  3.9588e-03, -2.5070e-07,  ..., -9.6302e-03,
         -1.5286e-03,  2.5940e-03],
        [-6.6739e-03,  3.3051e-03, -1.8213e-07,  ..., -9.1760e-03,
         -1.1105e-03,  3.5076e-04],
        ...,
        [-6.0055e-03,  1.5689e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6067e-03],
        [-6.0055e-03,  1.5689e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6067e-03],
        [-6.0055e-03,  1.5689e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6067e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-51832.1328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1960, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.4197, device='cuda:0')



h[100].sum tensor(-4.5019, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(112.2904, device='cuda:0')



h[200].sum tensor(-568.3156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.2000, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0213, 0.0000,  ..., 0.0000, 0.0000, 0.0291],
        [0.0000, 0.0189, 0.0000,  ..., 0.0000, 0.0000, 0.0210],
        [0.0000, 0.0194, 0.0000,  ..., 0.0000, 0.0000, 0.0225],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(322368.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8420, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8100, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7893, 0.0000],
        ...,
        [0.0000, 0.0012, 0.0000,  ..., 0.0168, 0.0845, 0.1074],
        [0.0000, 0.0012, 0.0000,  ..., 0.0168, 0.0845, 0.1074],
        [0.0000, 0.0012, 0.0000,  ..., 0.0168, 0.0845, 0.1074]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2764024.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1452.1023, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(107061.8359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2959.8521, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1650.2280, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(318.5223, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4682],
        [ 0.4715],
        [ 0.4882],
        ...,
        [-5.2410],
        [-5.2323],
        [-5.2305]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-831443.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6113],
        [0.6641],
        [0.5474],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(681.1772, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6113],
        [0.6641],
        [0.5474],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(681.1772, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.0441e-03,  4.2799e-03, -5.0750e-08,  ..., -9.8441e-03,
         -1.6917e-03,  3.6593e-03],
        [-7.5477e-03,  5.5873e-03, -7.5356e-08,  ..., -1.0753e-02,
         -2.5119e-03,  8.1492e-03],
        [-7.6032e-03,  5.7313e-03, -7.8068e-08,  ..., -1.0853e-02,
         -2.6023e-03,  8.6440e-03],
        ...,
        [-6.0055e-03,  1.5834e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6008e-03],
        [-6.0055e-03,  1.5834e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6008e-03],
        [-6.0055e-03,  1.5834e-03,  0.0000e+00,  ..., -7.9700e-03,
          0.0000e+00, -5.6008e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-52522.8359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7332, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.6032, device='cuda:0')



h[100].sum tensor(-3.7288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(94.8361, device='cuda:0')



h[200].sum tensor(-567.6760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.6405, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0217, 0.0000,  ..., 0.0000, 0.0000, 0.0302],
        [0.0000, 0.0219, 0.0000,  ..., 0.0000, 0.0000, 0.0310],
        [0.0000, 0.0197, 0.0000,  ..., 0.0000, 0.0000, 0.0234],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(311335.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 8.2288e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 8.2786e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 7.8463e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 7.7386e-04, 0.0000e+00,  ..., 1.7130e-02, 8.5675e-02,
         1.0568e-01],
        [0.0000e+00, 7.7386e-04, 0.0000e+00,  ..., 1.7130e-02, 8.5675e-02,
         1.0568e-01],
        [0.0000e+00, 7.7386e-04, 0.0000e+00,  ..., 1.7130e-02, 8.5675e-02,
         1.0568e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2677595., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1225.2932, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(107442.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2897.1599, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(894.9559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(100.0385, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4660],
        [ 0.4639],
        [ 0.4755],
        ...,
        [-5.2177],
        [-5.2092],
        [-5.2075]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-808887., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(652.0826, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(652.0826, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0056],
        ...,
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0015,  0.0000,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-52756.6719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.2533, device='cuda:0')



h[100].sum tensor(-3.4930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(90.7854, device='cuda:0')



h[200].sum tensor(-567.5229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.1182, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(309272.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0015, 0.0000,  ..., 0.0171, 0.0869, 0.1037],
        [0.0000, 0.0015, 0.0000,  ..., 0.0171, 0.0869, 0.1037],
        [0.0000, 0.0017, 0.0000,  ..., 0.0172, 0.0873, 0.1036],
        ...,
        [0.0000, 0.0012, 0.0000,  ..., 0.0170, 0.0861, 0.1038],
        [0.0000, 0.0012, 0.0000,  ..., 0.0170, 0.0861, 0.1038],
        [0.0000, 0.0012, 0.0000,  ..., 0.0170, 0.0861, 0.1038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2658863.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1306.3517, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(107524.6172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2899.6650, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(685.1908, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(144.7738, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.8604],
        [-5.2135],
        [-5.4076],
        ...,
        [-5.1867],
        [-5.1785],
        [-5.1768]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-695457.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(621.3097, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(621.3097, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0014,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0016, -0.0014,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0016, -0.0014,  ..., -0.0080,  0.0000, -0.0056],
        ...,
        [-0.0060,  0.0016, -0.0014,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0016, -0.0014,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0016, -0.0014,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-53409.3906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4968, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.8256, device='cuda:0')



h[100].sum tensor(-3.2442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(86.5011, device='cuda:0')



h[200].sum tensor(-567.3493, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.5081, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(301614.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0002, 0.0000,  ..., 0.0160, 0.0862, 0.1063],
        [0.0000, 0.0002, 0.0000,  ..., 0.0160, 0.0862, 0.1063],
        [0.0000, 0.0004, 0.0000,  ..., 0.0160, 0.0866, 0.1062],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0159, 0.0854, 0.1064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0159, 0.0854, 0.1064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0159, 0.0854, 0.1064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2612909.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1332.4178, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110415.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2993.3677, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(376.8416, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(349.9148, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.6057],
        [-5.5591],
        [-5.3865],
        ...,
        [-5.2674],
        [-5.2588],
        [-5.2570]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-908340.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(743.5371, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(743.5371, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015, -0.0027,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0015, -0.0027,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0063,  0.0022, -0.0029,  ..., -0.0084, -0.0004, -0.0033],
        ...,
        [-0.0060,  0.0015, -0.0027,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0015, -0.0027,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0015, -0.0027,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-52712.9609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.4964, device='cuda:0')



h[100].sum tensor(-3.8063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(103.5181, device='cuda:0')



h[200].sum tensor(-567.9767, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.9033, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(312084.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.1622, 0.0555],
        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.2703, 0.0229],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3898, 0.0067],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0849, 0.1063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0849, 0.1063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0849, 0.1063]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2669508.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1523.1204, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109954.7656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3051.9526, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1085.8469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(547.8143, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4287],
        [ 0.1366],
        [ 0.4695],
        ...,
        [-5.2840],
        [-5.2753],
        [-5.2734]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-912205.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 250.0 event: 3750 loss: tensor(477.1369, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(623.6949, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(623.6949, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0039,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0014, -0.0039,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0014, -0.0039,  ..., -0.0080,  0.0000, -0.0056],
        ...,
        [-0.0060,  0.0014, -0.0039,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0014, -0.0039,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0014, -0.0039,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-53948.7891, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.9363, device='cuda:0')



h[100].sum tensor(-3.1071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(86.8332, device='cuda:0')



h[200].sum tensor(-567.3439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.6329, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(301504.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0009, 0.0000,  ..., 0.0157, 0.0859, 0.1042],
        [0.0000, 0.0009, 0.0000,  ..., 0.0157, 0.0859, 0.1042],
        [0.0000, 0.0010, 0.0000,  ..., 0.0157, 0.0863, 0.1042],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0156, 0.0851, 0.1044],
        [0.0000, 0.0005, 0.0000,  ..., 0.0156, 0.0851, 0.1044],
        [0.0000, 0.0005, 0.0000,  ..., 0.0156, 0.0851, 0.1044]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2635068.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1615.0449, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110267.8906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(3012.5784, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(667.1784, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(572.4665, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.5866],
        [-5.5038],
        [-5.3113],
        ...,
        [-5.2575],
        [-5.2491],
        [-5.2474]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-762433.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(695.0833, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(695.0833, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0063,  0.0021, -0.0053,  ..., -0.0084, -0.0004, -0.0033],
        [-0.0060,  0.0015, -0.0049,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0015, -0.0049,  ..., -0.0080,  0.0000, -0.0056],
        ...,
        [-0.0060,  0.0015, -0.0049,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0015, -0.0049,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0015, -0.0049,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-53506.5977, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.2484, device='cuda:0')



h[100].sum tensor(-3.4101, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.7721, device='cuda:0')



h[200].sum tensor(-567.7258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.3681, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(308188.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0013, 0.2394, 0.0223],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.1697, 0.0492],
        [0.0000, 0.0000, 0.0000,  ..., 0.0107, 0.1186, 0.0832],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0846, 0.1053],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0846, 0.1053],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0846, 0.1053]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2642136.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1465.6482, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109842.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2939.1746, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(740.1135, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(630.4691, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.2736],
        [-1.4741],
        [-2.8587],
        ...,
        [-5.3162],
        [-5.3076],
        [-5.3058]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-864251.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(654.2928, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(654.2928, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015, -0.0059,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0015, -0.0059,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0015, -0.0059,  ..., -0.0080,  0.0000, -0.0056],
        ...,
        [-0.0060,  0.0015, -0.0059,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0015, -0.0059,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0015, -0.0059,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-53861.2031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.3559, device='cuda:0')



h[100].sum tensor(-3.1480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.0931, device='cuda:0')



h[200].sum tensor(-567.5223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.2339, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306894.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0012, 0.2354, 0.0226],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.2169, 0.0267],
        [0.0000, 0.0000, 0.0000,  ..., 0.0023, 0.1929, 0.0315],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0849, 0.1033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0849, 0.1033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0849, 0.1033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2617217.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1591.7134, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109152.0938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2789.1606, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(656.3187, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(748.5440, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2573],
        [ 0.5434],
        [ 0.6333],
        ...,
        [-5.2998],
        [-5.2364],
        [-5.0600]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-849737.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(667.7500, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(667.7500, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062,  0.0020, -0.0062,  ..., -0.0083, -0.0003, -0.0039],
        [-0.0060,  0.0015, -0.0059,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0015, -0.0059,  ..., -0.0080,  0.0000, -0.0056],
        ...,
        [-0.0060,  0.0015, -0.0059,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0015, -0.0059,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0015, -0.0059,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-53772.6992, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.9802, device='cuda:0')



h[100].sum tensor(-3.2001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.9667, device='cuda:0')



h[200].sum tensor(-567.5776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.9380, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(307810.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0081, 0.1447, 0.0643],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.1336, 0.0717],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.1018, 0.0928],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0849, 0.1033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0849, 0.1033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0849, 0.1033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2621436.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1592.2651, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109017.6562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2790.0957, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(665.2371, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(752.9709, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.2399],
        [-3.9082],
        [-4.4628],
        ...,
        [-5.3140],
        [-5.3055],
        [-5.3038]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-844118.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2703],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(691.0382, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2703],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(691.0382, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0068,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0063,  0.0021, -0.0072,  ..., -0.0084, -0.0004, -0.0033],
        [-0.0062,  0.0020, -0.0071,  ..., -0.0084, -0.0003, -0.0037],
        ...,
        [-0.0060,  0.0014, -0.0068,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0014, -0.0068,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0014, -0.0068,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-53508.0742, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7532, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.0607, device='cuda:0')



h[100].sum tensor(-3.2498, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.2090, device='cuda:0')



h[200].sum tensor(-567.7036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.1564, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(314899.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0071, 0.1644, 0.0472],
        [0.0000, 0.0000, 0.0000,  ..., 0.0023, 0.2301, 0.0217],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2901, 0.0007],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0851, 0.1001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0851, 0.1001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0851, 0.1001]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2648004., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1654.6127, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(107116.6562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2461.0457, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(935.3069, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(798.0736, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4814],
        [-0.3312],
        [ 0.2513],
        ...,
        [-5.2796],
        [-5.2714],
        [-5.2697]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-692315.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(801.1567, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(801.1567, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6315e-03,  3.0208e-03, -8.7704e-03,  ..., -9.0996e-03,
         -8.6798e-04,  4.0820e-06],
        [-6.0055e-03,  1.4558e-03, -7.5267e-03,  ..., -7.9700e-03,
          0.0000e+00, -5.5810e-03],
        [-6.5810e-03,  2.8943e-03, -8.6698e-03,  ..., -9.0083e-03,
         -7.9783e-04, -4.4730e-04],
        ...,
        [-6.0055e-03,  1.4558e-03, -7.5267e-03,  ..., -7.9700e-03,
          0.0000e+00, -5.5810e-03],
        [-6.0055e-03,  1.4558e-03, -7.5267e-03,  ..., -7.9700e-03,
          0.0000e+00, -5.5810e-03],
        [-6.0055e-03,  1.4558e-03, -7.5267e-03,  ..., -7.9700e-03,
          0.0000e+00, -5.5810e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-53136.6055, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1794, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.1696, device='cuda:0')



h[100].sum tensor(-3.7137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(111.5401, device='cuda:0')



h[200].sum tensor(-568.2926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.9181, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 1.1591e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         4.8620e-03],
        [0.0000e+00, 1.1279e-02, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         4.0820e-06],
        [0.0000e+00, 8.4363e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 5.8232e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.8232e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.8232e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(321743.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4407, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3262, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0844, 0.1000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0844, 0.1000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0844, 0.1000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2685929., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1640.1750, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(106635.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2654.1453, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1324.1233, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(770.6571, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7999],
        [ 0.8386],
        [ 0.8441],
        ...,
        [-5.3250],
        [-5.3167],
        [-5.3150]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-753462.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(692.3062, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(692.3062, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015, -0.0082,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0015, -0.0082,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0015, -0.0082,  ..., -0.0080,  0.0000, -0.0056],
        ...,
        [-0.0060,  0.0015, -0.0082,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0015, -0.0082,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0015, -0.0082,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-54618.9062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7786, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.1195, device='cuda:0')



h[100].sum tensor(-3.1467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.3855, device='cuda:0')



h[200].sum tensor(-567.7386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.2228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(309099.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.1204, 0.0765],
        [0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0896, 0.0970],
        [0.0000, 0.0000, 0.0000,  ..., 0.0158, 0.0844, 0.1007],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0832, 0.1010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0832, 0.1010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0832, 0.1010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2634109., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1569.8845, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(108440.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2839.9282, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(859.2466, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(702.4702, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9170],
        [-4.4068],
        [-5.3897],
        ...,
        [-5.3812],
        [-5.3701],
        [-5.3663]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-829790.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(771.8138, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(771.8138, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0070,  0.0039, -0.0113,  ..., -0.0098, -0.0014,  0.0037],
        [-0.0071,  0.0040, -0.0113,  ..., -0.0099, -0.0014,  0.0038],
        [-0.0068,  0.0033, -0.0107,  ..., -0.0094, -0.0010,  0.0013],
        ...,
        [-0.0060,  0.0014, -0.0089,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0014, -0.0089,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0014, -0.0089,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-54497.0703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.8083, device='cuda:0')



h[100].sum tensor(-3.4447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(107.4548, device='cuda:0')



h[200].sum tensor(-568.1592, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.3828, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0058],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0070],
        [0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        ...,
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(312504., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4725, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4921, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4125, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0818, 0.1025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0818, 0.1025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0149, 0.0818, 0.1025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2658218.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1586.3838, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109698.0938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2976.6665, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1090.1208, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(675.6718, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.8886],
        [ 0.9372],
        [ 0.8782],
        ...,
        [-5.4491],
        [-5.4402],
        [-5.4384]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-905591.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1008.4421, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1008.4421, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0094,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0014, -0.0094,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0014, -0.0094,  ..., -0.0080,  0.0000, -0.0056],
        ...,
        [-0.0060,  0.0014, -0.0094,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0014, -0.0094,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0014, -0.0094,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-52980.5000, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.9438, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(46.7866, device='cuda:0')



h[100].sum tensor(-4.3799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(140.3991, device='cuda:0')



h[200].sum tensor(-569.3489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(52.7636, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(339309.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0141, 0.0903, 0.0960],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.1121, 0.0814],
        [0.0000, 0.0000, 0.0000,  ..., 0.0062, 0.1520, 0.0549],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0818, 0.1014],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0818, 0.1014],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0818, 0.1014]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2831405.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1699.9772, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(107286.9688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2995.9678, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2468.8806, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(683.6799, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.4577],
        [-3.3846],
        [-2.1019],
        ...,
        [-5.4448],
        [-5.4361],
        [-5.4343]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-779268.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(609.0077, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(609.0077, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0063,  0.0020, -0.0106,  ..., -0.0084, -0.0003, -0.0033],
        [-0.0060,  0.0014, -0.0099,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0014, -0.0099,  ..., -0.0080,  0.0000, -0.0056],
        ...,
        [-0.0060,  0.0014, -0.0099,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0014, -0.0099,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0014, -0.0099,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-56607.5898, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4516, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.2549, device='cuda:0')



h[100].sum tensor(-2.5780, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(84.7884, device='cuda:0')



h[200].sum tensor(-567.2868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.8644, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(298709.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.2455, 0.0199],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.1651, 0.0464],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.1083, 0.0827],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0815, 0.1003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0815, 0.1003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0815, 0.1003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2594393., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1539.6399, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(111119.3359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2903.7383, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(461.6341, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(571.7538, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3451],
        [-1.8297],
        [-3.4302],
        ...,
        [-5.4734],
        [-5.4646],
        [-5.4628]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-857194.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 260.0 event: 3900 loss: tensor(417.7779, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(663.1602, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(663.1602, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0104,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0104,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0104,  ..., -0.0080,  0.0000, -0.0055],
        ...,
        [-0.0060,  0.0014, -0.0104,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0104,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0104,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-56408.2383, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.7673, device='cuda:0')



h[100].sum tensor(-2.7446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.3277, device='cuda:0')



h[200].sum tensor(-567.5591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.6978, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(302995.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0849, 0.0967],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0824, 0.0984],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0827, 0.0983],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0816, 0.0986],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0816, 0.0986],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0816, 0.0986]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2599737.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1554.5073, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109420.9062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2899.8931, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(597.7706, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(595.6368, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.1989],
        [-5.6006],
        [-5.7298],
        ...,
        [-5.4912],
        [-5.4825],
        [-5.4807]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-882132.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(759.9287, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(759.9287, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0108,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0108,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0108,  ..., -0.0080,  0.0000, -0.0055],
        ...,
        [-0.0060,  0.0014, -0.0108,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0108,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0108,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-55794.4844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.2569, device='cuda:0')



h[100].sum tensor(-3.0870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(105.8001, device='cuda:0')



h[200].sum tensor(-568.0632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.7609, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(315740.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0891, 0.0921],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0838, 0.0956],
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0831, 0.0963],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0819, 0.0966],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0819, 0.0966],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0819, 0.0966]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2685746., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1644.2394, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(106864.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2883.7969, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1149.0524, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(572.3076, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.1077],
        [-4.9530],
        [-5.4011],
        ...,
        [-5.4662],
        [-5.4578],
        [-5.4561]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-740886.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3884],
        [0.6343],
        [0.3738],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(657.2925, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3884],
        [0.6343],
        [0.3738],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(657.2925, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0071,  0.0040, -0.0143,  ..., -0.0099, -0.0013,  0.0040],
        [-0.0067,  0.0032, -0.0133,  ..., -0.0093, -0.0009,  0.0009],
        [-0.0070,  0.0038, -0.0140,  ..., -0.0097, -0.0012,  0.0031],
        ...,
        [-0.0060,  0.0014, -0.0112,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0112,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0112,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-56932.7891, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.4951, device='cuda:0')



h[100].sum tensor(-2.6257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.5107, device='cuda:0')



h[200].sum tensor(-567.5544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.3908, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0138, 0.0000,  ..., 0.0000, 0.0000, 0.0080],
        [0.0000, 0.0160, 0.0000,  ..., 0.0000, 0.0000, 0.0160],
        [0.0000, 0.0148, 0.0000,  ..., 0.0000, 0.0000, 0.0117],
        ...,
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(303971.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6419, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6442, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5905, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0812, 0.0970],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0812, 0.0970],
        [0.0000, 0.0000, 0.0000,  ..., 0.0145, 0.0812, 0.0970]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2612583.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1613.3696, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109971.0312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2865.4570, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(547.9746, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(511.0195, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7788],
        [ 0.7921],
        [ 0.8217],
        ...,
        [-5.4756],
        [-5.4675],
        [-5.4668]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-758518.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(627.7366, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(627.7366, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0115,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0115,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0115,  ..., -0.0080,  0.0000, -0.0055],
        ...,
        [-0.0060,  0.0014, -0.0115,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0115,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0115,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57559.1719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5275, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.1238, device='cuda:0')



h[100].sum tensor(-2.4460, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.3959, device='cuda:0')



h[200].sum tensor(-567.3917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.8444, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(301667.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0805, 0.0978],
        [0.0000, 0.0000, 0.0000,  ..., 0.0127, 0.0816, 0.0970],
        [0.0000, 0.0000, 0.0000,  ..., 0.0121, 0.0870, 0.0934],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0128, 0.0798, 0.0980],
        [0.0000, 0.0000, 0.0000,  ..., 0.0128, 0.0798, 0.0980],
        [0.0000, 0.0000, 0.0000,  ..., 0.0128, 0.0798, 0.0980]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2603831., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1596.8333, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(112233.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2914.3433, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(563.1260, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(549.6029, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.6543],
        [-5.4129],
        [-4.8976],
        ...,
        [-5.5593],
        [-5.5506],
        [-5.5488]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-901927.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(688.5387, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(688.5387, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0118,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0118,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0118,  ..., -0.0080,  0.0000, -0.0055],
        ...,
        [-0.0060,  0.0014, -0.0118,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0118,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0118,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57201.8164, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.9447, device='cuda:0')



h[100].sum tensor(-2.6231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(95.8610, device='cuda:0')



h[200].sum tensor(-567.6998, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.0257, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306859.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.0887, 0.0902],
        [0.0000, 0.0000, 0.0000,  ..., 0.0119, 0.0894, 0.0898],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.0956, 0.0859],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0801, 0.0960],
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0801, 0.0960],
        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0801, 0.0960]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2626262.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1638.5884, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(111070.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2906.8354, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(682.7727, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(530.6943, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.4654],
        [-4.1054],
        [-3.4774],
        ...,
        [-5.5458],
        [-5.5373],
        [-5.5353]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-821229.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(719.2560, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(719.2560, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064,  0.0024, -0.0134,  ..., -0.0087, -0.0005, -0.0019],
        [-0.0060,  0.0014, -0.0121,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0121,  ..., -0.0080,  0.0000, -0.0055],
        ...,
        [-0.0060,  0.0014, -0.0121,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0121,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0121,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57064.0625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8774, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.3699, device='cuda:0')



h[100].sum tensor(-2.6947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(100.1375, device='cuda:0')



h[200].sum tensor(-567.8752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.6328, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(312196.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2799, 0.0132],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.1959, 0.0222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.1952, 0.0200],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0043, 0.1442, 0.0496],
        [0.0000, 0.0000, 0.0000,  ..., 0.0079, 0.1064, 0.0760],
        [0.0000, 0.0000, 0.0000,  ..., 0.0129, 0.0804, 0.0941]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2654398.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1660.3440, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110395.4375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2918.7485, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(912.0421, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(538.9073, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7708],
        [ 0.7870],
        [ 0.8336],
        ...,
        [-2.4042],
        [-3.8849],
        [-4.8988]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-808207.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3687],
        [0.3198],
        [0.3198],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(731.7681, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3687],
        [0.3198],
        [0.3198],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(731.7681, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0063,  0.0022, -0.0133,  ..., -0.0085, -0.0003, -0.0028],
        [-0.0070,  0.0039, -0.0156,  ..., -0.0098, -0.0011,  0.0035],
        [-0.0070,  0.0039, -0.0156,  ..., -0.0098, -0.0011,  0.0034],
        ...,
        [-0.0060,  0.0014, -0.0124,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0124,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0124,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57100., device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.9503, device='cuda:0')



h[100].sum tensor(-2.6616, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(101.8795, device='cuda:0')



h[200].sum tensor(-567.9100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.2875, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0097],
        [0.0000, 0.0140, 0.0000,  ..., 0.0000, 0.0000, 0.0110],
        [0.0000, 0.0148, 0.0000,  ..., 0.0000, 0.0000, 0.0115],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(312622.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5531, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6302, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6611, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0812, 0.0922],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0812, 0.0922],
        [0.0000, 0.0000, 0.0000,  ..., 0.0134, 0.0812, 0.0922]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2653459.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1677.8794, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109847.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2901.8999, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(969.1072, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(517.2286, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6880],
        [ 0.6858],
        [ 0.6701],
        ...,
        [-5.5248],
        [-5.5001],
        [-5.4390]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-753016.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3723],
        [0.3076],
        [0.3792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(736.1877, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3723],
        [0.3076],
        [0.3792],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(736.1877, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0073,  0.0047, -0.0170,  ..., -0.0104, -0.0014,  0.0066],
        [-0.0070,  0.0039, -0.0159,  ..., -0.0098, -0.0011,  0.0035],
        [-0.0063,  0.0022, -0.0135,  ..., -0.0085, -0.0003, -0.0029],
        ...,
        [-0.0060,  0.0015, -0.0126,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0015, -0.0126,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0015, -0.0126,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57178.9609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.1554, device='cuda:0')



h[100].sum tensor(-2.6219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(102.4948, device='cuda:0')



h[200].sum tensor(-567.9354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.5187, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0154, 0.0000,  ..., 0.0000, 0.0000, 0.0134],
        [0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0099],
        [0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0000, 0.0049],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(310804.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5482, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4879, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3918, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0815, 0.0908],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0815, 0.0908],
        [0.0000, 0.0000, 0.0000,  ..., 0.0133, 0.0815, 0.0908]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2633945., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1686.3688, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110453.8438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2907.6191, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(709.2556, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(502.3478, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.8729],
        [ 0.9482],
        [ 1.0134],
        ...,
        [-5.5403],
        [-5.5321],
        [-5.5305]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-727012., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(872.1223, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(872.1223, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0067,  0.0032, -0.0152,  ..., -0.0093, -0.0008,  0.0011],
        [-0.0060,  0.0015, -0.0128,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0015, -0.0128,  ..., -0.0080,  0.0000, -0.0055],
        ...,
        [-0.0060,  0.0015, -0.0128,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0015, -0.0128,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0015, -0.0128,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-56525.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.4067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.4621, device='cuda:0')



h[100].sum tensor(-3.0234, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(121.4202, device='cuda:0')



h[200].sum tensor(-568.6067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(45.6311, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0076],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(320834.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4654, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3053, 0.0145],
        [0.0000, 0.0000, 0.0000,  ..., 0.0040, 0.1603, 0.0404],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.0910, 0.0840],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.1263, 0.0591],
        [0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.2154, 0.0237]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2676978.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1687.6331, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(111994.0469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2969.3462, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1148.6058, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(564.1122, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4252],
        [-0.2035],
        [-0.9096],
        ...,
        [-4.4823],
        [-3.0551],
        [-1.2670]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-855133.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(674.3522, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(674.3522, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0063,  0.0022, -0.0141,  ..., -0.0085, -0.0003, -0.0026],
        [-0.0060,  0.0014, -0.0130,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0130,  ..., -0.0080,  0.0000, -0.0055],
        ...,
        [-0.0060,  0.0014, -0.0130,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0130,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0014, -0.0130,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-58436.1719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6886, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.2865, device='cuda:0')



h[100].sum tensor(-2.2951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.8859, device='cuda:0')



h[200].sum tensor(-567.6143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.2834, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(305520.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0053, 0.1413, 0.0497],
        [0.0000, 0.0000, 0.0000,  ..., 0.0079, 0.1256, 0.0606],
        [0.0000, 0.0000, 0.0000,  ..., 0.0113, 0.0991, 0.0790],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0807, 0.0913],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0807, 0.0913],
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0807, 0.0913]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2620635.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1649.6040, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(113730.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2918.0103, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(540.6452, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(495.4662, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.8355],
        [-3.8836],
        [-4.5042],
        ...,
        [-5.6259],
        [-5.6169],
        [-5.6151]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-793829.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 270.0 event: 4050 loss: tensor(474.1170, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(817.7351, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(817.7351, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0066,  0.0028, -0.0151,  ..., -0.0090, -0.0006, -0.0004],
        [-0.0060,  0.0015, -0.0131,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0015, -0.0131,  ..., -0.0080,  0.0000, -0.0055],
        ...,
        [-0.0060,  0.0015, -0.0131,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0015, -0.0131,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0015, -0.0131,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57469.6406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.9388, device='cuda:0')



h[100].sum tensor(-2.7341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(113.8482, device='cuda:0')



h[200].sum tensor(-568.3627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.7855, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0041],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(318058.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3084, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3776, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5275, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0807, 0.0904],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0807, 0.0904],
        [0.0000, 0.0000, 0.0000,  ..., 0.0144, 0.0807, 0.0904]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2672770.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1690.3276, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(111610.2031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2921.9133, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1048.8778, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(537.1171, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7012],
        [ 0.6255],
        [ 0.5341],
        ...,
        [-5.6578],
        [-5.6488],
        [-5.6469]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-824562.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(742.9426, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(742.9426, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015, -0.0133,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0015, -0.0133,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0015, -0.0133,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0015, -0.0133,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0015, -0.0133,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0015, -0.0133,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-58255.0625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.4688, device='cuda:0')



h[100].sum tensor(-2.4374, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(103.4353, device='cuda:0')



h[200].sum tensor(-567.9919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.8722, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(311835.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0815, 0.0894],
        [0.0000, 0.0000, 0.0000,  ..., 0.0156, 0.0815, 0.0894],
        [0.0000, 0.0000, 0.0000,  ..., 0.0157, 0.0819, 0.0892],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0808, 0.0896],
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0808, 0.0896],
        [0.0000, 0.0000, 0.0000,  ..., 0.0155, 0.0808, 0.0896]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2645588.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1673.3569, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110988.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2875.9932, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(759.7039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(500.2845, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.0737],
        [-6.1088],
        [-6.1110],
        ...,
        [-5.6770],
        [-5.6682],
        [-5.6664]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-813783.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(835.6107, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(835.6107, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015, -0.0134,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0015, -0.0134,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0015, -0.0134,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0015, -0.0134,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0015, -0.0134,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0015, -0.0134,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57685.7148, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.3042, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.7681, device='cuda:0')



h[100].sum tensor(-2.6798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(116.3369, device='cuda:0')



h[200].sum tensor(-568.4651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.7207, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(318069.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0120, 0.1083, 0.0707],
        [0.0000, 0.0000, 0.0000,  ..., 0.0161, 0.0841, 0.0876],
        [0.0000, 0.0000, 0.0000,  ..., 0.0166, 0.0817, 0.0894],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0136, 0.0989, 0.0769],
        [0.0000, 0.0000, 0.0000,  ..., 0.0154, 0.0873, 0.0851],
        [0.0000, 0.0000, 0.0000,  ..., 0.0164, 0.0806, 0.0898]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2672387., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1711.7723, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109958.6406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2842.9983, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(992.8604, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(486.4141, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3881],
        [-4.6754],
        [-5.4406],
        ...,
        [-5.1247],
        [-5.3832],
        [-5.5606]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-770723.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(676.7633, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(676.7633, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0070,  0.0038, -0.0170,  ..., -0.0097, -0.0009,  0.0033],
        [-0.0065,  0.0026, -0.0151,  ..., -0.0088, -0.0004, -0.0014],
        [-0.0062,  0.0020, -0.0143,  ..., -0.0084, -0.0002, -0.0035],
        ...,
        [-0.0060,  0.0015, -0.0135,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0015, -0.0135,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0015, -0.0135,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-59186.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7165, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.3984, device='cuda:0')



h[100].sum tensor(-2.1246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(94.2215, device='cuda:0')



h[200].sum tensor(-567.6530, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.4095, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.0000, 0.0033],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(308732.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4102, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4022, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3382, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0804, 0.0903],
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0804, 0.0903],
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0804, 0.0903]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2653471.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1694.7461, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110662.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2801.6606, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(682.6882, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(429.6357, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.0839],
        [ 1.0963],
        [ 1.0701],
        ...,
        [-5.7023],
        [-5.6940],
        [-5.6924]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-727662.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(642.4899, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(642.4899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0063,  0.0023, -0.0147,  ..., -0.0085, -0.0003, -0.0028],
        [-0.0060,  0.0016, -0.0136,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0136,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0016, -0.0136,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0136,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0136,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-59732.5352, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5821, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.8083, device='cuda:0')



h[100].sum tensor(-1.9667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.4499, device='cuda:0')



h[200].sum tensor(-567.4672, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.6163, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(303579.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0089, 0.1382, 0.0506],
        [0.0000, 0.0000, 0.0000,  ..., 0.0111, 0.1238, 0.0608],
        [0.0000, 0.0000, 0.0000,  ..., 0.0153, 0.0939, 0.0814],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0795, 0.0909],
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0795, 0.0909],
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0795, 0.0909]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2616928.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1654.2592, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(111982.8281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2809.4824, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(460.4142, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(444.5887, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3989],
        [-4.1301],
        [-4.8739],
        ...,
        [-5.7828],
        [-5.7741],
        [-5.7724]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-863097.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(629.4998, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(629.4998, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0137,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0137,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0137,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0016, -0.0137,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0137,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0137,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-60006.5352, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.2056, device='cuda:0')



h[100].sum tensor(-1.8851, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.6413, device='cuda:0')



h[200].sum tensor(-567.4018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.9366, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(301685.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0110, 0.1117, 0.0689],
        [0.0000, 0.0000, 0.0000,  ..., 0.0137, 0.0993, 0.0779],
        [0.0000, 0.0000, 0.0000,  ..., 0.0104, 0.1237, 0.0611],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0790, 0.0916],
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0790, 0.0916],
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0790, 0.0916]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2601984., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1645.3271, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(112820.7578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2799.4365, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(397.3228, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(439.6820, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.9233],
        [-3.0204],
        [-2.1774],
        ...,
        [-5.8076],
        [-5.7989],
        [-5.7972]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-882978.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.9556],
        [0.0000],
        [0.4387],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(605.6528, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.9556],
        [0.0000],
        [0.4387],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(605.6528, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0138,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0073,  0.0047, -0.0187,  ..., -0.0104, -0.0012,  0.0065],
        [-0.0064,  0.0024, -0.0151,  ..., -0.0086, -0.0003, -0.0023],
        ...,
        [-0.0060,  0.0016, -0.0138,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0138,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0138,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-60195.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.0992, device='cuda:0')



h[100].sum tensor(-1.7721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(84.3213, device='cuda:0')



h[200].sum tensor(-567.2772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.6889, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0150, 0.0000,  ..., 0.0000, 0.0000, 0.0118],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0013],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0067],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(302110.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3789, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3540, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4389, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0179, 0.0797, 0.0910],
        [0.0000, 0.0000, 0.0000,  ..., 0.0179, 0.0797, 0.0910],
        [0.0000, 0.0000, 0.0000,  ..., 0.0179, 0.0797, 0.0910]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2618584.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1681.6118, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110769.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2744.6394, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(393.6470, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(381.9420, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7672],
        [ 0.9764],
        [ 1.0097],
        ...,
        [-5.7615],
        [-5.7532],
        [-5.7517]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-725486.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2661],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(934.2201, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2661],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(934.2201, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0139,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0063,  0.0022, -0.0148,  ..., -0.0084, -0.0002, -0.0031],
        [-0.0062,  0.0021, -0.0147,  ..., -0.0083, -0.0002, -0.0035],
        ...,
        [-0.0060,  0.0016, -0.0139,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0139,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0139,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57501.1719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.6879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(43.3431, device='cuda:0')



h[100].sum tensor(-2.6931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(130.0657, device='cuda:0')



h[200].sum tensor(-568.9952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(48.8802, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.0000, 0.0015],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(326807.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0082, 0.1528, 0.0408],
        [0.0000, 0.0000, 0.0000,  ..., 0.0035, 0.2397, 0.0196],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3767, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0794, 0.0906],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0794, 0.0906],
        [0.0000, 0.0000, 0.0000,  ..., 0.0174, 0.0794, 0.0906]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2709244.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1765.3802, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(108469.8125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2813.5537, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1268.8356, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(500.2435, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7786],
        [-0.0785],
        [ 0.3292],
        ...,
        [-5.4787],
        [-5.6493],
        [-5.7478]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-816457.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(742.4330, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(742.4330, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0140,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0140,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0140,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0016, -0.0140,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0140,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0140,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-59246.5195, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.4451, device='cuda:0')



h[100].sum tensor(-2.0690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(103.3643, device='cuda:0')



h[200].sum tensor(-567.9630, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.8455, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(312457.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0148, 0.0985, 0.0778],
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0824, 0.0890],
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0800, 0.0908],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0173, 0.0789, 0.0912],
        [0.0000, 0.0000, 0.0000,  ..., 0.0173, 0.0789, 0.0912],
        [0.0000, 0.0000, 0.0000,  ..., 0.0173, 0.0789, 0.0912]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2644581., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1723.3949, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109363.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2765.1099, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(689.8832, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(448.6813, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4391],
        [-4.2774],
        [-4.6930],
        ...,
        [-5.8051],
        [-5.7981],
        [-5.7977]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-790603.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(708.3690, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(708.3690, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0140,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0140,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0140,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0016, -0.0140,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0140,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0140,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-59612.8242, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8343, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.8647, device='cuda:0')



h[100].sum tensor(-1.9441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(98.6218, device='cuda:0')



h[200].sum tensor(-567.8157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.0632, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(309220.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0790, 0.0920],
        [0.0000, 0.0000, 0.0000,  ..., 0.0171, 0.0790, 0.0920],
        [0.0000, 0.0000, 0.0000,  ..., 0.0172, 0.0793, 0.0919],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0782, 0.0923],
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0782, 0.0923],
        [0.0000, 0.0000, 0.0000,  ..., 0.0170, 0.0782, 0.0923]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2627832.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1731.4984, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110028.0312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2750.5266, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(596.6233, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(433.5825, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.2073],
        [-6.2319],
        [-6.2454],
        ...,
        [-5.8291],
        [-5.8209],
        [-5.8193]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-770983.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 280.0 event: 4200 loss: tensor(478.7898, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(729.1576, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(729.1576, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0141,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0141,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0141,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0016, -0.0141,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0141,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0141,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-59652.6758, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.8292, device='cuda:0')



h[100].sum tensor(-1.9437, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(101.5161, device='cuda:0')



h[200].sum tensor(-567.8983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.1509, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(308614., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0147, 0.0885, 0.0856],
        [0.0000, 0.0000, 0.0000,  ..., 0.0160, 0.0811, 0.0909],
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0792, 0.0924],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0774, 0.0932],
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0774, 0.0932],
        [0.0000, 0.0000, 0.0000,  ..., 0.0163, 0.0774, 0.0932]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2613573.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1741.5557, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(110256.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2785.0112, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(448.1385, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(480.5328, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.2341],
        [-5.0391],
        [-5.2760],
        ...,
        [-5.8935],
        [-5.8848],
        [-5.8832]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-858424.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(587.8887, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(587.8887, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0142,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0142,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0142,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0016, -0.0142,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0142,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0142,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-60845.5703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.2751, device='cuda:0')



h[100].sum tensor(-1.5381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(81.8481, device='cuda:0')



h[200].sum tensor(-567.1895, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.7595, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(299068.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0121, 0.1151, 0.0671],
        [0.0000, 0.0000, 0.0000,  ..., 0.0152, 0.0942, 0.0818],
        [0.0000, 0.0000, 0.0000,  ..., 0.0158, 0.0900, 0.0847],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0773, 0.0931],
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0773, 0.0931],
        [0.0000, 0.0000, 0.0000,  ..., 0.0175, 0.0773, 0.0931]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2563664.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1679.1218, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(109788.6406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2724.0537, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(313.8499, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(443.5998, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4018],
        [-2.4446],
        [-3.1345],
        ...,
        [-5.8974],
        [-5.8889],
        [-5.8873]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-889846.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1014.1361, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1014.1361, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0142,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0142,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0142,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0017, -0.0142,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0142,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0142,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-57268.7930, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.9396, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(47.0508, device='cuda:0')



h[100].sum tensor(-2.5784, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(141.1919, device='cuda:0')



h[200].sum tensor(-569.3431, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(53.0615, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(333142.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0191, 0.0825, 0.0887],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0790, 0.0911],
        [0.0000, 0.0000, 0.0000,  ..., 0.0196, 0.0794, 0.0910],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0783, 0.0914],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0783, 0.0914],
        [0.0000, 0.0000, 0.0000,  ..., 0.0195, 0.0783, 0.0914]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2736097., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1898.5504, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(101737.5781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2725.2891, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1253.9393, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(468.1016, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.7429],
        [-6.0407],
        [-6.1515],
        ...,
        [-5.6735],
        [-5.7944],
        [-5.8284]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-647272.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(615.0874, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(615.0874, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0143,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0143,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0143,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0017, -0.0143,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0143,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0143,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-60379.9062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4839, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.5370, device='cuda:0')



h[100].sum tensor(-1.5388, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(85.6348, device='cuda:0')



h[200].sum tensor(-567.3315, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.1825, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306063.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0190, 0.0907, 0.0830],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.1241, 0.0599],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.1755, 0.0314],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0204, 0.0782, 0.0912],
        [0.0000, 0.0000, 0.0000,  ..., 0.0204, 0.0782, 0.0912],
        [0.0000, 0.0000, 0.0000,  ..., 0.0200, 0.0809, 0.0894]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2600554.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1730.6545, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(103833.3359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2442.9832, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(437.4506, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(408.1378, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.2513],
        [-1.9276],
        [-0.6644],
        ...,
        [-5.7556],
        [-5.4933],
        [-4.8762]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-706668.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(658.8330, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(658.8330, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0063,  0.0023, -0.0152,  ..., -0.0084, -0.0002, -0.0031],
        [-0.0060,  0.0017, -0.0143,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0143,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0017, -0.0143,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0143,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0143,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-60367.1484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.5665, device='cuda:0')



h[100].sum tensor(-1.6041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.7252, device='cuda:0')



h[200].sum tensor(-567.5417, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.4714, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306911.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0022, 0.2175, 0.0118],
        [0.0000, 0.0000, 0.0000,  ..., 0.0063, 0.1872, 0.0263],
        [0.0000, 0.0000, 0.0000,  ..., 0.0142, 0.1250, 0.0605],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0769, 0.0929],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0769, 0.0929],
        [0.0000, 0.0000, 0.0000,  ..., 0.0202, 0.0769, 0.0929]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2614689.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1771.2208, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(104285.9609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2639.6470, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(487.4824, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(399.6735, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6084],
        [-1.3262],
        [-2.5905],
        ...,
        [-5.9102],
        [-5.9019],
        [-5.9005]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-769977.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5029],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.3301]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(784.0759, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5029],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.3301]], device='cuda:0') 
g.ndata[nfet].sum tensor(784.0759, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0065,  0.0028, -0.0161,  ..., -0.0088, -0.0003, -0.0011],
        [-0.0064,  0.0026, -0.0158,  ..., -0.0087, -0.0003, -0.0018],
        [-0.0078,  0.0059, -0.0212,  ..., -0.0112, -0.0013,  0.0110],
        ...,
        [-0.0060,  0.0017, -0.0143,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0066,  0.0030, -0.0165,  ..., -0.0090, -0.0004, -0.0003],
        [-0.0063,  0.0023, -0.0153,  ..., -0.0084, -0.0002, -0.0031]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-59428.4102, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0972, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.3772, device='cuda:0')



h[100].sum tensor(-1.8646, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(109.1620, device='cuda:0')



h[200].sum tensor(-568.1788, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.0243, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0000, 0.0039],
        [0.0000, 0.0176, 0.0000,  ..., 0.0000, 0.0000, 0.0218],
        [0.0000, 0.0150, 0.0000,  ..., 0.0000, 0.0000, 0.0136],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.0000, 0.0002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(315978.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4400, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6832, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7463, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0056, 0.1972, 0.0281],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.2446, 0.0101],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2967, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2657485.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1793.4014, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(105335.6797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2569.3857, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(752.7363, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(437.0107, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7928],
        [ 0.7365],
        [ 0.6849],
        ...,
        [-2.1239],
        [-0.5463],
        [ 0.0267]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-836797.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(812.3116, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(812.3116, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0143,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0143,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0143,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0017, -0.0143,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0143,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0143,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-59212.7383, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1940, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.6872, device='cuda:0')



h[100].sum tensor(-1.9270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(113.0931, device='cuda:0')



h[200].sum tensor(-568.3127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.5017, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(316619.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 4.4655e-04, 0.0000e+00,  ..., 1.9518e-02, 7.6358e-02,
         9.4227e-02],
        [0.0000e+00, 4.4655e-04, 0.0000e+00,  ..., 1.9518e-02, 7.6358e-02,
         9.4227e-02],
        [0.0000e+00, 6.2894e-04, 0.0000e+00,  ..., 1.9570e-02, 7.6718e-02,
         9.4102e-02],
        ...,
        [0.0000e+00, 8.1766e-05, 0.0000e+00,  ..., 1.9412e-02, 7.5639e-02,
         9.4477e-02],
        [0.0000e+00, 8.1766e-05, 0.0000e+00,  ..., 1.9412e-02, 7.5639e-02,
         9.4477e-02],
        [0.0000e+00, 8.1766e-05, 0.0000e+00,  ..., 1.9412e-02, 7.5639e-02,
         9.4477e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2648754.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1787.2216, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(105801.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2567.5640, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(741.1336, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(442.5432, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.6593],
        [-5.5860],
        [-5.4933],
        ...,
        [-5.9538],
        [-5.9415],
        [-5.9435]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-833844., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(633.8058, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(633.8058, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062,  0.0022, -0.0151,  ..., -0.0083, -0.0001, -0.0036],
        [-0.0062,  0.0022, -0.0151,  ..., -0.0083, -0.0001, -0.0036],
        [-0.0060,  0.0017, -0.0144,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0017, -0.0144,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0144,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0144,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-60837.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5561, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.4054, device='cuda:0')



h[100].sum tensor(-1.4817, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(88.2408, device='cuda:0')



h[200].sum tensor(-567.4313, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.1619, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(304256.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.2070, 0.0161],
        [0.0000, 0.0000, 0.0000,  ..., 0.0028, 0.1904, 0.0210],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.1560, 0.0406],
        ...,
        [0.0000, 0.0005, 0.0000,  ..., 0.0186, 0.0744, 0.0960],
        [0.0000, 0.0005, 0.0000,  ..., 0.0186, 0.0744, 0.0960],
        [0.0000, 0.0005, 0.0000,  ..., 0.0186, 0.0744, 0.0960]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2613156.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1762.6299, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(108112.8516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2603.3013, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(436.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(407.8690, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8780],
        [-1.3093],
        [-2.3824],
        ...,
        [-6.0551],
        [-6.0463],
        [-6.0446]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-922251.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(730.3311, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(730.3311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0144,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0144,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0144,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0017, -0.0144,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0144,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0144,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-60248.1719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.8837, device='cuda:0')



h[100].sum tensor(-1.6671, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(101.6795, device='cuda:0')



h[200].sum tensor(-567.9240, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.2123, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(315458.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0006, 0.0000,  ..., 0.0182, 0.0748, 0.0954],
        [0.0000, 0.0006, 0.0000,  ..., 0.0182, 0.0748, 0.0954],
        [0.0000, 0.0008, 0.0000,  ..., 0.0182, 0.0751, 0.0953],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0181, 0.0741, 0.0957],
        [0.0000, 0.0002, 0.0000,  ..., 0.0181, 0.0741, 0.0957],
        [0.0000, 0.0002, 0.0000,  ..., 0.0181, 0.0741, 0.0957]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2676784., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1805.6892, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(106866.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2725.5801, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(989.7704, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(451.7961, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.0827],
        [-6.2996],
        [-6.4072],
        ...,
        [-6.0840],
        [-6.0752],
        [-6.0736]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-972595.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(647.3217, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(647.3217, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0144,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0144,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0144,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0017, -0.0144,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0144,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0144,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-60894.6836, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.0325, device='cuda:0')



h[100].sum tensor(-1.4462, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(90.1226, device='cuda:0')



h[200].sum tensor(-567.5038, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.8691, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(307296.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0007, 0.0000,  ..., 0.0175, 0.0893, 0.0842],
        [0.0000, 0.0014, 0.0000,  ..., 0.0188, 0.0795, 0.0908],
        [0.0000, 0.0016, 0.0000,  ..., 0.0193, 0.0770, 0.0926],
        ...,
        [0.0000, 0.0011, 0.0000,  ..., 0.0191, 0.0759, 0.0930],
        [0.0000, 0.0011, 0.0000,  ..., 0.0191, 0.0759, 0.0930],
        [0.0000, 0.0011, 0.0000,  ..., 0.0191, 0.0759, 0.0930]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2621661., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1811.9363, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(105316.5781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2656.3511, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(553.6208, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(357.5611, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.7070],
        [-5.8657],
        [-5.7441],
        ...,
        [-6.0007],
        [-5.9923],
        [-5.9908]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-777906.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 290.0 event: 4350 loss: tensor(416.5836, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4370],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(662.5530, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4370],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(662.5530, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0065,  0.0028, -0.0162,  ..., -0.0088, -0.0003, -0.0012],
        [-0.0064,  0.0027, -0.0160,  ..., -0.0087, -0.0003, -0.0017],
        [-0.0060,  0.0018, -0.0144,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0018, -0.0144,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0144,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0144,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-60874.3828, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.7391, device='cuda:0')



h[100].sum tensor(-1.4412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.2431, device='cuda:0')



h[200].sum tensor(-567.5703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.6660, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0058],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(308114.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3779, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0037, 0.2446, 0.0194],
        [0.0000, 0.0000, 0.0000,  ..., 0.0089, 0.1615, 0.0410],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0188, 0.0762, 0.0915],
        [0.0000, 0.0004, 0.0000,  ..., 0.0188, 0.0762, 0.0915],
        [0.0000, 0.0004, 0.0000,  ..., 0.0188, 0.0762, 0.0915]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2609214.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1811.4701, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(104022.4531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2649.5593, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(513.9470, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(387.6345, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5994],
        [-0.4467],
        [-1.9993],
        ...,
        [-6.0017],
        [-5.9936],
        [-5.9922]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-780787.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(733.9073, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(733.9073, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0063,  0.0023, -0.0154,  ..., -0.0084, -0.0002, -0.0031],
        ...,
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-60451.4531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9264, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.0496, device='cuda:0')



h[100].sum tensor(-1.5626, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(102.1774, device='cuda:0')



h[200].sum tensor(-567.9429, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.3994, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(314320.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0128, 0.1150, 0.0651],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.1632, 0.0321],
        [0.0000, 0.0000, 0.0000,  ..., 0.0030, 0.2236, 0.0158],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0757, 0.0913],
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0757, 0.0913],
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0757, 0.0913]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2633252., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1856.4685, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(103889.2500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2702.5134, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(638.9627, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(442.7493, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7830],
        [-0.1304],
        [ 0.3926],
        ...,
        [-6.0246],
        [-6.0341],
        [-6.0343]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-808581.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(656.1218, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(656.1218, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0065,  0.0029, -0.0164,  ..., -0.0089, -0.0003, -0.0008],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-61113.0977, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.4407, device='cuda:0')



h[100].sum tensor(-1.3923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.3478, device='cuda:0')



h[200].sum tensor(-567.5326, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.3295, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306403.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.2122, 0.0126],
        [0.0000, 0.0000, 0.0000,  ..., 0.0050, 0.1828, 0.0272],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.1156, 0.0642],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0757, 0.0913],
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0757, 0.0913],
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.0757, 0.0913]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2586514., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1786.4741, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(105414.6562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2673.7793, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(589.4391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(433.2703, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0900],
        [-1.3340],
        [-2.8054],
        ...,
        [-6.0438],
        [-6.0357],
        [-6.0343]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-886070.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(709.5038, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(709.5038, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0017, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0145,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-60812.3672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.9174, device='cuda:0')



h[100].sum tensor(-1.4635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(98.7798, device='cuda:0')



h[200].sum tensor(-567.7874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.1226, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(310187.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0055, 0.1699, 0.0363],
        [0.0000, 0.0005, 0.0000,  ..., 0.0117, 0.1065, 0.0697],
        [0.0000, 0.0012, 0.0000,  ..., 0.0156, 0.0877, 0.0829],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0173, 0.0760, 0.0908],
        [0.0000, 0.0007, 0.0000,  ..., 0.0173, 0.0760, 0.0908],
        [0.0000, 0.0007, 0.0000,  ..., 0.0173, 0.0760, 0.0908]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2615308.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1861.7559, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(104741.9688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2703.0117, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(408.2628, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(399.3778, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1157],
        [-3.4709],
        [-4.1252],
        ...,
        [-6.0371],
        [-6.0290],
        [-6.0276]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-733865.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(761.7620, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(761.7620, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0065,  0.0029, -0.0164,  ..., -0.0089, -0.0003, -0.0008],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-60544.0117, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0063, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.3419, device='cuda:0')



h[100].sum tensor(-1.5349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(106.0554, device='cuda:0')



h[200].sum tensor(-568.0533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.8568, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(312876., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 7.8030e-04, 2.5028e-01,
         9.9563e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.9213e-03, 1.9806e-01,
         2.6309e-02],
        [0.0000e+00, 6.9072e-05, 0.0000e+00,  ..., 1.0158e-02, 1.1659e-01,
         6.2400e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6056e-02, 7.5441e-02,
         9.0740e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6056e-02, 7.5441e-02,
         9.0740e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6056e-02, 7.5441e-02,
         9.0740e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2609446.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1855.5699, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(105139.1016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2731.2173, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(573.3053, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(468.2063, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.2604],
        [-0.9198],
        [-2.6395],
        ...,
        [-5.9421],
        [-5.8390],
        [-5.7743]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-847843.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(726.4667, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(726.4667, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-60871.4453, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.7044, device='cuda:0')



h[100].sum tensor(-1.4343, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(101.1414, device='cuda:0')



h[200].sum tensor(-567.8855, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.0101, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(311158.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0007, 0.0000,  ..., 0.0161, 0.0768, 0.0897],
        [0.0000, 0.0007, 0.0000,  ..., 0.0161, 0.0768, 0.0897],
        [0.0000, 0.0005, 0.0000,  ..., 0.0145, 0.0882, 0.0818],
        ...,
        [0.0000, 0.0003, 0.0000,  ..., 0.0161, 0.0760, 0.0900],
        [0.0000, 0.0003, 0.0000,  ..., 0.0161, 0.0760, 0.0900],
        [0.0000, 0.0003, 0.0000,  ..., 0.0161, 0.0760, 0.0900]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2604364.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1858.9547, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(104305.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2706.1375, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(533.8984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(439.6657, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.2798],
        [-6.0809],
        [-5.5465],
        ...,
        [-6.0883],
        [-6.0804],
        [-6.0791]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-813640.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(666.7825, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(666.7825, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-61394.5703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.9353, device='cuda:0')



h[100].sum tensor(-1.2836, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.8320, device='cuda:0')



h[200].sum tensor(-567.5752, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.8873, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(307555., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0012, 0.0000,  ..., 0.0158, 0.0811, 0.0865],
        [0.0000, 0.0014, 0.0000,  ..., 0.0162, 0.0788, 0.0881],
        [0.0000, 0.0017, 0.0000,  ..., 0.0164, 0.0781, 0.0888],
        ...,
        [0.0000, 0.0012, 0.0000,  ..., 0.0162, 0.0770, 0.0892],
        [0.0000, 0.0012, 0.0000,  ..., 0.0162, 0.0770, 0.0892],
        [0.0000, 0.0012, 0.0000,  ..., 0.0162, 0.0770, 0.0892]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2589425.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1857.7089, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(104297.0938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2672.1060, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(463.7758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(384.2133, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.4274],
        [-5.0655],
        [-5.5376],
        ...,
        [-6.0684],
        [-6.0606],
        [-6.0593]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-729217.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2690],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(522.6619, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2690],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(522.6619, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064,  0.0026, -0.0160,  ..., -0.0087, -0.0002, -0.0019],
        [-0.0065,  0.0029, -0.0165,  ..., -0.0089, -0.0003, -0.0008],
        [-0.0068,  0.0035, -0.0175,  ..., -0.0094, -0.0004,  0.0016],
        ...,
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-62689.2070, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.1346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.2489, device='cuda:0')



h[100].sum tensor(-0.9856, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(72.7670, device='cuda:0')



h[200].sum tensor(-566.8489, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.3467, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0118, 0.0000,  ..., 0.0000, 0.0000, 0.0032],
        [0.0000, 0.0148, 0.0000,  ..., 0.0000, 0.0000, 0.0112],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(296529.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4258, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4736, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5659, 0.0000],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0150, 0.0765, 0.0894],
        [0.0000, 0.0007, 0.0000,  ..., 0.0150, 0.0765, 0.0894],
        [0.0000, 0.0007, 0.0000,  ..., 0.0150, 0.0765, 0.0894]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2525604., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1790.2141, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(106402.5078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2662.7664, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(201.6733, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(398.1032, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.0415],
        [ 1.0083],
        [ 0.9771],
        ...,
        [-6.1278],
        [-6.1197],
        [-6.1183]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-869363.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1015.3805, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1015.3805, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0062,  0.0023, -0.0155,  ..., -0.0084, -0.0001, -0.0032],
        ...,
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0145,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-58625.4648, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.9962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(47.1085, device='cuda:0')



h[100].sum tensor(-1.8894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(141.3651, device='cuda:0')



h[200].sum tensor(-569.4211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(53.1266, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(339174.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0004, 0.0000,  ..., 0.0101, 0.1042, 0.0705],
        [0.0000, 0.0000, 0.0000,  ..., 0.0052, 0.1566, 0.0377],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.2267, 0.0146],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0009, 0.2001, 0.0202],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2398, 0.0093],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.1943, 0.0239]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2743002.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1997.5862, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(100959.3594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2746.9092, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1617.2952, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(522.3874, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.2892],
        [-1.8766],
        [-0.4715],
        ...,
        [-0.8328],
        [-0.2465],
        [-1.1225]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-812528.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(703.0710, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(703.0710, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-61406.5859, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.6190, device='cuda:0')



h[100].sum tensor(-1.2623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(97.8842, device='cuda:0')



h[200].sum tensor(-567.7651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.7860, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(311459.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0023, 0.0000,  ..., 0.0150, 0.0764, 0.0905],
        [0.0000, 0.0023, 0.0000,  ..., 0.0150, 0.0764, 0.0905],
        [0.0000, 0.0025, 0.0000,  ..., 0.0150, 0.0767, 0.0903],
        ...,
        [0.0000, 0.0019, 0.0000,  ..., 0.0149, 0.0757, 0.0908],
        [0.0000, 0.0019, 0.0000,  ..., 0.0149, 0.0757, 0.0908],
        [0.0000, 0.0019, 0.0000,  ..., 0.0149, 0.0757, 0.0908]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2625888., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1917.1537, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(104381.4531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2670.5532, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(570.5630, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(371.6277, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.3195],
        [-6.1757],
        [-5.8980],
        ...,
        [-6.1864],
        [-6.1778],
        [-6.1759]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-712294., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 300.0 event: 4500 loss: tensor(411.8202, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2444]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(778.0095, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.2444]], device='cuda:0') 
g.ndata[nfet].sum tensor(778.0095, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0062,  0.0023, -0.0155,  ..., -0.0084, -0.0001, -0.0032],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-61004.0938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.0957, device='cuda:0')



h[100].sum tensor(-1.3651, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(108.3174, device='cuda:0')



h[200].sum tensor(-568.1509, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.7069, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(313067.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0011, 0.0000,  ..., 0.0135, 0.0760, 0.0909],
        [0.0000, 0.0011, 0.0000,  ..., 0.0136, 0.0751, 0.0916],
        [0.0000, 0.0013, 0.0000,  ..., 0.0137, 0.0749, 0.0918],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0083, 0.1066, 0.0697],
        [0.0000, 0.0000, 0.0000,  ..., 0.0067, 0.1161, 0.0632],
        [0.0000, 0.0000, 0.0000,  ..., 0.0017, 0.1462, 0.0424]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2605332., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1899.8323, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(105957.2500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2698.3354, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(648.6965, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(460.1037, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.8801],
        [-6.2072],
        [-6.3807],
        ...,
        [-5.6800],
        [-5.4674],
        [-5.3509]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-900173.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(790.5546, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(790.5546, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0067,  0.0032, -0.0172,  ..., -0.0092, -0.0004,  0.0008],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-60852.9688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.6777, device='cuda:0')



h[100].sum tensor(-1.3667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(110.0640, device='cuda:0')



h[200].sum tensor(-568.2520, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.3633, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0008],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0008],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(315775.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2583, 0.0073],
        [0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.2268, 0.0190],
        [0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.1275, 0.0553],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0144, 0.0737, 0.0928],
        [0.0000, 0.0013, 0.0000,  ..., 0.0144, 0.0737, 0.0928],
        [0.0000, 0.0013, 0.0000,  ..., 0.0144, 0.0737, 0.0928]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2626209.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1924.8281, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(105128.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2664.9604, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(775.5656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(432.9762, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4251],
        [-0.3096],
        [-1.1839],
        ...,
        [-6.2962],
        [-6.2873],
        [-6.2857]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-830483.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(703.7341, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(703.7341, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0067,  0.0032, -0.0173,  ..., -0.0093, -0.0004,  0.0011],
        [-0.0063,  0.0024, -0.0158,  ..., -0.0085, -0.0002, -0.0025],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-61551.5078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.6497, device='cuda:0')



h[100].sum tensor(-1.1837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(97.9765, device='cuda:0')



h[200].sum tensor(-567.7919, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.8207, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0024],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(310588.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4097, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3038, 0.0091],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2548, 0.0000],
        ...,
        [0.0000, 0.0023, 0.0000,  ..., 0.0153, 0.0739, 0.0930],
        [0.0000, 0.0023, 0.0000,  ..., 0.0153, 0.0739, 0.0930],
        [0.0000, 0.0023, 0.0000,  ..., 0.0153, 0.0739, 0.0930]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2614871., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1916.3477, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(104668.8047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2598.0212, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(650.8280, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(356.9385, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.9834],
        [ 1.0297],
        [ 1.0948],
        ...,
        [-6.2596],
        [-6.2510],
        [-6.2495]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-712283.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(641.9656, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(641.9656, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0067,  0.0032, -0.0172,  ..., -0.0092, -0.0003,  0.0009],
        ...,
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-62230.3906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5795, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.7840, device='cuda:0')



h[100].sum tensor(-1.0506, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.3769, device='cuda:0')



h[200].sum tensor(-567.4636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.5889, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0009],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0009],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(308823.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0077, 0.1262, 0.0573],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.2287, 0.0194],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2605, 0.0073],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0166, 0.0740, 0.0935],
        [0.0000, 0.0004, 0.0000,  ..., 0.0166, 0.0740, 0.0935],
        [0.0000, 0.0004, 0.0000,  ..., 0.0166, 0.0740, 0.0935]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2613911.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1869.2755, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(104361.9062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2622.7427, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(781.6365, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(444.2657, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3020],
        [-1.5785],
        [-0.5073],
        ...,
        [-6.3314],
        [-6.3223],
        [-6.3206]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-883106.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2462],
        [0.0000],
        [0.5493],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1011.5923, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2462],
        [0.0000],
        [0.5493],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(1011.5923, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062,  0.0023, -0.0155,  ..., -0.0084, -0.0001, -0.0031],
        [-0.0068,  0.0033, -0.0175,  ..., -0.0093, -0.0004,  0.0015],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-59088.9141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-616.0291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(46.9328, device='cuda:0')



h[100].sum tensor(-1.6507, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(140.8377, device='cuda:0')



h[200].sum tensor(-569.4667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(52.9284, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0015],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0132, 0.0000,  ..., 0.0000, 0.0000, 0.0070],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(337706.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.5400e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.4863e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.1631e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 3.5490e-04, 0.0000e+00,  ..., 1.6858e-02, 7.3594e-02,
         9.4532e-02],
        [0.0000e+00, 3.5490e-04, 0.0000e+00,  ..., 1.6858e-02, 7.3594e-02,
         9.4532e-02],
        [0.0000e+00, 3.5490e-04, 0.0000e+00,  ..., 1.6858e-02, 7.3594e-02,
         9.4532e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2728232., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1954.0582, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(102977.9688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2678.9341, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1641.4644, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(538.7238, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.9209],
        [ 1.0254],
        [ 1.0314],
        ...,
        [-6.3570],
        [-6.3478],
        [-6.3460]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-883654., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3728],
        [0.2800],
        [0.5835],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(748.0365, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3728],
        [0.2800],
        [0.5835],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(748.0365, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0071,  0.0038, -0.0186,  ..., -0.0099, -0.0005,  0.0042],
        [-0.0074,  0.0046, -0.0201,  ..., -0.0105, -0.0007,  0.0077],
        [-0.0073,  0.0043, -0.0197,  ..., -0.0104, -0.0006,  0.0067],
        ...,
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-61467.7891, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9725, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.7051, device='cuda:0')



h[100].sum tensor(-1.1665, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(104.1445, device='cuda:0')



h[200].sum tensor(-568.0067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.1387, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0170, 0.0000,  ..., 0.0000, 0.0000, 0.0252],
        [0.0000, 0.0178, 0.0000,  ..., 0.0000, 0.0000, 0.0285],
        [0.0000, 0.0197, 0.0000,  ..., 0.0000, 0.0000, 0.0372],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(312860.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8052, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8842, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.9411, 0.0000],
        ...,
        [0.0000, 0.0022, 0.0000,  ..., 0.0163, 0.0733, 0.0956],
        [0.0000, 0.0022, 0.0000,  ..., 0.0163, 0.0733, 0.0956],
        [0.0000, 0.0022, 0.0000,  ..., 0.0163, 0.0733, 0.0956]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2619541.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1936.2604, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(105142.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2615.6091, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(794.5979, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(378.8097, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.9062],
        [ 0.8820],
        [ 0.8709],
        ...,
        [-6.3036],
        [-6.2945],
        [-6.2929]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-724074.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(712.9070, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(712.9070, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-61909.9766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8607, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.0753, device='cuda:0')



h[100].sum tensor(-1.0926, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(99.2536, device='cuda:0')



h[200].sum tensor(-567.8521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.3007, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(308434.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0015, 0.0000,  ..., 0.0166, 0.0758, 0.0947],
        [0.0000, 0.0015, 0.0000,  ..., 0.0169, 0.0739, 0.0960],
        [0.0000, 0.0017, 0.0000,  ..., 0.0149, 0.0809, 0.0914],
        ...,
        [0.0000, 0.0011, 0.0000,  ..., 0.0170, 0.0724, 0.0969],
        [0.0000, 0.0011, 0.0000,  ..., 0.0170, 0.0724, 0.0969],
        [0.0000, 0.0011, 0.0000,  ..., 0.0170, 0.0724, 0.0969]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2595998., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1924.3291, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(106194.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2652.1763, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(444.6491, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(411.5549, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.6760],
        [-5.7800],
        [-5.6064],
        ...,
        [-6.3526],
        [-6.3432],
        [-6.3413]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-760612., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(704.2026, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(704.2026, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0063,  0.0022, -0.0157,  ..., -0.0085, -0.0001, -0.0028],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-62276.4844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.6715, device='cuda:0')



h[100].sum tensor(-1.0468, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(98.0418, device='cuda:0')



h[200].sum tensor(-567.7849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.8452, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306243.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0116, 0.1199, 0.0647],
        [0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.1257, 0.0606],
        [0.0000, 0.0000, 0.0000,  ..., 0.0059, 0.1597, 0.0377],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0714, 0.0976],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0714, 0.0976],
        [0.0000, 0.0000, 0.0000,  ..., 0.0185, 0.0714, 0.0976]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2555138.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1844.5413, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(105466.0859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2689.7126, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(618.5806, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(608.0116, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7313],
        [-2.8883],
        [-2.9836],
        ...,
        [-6.4700],
        [-6.4599],
        [-6.4579]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1142907.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(669.5002, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(669.5002, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-62465.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6799, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.0614, device='cuda:0')



h[100].sum tensor(-0.9699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.2104, device='cuda:0')



h[200].sum tensor(-567.6023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.0295, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(304737.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0210, 0.0826, 0.0898],
        [0.0000, 0.0000, 0.0000,  ..., 0.0188, 0.0993, 0.0784],
        [0.0000, 0.0000, 0.0000,  ..., 0.0177, 0.1083, 0.0724],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0220, 0.0740, 0.0955],
        [0.0000, 0.0000, 0.0000,  ..., 0.0220, 0.0740, 0.0955],
        [0.0000, 0.0000, 0.0000,  ..., 0.0220, 0.0740, 0.0955]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2542506., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1833.1833, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(102325.1875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2612.4531, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(536.2051, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(531.1246, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.1003],
        [-2.8458],
        [-2.7842],
        ...,
        [-6.3477],
        [-6.3381],
        [-6.3362]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-921259.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(667.4698, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(667.4698, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-62297.8359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6732, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.9672, device='cuda:0')



h[100].sum tensor(-0.9439, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.9277, device='cuda:0')



h[200].sum tensor(-567.5930, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.9233, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306388.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0266, 0.0789, 0.0929],
        [0.0000, 0.0000, 0.0000,  ..., 0.0266, 0.0789, 0.0929],
        [0.0000, 0.0002, 0.0000,  ..., 0.0267, 0.0792, 0.0927],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0265, 0.0781, 0.0932],
        [0.0000, 0.0000, 0.0000,  ..., 0.0265, 0.0781, 0.0932],
        [0.0000, 0.0000, 0.0000,  ..., 0.0265, 0.0781, 0.0932]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2572209., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1876.6445, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(98256.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2516.3037, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(556.4385, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(349.0569, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.0047],
        [-6.1349],
        [-6.2315],
        ...,
        [-6.1559],
        [-6.1472],
        [-6.1456]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-563014.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 310.0 event: 4650 loss: tensor(480.9356, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(840.9241, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(840.9241, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0063,  0.0020, -0.0156,  ..., -0.0084, -0.0001, -0.0030],
        [-0.0065,  0.0026, -0.0166,  ..., -0.0089, -0.0002, -0.0006],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-60914.4062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.3143, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.0146, device='cuda:0')



h[100].sum tensor(-1.1599, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(117.0766, device='cuda:0')



h[200].sum tensor(-568.4791, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.9988, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(316620.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.2898, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0072, 0.2420, 0.0102],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.1963, 0.0264],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0284, 0.0770, 0.0921],
        [0.0000, 0.0000, 0.0000,  ..., 0.0284, 0.0770, 0.0921],
        [0.0000, 0.0000, 0.0000,  ..., 0.0284, 0.0770, 0.0921]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2579883.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1905.5099, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(93223.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2454.2144, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(787.9780, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(445.1977, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.0647],
        [ 0.3575],
        [-0.9004],
        ...,
        [-6.1640],
        [-6.1553],
        [-6.1537]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-625935.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4651],
        [0.0000],
        [0.6201],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(604.2046, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4651],
        [0.0000],
        [0.6201],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(604.2046, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0070,  0.0036, -0.0186,  ..., -0.0099, -0.0004,  0.0042],
        [-0.0078,  0.0051, -0.0216,  ..., -0.0112, -0.0008,  0.0111],
        [-0.0064,  0.0022, -0.0160,  ..., -0.0086, -0.0001, -0.0022],
        ...,
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-63118.5859, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.0320, device='cuda:0')



h[100].sum tensor(-0.8130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(84.1197, device='cuda:0')



h[200].sum tensor(-567.2675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.6131, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0142, 0.0000,  ..., 0.0000, 0.0000, 0.0173],
        [0.0000, 0.0124, 0.0000,  ..., 0.0000, 0.0000, 0.0099],
        [0.0000, 0.0157, 0.0000,  ..., 0.0000, 0.0000, 0.0233],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(297161.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5718, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6061, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6656, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0293, 0.0747, 0.0934],
        [0.0000, 0.0000, 0.0000,  ..., 0.0293, 0.0747, 0.0934],
        [0.0000, 0.0000, 0.0000,  ..., 0.0293, 0.0747, 0.0934]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2484829.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1840.9291, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(93906.1016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2406.2905, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(274.8582, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(450.4165, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.9401],
        [ 0.9746],
        [ 0.9815],
        ...,
        [-6.2463],
        [-6.2373],
        [-6.2356]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-747212., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(722.6207, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(722.6207, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-62450.3555, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8779, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.5260, device='cuda:0')



h[100].sum tensor(-0.9494, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(100.6060, device='cuda:0')



h[200].sum tensor(-567.8760, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.8089, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(303295.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0300, 0.0751, 0.0947],
        [0.0000, 0.0000, 0.0000,  ..., 0.0301, 0.0740, 0.0955],
        [0.0000, 0.0000, 0.0000,  ..., 0.0303, 0.0738, 0.0958],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0300, 0.0728, 0.0962],
        [0.0000, 0.0000, 0.0000,  ..., 0.0300, 0.0728, 0.0962],
        [0.0000, 0.0000, 0.0000,  ..., 0.0300, 0.0728, 0.0962]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2509430., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1848.9126, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(95112.9297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2431.4961, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(513.3204, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(492.7868, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.9969],
        [-6.2166],
        [-6.3881],
        ...,
        [-6.3422],
        [-6.3330],
        [-6.3312]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-832715.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(577.7344, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(577.7344, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0063,  0.0019, -0.0157,  ..., -0.0085, -0.0001, -0.0027],
        ...,
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-63709.2695, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3359, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.8040, device='cuda:0')



h[100].sum tensor(-0.7386, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(80.4344, device='cuda:0')



h[200].sum tensor(-567.1270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.2282, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(292923.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0292, 0.0891, 0.0868],
        [0.0000, 0.0000, 0.0000,  ..., 0.0241, 0.1282, 0.0592],
        [0.0000, 0.0000, 0.0000,  ..., 0.0180, 0.1762, 0.0299],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0310, 0.0723, 0.0986],
        [0.0000, 0.0000, 0.0000,  ..., 0.0310, 0.0723, 0.0986],
        [0.0000, 0.0000, 0.0000,  ..., 0.0310, 0.0723, 0.0986]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2474168., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1660.5872, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(98275.0547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2462.3096, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(214.4156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(416.0925, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5147],
        [-2.4585],
        [-1.2744],
        ...,
        [-6.3929],
        [-6.3837],
        [-6.3820]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-835587.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3083],
        [0.2983],
        [0.3076],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(729.7410, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3083],
        [0.2983],
        [0.3076],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(729.7410, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0070,  0.0033, -0.0186,  ..., -0.0098, -0.0004,  0.0040],
        [-0.0070,  0.0034, -0.0187,  ..., -0.0099, -0.0004,  0.0042],
        [-0.0068,  0.0029, -0.0177,  ..., -0.0094, -0.0003,  0.0020],
        ...,
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-62415.5664, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.8563, device='cuda:0')



h[100].sum tensor(-0.9103, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(101.5973, device='cuda:0')



h[200].sum tensor(-567.9037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.1814, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0137, 0.0000,  ..., 0.0000, 0.0000, 0.0181],
        [0.0000, 0.0130, 0.0000,  ..., 0.0000, 0.0000, 0.0147],
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0075],
        ...,
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(305579.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6626, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6029, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4953, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0322, 0.0704, 0.1005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0322, 0.0704, 0.1005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0322, 0.0704, 0.1005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2525252.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1545.1183, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(97183.0703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2507.1750, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(461.2620, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(441.1899, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.1453],
        [ 1.1779],
        [ 1.2397],
        ...,
        [-6.4899],
        [-6.4808],
        [-6.4792]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-889168.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(687.6119, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(687.6119, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-62384.7188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.9017, device='cuda:0')



h[100].sum tensor(-0.8491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(95.7319, device='cuda:0')



h[200].sum tensor(-567.7405, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.9772, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(305890., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0234, 0.1493, 0.0489],
        [0.0000, 0.0000, 0.0000,  ..., 0.0322, 0.0933, 0.0833],
        [0.0000, 0.0000, 0.0000,  ..., 0.0344, 0.0752, 0.0960],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0345, 0.0705, 0.0990],
        [0.0000, 0.0000, 0.0000,  ..., 0.0345, 0.0705, 0.0990],
        [0.0000, 0.0000, 0.0000,  ..., 0.0345, 0.0705, 0.0990]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2499166., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1420.7407, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(93392.4062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2444.5181, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(439.4294, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(410.0942, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7534],
        [-4.3807],
        [-5.3694],
        ...,
        [-6.4731],
        [-6.4645],
        [-6.4631]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-861027.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(628.3615, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(628.3615, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.4896e-03, -1.4631e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2417e-03],
        [-6.2130e-03,  1.8998e-03, -1.5432e-02,  ..., -8.3444e-03,
         -7.7507e-05, -3.3481e-03],
        [-6.2130e-03,  1.8998e-03, -1.5432e-02,  ..., -8.3444e-03,
         -7.7507e-05, -3.3481e-03],
        ...,
        [-6.0055e-03,  1.4896e-03, -1.4631e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2417e-03],
        [-6.0055e-03,  1.4896e-03, -1.4631e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2417e-03],
        [-6.0055e-03,  1.4896e-03, -1.4631e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2417e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-62675.9453, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.1528, device='cuda:0')



h[100].sum tensor(-0.7480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.4829, device='cuda:0')



h[200].sum tensor(-567.3958, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.8771, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(303726.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0260, 0.1744, 0.0320],
        [0.0000, 0.0000, 0.0000,  ..., 0.0223, 0.2063, 0.0148],
        [0.0000, 0.0000, 0.0000,  ..., 0.0237, 0.1920, 0.0181],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0368, 0.0716, 0.0971],
        [0.0000, 0.0000, 0.0000,  ..., 0.0368, 0.0716, 0.0971],
        [0.0000, 0.0000, 0.0000,  ..., 0.0368, 0.0716, 0.0971]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2489584.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1462.1572, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(91414.6797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2389.2944, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(310.3059, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(227.7375, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5835],
        [-0.6309],
        [-0.5513],
        ...,
        [-6.3881],
        [-6.3806],
        [-6.3805]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-712648.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(626.2655, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(626.2655, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-62726.6016, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.0556, device='cuda:0')



h[100].sum tensor(-0.7305, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.1911, device='cuda:0')



h[200].sum tensor(-567.4000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.7674, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(301652.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0369, 0.0725, 0.0980],
        [0.0000, 0.0000, 0.0000,  ..., 0.0369, 0.0725, 0.0980],
        [0.0000, 0.0000, 0.0000,  ..., 0.0370, 0.0729, 0.0979],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0367, 0.0719, 0.0983],
        [0.0000, 0.0000, 0.0000,  ..., 0.0367, 0.0719, 0.0983],
        [0.0000, 0.0000, 0.0000,  ..., 0.0367, 0.0719, 0.0983]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2478379., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1578.2943, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(92856.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2355.8823, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(225.1254, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(53.5449, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.5336],
        [-6.4502],
        [-6.3049],
        ...,
        [-6.4218],
        [-6.4139],
        [-6.4127]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-683029.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(798.7103, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(798.7103, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-61750.4727, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.0561, device='cuda:0')



h[100].sum tensor(-0.9049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(111.1995, device='cuda:0')



h[200].sum tensor(-568.2704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.7901, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(316804.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0339, 0.0769, 0.0965],
        [0.0000, 0.0000, 0.0000,  ..., 0.0344, 0.0718, 0.1001],
        [0.0000, 0.0001, 0.0000,  ..., 0.0346, 0.0711, 0.1007],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0344, 0.0701, 0.1011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0344, 0.0701, 0.1011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0344, 0.0701, 0.1011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2577140.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1797.2980, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(94175.2734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2365.4971, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(801.0627, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(93.8094, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.5321],
        [-6.1675],
        [-6.5015],
        ...,
        [-6.5624],
        [-6.5539],
        [-6.5525]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-863092.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(802.8391, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(802.8391, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-62053.7188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.2477, device='cuda:0')



h[100].sum tensor(-0.8962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(111.7743, device='cuda:0')



h[200].sum tensor(-568.3345, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.0061, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(311349.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.2303e-02, 6.9126e-02,
         1.0361e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.2303e-02, 6.9126e-02,
         1.0361e-01],
        [0.0000e+00, 8.5216e-05, 0.0000e+00,  ..., 3.2388e-02, 6.9450e-02,
         1.0349e-01],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.2135e-02, 6.8477e-02,
         1.0384e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.2135e-02, 6.8477e-02,
         1.0384e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.2135e-02, 6.8477e-02,
         1.0384e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2542298., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1926.1964, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(96666.9375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2379.3276, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(537.7687, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(44.0153, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.3696],
        [-6.6945],
        [-6.8914],
        ...,
        [-6.6978],
        [-6.6890],
        [-6.6875]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1006145.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 320.0 event: 4800 loss: tensor(465.5923, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(654.0823, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(654.0823, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-63665.1953, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.3461, device='cuda:0')



h[100].sum tensor(-0.6983, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.0638, device='cuda:0')



h[200].sum tensor(-567.4993, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.2228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(294924.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0008, 0.0000,  ..., 0.0325, 0.0710, 0.1020],
        [0.0000, 0.0008, 0.0000,  ..., 0.0322, 0.0738, 0.1001],
        [0.0000, 0.0004, 0.0000,  ..., 0.0309, 0.0862, 0.0918],
        ...,
        [0.0000, 0.0004, 0.0000,  ..., 0.0324, 0.0704, 0.1022],
        [0.0000, 0.0004, 0.0000,  ..., 0.0324, 0.0704, 0.1022],
        [0.0000, 0.0004, 0.0000,  ..., 0.0324, 0.0704, 0.1022]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2449748.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1880.7205, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(97813.6562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2323.2031, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(178.0215, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(28.1531, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.7688],
        [-5.3974],
        [-4.3506],
        ...,
        [-6.6414],
        [-6.6329],
        [-6.6315]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-908175.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3025],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(634.3517, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3025],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(634.3517, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2081e-03,  1.7018e-03, -1.5420e-02,  ..., -8.3355e-03,
         -6.7026e-05, -3.3964e-03],
        [-6.2920e-03,  1.8623e-03, -1.5744e-02,  ..., -8.4869e-03,
         -9.4790e-05, -2.6312e-03],
        [-6.7279e-03,  2.6961e-03, -1.7426e-02,  ..., -9.2734e-03,
         -2.3900e-04,  1.3438e-03],
        ...,
        [-6.0055e-03,  1.3143e-03, -1.4638e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2439e-03],
        [-6.0055e-03,  1.3143e-03, -1.4638e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2439e-03],
        [-6.0055e-03,  1.3143e-03, -1.4638e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2439e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-64045.5117, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5504, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.4307, device='cuda:0')



h[100].sum tensor(-0.6654, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(88.3168, device='cuda:0')



h[200].sum tensor(-567.4235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.1905, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0013],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0019],
        ...,
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(294245.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.8929e-04, 3.1495e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.4500e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.9954e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.9772e-02, 7.2685e-02,
         9.8561e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.9772e-02, 7.2685e-02,
         9.8561e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 2.9772e-02, 7.2685e-02,
         9.8561e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2445315.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2002.3400, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(94961.2734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2285.0083, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(176.3889, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(127.9072, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.1734],
        [ 1.1812],
        [ 1.1004],
        ...,
        [-6.6089],
        [-6.6008],
        [-6.5995]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-853306.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(804.0508, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(804.0508, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-62829.5391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.3039, device='cuda:0')



h[100].sum tensor(-0.8200, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(111.9430, device='cuda:0')



h[200].sum tensor(-568.2808, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.0695, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(308563.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.5457e-02, 1.6978e-01,
         3.8616e-02],
        [0.0000e+00, 1.6855e-04, 0.0000e+00,  ..., 2.4583e-02, 1.0846e-01,
         7.2931e-02],
        [0.0000e+00, 7.2427e-04, 0.0000e+00,  ..., 2.8446e-02, 8.3812e-02,
         8.9992e-02],
        ...,
        [0.0000e+00, 3.0634e-04, 0.0000e+00,  ..., 2.9599e-02, 7.4558e-02,
         9.6041e-02],
        [0.0000e+00, 3.0634e-04, 0.0000e+00,  ..., 2.9599e-02, 7.4558e-02,
         9.6041e-02],
        [0.0000e+00, 3.0634e-04, 0.0000e+00,  ..., 2.9599e-02, 7.4558e-02,
         9.6041e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2511351.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2069.3848, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(92161.0859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2270.5920, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(599.2964, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(217.9636, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0035],
        [-3.4521],
        [-4.4940],
        ...,
        [-6.5384],
        [-6.5306],
        [-6.5295]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-786395.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2842],
        [0.2742],
        [0.2646],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(621.1960, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2842],
        [0.2742],
        [0.2646],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(621.1960, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0068,  0.0028, -0.0176,  ..., -0.0094, -0.0002,  0.0017],
        [-0.0070,  0.0032, -0.0184,  ..., -0.0097, -0.0003,  0.0036],
        [-0.0071,  0.0035, -0.0190,  ..., -0.0100, -0.0004,  0.0051],
        ...,
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-64658.0977, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4949, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.8204, device='cuda:0')



h[100].sum tensor(-0.6175, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(86.4853, device='cuda:0')



h[200].sum tensor(-567.3467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.5022, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0094],
        [0.0000, 0.0125, 0.0000,  ..., 0.0000, 0.0000, 0.0136],
        [0.0000, 0.0122, 0.0000,  ..., 0.0000, 0.0000, 0.0124],
        ...,
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(291417.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5456, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5878, 0.0000],
        ...,
        [0.0000, 0.0011, 0.0000,  ..., 0.0300, 0.0758, 0.0943],
        [0.0000, 0.0011, 0.0000,  ..., 0.0300, 0.0758, 0.0943],
        [0.0000, 0.0011, 0.0000,  ..., 0.0300, 0.0758, 0.0943]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2399517.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1991.0540, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(93145.7656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2230.7122, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(156.3178, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(134.4374, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.1118],
        [ 1.1476],
        [ 1.1566],
        ...,
        [-6.4691],
        [-6.4602],
        [-6.4580]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-695979.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(661.0267, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(661.0267, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-64716.5039, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.6683, device='cuda:0')



h[100].sum tensor(-0.6382, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.0306, device='cuda:0')



h[200].sum tensor(-567.5377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.5862, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(295298.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0014, 0.0000,  ..., 0.0289, 0.0770, 0.0928],
        [0.0000, 0.0014, 0.0000,  ..., 0.0289, 0.0770, 0.0928],
        [0.0000, 0.0016, 0.0000,  ..., 0.0289, 0.0774, 0.0927],
        ...,
        [0.0000, 0.0010, 0.0000,  ..., 0.0287, 0.0763, 0.0931],
        [0.0000, 0.0010, 0.0000,  ..., 0.0287, 0.0763, 0.0931],
        [0.0000, 0.0010, 0.0000,  ..., 0.0287, 0.0763, 0.0931]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2421769.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2061.3643, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(92261.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2257.8936, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(166.5829, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(131.5967, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.3616],
        [-6.1926],
        [-5.8198],
        ...,
        [-6.4476],
        [-6.3197],
        [-5.9427]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-676171., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(644.8204, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(644.8204, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-65302.9766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.9164, device='cuda:0')



h[100].sum tensor(-0.6095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.7743, device='cuda:0')



h[200].sum tensor(-567.4697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.7382, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(293124.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0014, 0.0000,  ..., 0.0263, 0.0818, 0.0896],
        [0.0000, 0.0014, 0.0000,  ..., 0.0274, 0.0760, 0.0936],
        [0.0000, 0.0016, 0.0000,  ..., 0.0275, 0.0764, 0.0935],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0273, 0.0753, 0.0939],
        [0.0000, 0.0009, 0.0000,  ..., 0.0273, 0.0753, 0.0939],
        [0.0000, 0.0009, 0.0000,  ..., 0.0267, 0.0795, 0.0911]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2405177., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2051.3301, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(95229.1641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2312.9629, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(136.4040, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(151.2477, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.9101],
        [-6.4031],
        [-6.6283],
        ...,
        [-6.5063],
        [-6.3502],
        [-5.9806]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-773633.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2612],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(632.5498, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2612],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(632.5498, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0065,  0.0023, -0.0166,  ..., -0.0089, -0.0001, -0.0007],
        [-0.0065,  0.0023, -0.0166,  ..., -0.0089, -0.0002, -0.0006],
        ...,
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-65871.2500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5459, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.3471, device='cuda:0')



h[100].sum tensor(-0.5846, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(88.0660, device='cuda:0')



h[200].sum tensor(-567.4172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.0962, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(292100.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0087, 0.2241, 0.0242],
        [0.0000, 0.0000, 0.0000,  ..., 0.0024, 0.3071, 0.0077],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3656, 0.0000],
        ...,
        [0.0000, 0.0018, 0.0000,  ..., 0.0277, 0.0732, 0.0964],
        [0.0000, 0.0018, 0.0000,  ..., 0.0277, 0.0732, 0.0964],
        [0.0000, 0.0018, 0.0000,  ..., 0.0277, 0.0732, 0.0964]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2405927.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1986.1129, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(99683.4062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2387.7646, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(156.6787, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(162.3496, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0838],
        [ 0.7854],
        [ 1.0690],
        ...,
        [-6.6270],
        [-6.6192],
        [-6.6181]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-851960.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6631],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(822.8882, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6631],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(822.8882, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0065,  0.0023, -0.0166,  ..., -0.0089, -0.0001, -0.0006],
        [-0.0066,  0.0025, -0.0171,  ..., -0.0091, -0.0002,  0.0005],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-64579.2734, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2578, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.1779, device='cuda:0')



h[100].sum tensor(-0.7428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(114.5656, device='cuda:0')



h[200].sum tensor(-568.4009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.0551, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0059],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0012],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0005],
        ...,
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(309150.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4497, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0025, 0.2934, 0.0107],
        [0.0000, 0.0000, 0.0000,  ..., 0.0103, 0.1865, 0.0350],
        ...,
        [0.0000, 0.0020, 0.0000,  ..., 0.0273, 0.0723, 0.0968],
        [0.0000, 0.0020, 0.0000,  ..., 0.0273, 0.0723, 0.0968],
        [0.0000, 0.0020, 0.0000,  ..., 0.0273, 0.0723, 0.0968]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2498175.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2024.8613, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(99492.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2422.1899, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(497.4936, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(207.6053, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7458],
        [-0.1050],
        [-1.6939],
        ...,
        [-6.6779],
        [-6.6700],
        [-6.6687]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-903495., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(732.2382, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(732.2382, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.3896e-03, -1.4643e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2432e-03],
        [-6.0055e-03,  1.3896e-03, -1.4643e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2432e-03],
        [-6.2430e-03,  1.8421e-03, -1.5560e-02,  ..., -8.3985e-03,
         -6.6114e-05, -3.0807e-03],
        ...,
        [-6.0055e-03,  1.3896e-03, -1.4643e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2432e-03],
        [-6.0055e-03,  1.3896e-03, -1.4643e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2432e-03],
        [-6.0055e-03,  1.3896e-03, -1.4643e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2432e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-65542.2578, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.9722, device='cuda:0')



h[100].sum tensor(-0.6413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(101.9450, device='cuda:0')



h[200].sum tensor(-567.9189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.3121, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(299042.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0014, 0.0000,  ..., 0.0225, 0.1058, 0.0744],
        [0.0000, 0.0000, 0.0000,  ..., 0.0150, 0.1577, 0.0410],
        [0.0000, 0.0000, 0.0000,  ..., 0.0068, 0.2271, 0.0177],
        ...,
        [0.0000, 0.0023, 0.0000,  ..., 0.0274, 0.0726, 0.0961],
        [0.0000, 0.0023, 0.0000,  ..., 0.0274, 0.0726, 0.0961],
        [0.0000, 0.0023, 0.0000,  ..., 0.0274, 0.0726, 0.0961]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2431843.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2025.5133, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(97855.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2356.8545, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(173.6910, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(151.0401, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3906],
        [-1.7315],
        [-0.3193],
        ...,
        [-6.6618],
        [-6.6523],
        [-6.6495]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-800660.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7988],
        [0.3286],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(650.9916, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.7988],
        [0.3286],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(650.9916, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3167e-03,  2.0856e-03, -1.5845e-02,  ..., -8.5316e-03,
         -8.4518e-05, -2.4209e-03],
        [-6.7621e-03,  2.9370e-03, -1.7565e-02,  ..., -9.3352e-03,
         -2.0545e-04,  1.6284e-03],
        [-6.3167e-03,  2.0856e-03, -1.5845e-02,  ..., -8.5316e-03,
         -8.4518e-05, -2.4209e-03],
        ...,
        [-6.0055e-03,  1.4906e-03, -1.4643e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2509e-03],
        [-6.0055e-03,  1.4906e-03, -1.4643e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2509e-03],
        [-6.0055e-03,  1.4906e-03, -1.4643e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2509e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-66240.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6235, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.2027, device='cuda:0')



h[100].sum tensor(-0.5591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(90.6335, device='cuda:0')



h[200].sum tensor(-567.5244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.0611, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0117, 0.0000,  ..., 0.0000, 0.0000, 0.0063],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0016],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(294305.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3356, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2894, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0058, 0.2248, 0.0216],
        ...,
        [0.0000, 0.0022, 0.0000,  ..., 0.0269, 0.0734, 0.0941],
        [0.0000, 0.0022, 0.0000,  ..., 0.0269, 0.0734, 0.0941],
        [0.0000, 0.0022, 0.0000,  ..., 0.0269, 0.0734, 0.0941]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2399453., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2021.4707, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(95193.6406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2260.3699, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(147.4326, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(116.9044, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.8427],
        [ 0.2285],
        [-1.1641],
        ...,
        [-6.6292],
        [-6.6212],
        [-6.6201]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-759593.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 330.0 event: 4950 loss: tensor(458.5934, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.7700],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(651.1021, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.7700],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(651.1021, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064,  0.0024, -0.0163,  ..., -0.0088, -0.0001, -0.0013],
        [-0.0067,  0.0029, -0.0175,  ..., -0.0093, -0.0002,  0.0014],
        [-0.0067,  0.0028, -0.0173,  ..., -0.0092, -0.0002,  0.0009],
        ...,
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-66433.2969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.2079, device='cuda:0')



h[100].sum tensor(-0.5418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(90.6489, device='cuda:0')



h[200].sum tensor(-567.5059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.0669, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0124, 0.0000,  ..., 0.0000, 0.0000, 0.0086],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0014],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(292071.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.3136e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.5295e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 2.9894e-01,
         8.8461e-05],
        ...,
        [0.0000e+00, 2.4694e-03, 0.0000e+00,  ..., 2.6933e-02, 7.3299e-02,
         9.3765e-02],
        [0.0000e+00, 2.4694e-03, 0.0000e+00,  ..., 2.6933e-02, 7.3299e-02,
         9.3765e-02],
        [0.0000e+00, 2.4694e-03, 0.0000e+00,  ..., 2.6933e-02, 7.3299e-02,
         9.3765e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2380344.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2012.6503, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(94291.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2216.4966, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(88.5095, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(92.5745, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.1363],
        [ 1.1565],
        [ 0.9569],
        ...,
        [-6.3462],
        [-6.5291],
        [-6.5872]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-708301., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(751.5261, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(751.5261, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0068,  0.0030, -0.0175,  ..., -0.0093, -0.0002,  0.0015],
        [-0.0067,  0.0030, -0.0175,  ..., -0.0093, -0.0002,  0.0014],
        [-0.0067,  0.0030, -0.0175,  ..., -0.0093, -0.0002,  0.0015],
        ...,
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-65960.2031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.8670, device='cuda:0')



h[100].sum tensor(-0.6076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(104.6303, device='cuda:0')



h[200].sum tensor(-568.0084, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.3213, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0035],
        [0.0000, 0.0123, 0.0000,  ..., 0.0000, 0.0000, 0.0080],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(299441.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6515, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6503, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6721, 0.0000],
        ...,
        [0.0000, 0.0025, 0.0000,  ..., 0.0264, 0.0718, 0.0955],
        [0.0000, 0.0025, 0.0000,  ..., 0.0264, 0.0718, 0.0955],
        [0.0000, 0.0025, 0.0000,  ..., 0.0264, 0.0718, 0.0955]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2422425.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2042.3519, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(95048.4531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2253.1187, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(165.6097, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(134.5787, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7649],
        [ 0.7877],
        [ 0.8040],
        ...,
        [-6.7079],
        [-6.6996],
        [-6.6984]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-754245.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(849.3729, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(849.3729, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0071,  0.0036, -0.0189,  ..., -0.0100, -0.0003,  0.0048],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-65445.7227, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.3714, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.4066, device='cuda:0')



h[100].sum tensor(-0.6769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(118.2529, device='cuda:0')



h[200].sum tensor(-568.5579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(44.4408, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0124, 0.0000,  ..., 0.0000, 0.0000, 0.0139],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0066],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306967.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5963, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4425, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0011, 0.2793, 0.0061],
        ...,
        [0.0000, 0.0023, 0.0000,  ..., 0.0255, 0.0701, 0.0976],
        [0.0000, 0.0023, 0.0000,  ..., 0.0255, 0.0701, 0.0976],
        [0.0000, 0.0023, 0.0000,  ..., 0.0255, 0.0701, 0.0976]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2458106., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2067.7598, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(97058.1016, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2311.1567, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(234.4554, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(186.1088, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.9288],
        [ 0.9904],
        [ 1.0349],
        ...,
        [-6.8012],
        [-6.7924],
        [-6.7909]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-842678.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(718.6407, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(718.6407, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0065,  0.0025, -0.0166,  ..., -0.0089, -0.0001, -0.0006],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-66792.2969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.3413, device='cuda:0')



h[100].sum tensor(-0.5536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(100.0519, device='cuda:0')



h[200].sum tensor(-567.8547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.6007, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(291654.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0026, 0.2420, 0.0119],
        [0.0000, 0.0000, 0.0000,  ..., 0.0078, 0.1950, 0.0293],
        [0.0000, 0.0013, 0.0000,  ..., 0.0180, 0.1164, 0.0675],
        ...,
        [0.0000, 0.0023, 0.0000,  ..., 0.0250, 0.0693, 0.0981],
        [0.0000, 0.0023, 0.0000,  ..., 0.0245, 0.0729, 0.0958],
        [0.0000, 0.0012, 0.0000,  ..., 0.0221, 0.0897, 0.0849]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2370098.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1997.6385, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(98984.7188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2283.1733, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(83.1405, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(159.0895, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1369],
        [-1.1018],
        [-2.8535],
        ...,
        [-6.6608],
        [-6.2883],
        [-5.5454]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-901147.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2781],
        [0.2754],
        [0.2529],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(746.4032, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2781],
        [0.2754],
        [0.2529],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(746.4032, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5604e-03,  2.6456e-03, -1.6788e-02,  ..., -8.9713e-03,
         -1.3291e-04, -2.8423e-04],
        [-6.5084e-03,  2.5483e-03, -1.6587e-02,  ..., -8.8775e-03,
         -1.2045e-04, -7.5417e-04],
        [-6.2663e-03,  2.0956e-03, -1.5652e-02,  ..., -8.4406e-03,
         -6.2469e-05, -2.9409e-03],
        ...,
        [-6.0055e-03,  1.6079e-03, -1.4645e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2969e-03],
        [-6.0055e-03,  1.6079e-03, -1.4645e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2969e-03],
        [-6.0055e-03,  1.6079e-03, -1.4645e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2969e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-66564.4531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.6293, device='cuda:0')



h[100].sum tensor(-0.5627, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(103.9171, device='cuda:0')



h[200].sum tensor(-568.0145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.0532, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0106, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0003],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(298716.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4967, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4681, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4231, 0.0000],
        ...,
        [0.0000, 0.0025, 0.0000,  ..., 0.0252, 0.0698, 0.0966],
        [0.0000, 0.0025, 0.0000,  ..., 0.0252, 0.0698, 0.0966],
        [0.0000, 0.0025, 0.0000,  ..., 0.0252, 0.0698, 0.0966]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2419573.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2046.3866, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(95464.2969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2247.3032, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(138.9905, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(147.1795, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.1039],
        [ 1.1055],
        [ 1.1142],
        ...,
        [-6.6380],
        [-6.6506],
        [-6.6752]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-788244.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6064],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(557.6959, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6064],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(557.6959, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0066,  0.0027, -0.0169,  ..., -0.0090, -0.0001, -0.0001],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0071,  0.0036, -0.0188,  ..., -0.0099, -0.0003,  0.0044],
        ...,
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-68140.8672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.8743, device='cuda:0')



h[100].sum tensor(-0.4206, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(77.6445, device='cuda:0')



h[200].sum tensor(-567.0458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.1797, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0120, 0.0000,  ..., 0.0000, 0.0000, 0.0070],
        [0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.0000, 0.0062],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(284088.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0064, 0.2157, 0.0261],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3753, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4061, 0.0000],
        ...,
        [0.0000, 0.0025, 0.0000,  ..., 0.0252, 0.0698, 0.0966],
        [0.0000, 0.0025, 0.0000,  ..., 0.0252, 0.0698, 0.0966],
        [0.0000, 0.0025, 0.0000,  ..., 0.0252, 0.0698, 0.0966]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2347302.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-1978.4349, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(97296.7344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2221.9883, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(56.4986, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(111.0800, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.1335],
        [ 0.2927],
        [ 0.9154],
        ...,
        [-6.7979],
        [-6.7894],
        [-6.7880]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-837798.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(849.8733, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(849.8733, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.7089e-03, -1.4645e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3004e-03],
        [-6.0055e-03,  1.7089e-03, -1.4645e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3004e-03],
        [-6.2603e-03,  2.1868e-03, -1.5629e-02,  ..., -8.4298e-03,
         -5.9503e-05, -3.0003e-03],
        ...,
        [-6.0055e-03,  1.7089e-03, -1.4645e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3004e-03],
        [-6.0055e-03,  1.7089e-03, -1.4645e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3004e-03],
        [-6.0055e-03,  1.7089e-03, -1.4645e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3004e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-65725.7656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.3372, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.4298, device='cuda:0')



h[100].sum tensor(-0.6190, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(118.3226, device='cuda:0')



h[200].sum tensor(-568.5106, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(44.4670, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(301505.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0002, 0.0000,  ..., 0.0186, 0.0958, 0.0784],
        [0.0000, 0.0000, 0.0000,  ..., 0.0160, 0.1135, 0.0670],
        [0.0000, 0.0000, 0.0000,  ..., 0.0132, 0.1295, 0.0565],
        ...,
        [0.0000, 0.0016, 0.0000,  ..., 0.0225, 0.0704, 0.0941],
        [0.0000, 0.0016, 0.0000,  ..., 0.0225, 0.0704, 0.0941],
        [0.0000, 0.0016, 0.0000,  ..., 0.0225, 0.0704, 0.0941]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2398770.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2120.3450, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(92615.3672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2204.5215, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(70.1379, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(164.4020, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.9968],
        [-3.8720],
        [-3.2346],
        ...,
        [-6.7867],
        [-6.7757],
        [-6.7685]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-803984.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(706.3986, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(706.3986, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67110.9688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8126, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.7733, device='cuda:0')



h[100].sum tensor(-0.5021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(98.3475, device='cuda:0')



h[200].sum tensor(-567.7857, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.9601, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(291746.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0025, 0.0000,  ..., 0.0232, 0.0784, 0.0891],
        [0.0000, 0.0011, 0.0000,  ..., 0.0185, 0.1065, 0.0704],
        [0.0000, 0.0000, 0.0000,  ..., 0.0088, 0.1742, 0.0329],
        ...,
        [0.0000, 0.0024, 0.0000,  ..., 0.0243, 0.0705, 0.0940],
        [0.0000, 0.0024, 0.0000,  ..., 0.0243, 0.0705, 0.0940],
        [0.0000, 0.0024, 0.0000,  ..., 0.0243, 0.0705, 0.0940]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2356955.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2034.9366, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(93367.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2180.6729, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(61.8992, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(111.9946, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.4858],
        [-2.8829],
        [-1.0795],
        ...,
        [-6.7190],
        [-6.7109],
        [-6.7097]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-716245.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(923.4279, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(923.4279, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-65654.9062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.5806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(42.8424, device='cuda:0')



h[100].sum tensor(-0.6328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(128.5631, device='cuda:0')



h[200].sum tensor(-568.8469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(48.3155, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(311808.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0026, 0.0000,  ..., 0.0235, 0.0707, 0.0939],
        [0.0000, 0.0026, 0.0000,  ..., 0.0235, 0.0707, 0.0939],
        [0.0000, 0.0028, 0.0000,  ..., 0.0236, 0.0710, 0.0938],
        ...,
        [0.0000, 0.0021, 0.0000,  ..., 0.0234, 0.0700, 0.0942],
        [0.0000, 0.0021, 0.0000,  ..., 0.0234, 0.0700, 0.0942],
        [0.0000, 0.0021, 0.0000,  ..., 0.0234, 0.0700, 0.0942]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2458469., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2080.5322, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(92333.4844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2220.7261, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(223.9520, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(183.0958, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.7472],
        [-6.8190],
        [-6.8793],
        ...,
        [-6.7359],
        [-6.7278],
        [-6.7266]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-720207.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(752.6929, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(752.6929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67368.3750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9606, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.9212, device='cuda:0')



h[100].sum tensor(-0.5032, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(104.7927, device='cuda:0')



h[200].sum tensor(-567.9902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.3823, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0001],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(299264.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0006, 0.0000,  ..., 0.0171, 0.0757, 0.0897],
        [0.0000, 0.0003, 0.0000,  ..., 0.0130, 0.0969, 0.0752],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.1614, 0.0396],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0074, 0.1317, 0.0534],
        [0.0000, 0.0000, 0.0000,  ..., 0.0106, 0.1135, 0.0652],
        [0.0000, 0.0000, 0.0000,  ..., 0.0146, 0.0903, 0.0804]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2395944.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2142.6919, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(93656.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2239.2959, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(166.6769, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(212.8497, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.6400],
        [-4.1247],
        [-2.1497],
        ...,
        [-4.4886],
        [-4.7211],
        [-5.3167]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-947165.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 340.0 event: 5100 loss: tensor(431.5517, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(583.3171, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(583.3171, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.1968e-03,  1.9700e-03, -1.5385e-02,  ..., -8.3152e-03,
         -4.0352e-05, -3.5879e-03],
        [-6.0055e-03,  1.6281e-03, -1.4646e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3110e-03],
        [-6.0055e-03,  1.6281e-03, -1.4646e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3110e-03],
        ...,
        [-6.0055e-03,  1.6281e-03, -1.4646e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3110e-03],
        [-6.0055e-03,  1.6281e-03, -1.4646e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3110e-03],
        [-6.0055e-03,  1.6281e-03, -1.4646e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3110e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-68854.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3749, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.0630, device='cuda:0')



h[100].sum tensor(-0.3860, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(81.2116, device='cuda:0')



h[200].sum tensor(-567.1809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.5203, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(284142.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0018, 0.2283, 0.0144],
        [0.0000, 0.0000, 0.0000,  ..., 0.0060, 0.1661, 0.0328],
        [0.0000, 0.0007, 0.0000,  ..., 0.0138, 0.1023, 0.0719],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0197, 0.0698, 0.0937],
        [0.0000, 0.0009, 0.0000,  ..., 0.0197, 0.0698, 0.0937],
        [0.0000, 0.0009, 0.0000,  ..., 0.0197, 0.0698, 0.0937]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2318500.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2081.4170, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(93575.3359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2187.5684, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(47.7880, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(142.2350, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3338],
        [-1.0176],
        [-2.8289],
        ...,
        [-6.7746],
        [-6.7652],
        [-6.7636]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-839102., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6313],
        [0.5386],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(842.8430, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6313],
        [0.5386],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(842.8430, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5156e-03,  2.5002e-03, -1.6616e-02,  ..., -8.8904e-03,
         -1.0485e-04, -7.2388e-04],
        [-6.6035e-03,  2.6555e-03, -1.6955e-02,  ..., -9.0490e-03,
         -1.2291e-04,  6.6816e-05],
        [-6.5156e-03,  2.5002e-03, -1.6616e-02,  ..., -8.8904e-03,
         -1.0485e-04, -7.2388e-04],
        ...,
        [-6.0055e-03,  1.5988e-03, -1.4646e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3141e-03],
        [-6.0055e-03,  1.5988e-03, -1.4646e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3141e-03],
        [-6.0055e-03,  1.5988e-03, -1.4646e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3141e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-66840.9766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.1037, device='cuda:0')



h[100].sum tensor(-0.5355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(117.3438, device='cuda:0')



h[200].sum tensor(-568.4564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(44.0992, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0160, 0.0000,  ..., 0.0000, 0.0000, 0.0277],
        [0.0000, 0.0124, 0.0000,  ..., 0.0000, 0.0000, 0.0107],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0017],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(303206., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7575, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6661, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5866, 0.0000],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0203, 0.0707, 0.0923],
        [0.0000, 0.0013, 0.0000,  ..., 0.0203, 0.0707, 0.0923],
        [0.0000, 0.0013, 0.0000,  ..., 0.0203, 0.0707, 0.0923]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2402772.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2170.3813, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(89024.2500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2175.0315, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(133.0420, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(149.6071, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.1318],
        [ 1.1279],
        [ 1.1272],
        ...,
        [-6.6947],
        [-6.6855],
        [-6.6839]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-649860.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(654.2903, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(654.2903, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.7357e-03, -1.4646e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3143e-03],
        [-6.0055e-03,  1.7357e-03, -1.4646e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3143e-03],
        [-6.2501e-03,  2.1674e-03, -1.5591e-02,  ..., -8.4114e-03,
         -4.9008e-05, -3.1140e-03],
        ...,
        [-6.0055e-03,  1.7357e-03, -1.4646e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3143e-03],
        [-6.0055e-03,  1.7357e-03, -1.4646e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3143e-03],
        [-6.0055e-03,  1.7357e-03, -1.4646e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3143e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-68615.8984, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5988, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.3558, device='cuda:0')



h[100].sum tensor(-0.4037, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.0928, device='cuda:0')



h[200].sum tensor(-567.4902, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.2337, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(292405.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0098, 0.1001, 0.0707],
        [0.0000, 0.0000, 0.0000,  ..., 0.0051, 0.1550, 0.0372],
        [0.0000, 0.0000, 0.0000,  ..., 0.0002, 0.2277, 0.0141],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0710, 0.0903],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0710, 0.0903],
        [0.0000, 0.0000, 0.0000,  ..., 0.0151, 0.0710, 0.0903]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2348036.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2231.8560, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(87260.4062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2126.7048, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(134.6758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(183.6269, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5567],
        [-1.7636],
        [-0.2443],
        ...,
        [-6.7726],
        [-6.7629],
        [-6.7611]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-875404.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(682.5081, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(682.5081, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2511e-03,  2.1024e-03, -1.5595e-02,  ..., -8.4132e-03,
         -4.7953e-05, -3.1071e-03],
        [-6.2511e-03,  2.1024e-03, -1.5595e-02,  ..., -8.4132e-03,
         -4.7953e-05, -3.1071e-03],
        [-6.0055e-03,  1.6755e-03, -1.4646e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3150e-03],
        ...,
        [-6.0055e-03,  1.6755e-03, -1.4646e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3150e-03],
        [-6.0055e-03,  1.6755e-03, -1.4646e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3150e-03],
        [-6.0055e-03,  1.6755e-03, -1.4646e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3150e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-68525.4844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7260, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.6649, device='cuda:0')



h[100].sum tensor(-0.4139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(95.0214, device='cuda:0')



h[200].sum tensor(-567.6660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.7101, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(293153., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0804e-04, 1.7776e-01,
         2.1664e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.6834e-03, 1.5766e-01,
         3.1450e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 4.4196e-03, 1.3538e-01,
         4.6665e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.4588e-02, 7.0521e-02,
         9.0364e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.4588e-02, 7.0521e-02,
         9.0364e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.4588e-02, 7.0521e-02,
         9.0364e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2349181., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2267.6924, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(86459.1250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2117.5444, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(140.3586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(197.9030, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8785],
        [-3.2932],
        [-4.6395],
        ...,
        [-6.7547],
        [-6.7452],
        [-6.7435]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-811303.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4944],
        [0.5615],
        [0.6328],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(694.0161, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4944],
        [0.5615],
        [0.6328],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(694.0161, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0065,  0.0024, -0.0167,  ..., -0.0089, -0.0001, -0.0005],
        [-0.0071,  0.0033, -0.0188,  ..., -0.0099, -0.0002,  0.0043],
        [-0.0065,  0.0024, -0.0167,  ..., -0.0089, -0.0001, -0.0005],
        ...,
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-68797.3125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7798, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.1988, device='cuda:0')



h[100].sum tensor(-0.4115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.6236, device='cuda:0')



h[200].sum tensor(-567.7404, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.3122, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0125, 0.0000,  ..., 0.0000, 0.0000, 0.0130],
        [0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0000, 0.0050],
        [0.0000, 0.0126, 0.0000,  ..., 0.0000, 0.0000, 0.0135],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(290690.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.5742e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.1017e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.6023e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 4.7459e-04, 0.0000e+00,  ..., 1.7050e-02, 6.9063e-02,
         9.2678e-02],
        [0.0000e+00, 4.7459e-04, 0.0000e+00,  ..., 1.7050e-02, 6.9063e-02,
         9.2678e-02],
        [0.0000e+00, 4.7459e-04, 0.0000e+00,  ..., 1.7050e-02, 6.9063e-02,
         9.2678e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2337889., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2214.7378, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(87998.7656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2125.6711, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(59.5443, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(170.5605, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.2133],
        [ 1.2128],
        [ 1.2077],
        ...,
        [-6.4640],
        [-6.5532],
        [-6.6322]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-610135.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(652.9395, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(652.9395, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-69712.9844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.2931, device='cuda:0')



h[100].sum tensor(-0.3743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(90.9047, device='cuda:0')



h[200].sum tensor(-567.5064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.1630, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(283308.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0096, 0.0747, 0.0876],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0669, 0.0930],
        [0.0000, 0.0000, 0.0000,  ..., 0.0122, 0.0673, 0.0928],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0121, 0.0663, 0.0933],
        [0.0000, 0.0000, 0.0000,  ..., 0.0121, 0.0663, 0.0933],
        [0.0000, 0.0000, 0.0000,  ..., 0.0121, 0.0663, 0.0933]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2269557.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2247.9707, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(87607.2500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2063.7478, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(53.5691, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(234.7918, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.6495],
        [-6.4527],
        [-6.8210],
        ...,
        [-6.8511],
        [-6.8417],
        [-6.8401]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-862997.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4395],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(931.8173, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4395],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(931.8173, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.6268e-03, -1.4646e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3016e-03],
        [-6.4217e-03,  2.3353e-03, -1.6254e-02,  ..., -8.7210e-03,
         -7.5177e-05, -1.5603e-03],
        [-6.3378e-03,  2.1924e-03, -1.5929e-02,  ..., -8.5695e-03,
         -6.0016e-05, -2.3148e-03],
        ...,
        [-6.0055e-03,  1.6268e-03, -1.4646e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3016e-03],
        [-6.0055e-03,  1.6268e-03, -1.4646e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3016e-03],
        [-6.0055e-03,  1.6268e-03, -1.4646e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3016e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-67807.8516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.6598, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(43.2316, device='cuda:0')



h[100].sum tensor(-0.5231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(129.7311, device='cuda:0')



h[200].sum tensor(-568.9564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(48.7545, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0000, 0.0016],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(312905.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0014, 0.1446, 0.0430],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2078, 0.0239],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3266, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0642, 0.0945],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0642, 0.0945],
        [0.0000, 0.0000, 0.0000,  ..., 0.0095, 0.0642, 0.0945]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2450081.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2436.4004, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(83600.8438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2094.3965, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(324.1741, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(338.8065, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3144],
        [-0.2786],
        [ 0.6589],
        ...,
        [-6.9310],
        [-6.9213],
        [-6.9196]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-880663.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(631.8638, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(631.8638, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5345e-03,  2.3685e-03, -1.6689e-02,  ..., -8.9245e-03,
         -9.3093e-05, -5.5502e-04],
        [-6.5345e-03,  2.3685e-03, -1.6689e-02,  ..., -8.9245e-03,
         -9.3093e-05, -5.5502e-04],
        [-6.0055e-03,  1.4908e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3059e-03],
        ...,
        [-6.0055e-03,  1.4908e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3059e-03],
        [-6.0055e-03,  1.4908e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3059e-03],
        [-6.0055e-03,  1.4908e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3059e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-70601.0156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5588, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.3153, device='cuda:0')



h[100].sum tensor(-0.3470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.9705, device='cuda:0')



h[200].sum tensor(-567.4349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.0603, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(279829.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2309, 0.0112],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2570, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2849, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0630, 0.0968],
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0630, 0.0968],
        [0.0000, 0.0000, 0.0000,  ..., 0.0094, 0.0630, 0.0968]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2244850., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2247.2297, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(88186.7656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2018.9336, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(45.1823, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(261.4708, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5849],
        [ 1.2817],
        [ 1.3980],
        ...,
        [-6.9078],
        [-6.8983],
        [-6.8966]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-823681., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(562.8892, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(562.8892, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2253e-03,  1.7890e-03, -1.5495e-02,  ..., -8.3665e-03,
         -3.7676e-05, -3.3353e-03],
        [-6.0055e-03,  1.4306e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3076e-03],
        [-6.0055e-03,  1.4306e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3076e-03],
        ...,
        [-6.0055e-03,  1.4306e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3076e-03],
        [-6.0055e-03,  1.4306e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3076e-03],
        [-6.0055e-03,  1.4306e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3076e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-71499.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.1152, device='cuda:0')



h[100].sum tensor(-0.2999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(78.3676, device='cuda:0')



h[200].sum tensor(-567.0700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.4514, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(275806.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.8104e-06, 1.4731e-01,
         3.8848e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 8.2610e-04, 1.3245e-01,
         4.9128e-02],
        [0.0000e+00, 6.4783e-05, 0.0000e+00,  ..., 3.3975e-03, 8.9807e-02,
         7.8883e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 8.6232e-03, 6.1420e-02,
         9.8348e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 8.6232e-03, 6.1420e-02,
         9.8348e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 8.6232e-03, 6.1420e-02,
         9.8348e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2232001., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2231.6367, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(88318.3047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1991.5839, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(39.6310, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(262.5011, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7986],
        [-3.3322],
        [-4.2789],
        ...,
        [-6.9254],
        [-6.9157],
        [-6.9138]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-789420.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(625.3671, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(625.3671, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-71371.3906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.0139, device='cuda:0')



h[100].sum tensor(-0.3223, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.0660, device='cuda:0')



h[200].sum tensor(-567.3716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.7204, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(277790.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0046, 0.0639, 0.0949],
        [0.0000, 0.0000, 0.0000,  ..., 0.0054, 0.0602, 0.0976],
        [0.0000, 0.0000, 0.0000,  ..., 0.0049, 0.0633, 0.0956],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0585, 0.0986],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0585, 0.0986],
        [0.0000, 0.0000, 0.0000,  ..., 0.0057, 0.0585, 0.0986]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2217531.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2285.8738, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(86342.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1960.3834, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(31.8159, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(328.9341, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.1696],
        [-6.3279],
        [-6.0839],
        ...,
        [-7.0333],
        [-7.0235],
        [-7.0218]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-924892.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 350.0 event: 5250 loss: tensor(465.0651, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5195],
        [0.6719],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(816.7725, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5195],
        [0.6719],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(816.7725, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6418e-03,  2.4806e-03, -1.7104e-02,  ..., -9.1182e-03,
         -1.0352e-04,  3.9296e-04],
        [-6.4976e-03,  2.2519e-03, -1.6547e-02,  ..., -8.8579e-03,
         -8.0047e-05, -9.0075e-04],
        [-6.6418e-03,  2.4806e-03, -1.7104e-02,  ..., -9.1182e-03,
         -1.0352e-04,  3.9296e-04],
        ...,
        [-6.0055e-03,  1.4717e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3126e-03],
        [-6.0055e-03,  1.4717e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3126e-03],
        [-6.0055e-03,  1.4717e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3126e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-69997.1328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.8941, device='cuda:0')



h[100].sum tensor(-0.4068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(113.7141, device='cuda:0')



h[200].sum tensor(-568.3203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.7351, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0037],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(291938.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3009, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3157, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2269, 0.0158],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0577, 0.0982],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0577, 0.0982],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0577, 0.0982]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2262626., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2370.0391, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(83145.6406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1948.0496, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(73.0856, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(396.1010, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1450],
        [ 0.4607],
        [ 0.0307],
        ...,
        [-7.0205],
        [-7.0125],
        [-7.0100]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-892761.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(868.5375, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(868.5375, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-69599.1562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.4266, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.2958, device='cuda:0')



h[100].sum tensor(-0.4258, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(120.9211, device='cuda:0')



h[200].sum tensor(-568.6342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(45.4435, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(295823.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0019, 0.0756, 0.0862],
        [0.0000, 0.0000, 0.0000,  ..., 0.0033, 0.0611, 0.0962],
        [0.0000, 0.0000, 0.0000,  ..., 0.0038, 0.0588, 0.0979],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0579, 0.0983],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0579, 0.0983],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0579, 0.0983]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2272161., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2375.5073, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(82640.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1940.7284, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(44.9918, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(378.8009, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.5859],
        [-5.7699],
        [-6.3602],
        ...,
        [-6.9169],
        [-6.9078],
        [-6.9062]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-672876.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(620.3419, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(620.3419, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-71704.6406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.7807, device='cuda:0')



h[100].sum tensor(-0.3023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(86.3663, device='cuda:0')



h[200].sum tensor(-567.3414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.4575, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(275528.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0010, 0.0868, 0.0781],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1114, 0.0609],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1351, 0.0442],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0579, 0.0983],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0579, 0.0983],
        [0.0000, 0.0000, 0.0000,  ..., 0.0039, 0.0579, 0.0983]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2178457.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2290.1460, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(84202.2734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1896.6912, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(9.7494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(335.0011, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.3472],
        [-3.5158],
        [-2.7748],
        ...,
        [-6.9169],
        [-6.9078],
        [-6.9062]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-732934.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(775.0824, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(775.0824, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-70533.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.9599, device='cuda:0')



h[100].sum tensor(-0.3686, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(107.9099, device='cuda:0')



h[200].sum tensor(-568.1425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.5538, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(289614.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0821, 0.0814],
        [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0823, 0.0812],
        [0.0000, 0.0000, 0.0000,  ..., 0.0007, 0.0892, 0.0764],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0575, 0.0984],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0575, 0.0984],
        [0.0000, 0.0000, 0.0000,  ..., 0.0027, 0.0575, 0.0984]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2246163., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2384.9302, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(81565.9297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1913.4448, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(30.6855, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(376.0291, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.6001],
        [-2.1778],
        [-1.5421],
        ...,
        [-6.8699],
        [-6.8610],
        [-6.8595]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-626926.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2612],
        [0.0000],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(647.9766, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2612],
        [0.0000],
        [0.2466],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(647.9766, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.1974e-03,  1.5262e-03, -1.5388e-02,  ..., -8.3163e-03,
         -2.8844e-05, -3.6172e-03],
        [-6.8813e-03,  2.4987e-03, -1.8028e-02,  ..., -9.5504e-03,
         -1.3163e-04,  2.5031e-03],
        [-6.2085e-03,  1.5419e-03, -1.5430e-02,  ..., -8.3362e-03,
         -3.0504e-05, -3.5183e-03],
        ...,
        [-6.0055e-03,  1.2533e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3346e-03],
        [-6.0055e-03,  1.2533e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3346e-03],
        [-6.0055e-03,  1.2533e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3346e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-71880.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5987, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.0628, device='cuda:0')



h[100].sum tensor(-0.2996, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(90.2138, device='cuda:0')



h[200].sum tensor(-567.4901, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.9034, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0025],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0030],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(277487.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3536, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3643, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4257, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0551, 0.0992],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0551, 0.0992],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0551, 0.0992]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2158072.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2366.2915, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(81565.2422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1867.6578, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(7.9771, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(441.1532, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.2645],
        [ 1.2662],
        [ 1.2508],
        ...,
        [-6.9743],
        [-6.9650],
        [-6.9634]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-874544.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9429],
        [0.8540],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(543.7529, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9429],
        [0.8540],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(543.7529, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0069,  0.0025, -0.0181,  ..., -0.0096, -0.0001,  0.0027],
        [-0.0068,  0.0024, -0.0178,  ..., -0.0094, -0.0001,  0.0019],
        [-0.0069,  0.0025, -0.0181,  ..., -0.0096, -0.0001,  0.0027],
        ...,
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-72831.0703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.2274, device='cuda:0')



h[100].sum tensor(-0.2436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(75.7033, device='cuda:0')



h[200].sum tensor(-566.9464, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.4502, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0031],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0137],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0138],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(269506.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3386, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4336, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4307, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0533, 0.1004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0533, 0.1004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0533, 0.1004]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2090951.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2443.3384, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(82623.1250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1654.1333, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4.4692, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(331.2740, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.2297],
        [ 1.1780],
        [ 1.0964],
        ...,
        [-7.0032],
        [-6.9939],
        [-6.9923]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-946766.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(677.4012, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(677.4012, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6243e-03,  2.0652e-03, -1.7036e-02,  ..., -9.0865e-03,
         -8.8187e-05,  2.3077e-04],
        [-6.0055e-03,  1.1968e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3140e-03],
        [-6.0055e-03,  1.1968e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3140e-03],
        ...,
        [-6.0055e-03,  1.1968e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3140e-03],
        [-6.0055e-03,  1.1968e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3140e-03],
        [-6.0055e-03,  1.1968e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3140e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-71473.0469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.4280, device='cuda:0')



h[100].sum tensor(-0.2977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(94.3104, device='cuda:0')



h[200].sum tensor(-567.6548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.4429, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(287744.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1598, 0.0322],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1329, 0.0486],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0784, 0.0825],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0683, 0.0900],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0562, 0.0983],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0526, 0.1008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2171519.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2456.4546, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(80848.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1343.1030, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(12.7600, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(30.3747, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4758],
        [-2.1198],
        [-3.8571],
        ...,
        [-6.0439],
        [-6.5161],
        [-6.7836]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-793552.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(643.9083, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(643.9083, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-71560.6328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.8741, device='cuda:0')



h[100].sum tensor(-0.2738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.6474, device='cuda:0')



h[200].sum tensor(-567.4663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.6905, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(285961.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0547, 0.0997],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0518, 0.1018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0521, 0.1016],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0513, 0.1020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0564, 0.0984],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0926, 0.0728]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2142208.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2417.6675, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(82317.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(955.3297, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4.3045, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-362.5898, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.7198],
        [-6.3935],
        [-6.7299],
        ...,
        [-6.3860],
        [-5.4676],
        [-3.6650]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-727867.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(724.5037, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(724.5037, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-71615.5625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.6133, device='cuda:0')



h[100].sum tensor(-0.2981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(100.8681, device='cuda:0')



h[200].sum tensor(-567.8585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.9074, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(286576.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0586, 0.0964],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0784, 0.0826],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1065, 0.0628],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0467, 0.1046],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0467, 0.1046],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0467, 0.1046]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2147195.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2476.6892, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(82874.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1266.6890, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(4.4483, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(31.0224, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.3891],
        [-3.8030],
        [-3.1818],
        ...,
        [-7.1011],
        [-7.0916],
        [-7.0899]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-994999.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(685.2053, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(685.2053, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0069,  0.0024, -0.0180,  ..., -0.0095, -0.0001,  0.0024],
        ...,
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-72256.9609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.7901, device='cuda:0')



h[100].sum tensor(-0.2771, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(95.3969, device='cuda:0')



h[200].sum tensor(-567.6963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.8513, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0016],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(279399.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1915, 0.0190],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3138, 0.0068],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3832, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0460, 0.1036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0460, 0.1036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0460, 0.1036]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2113073.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2610.4248, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(82055.9688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1479.1273, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.5559, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(283.8680, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.0344],
        [ 0.9801],
        [ 0.9272],
        ...,
        [-7.0925],
        [-7.0831],
        [-7.0815]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-941307.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 360.0 event: 5400 loss: tensor(428.7200, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8760],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(956.9963, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.8760],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(956.9963, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.2159e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2687e-03],
        [-6.8351e-03,  2.3644e-03, -1.7850e-02,  ..., -9.4670e-03,
         -1.0624e-04,  2.1891e-03],
        [-6.2874e-03,  1.6061e-03, -1.5735e-02,  ..., -8.4786e-03,
         -3.6094e-05, -2.7349e-03],
        ...,
        [-6.0055e-03,  1.2159e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2687e-03],
        [-6.0055e-03,  1.2159e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2687e-03],
        [-6.0055e-03,  1.2159e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2687e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-70177.9688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.7623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(44.3998, device='cuda:0')



h[100].sum tensor(-0.3769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(133.2366, device='cuda:0')



h[200].sum tensor(-569.0980, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(50.0719, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0022],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0104, 0.0000,  ..., 0.0000, 0.0000, 0.0150],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(300356.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 2.5803e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.0291e-01,
         3.2273e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.3942e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.7824e-02,
         1.0125e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.7824e-02,
         1.0125e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.7824e-02,
         1.0125e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2211866.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2693.4053, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(78498.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1516.7821, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(2.4812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(366.2127, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.3110],
        [ 1.2723],
        [ 1.2319],
        ...,
        [-6.9837],
        [-6.9748],
        [-6.9734]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-715898., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(736.3228, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3174],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(736.3228, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3061e-03,  1.5946e-03, -1.5807e-02,  ..., -8.5124e-03,
         -3.7468e-05, -2.5625e-03],
        [-6.0055e-03,  1.1854e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2654e-03],
        [-6.3061e-03,  1.5946e-03, -1.5807e-02,  ..., -8.5124e-03,
         -3.7468e-05, -2.5625e-03],
        ...,
        [-6.0055e-03,  1.1854e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2654e-03],
        [-6.0055e-03,  1.1854e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2654e-03],
        [-6.0055e-03,  1.1854e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2654e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-72403.7578, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9291, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.1617, device='cuda:0')



h[100].sum tensor(-0.2807, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(102.5136, device='cuda:0')



h[200].sum tensor(-567.9467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.5258, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(285863.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1044, 0.0619],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1490, 0.0312],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1251, 0.0476],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0481, 0.1006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0481, 0.1006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0481, 0.1006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2155412.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2584.2073, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(79915.9219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1525.3770, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.1023, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(366.0311, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.1631],
        [-2.8907],
        [-1.7583],
        ...,
        [-6.8566],
        [-6.7824],
        [-6.7001]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-668236.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(695.7719, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(695.7719, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.2160e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2692e-03],
        [-6.0055e-03,  1.2160e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2692e-03],
        [-6.2077e-03,  1.4857e-03, -1.5428e-02,  ..., -8.3348e-03,
         -2.4532e-05, -3.4515e-03],
        ...,
        [-6.0055e-03,  1.2160e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2692e-03],
        [-6.0055e-03,  1.2160e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2692e-03],
        [-6.0055e-03,  1.2160e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2692e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-73309.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.2803, device='cuda:0')



h[100].sum tensor(-0.2582, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.8680, device='cuda:0')



h[200].sum tensor(-567.7420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.4041, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(280732.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0615, 0.0891],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1090, 0.0550],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1517, 0.0295],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0437, 0.1018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0437, 0.1018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0437, 0.1018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2119448.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2480.4016, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(81914.1875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1621.9780, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(504.7689, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.8491],
        [-2.5715],
        [-1.0811],
        ...,
        [-7.0607],
        [-7.0519],
        [-7.0510]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-884879., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(692.5455, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(692.5455, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-73804.9531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.1306, device='cuda:0')



h[100].sum tensor(-0.2505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.4188, device='cuda:0')



h[200].sum tensor(-567.7342, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.2353, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(274752.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1833, 0.0123],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1860, 0.0112],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2731, 0.0057],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0712, 0.0806],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0476, 0.0976],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0406, 0.1026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2076528.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2419.7773, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(83798.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1704.8889, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.0656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(576.4869, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.8932],
        [ 0.9083],
        [ 0.9109],
        ...,
        [-4.9432],
        [-6.1011],
        [-6.7321]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-983916.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(608.3149, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(608.3149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-74717.5391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4613, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.2227, device='cuda:0')



h[100].sum tensor(-0.2138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(84.6919, device='cuda:0')



h[200].sum tensor(-567.3002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.8282, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268303.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0479, 0.0966],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0429, 0.1000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0421, 0.1006],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0414, 0.1010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0414, 0.1010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0414, 0.1010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2039707., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2353.4624, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(84459.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1720.3447, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.4411, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(577.1513, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.8511],
        [-5.7319],
        [-6.2606],
        ...,
        [-7.0803],
        [-7.0715],
        [-7.0702]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-926216.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4812],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(740.8203, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4812],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(740.8203, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.1637e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2692e-03],
        [-6.4613e-03,  1.7339e-03, -1.6406e-02,  ..., -8.7923e-03,
         -5.0970e-05, -1.1724e-03],
        [-6.3075e-03,  1.5416e-03, -1.5813e-02,  ..., -8.5149e-03,
         -3.3773e-05, -2.5546e-03],
        ...,
        [-6.0055e-03,  1.1637e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2692e-03],
        [-6.0055e-03,  1.1637e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2692e-03],
        [-6.0055e-03,  1.1637e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2692e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-73702.9531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9512, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.3703, device='cuda:0')



h[100].sum tensor(-0.2529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(103.1398, device='cuda:0')



h[200].sum tensor(-567.9773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.7611, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0048],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(283327.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1260, 0.0471],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2076, 0.0241],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3620, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0439, 0.0982],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0439, 0.0982],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0439, 0.0982]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2117925.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2431.6162, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(81565., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1694.1340, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.0116, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(564.8250, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8429],
        [-0.2658],
        [ 0.7827],
        ...,
        [-6.9816],
        [-6.9731],
        [-6.9718]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-726072.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(690.5193, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(690.5193, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-74286.2500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.0366, device='cuda:0')



h[100].sum tensor(-0.2297, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.1367, device='cuda:0')



h[200].sum tensor(-567.7277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.1293, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(277796.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0449, 0.0957],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0449, 0.0957],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0451, 0.0956],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0444, 0.0961],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0444, 0.0961],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0444, 0.0961]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2074920.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2434.9519, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(80823.6406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1648.2528, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.0036, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(551.8994, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.9534],
        [-5.5013],
        [-4.6734],
        ...,
        [-6.9588],
        [-6.9505],
        [-6.9493]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-710583.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9438],
        [0.7729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(793.1112, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9438],
        [0.7729],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(793.1112, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.1960e-03,  2.7130e-03, -1.9243e-02,  ..., -1.0118e-02,
         -1.2607e-04,  5.4491e-03],
        [-7.0342e-03,  2.5148e-03, -1.8618e-02,  ..., -9.8262e-03,
         -1.0893e-04,  3.9929e-03],
        [-6.8994e-03,  2.3498e-03, -1.8098e-02,  ..., -9.5830e-03,
         -9.4659e-05,  2.7808e-03],
        ...,
        [-6.0055e-03,  1.2552e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2615e-03],
        [-6.0055e-03,  1.2552e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2615e-03],
        [-6.0055e-03,  1.2552e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2615e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-73772.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1274, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.7964, device='cuda:0')



h[100].sum tensor(-0.2542, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(110.4199, device='cuda:0')



h[200].sum tensor(-568.2207, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.4971, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0071],
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0220],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0000, 0.0161],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(282581.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3985, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5485, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5526, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0439, 0.0953],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0439, 0.0953],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0439, 0.0953]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2073558.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2459.6316, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(80425.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1649.1958, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.0494, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(589.6373, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7252],
        [ 0.9043],
        [ 0.9129],
        ...,
        [-6.9934],
        [-6.9923],
        [-6.9969]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-764059.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(624.6146, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(624.6146, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-75485.9141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5022, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.9790, device='cuda:0')



h[100].sum tensor(-0.1943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(86.9612, device='cuda:0')



h[200].sum tensor(-567.3567, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.6810, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268815.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0654, 0.0800],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1187, 0.0426],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2411, 0.0165],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0429, 0.0959],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0429, 0.0959],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0429, 0.0959]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2010449.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2425.0232, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(82148.0312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1650.1644, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(558.4207, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.2144],
        [-1.3860],
        [ 0.0072],
        ...,
        [-7.0871],
        [-7.0785],
        [-7.0771]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-827688.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4111],
        [0.3630],
        [0.4265],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(641.5825, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4111],
        [0.3630],
        [0.4265],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(641.5825, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3493e-03,  1.6344e-03, -1.5974e-02,  ..., -8.5904e-03,
         -3.4464e-05, -2.1529e-03],
        [-6.7988e-03,  2.1710e-03, -1.7709e-02,  ..., -9.4015e-03,
         -7.9519e-05,  1.8952e-03],
        [-6.3493e-03,  1.6344e-03, -1.5974e-02,  ..., -8.5904e-03,
         -3.4464e-05, -2.1529e-03],
        ...,
        [-6.0055e-03,  1.2240e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2494e-03],
        [-6.0055e-03,  1.2240e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2494e-03],
        [-6.0055e-03,  1.2240e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2494e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-75605.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5643, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.7662, device='cuda:0')



h[100].sum tensor(-0.1939, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.3235, device='cuda:0')



h[200].sum tensor(-567.4425, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.5688, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(269033.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2939, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2821, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2654, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0409, 0.0979],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0409, 0.0979],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0409, 0.0979]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2006097.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2413.4080, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(83720.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1683.8486, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(542.7009, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.4901],
        [ 1.2206],
        [ 0.5067],
        ...,
        [-7.1426],
        [-7.1338],
        [-7.1325]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-833785.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 370.0 event: 5550 loss: tensor(463.7575, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(843.4880, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(843.4880, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5354e-03,  1.8009e-03, -1.6693e-02,  ..., -8.9262e-03,
         -5.1671e-05, -4.6312e-04],
        [-6.0055e-03,  1.1747e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2393e-03],
        [-6.0055e-03,  1.1747e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2393e-03],
        ...,
        [-6.0055e-03,  1.1747e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2393e-03],
        [-6.0055e-03,  1.1747e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2393e-03],
        [-6.0055e-03,  1.1747e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2393e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-74115.3438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.3244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.1336, device='cuda:0')



h[100].sum tensor(-0.2491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(117.4336, device='cuda:0')



h[200].sum tensor(-568.4929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(44.1329, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(291122.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2328, 0.0102],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1790, 0.0267],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0878, 0.0654],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0397, 0.0997],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0397, 0.0997],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0397, 0.0997]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2149035.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2531.5898, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(82615.5391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1757.4645, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(550.8542, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7239],
        [-0.2957],
        [-1.9816],
        ...,
        [-7.1838],
        [-7.1740],
        [-7.1711]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-766184.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(608.0802, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(608.0802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2136e-03,  1.4038e-03, -1.5450e-02,  ..., -8.3454e-03,
         -1.9734e-05, -3.3631e-03],
        [-6.0055e-03,  1.1624e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2386e-03],
        [-6.0055e-03,  1.1624e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2386e-03],
        ...,
        [-6.0055e-03,  1.1624e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2386e-03],
        [-6.0055e-03,  1.1624e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2386e-03],
        [-6.0055e-03,  1.1624e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2386e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-76393.7188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.2119, device='cuda:0')



h[100].sum tensor(-0.1759, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(84.6592, device='cuda:0')



h[200].sum tensor(-567.3131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.8159, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(265056.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1782, 0.0166],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1469, 0.0333],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0871, 0.0680],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0383, 0.1018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0383, 0.1018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0383, 0.1018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1983741.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2386.4272, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(87796.5234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1750.0718, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(515.5410, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6539],
        [-1.6412],
        [-3.1708],
        ...,
        [-7.2999],
        [-7.2908],
        [-7.2893]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-971721.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(655.8273, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(655.8273, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-76231.6250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.4271, device='cuda:0')



h[100].sum tensor(-0.1828, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.3068, device='cuda:0')



h[200].sum tensor(-567.5317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.3141, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(270919.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0387, 0.1017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0387, 0.1017],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0389, 0.1015],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0382, 0.1020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0382, 0.1020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0382, 0.1020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2034197.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2457.5522, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(86231.9062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1770.8523, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.8027, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(518.7141, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.0622],
        [-7.1962],
        [-7.3008],
        ...,
        [-7.3481],
        [-7.3389],
        [-7.3374]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-940640.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4412],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(758.1050, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4412],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(758.1050, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.4233e-03,  1.6925e-03, -1.6260e-02,  ..., -8.7239e-03,
         -3.7493e-05, -1.4643e-03],
        [-6.0055e-03,  1.2181e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2334e-03],
        [-6.4233e-03,  1.6925e-03, -1.6260e-02,  ..., -8.7239e-03,
         -3.7493e-05, -1.4643e-03],
        ...,
        [-6.0055e-03,  1.2181e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2334e-03],
        [-6.0055e-03,  1.2181e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2334e-03],
        [-6.0055e-03,  1.2181e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2334e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-75330.6094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.1722, device='cuda:0')



h[100].sum tensor(-0.2076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(105.5462, device='cuda:0')



h[200].sum tensor(-568.0990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.6655, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(278398.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1136, 0.0497],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1660, 0.0134],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1138, 0.0495],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0395, 0.1005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0395, 0.1005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0395, 0.1005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2045520.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2493.2705, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(84218.0234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1747.9358, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(542.6627, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.4690],
        [-2.9578],
        [-3.5223],
        ...,
        [-7.3153],
        [-7.3065],
        [-7.3051]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-898339.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(631.9487, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(631.9487, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-76330.6172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.3192, device='cuda:0')



h[100].sum tensor(-0.1677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.9823, device='cuda:0')



h[200].sum tensor(-567.4375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.0648, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(269257.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0554, 0.0888],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0793, 0.0718],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1186, 0.0443],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0417, 0.0983],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0417, 0.0983],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0417, 0.0983]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1991435.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2476.6919, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(82484.2266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1677.2770, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(493.0829, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0290],
        [-1.8484],
        [-0.5567],
        ...,
        [-7.2299],
        [-7.2215],
        [-7.2203]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-803917.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2576],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(647.5200, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2576],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(647.5200, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2506e-03,  1.5724e-03, -1.5593e-02,  ..., -8.4122e-03,
         -2.0801e-05, -3.0170e-03],
        [-6.2494e-03,  1.5711e-03, -1.5589e-02,  ..., -8.4102e-03,
         -2.0703e-05, -3.0275e-03],
        [-6.0055e-03,  1.3000e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2292e-03],
        ...,
        [-6.0055e-03,  1.3000e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2292e-03],
        [-6.0055e-03,  1.3000e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2292e-03],
        [-6.0055e-03,  1.3000e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2292e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-76230.3750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.0417, device='cuda:0')



h[100].sum tensor(-0.1658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(90.1502, device='cuda:0')



h[200].sum tensor(-567.4962, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.8795, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(270604.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1961, 0.0063],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1466, 0.0251],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1364, 0.0325],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0578, 0.0861],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0843, 0.0673],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1678, 0.0261]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1999475.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2546.4287, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(79897.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1654.3676, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(479.3142, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7240],
        [-1.7197],
        [-2.5556],
        ...,
        [-5.8155],
        [-4.4296],
        [-2.6417]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-730603.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(953.6681, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(953.6681, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-73872.8906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.7164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(44.2454, device='cuda:0')



h[100].sum tensor(-0.2357, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(132.7733, device='cuda:0')



h[200].sum tensor(-569.0345, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(49.8977, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(298831.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0426, 0.0965],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0426, 0.0965],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0460, 0.0943],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0422, 0.0969],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0422, 0.0969],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0422, 0.0969]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2143514.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2594.6892, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(78204.2734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1688.3792, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(606.7925, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.8479],
        [-6.6917],
        [-6.2897],
        ...,
        [-7.2803],
        [-7.2719],
        [-7.2708]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-891516.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(526.9036, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(526.9036, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-77615.3438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.1526, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.4457, device='cuda:0')



h[100].sum tensor(-0.1271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(73.3575, device='cuda:0')



h[200].sum tensor(-566.8735, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.5686, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(262099.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0454, 0.0945],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0428, 0.0963],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0430, 0.0961],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0424, 0.0966],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0424, 0.0966],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0424, 0.0966]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1953362., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2506.3477, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(80317.4297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1619.9529, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(487.3780, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.2369],
        [-6.7444],
        [-6.9760],
        ...,
        [-7.2645],
        [-7.2562],
        [-7.2551]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-877371.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3005],
        [0.3391],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(579.9313, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3005],
        [0.3391],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(579.9313, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5981e-03,  1.8832e-03, -1.6934e-02,  ..., -9.0394e-03,
         -4.6235e-05,  9.9386e-05],
        [-6.2901e-03,  1.5675e-03, -1.5746e-02,  ..., -8.4836e-03,
         -2.2207e-05, -2.6786e-03],
        [-6.3267e-03,  1.6049e-03, -1.5887e-02,  ..., -8.5495e-03,
         -2.5057e-05, -2.3491e-03],
        ...,
        [-6.0055e-03,  1.2758e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2460e-03],
        [-6.0055e-03,  1.2758e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2460e-03],
        [-6.0055e-03,  1.2758e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2460e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-77363.3906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.9059, device='cuda:0')



h[100].sum tensor(-0.1355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(80.7402, device='cuda:0')



h[200].sum tensor(-567.1375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.3431, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 7.7754e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         5.9958e-03],
        [0.0000e+00, 6.8154e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         9.9386e-05],
        [0.0000e+00, 5.6636e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 5.1030e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.1030e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.1030e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(265198.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4164, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2756, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1628, 0.0297],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0429, 0.0963],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0429, 0.0963],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0429, 0.0963]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1967030.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2542.0706, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(79771.4375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1613.9495, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(466.7077, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.8977],
        [ 0.2689],
        [-1.1651],
        ...,
        [-7.2162],
        [-7.2081],
        [-7.2071]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-765234.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5474],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(754.3313, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5474],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(754.3313, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5239e-03,  1.8426e-03, -1.6648e-02,  ..., -8.9054e-03,
         -3.9320e-05, -5.8161e-04],
        [-6.0055e-03,  1.3252e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2554e-03],
        [-6.5239e-03,  1.8426e-03, -1.6648e-02,  ..., -8.9054e-03,
         -3.9320e-05, -5.8161e-04],
        ...,
        [-6.0055e-03,  1.3252e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2554e-03],
        [-6.0055e-03,  1.3252e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2554e-03],
        [-6.0055e-03,  1.3252e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2554e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-76189.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9989, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.9972, device='cuda:0')



h[100].sum tensor(-0.1719, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(105.0209, device='cuda:0')



h[200].sum tensor(-568.0432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.4681, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(277600.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1558, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2074, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1374, 0.0368],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0422, 0.0969],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0422, 0.0969],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0422, 0.0969]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2022849.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2612.0410, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(78703.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1647.8317, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(523.8439, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.0559],
        [ 0.4595],
        [-0.9859],
        ...,
        [-7.2986],
        [-7.2919],
        [-7.2920]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-849697.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3098],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(856.0952, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3098],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(856.0952, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.3252e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2554e-03],
        [-6.2989e-03,  1.6180e-03, -1.5780e-02,  ..., -8.4994e-03,
         -2.2256e-05, -2.6100e-03],
        [-6.2245e-03,  1.5438e-03, -1.5492e-02,  ..., -8.3652e-03,
         -1.6611e-05, -3.2809e-03],
        ...,
        [-6.0055e-03,  1.3252e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2554e-03],
        [-6.0055e-03,  1.3252e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2554e-03],
        [-6.0055e-03,  1.3252e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2554e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-75368.0781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.3642, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.7185, device='cuda:0')



h[100].sum tensor(-0.1943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(119.1888, device='cuda:0')



h[200].sum tensor(-568.5480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(44.7925, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(285284.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1142, 0.0474],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1876, 0.0229],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2871, 0.0006],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0422, 0.0969],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0422, 0.0969],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0422, 0.0969]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2053852.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2595.6660, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(78561.9688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1652.4143, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(561.5728, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8071],
        [-0.1533],
        [ 0.8672],
        ...,
        [-7.3046],
        [-7.2963],
        [-7.2951]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-880544.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3162],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(930.6744, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3162],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(930.6744, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.2994e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2626e-03],
        [-6.5049e-03,  1.7810e-03, -1.6574e-02,  ..., -8.8711e-03,
         -3.6821e-05, -7.6161e-04],
        [-6.2054e-03,  1.4922e-03, -1.5419e-02,  ..., -8.3307e-03,
         -1.4742e-05, -3.4605e-03],
        ...,
        [-6.0055e-03,  1.2994e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2626e-03],
        [-6.0055e-03,  1.2994e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2626e-03],
        [-6.0055e-03,  1.2994e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2626e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-74939.9375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.6448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(43.1786, device='cuda:0')



h[100].sum tensor(-0.2054, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(129.5720, device='cuda:0')



h[200].sum tensor(-568.9355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(48.6947, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0132],
        ...,
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(296879., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1613, 0.0331],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2568, 0.0151],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4795, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0425, 0.0970],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0425, 0.0970],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0425, 0.0970]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2156163., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2697.7964, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(76713.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1677.4746, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(1.1420, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(581.9927, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0681],
        [-0.3001],
        [ 0.7993],
        ...,
        [-7.3383],
        [-7.3298],
        [-7.3287]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-853610.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(773.3264, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(773.3264, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-76482.3516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.8784, device='cuda:0')



h[100].sum tensor(-0.1664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(107.6654, device='cuda:0')



h[200].sum tensor(-568.1456, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.4619, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(279246.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0445, 0.0956],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0496, 0.0923],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0649, 0.0821],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0434, 0.0964],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0434, 0.0964],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0434, 0.0964]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2032515.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2616.7080, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(77604.6172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1610.7861, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(528.7799, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.4704],
        [-5.8268],
        [-4.6992],
        ...,
        [-7.2993],
        [-7.2911],
        [-7.2900]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-790313.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6206],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(697.3671, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6206],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(697.3671, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.3947e-03,  2.4035e-03, -2.0008e-02,  ..., -1.0477e-02,
         -9.6782e-05,  7.2669e-03],
        [-6.8402e-03,  1.8932e-03, -1.7868e-02,  ..., -9.4762e-03,
         -5.8152e-05,  2.2668e-03],
        [-6.6349e-03,  1.7042e-03, -1.7076e-02,  ..., -9.1057e-03,
         -4.3848e-05,  4.1536e-04],
        ...,
        [-6.0055e-03,  1.1249e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2601e-03],
        [-6.0055e-03,  1.1249e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2601e-03],
        [-6.0055e-03,  1.1249e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2601e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-77364.2500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.3543, device='cuda:0')



h[100].sum tensor(-0.1449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(97.0901, device='cuda:0')



h[200].sum tensor(-567.7363, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.4876, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0301],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0140],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0108],
        ...,
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(274082.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8204, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6927, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5704, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0440, 0.0959],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0440, 0.0959],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0440, 0.0959]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2015854.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2598.3103, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(77352.1094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1572.5590, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(489.1519, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.0407],
        [ 1.0793],
        [ 1.0840],
        ...,
        [-7.2748],
        [-7.2663],
        [-7.2652]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-680386.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(664.9961, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(664.9961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-77969.0625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6715, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.8525, device='cuda:0')



h[100].sum tensor(-0.1349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.5833, device='cuda:0')



h[200].sum tensor(-567.5908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.7939, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(272652.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0611, 0.0838],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0453, 0.0945],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0427, 0.0962],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0421, 0.0967],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0421, 0.0967],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0421, 0.0967]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2001924.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2562.7231, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(77577.8281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1562.1780, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(541.8099, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.0633],
        [-6.2342],
        [-6.8351],
        ...,
        [-7.4427],
        [-7.4341],
        [-7.4329]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-920617.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(792.7361, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(792.7361, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-77150.9219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.7790, device='cuda:0')



h[100].sum tensor(-0.1565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(110.3677, device='cuda:0')



h[200].sum tensor(-568.2565, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.4775, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(283526.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0416, 0.0971],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0416, 0.0971],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0418, 0.0970],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0412, 0.0975],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0412, 0.0975],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0412, 0.0975]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2055421.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2609.0107, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(76396.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1572.3401, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(572.3395, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.0590],
        [-7.2060],
        [-7.2407],
        ...,
        [-7.4839],
        [-7.4753],
        [-7.4741]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-904141.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3342],
        [0.3333],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(731.8540, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3342],
        [0.3333],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(731.8540, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3221e-03,  1.4261e-03, -1.5869e-02,  ..., -8.5412e-03,
         -2.0243e-05, -2.3645e-03],
        [-6.3211e-03,  1.4253e-03, -1.5865e-02,  ..., -8.5395e-03,
         -2.0184e-05, -2.3729e-03],
        [-6.8113e-03,  1.8773e-03, -1.7757e-02,  ..., -9.4241e-03,
         -5.1531e-05,  2.0605e-03],
        ...,
        [-6.0055e-03,  1.1342e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2274e-03],
        [-6.0055e-03,  1.1342e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2274e-03],
        [-6.0055e-03,  1.1342e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2274e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-77867.3750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9219, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.9543, device='cuda:0')



h[100].sum tensor(-0.1400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(101.8915, device='cuda:0')



h[200].sum tensor(-567.9366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.2920, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        ...,
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(274975.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1853, 0.0190],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2733, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2980, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0413, 0.0984],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0413, 0.0984],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0413, 0.0984]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2006472.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2596.4844, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(77903.7656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1550.8738, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(505.3272, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.9484],
        [ 1.3633],
        [ 1.5061],
        ...,
        [-7.3836],
        [-7.3870],
        [-7.3873]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-747148., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2820],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(552.2292, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2820],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(552.2292, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2726e-03,  1.3130e-03, -1.5678e-02,  ..., -8.4519e-03,
         -1.6595e-05, -2.7933e-03],
        [-6.0055e-03,  1.0671e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2116e-03],
        [-6.2726e-03,  1.3130e-03, -1.5678e-02,  ..., -8.4519e-03,
         -1.6595e-05, -2.7933e-03],
        ...,
        [-6.0055e-03,  1.0671e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2116e-03],
        [-6.0055e-03,  1.0671e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2116e-03],
        [-6.0055e-03,  1.0671e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2116e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-79672.7656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2454, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.6207, device='cuda:0')



h[100].sum tensor(-0.1021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(76.8834, device='cuda:0')



h[200].sum tensor(-567.0018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.8937, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(260296.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4246, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3294, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1965, 0.0225],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0397, 0.1003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0397, 0.1003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0397, 0.1003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1932759.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2491.8862, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(81135.6328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1525.9358, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(467.5669, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.2511],
        [ 1.0999],
        [ 0.5195],
        ...,
        [-7.4363],
        [-7.4279],
        [-7.4268]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-794729., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(761.7009, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(761.7009, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-77892.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.3391, device='cuda:0')



h[100].sum tensor(-0.1416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(106.0469, device='cuda:0')



h[200].sum tensor(-568.0952, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.8536, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(279336.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0401, 0.0999],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0401, 0.0999],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0404, 0.0998],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0397, 0.1003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0397, 0.1003],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0397, 0.1003]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2038891.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2569.9050, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(79267., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1560.5669, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(519.8234, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.1574],
        [-7.2146],
        [-7.2504],
        ...,
        [-7.4153],
        [-7.4161],
        [-7.4155]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-761604.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2488],
        [0.2424],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(846.4626, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2488],
        [0.2424],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(846.4626, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5833e-03,  1.7299e-03, -1.6877e-02,  ..., -9.0127e-03,
         -3.4888e-05,  3.0095e-05],
        [-6.2411e-03,  1.4131e-03, -1.5556e-02,  ..., -8.3951e-03,
         -1.4226e-05, -3.0703e-03],
        [-6.2351e-03,  1.4075e-03, -1.5533e-02,  ..., -8.3843e-03,
         -1.3863e-05, -3.1248e-03],
        ...,
        [-6.0055e-03,  1.1950e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2050e-03],
        [-6.0055e-03,  1.1950e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2050e-03],
        [-6.0055e-03,  1.1950e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2050e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-77602.7344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.3334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.2716, device='cuda:0')



h[100].sum tensor(-0.1518, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(117.8477, device='cuda:0')



h[200].sum tensor(-568.5052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(44.2885, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 6.0092e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.0526e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         3.0095e-05],
        [0.0000e+00, 5.1717e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 4.7800e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 4.7800e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 4.7800e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(283627., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2363, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2005, 0.0109],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1256, 0.0410],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0364, 0.1020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0364, 0.1020],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0364, 0.1020]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2059390.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2590.6155, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(79230.0938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1575.3051, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(581.4999, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.9250],
        [-0.1822],
        [-1.9177],
        ...,
        [-7.6047],
        [-7.5961],
        [-7.5949]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-954582.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 390.0 event: 5850 loss: tensor(841.2592, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(616.9645, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(616.9645, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.1249e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2400e-03],
        [-6.0055e-03,  1.1249e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2400e-03],
        [-6.2920e-03,  1.3697e-03, -1.5753e-02,  ..., -8.4869e-03,
         -1.6806e-05, -2.6494e-03],
        ...,
        [-6.0055e-03,  1.1249e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2400e-03],
        [-6.0055e-03,  1.1249e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2400e-03],
        [-6.0055e-03,  1.1249e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2400e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-79824.8672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4838, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.6240, device='cuda:0')



h[100].sum tensor(-0.1074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(85.8961, device='cuda:0')



h[200].sum tensor(-567.3314, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.2808, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264330.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0608, 0.0863],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1135, 0.0508],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2063, 0.0269],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0364, 0.1026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0364, 0.1026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0364, 0.1026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1955460.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2468.3792, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(81164.6328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1521.5718, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.6660, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(562.3824, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.3570],
        [-2.2944],
        [-0.4636],
        ...,
        [-7.6270],
        [-7.6182],
        [-7.6169]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1050461.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(628.9801, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(628.9801, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-79681.1328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5366, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.1815, device='cuda:0')



h[100].sum tensor(-0.1066, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.5690, device='cuda:0')



h[200].sum tensor(-567.4044, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.9094, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(266368.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0605, 0.0857],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0435, 0.0972],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0400, 0.0995],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0393, 0.1000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0393, 0.1000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0393, 0.1000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1959416.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2545.4314, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(77786.0156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1471.4736, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(516.5602, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.3101],
        [-5.6724],
        [-6.4463],
        ...,
        [-7.4641],
        [-7.4554],
        [-7.4541]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-758116.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2466],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(626.4885, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2466],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(626.4885, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.4537e-03,  1.3719e-03, -1.6377e-02,  ..., -8.7787e-03,
         -2.4813e-05, -1.2291e-03],
        [-6.0055e-03,  1.0202e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2753e-03],
        [-6.2390e-03,  1.2035e-03, -1.5548e-02,  ..., -8.3914e-03,
         -1.2929e-05, -3.1670e-03],
        ...,
        [-6.0055e-03,  1.0202e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2753e-03],
        [-6.0055e-03,  1.0202e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2753e-03],
        [-6.0055e-03,  1.0202e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2753e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-79720.2500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.0659, device='cuda:0')



h[100].sum tensor(-0.1036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.2221, device='cuda:0')



h[200].sum tensor(-567.4091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.7791, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268613.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1470, 0.0274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1653, 0.0149],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1008, 0.0566],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0405, 0.0983],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0405, 0.0983],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0405, 0.0983]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1960205.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2552.6797, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(75622.1797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1418.4783, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(555.1266, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.8941],
        [-2.4729],
        [-3.6169],
        ...,
        [-7.4306],
        [-7.4220],
        [-7.4207]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-775365.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(707.3094, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(707.3094, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-79104.8281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8341, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.8156, device='cuda:0')



h[100].sum tensor(-0.1131, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(98.4743, device='cuda:0')



h[200].sum tensor(-567.8155, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.0078, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(274658.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0415, 0.0967],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0425, 0.0960],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0453, 0.0942],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0411, 0.0970],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0411, 0.0970],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0411, 0.0970]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1983627.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2618.2256, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(73291.6172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1394.6022, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(576.9853, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.8641],
        [-6.6291],
        [-6.2224],
        ...,
        [-7.4190],
        [-7.4114],
        [-7.4108]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-739808.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(613.9088, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(613.9088, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-80070.5156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4728, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.4823, device='cuda:0')



h[100].sum tensor(-0.0947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(85.4707, device='cuda:0')



h[200].sum tensor(-567.3160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.1209, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(266037.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0401, 0.0974],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0401, 0.0974],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0403, 0.0973],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0396, 0.0978],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0396, 0.0978],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0396, 0.0978]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1930189.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2546.6687, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(75191.7891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1372.0999, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(557.8839, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.9767],
        [-7.0179],
        [-7.0182],
        ...,
        [-7.4509],
        [-7.4425],
        [-7.4413]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-765345.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(703.2878, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(703.2878, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-79511., device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.6290, device='cuda:0')



h[100].sum tensor(-0.1051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(97.9144, device='cuda:0')



h[200].sum tensor(-567.7679, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.7974, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(271984.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1746, 0.0220],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1095, 0.0482],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0738, 0.0738],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0370, 0.0994],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0370, 0.0994],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0370, 0.0994]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1960448., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2535.7302, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(77209.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1399.2363, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(564.8624, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7685],
        [ 0.1616],
        [-0.4412],
        ...,
        [-7.4441],
        [-7.4372],
        [-7.4411]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-782141.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(879.1224, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(879.1224, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  9.8828e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2687e-03],
        [-6.0055e-03,  9.8828e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2687e-03],
        [-6.3829e-03,  1.2819e-03, -1.6103e-02,  ..., -8.6509e-03,
         -1.8591e-05, -1.8553e-03],
        ...,
        [-6.0055e-03,  9.8828e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2687e-03],
        [-6.0055e-03,  9.8828e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2687e-03],
        [-6.0055e-03,  9.8828e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2687e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-78303.8125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.4445, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.7868, device='cuda:0')



h[100].sum tensor(-0.1272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(122.3948, device='cuda:0')



h[200].sum tensor(-568.6588, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(45.9974, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(282918.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0544, 0.0877],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1129, 0.0526],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1959, 0.0278],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0341, 0.1018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0341, 0.1018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0341, 0.1018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2015686., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2573.9092, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(79027.6641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1459.9803, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(585.1875, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.3574],
        [-3.3750],
        [-1.2762],
        ...,
        [-7.6032],
        [-7.5951],
        [-7.5942]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-807023.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(603.9034, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(603.9034, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-80833.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.0181, device='cuda:0')



h[100].sum tensor(-0.0849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(84.0777, device='cuda:0')



h[200].sum tensor(-567.2611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.5974, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(260922.4219, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0498, 0.0914],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0392, 0.0986],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0334, 0.1025],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0308, 0.1043],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0308, 0.1043],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0308, 0.1043]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1918470.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2442.8833, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(84006.3594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1458.1139, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(533.3729, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.2717],
        [-5.6876],
        [-5.8763],
        ...,
        [-7.7383],
        [-7.7299],
        [-7.7288]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-980574.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6294],
        [0.6362],
        [0.5654],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(681.1509, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6294],
        [0.6362],
        [0.5654],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(681.1509, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6081e-03,  1.4667e-03, -1.6972e-02,  ..., -9.0573e-03,
         -2.7992e-05,  2.0873e-04],
        [-7.5211e-03,  2.1873e-03, -2.0495e-02,  ..., -1.0705e-02,
         -7.0407e-05,  8.4824e-03],
        [-7.4546e-03,  2.1348e-03, -2.0238e-02,  ..., -1.0585e-02,
         -6.7318e-05,  7.8798e-03],
        ...,
        [-6.0055e-03,  9.9121e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2515e-03],
        [-6.0055e-03,  9.9121e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2515e-03],
        [-6.0055e-03,  9.9121e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2515e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-80169.0234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.6020, device='cuda:0')



h[100].sum tensor(-0.0931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(94.8324, device='cuda:0')



h[200].sum tensor(-567.6652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.6391, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0254],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0332],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0301],
        ...,
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(265849.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7765, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8194, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7361, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0308, 0.1040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0308, 0.1040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0308, 0.1040]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1931638.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2471.8108, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(83081.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1450.4224, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(536.2286, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6980],
        [ 0.7136],
        [ 0.7924],
        ...,
        [-7.7448],
        [-7.7365],
        [-7.7355]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-957771.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(628.4335, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(628.4335, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-80475.5703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.1561, device='cuda:0')



h[100].sum tensor(-0.0830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.4929, device='cuda:0')



h[200].sum tensor(-567.3810, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.8808, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(265474.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0416, 0.0957],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0440, 0.0941],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0488, 0.0909],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0326, 0.1019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0326, 0.1019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0326, 0.1019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1933584.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2499.2603, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(80523.0469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1404.4502, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(507.6286, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.8169],
        [-5.8161],
        [-5.8047],
        ...,
        [-7.6852],
        [-7.6774],
        [-7.6766]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-844623.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 400.0 event: 6000 loss: tensor(455.2220, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(541.7950, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(541.7950, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-81112.3672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2189, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.1366, device='cuda:0')



h[100].sum tensor(-0.0700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(75.4308, device='cuda:0')



h[200].sum tensor(-566.9652, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.3478, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(259773.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1557, 0.0399],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0675, 0.0766],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0450, 0.0928],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0339, 0.1007],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0339, 0.1007],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0339, 0.1007]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1899209., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2494.7153, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(79305.0312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1364.1951, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(492.8099, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.7429],
        [-2.0378],
        [-3.3585],
        ...,
        [-7.6765],
        [-7.6689],
        [-7.6684]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-834507.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(664.5807, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(664.5807, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.0354e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2330e-03],
        [-6.0055e-03,  1.0354e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2330e-03],
        [-6.5143e-03,  1.4349e-03, -1.6610e-02,  ..., -8.8882e-03,
         -2.1631e-05, -6.1426e-04],
        ...,
        [-6.0055e-03,  1.0354e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2330e-03],
        [-6.0055e-03,  1.0354e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2330e-03],
        [-6.0055e-03,  1.0354e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2330e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-80307.4375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6644, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.8332, device='cuda:0')



h[100].sum tensor(-0.0829, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.5254, device='cuda:0')



h[200].sum tensor(-567.5809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.7721, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(267994.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0744, 0.0721],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1522, 0.0311],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1965, 0.0125],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0348, 0.0999],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0348, 0.0999],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0348, 0.0999]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1937287.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2562.9253, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(77506.3984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1368.3010, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(492.9650, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.7082],
        [-1.5294],
        [ 0.0815],
        ...,
        [-7.6874],
        [-7.6794],
        [-7.6785]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-774194.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(746.7725, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(746.7725, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-79943.3906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.6465, device='cuda:0')



h[100].sum tensor(-0.0903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(103.9685, device='cuda:0')



h[200].sum tensor(-568.0011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.0726, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(279026.4688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0363, 0.0999],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0386, 0.0983],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0463, 0.0929],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0353, 0.1006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0353, 0.1006],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0353, 0.1006]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2011325.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2573.5532, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(77682.6641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1395.3323, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(522.2339, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.9088],
        [-6.4327],
        [-5.5527],
        ...,
        [-7.7506],
        [-7.7423],
        [-7.7413]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-845971.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(755.4601, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(755.4601, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-80291.3203, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.0495, device='cuda:0')



h[100].sum tensor(-0.0877, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(105.1780, device='cuda:0')



h[200].sum tensor(-568.0090, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.5271, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(274139.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0507, 0.0916],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0706, 0.0781],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0981, 0.0590],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0357, 0.1018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0357, 0.1018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0357, 0.1018]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1991827.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2590.5837, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(78657.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1413.7021, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(475.4890, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.8914],
        [-2.5187],
        [-1.0588],
        ...,
        [-7.8324],
        [-7.8240],
        [-7.8230]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-850563.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(664.2594, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(664.2594, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  9.9496e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2549e-03],
        [-6.0055e-03,  9.9496e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2549e-03],
        [-6.2255e-03,  1.1472e-03, -1.5496e-02,  ..., -8.3669e-03,
         -8.5508e-06, -3.2610e-03],
        ...,
        [-6.0055e-03,  9.9496e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2549e-03],
        [-6.0055e-03,  9.9496e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2549e-03],
        [-6.0055e-03,  9.9496e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2549e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-81309.3281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.8183, device='cuda:0')



h[100].sum tensor(-0.0757, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.4807, device='cuda:0')



h[200].sum tensor(-567.5884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.7553, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264967.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0781, 0.0735],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1405, 0.0359],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1729, 0.0177],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0360, 0.1028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0360, 0.1028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0360, 0.1028]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1950574.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2575.8120, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(80239.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1421.8424, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(431.2676, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.1468],
        [-2.3158],
        [-1.0927],
        ...,
        [-7.8991],
        [-7.7984],
        [-7.4869]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-916466.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(609.7260, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(609.7260, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-81980.0156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4660, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.2882, device='cuda:0')



h[100].sum tensor(-0.0673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(84.8884, device='cuda:0')



h[200].sum tensor(-567.3068, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.9020, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(260895.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0530, 0.0915],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0843, 0.0693],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1663, 0.0296],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0370, 0.1027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0370, 0.1027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0370, 0.1027]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1934287.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2582.3167, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(79830.7891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1407.5679, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(418.1699, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.0598],
        [-3.1964],
        [-1.1152],
        ...,
        [-7.9581],
        [-7.9487],
        [-7.9474]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-972818., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(650.0696, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(650.0696, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-81678.9062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.1600, device='cuda:0')



h[100].sum tensor(-0.0687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(90.5051, device='cuda:0')



h[200].sum tensor(-567.4745, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.0129, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(262900.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0625, 0.0843],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0653, 0.0823],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0688, 0.0799],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0391, 0.1009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0391, 0.1009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0391, 0.1009]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1931477.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2650.6440, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(76708.0234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1348.0620, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(377.5823, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.8802],
        [-3.0247],
        [-2.1443],
        ...,
        [-7.8500],
        [-7.8411],
        [-7.8398]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-847568.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3999],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(615.9929, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3999],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(615.9929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.1705e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2628e-03],
        [-6.8914e-03,  1.7396e-03, -1.8064e-02,  ..., -9.5685e-03,
         -3.1474e-05,  2.7620e-03],
        [-6.5126e-03,  1.4963e-03, -1.6603e-02,  ..., -8.8850e-03,
         -1.8017e-05, -6.6905e-04],
        ...,
        [-6.0055e-03,  1.1705e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2628e-03],
        [-6.0055e-03,  1.1705e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2628e-03],
        [-6.0055e-03,  1.1705e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2628e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-81759.2500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.5790, device='cuda:0')



h[100].sum tensor(-0.0638, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(85.7609, device='cuda:0')



h[200].sum tensor(-567.3358, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.2299, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0009],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0207],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(263247.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3007, 0.0154],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4619, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6930, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0411, 0.0988],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0411, 0.0988],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0411, 0.0988]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1915449.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2681.5010, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(74158.8906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1274.6729, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(341.5697, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5510],
        [ 0.8052],
        [ 0.8292],
        ...,
        [-7.7096],
        [-7.7014],
        [-7.7004]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-744637.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(702.2404, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(702.2404, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6365e-03,  1.6792e-03, -1.7081e-02,  ..., -9.1085e-03,
         -2.1751e-05,  4.6424e-04],
        [-6.0055e-03,  1.2686e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2548e-03],
        [-6.0055e-03,  1.2686e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2548e-03],
        ...,
        [-6.0055e-03,  1.2686e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2548e-03],
        [-6.0055e-03,  1.2686e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2548e-03],
        [-6.0055e-03,  1.2686e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2548e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-80980.5938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.5804, device='cuda:0')



h[100].sum tensor(-0.0702, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(97.7686, device='cuda:0')



h[200].sum tensor(-567.7622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.7426, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0005],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0005],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(271392., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2194, 0.0092],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1886, 0.0224],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0968, 0.0584],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0418, 0.0976],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0418, 0.0976],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0418, 0.0976]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1952335.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2772.4365, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(71025.0625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1244.1672, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(338.8127, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7163],
        [-0.3813],
        [-1.8313],
        ...,
        [-7.6426],
        [-7.6348],
        [-7.6339]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-698228.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(659.4929, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(659.4929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-81352.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6466, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.5971, device='cuda:0')



h[100].sum tensor(-0.0641, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.8171, device='cuda:0')



h[200].sum tensor(-567.5564, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.5059, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(266703., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0421, 0.0973],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0523, 0.0904],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0768, 0.0736],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0411, 0.0981],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0411, 0.0981],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0411, 0.0981]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1906478., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2744.1802, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(71723.1641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1219.1031, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0.0560, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(320.1760, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.2592],
        [-5.2652],
        [-3.8265],
        ...,
        [-7.6571],
        [-7.6495],
        [-7.6487]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-759484., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 410.0 event: 6150 loss: tensor(446.2593, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(667.6072, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(667.6072, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.3901e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2320e-03],
        [-6.0055e-03,  1.3901e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2320e-03],
        [-6.2691e-03,  1.5672e-03, -1.5664e-02,  ..., -8.4456e-03,
         -8.5533e-06, -2.8385e-03],
        ...,
        [-6.0055e-03,  1.3901e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2320e-03],
        [-6.0055e-03,  1.3901e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2320e-03],
        [-6.0055e-03,  1.3901e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2320e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-81587.0781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.9736, device='cuda:0')



h[100].sum tensor(-0.0631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.9468, device='cuda:0')



h[200].sum tensor(-567.6105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.9305, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        ...,
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268954., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0679, 0.0807],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1530, 0.0414],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3315, 0.0153],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0392, 0.1002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0392, 0.1002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0392, 0.1002]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1923676.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2745.4121, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(72744.4766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1238.9471, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(302.5123, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.9273],
        [-0.5899],
        [ 0.2924],
        ...,
        [-7.7178],
        [-7.7102],
        [-7.7095]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-788217., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(525.0773, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(525.0773, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-83335.5703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.1482, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.3609, device='cuda:0')



h[100].sum tensor(-0.0479, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(73.1032, device='cuda:0')



h[200].sum tensor(-566.8677, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.4730, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258122.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0372, 0.1027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0372, 0.1027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0374, 0.1025],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0368, 0.1030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0368, 0.1030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0368, 0.1030]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1878401.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2641.3208, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(75838.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1241.6837, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(270.9594, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.0022],
        [-7.2774],
        [-7.4168],
        ...,
        [-7.8153],
        [-7.8077],
        [-7.8070]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-917864.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9775],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(609.2792, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.9775],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(609.2792, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.4446e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2159e-03],
        [-6.9313e-03,  2.0731e-03, -1.8218e-02,  ..., -9.6406e-03,
         -2.8268e-05,  3.2011e-03],
        [-6.0055e-03,  1.4446e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2159e-03],
        ...,
        [-6.0055e-03,  1.4446e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2159e-03],
        [-6.0055e-03,  1.4446e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2159e-03],
        [-6.0055e-03,  1.4446e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2159e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-83120.4141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.2675, device='cuda:0')



h[100].sum tensor(-0.0538, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(84.8261, device='cuda:0')



h[200].sum tensor(-567.2964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.8787, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0032],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0017],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0163],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(261659.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1700, 0.0316],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2238, 0.0176],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3975, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0350, 0.1053],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0350, 0.1053],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0350, 0.1053]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1884251.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2631.2935, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(77402.7188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1276.7712, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(281.2350, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.2673],
        [ 1.3769],
        [ 1.4112],
        ...,
        [-7.9111],
        [-7.9036],
        [-7.9029]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-957656., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(651.0555, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(651.0555, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-83249.7266, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.2057, device='cuda:0')



h[100].sum tensor(-0.0559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(90.6424, device='cuda:0')



h[200].sum tensor(-567.5215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.0645, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268226.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0347, 0.1056],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0347, 0.1056],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0349, 0.1055],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0344, 0.1059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0344, 0.1059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0344, 0.1059]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1924167.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2664.0005, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(76515.5625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1287.5282, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(288.9359, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.6596],
        [-7.6706],
        [-7.6632],
        ...,
        [-7.9465],
        [-7.9394],
        [-7.9388]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-933856.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2705],
        [0.0000],
        [0.5278],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(629.6385, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2705],
        [0.0000],
        [0.5278],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(629.6385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2536e-03,  1.6338e-03, -1.5604e-02,  ..., -8.4176e-03,
         -7.1252e-06, -2.9410e-03],
        [-7.0097e-03,  2.1575e-03, -1.8520e-02,  ..., -9.7820e-03,
         -2.8842e-05,  3.9427e-03],
        [-6.0055e-03,  1.4620e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1996e-03],
        ...,
        [-6.0055e-03,  1.4620e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1996e-03],
        [-6.0055e-03,  1.4620e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1996e-03],
        [-6.0055e-03,  1.4620e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1996e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-83789., device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5383, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.2121, device='cuda:0')



h[100].sum tensor(-0.0523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.6607, device='cuda:0')



h[200].sum tensor(-567.4067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.9439, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0039],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0064],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264807.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 2.9590e-01,
         1.8949e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 2.8416e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.3499e-01,
         0.0000e+00],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.5010e-02,
         1.0508e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.5010e-02,
         1.0508e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.5010e-02,
         1.0508e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1895074.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2677.2559, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(75437.6562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1258.1523, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(259.9020, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.0570],
        [ 1.6007],
        [ 1.7067],
        ...,
        [-7.9465],
        [-7.9399],
        [-7.9395]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-886868.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(541.5868, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(541.5868, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-84861.3750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.1269, device='cuda:0')



h[100].sum tensor(-0.0434, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(75.4018, device='cuda:0')



h[200].sum tensor(-566.9472, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.3369, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258267.9844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0360, 0.1040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0360, 0.1040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0362, 0.1038],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0357, 0.1043],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0357, 0.1043],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0357, 0.1043]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1854378.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2640.8794, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(75212.4375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1221.2781, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(238.8349, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.0090],
        [-7.2561],
        [-7.4281],
        ...,
        [-7.9521],
        [-7.9458],
        [-7.9456]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-889448., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(615.4276, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(615.4276, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6982e-03,  1.8831e-03, -1.7319e-02,  ..., -9.2198e-03,
         -1.8708e-05,  1.1146e-03],
        [-6.0055e-03,  1.4094e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1958e-03],
        [-6.0055e-03,  1.4094e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1958e-03],
        ...,
        [-6.0055e-03,  1.4094e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1958e-03],
        [-6.0055e-03,  1.4094e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1958e-03],
        [-6.0055e-03,  1.4094e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1958e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-84591.5000, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.5527, device='cuda:0')



h[100].sum tensor(-0.0476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(85.6822, device='cuda:0')



h[200].sum tensor(-567.3112, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.2003, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264637., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2345, 0.0123],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1845, 0.0289],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0839, 0.0723],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0361, 0.1038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0361, 0.1038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0361, 0.1038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1888569.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2685.2520, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(73998.2656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1214.5793, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(233.2928, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6317],
        [-0.9554],
        [-3.2030],
        ...,
        [-7.9536],
        [-7.9477],
        [-7.9477]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-814474.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(561.1880, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3616],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(561.1880, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6266e-03,  1.8342e-03, -1.7043e-02,  ..., -9.0907e-03,
         -1.6775e-05,  4.6246e-04],
        [-6.0055e-03,  1.4094e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1958e-03],
        [-6.3480e-03,  1.6436e-03, -1.5968e-02,  ..., -8.5879e-03,
         -9.2494e-06, -2.0760e-03],
        ...,
        [-6.0055e-03,  1.4094e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1958e-03],
        [-6.0055e-03,  1.4094e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1958e-03],
        [-6.0055e-03,  1.4094e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1958e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-85012.4062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2814, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0363, device='cuda:0')



h[100].sum tensor(-0.0436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(78.1307, device='cuda:0')



h[200].sum tensor(-567.0516, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.3624, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0005],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(261089.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1723, 0.0199],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1893, 0.0119],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1106, 0.0554],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0361, 0.1038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0361, 0.1038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0361, 0.1038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1878008.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2692.3423, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(73926.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1213.0360, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(215.5574, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1934],
        [-0.9889],
        [-3.0367],
        ...,
        [-7.9536],
        [-7.9477],
        [-7.9477]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-793131.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(696.6160, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(696.6160, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-84091.0938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7849, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.3195, device='cuda:0')



h[100].sum tensor(-0.0525, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.9855, device='cuda:0')



h[200].sum tensor(-567.7474, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.4483, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(274624.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0418, 0.1009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0506, 0.0955],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0556, 0.0924],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0383, 0.1031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0362, 0.1044],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0362, 0.1044]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1943245.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2914.0845, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(73220.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1409.0527, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(479.5176, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.5981],
        [-6.2538],
        [-5.9818],
        ...,
        [-6.6899],
        [-7.3431],
        [-7.7601]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-766668.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(615.3260, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(615.3260, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.3233e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2046e-03],
        [-6.0055e-03,  1.3233e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2046e-03],
        [-6.9244e-03,  1.9209e-03, -1.8191e-02,  ..., -9.6281e-03,
         -2.3333e-05,  3.1663e-03],
        ...,
        [-6.0055e-03,  1.3233e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2046e-03],
        [-6.0055e-03,  1.3233e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2046e-03],
        [-6.0055e-03,  1.3233e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2046e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-84967.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.5480, device='cuda:0')



h[100].sum tensor(-0.0452, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(85.6680, device='cuda:0')



h[200].sum tensor(-567.3536, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.1950, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0032],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0016],
        ...,
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(269589.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0766, 0.0780],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1645, 0.0334],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2185, 0.0151],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0360, 0.1055],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0360, 0.1055],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0360, 0.1055]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1901540., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3016.2200, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(75680.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1590.1495, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(716.1847, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.1994],
        [-0.4136],
        [ 0.6651],
        ...,
        [-8.0715],
        [-8.0705],
        [-8.0762]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-859160.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 420.0 event: 6300 loss: tensor(375.7865, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(692.3978, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(692.3978, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-84550.1562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.1238, device='cuda:0')



h[100].sum tensor(-0.0488, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.3982, device='cuda:0')



h[200].sum tensor(-567.7173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.2276, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(275145.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0491, 0.0976],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0394, 0.1038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0397, 0.1036],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0356, 0.1062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0356, 0.1062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0356, 0.1062]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1934443.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3220.7585, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(76623.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1797.6553, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(969.2291, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.4371],
        [-5.7404],
        [-6.3767],
        ...,
        [-8.1609],
        [-8.1535],
        [-8.1528]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-874389.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2864],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(616.4651, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2864],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(616.4651, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.4850e-03,  1.5552e-03, -1.6496e-02,  ..., -8.8352e-03,
         -1.1442e-05, -8.4711e-04],
        [-6.2137e-03,  1.3878e-03, -1.5450e-02,  ..., -8.3457e-03,
         -4.9694e-06, -3.3176e-03],
        [-6.6511e-03,  1.6577e-03, -1.7137e-02,  ..., -9.1350e-03,
         -1.5407e-05,  6.6621e-04],
        ...,
        [-6.0055e-03,  1.2593e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2145e-03],
        [-6.0055e-03,  1.2593e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2145e-03],
        [-6.0055e-03,  1.2593e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2145e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-84941.8984, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4913, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.6009, device='cuda:0')



h[100].sum tensor(-0.0422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(85.8266, device='cuda:0')



h[200].sum tensor(-567.3418, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.2546, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0062],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0140],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(276701., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2782, 0.0143],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4658, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6129, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0351, 0.1065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0351, 0.1065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0351, 0.1065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1934750.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3414.7656, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(78411.0469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(2099.2766, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(986.4490, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.3907],
        [ 1.0698],
        [ 1.1546],
        ...,
        [-8.1958],
        [-8.1878],
        [-8.1870]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-922901., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(490.5768, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(490.5768, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-86705.8047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.0242, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.7603, device='cuda:0')



h[100].sum tensor(-0.0325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(68.3000, device='cuda:0')



h[200].sum tensor(-566.6963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.6679, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(261578.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1162, 0.0541],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0635, 0.0878],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0442, 0.1001],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0360, 0.1055],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0360, 0.1055],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0360, 0.1055]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1891883.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3166.7954, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(78836.0234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1792.5917, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(899.4273, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.5602],
        [-5.9242],
        [-6.7959],
        ...,
        [-8.1862],
        [-8.1781],
        [-8.1773]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-966808.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2810],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(588.2004, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2810],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(588.2004, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2716e-03,  1.4432e-03, -1.5673e-02,  ..., -8.4502e-03,
         -5.9671e-06, -2.7882e-03],
        [-6.2192e-03,  1.4111e-03, -1.5471e-02,  ..., -8.3556e-03,
         -4.7917e-06, -3.2661e-03],
        [-6.4854e-03,  1.5742e-03, -1.6498e-02,  ..., -8.8359e-03,
         -1.0759e-05, -8.4010e-04],
        ...,
        [-6.0055e-03,  1.2801e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2142e-03],
        [-6.0055e-03,  1.2801e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2142e-03],
        [-6.0055e-03,  1.2801e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2142e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-86516.3438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.2895, device='cuda:0')



h[100].sum tensor(-0.0376, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(81.8915, device='cuda:0')



h[200].sum tensor(-567.1844, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.7758, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(266670.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1546, 0.0340],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2031, 0.0073],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1698, 0.0220],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0374, 0.1033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0374, 0.1033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0374, 0.1033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1917397.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3046.9119, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(76172.9219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1561.8380, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(590.1509, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3555],
        [-0.2592],
        [-0.5582],
        ...,
        [-8.0922],
        [-8.0846],
        [-8.0839]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-826356.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2668],
        [0.0000],
        [0.6699],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(654.3776, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2668],
        [0.0000],
        [0.6699],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(654.3776, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2190e-03,  1.4495e-03, -1.5471e-02,  ..., -8.3553e-03,
         -4.6399e-06, -3.2655e-03],
        [-7.4168e-03,  2.1888e-03, -2.0090e-02,  ..., -1.0517e-02,
         -3.0667e-05,  7.6574e-03],
        [-7.2829e-03,  2.1062e-03, -1.9573e-02,  ..., -1.0275e-02,
         -2.7758e-05,  6.4365e-03],
        ...,
        [-6.0055e-03,  1.3177e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2128e-03],
        [-6.0055e-03,  1.3177e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2128e-03],
        [-6.0055e-03,  1.3177e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2128e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-86329.0234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6556, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.3598, device='cuda:0')



h[100].sum tensor(-0.0410, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.1049, device='cuda:0')



h[200].sum tensor(-567.5687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.2383, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0115],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0103],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0173],
        ...,
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(274026.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4587, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5080, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5321, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0386, 0.1011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0386, 0.1011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0386, 0.1011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1963017.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3003.5220, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(73705.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1418.6093, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(410.3781, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.3396],
        [ 1.3597],
        [ 1.3098],
        ...,
        [-7.9950],
        [-7.9956],
        [-8.0037]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-776078.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(608.2659, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(608.2659, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.3169e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2071e-03],
        [-6.0055e-03,  1.3169e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2071e-03],
        [-6.0055e-03,  1.3169e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2071e-03],
        ...,
        [-6.0055e-03,  1.3169e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2071e-03],
        [-6.0055e-03,  1.3169e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2071e-03],
        [-6.4085e-03,  1.5688e-03, -1.6201e-02,  ..., -8.6972e-03,
         -8.4871e-06, -1.5292e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-87064.6250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4535, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.2205, device='cuda:0')



h[100].sum tensor(-0.0365, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(84.6851, device='cuda:0')



h[200].sum tensor(-567.2894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.8256, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(269435.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.8535e-02,
         1.0006e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.8535e-02,
         1.0006e-01],
        [0.0000e+00, 4.5750e-05, 0.0000e+00,  ..., 0.0000e+00, 3.8735e-02,
         9.9883e-02],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.9957e-02,
         8.6286e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.1408e-01,
         5.2071e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.8273e-01,
         2.7568e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1939749.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3031.6074, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(73940.7812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1376.3344, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(40.7892, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.0219],
        [-7.2872],
        [-7.4321],
        ...,
        [-6.0981],
        [-4.2809],
        [-2.1359]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-757954.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(550.3356, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.3042],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(550.3356, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.3134e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2038e-03],
        [-6.2936e-03,  1.4943e-03, -1.5758e-02,  ..., -8.4898e-03,
         -5.8792e-06, -2.5731e-03],
        [-6.1986e-03,  1.4347e-03, -1.5392e-02,  ..., -8.3185e-03,
         -3.9412e-06, -3.4403e-03],
        ...,
        [-6.0055e-03,  1.3134e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2038e-03],
        [-6.0055e-03,  1.3134e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2038e-03],
        [-6.0055e-03,  1.3134e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2038e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-87819.1641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.5328, device='cuda:0')



h[100].sum tensor(-0.0317, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(76.6198, device='cuda:0')



h[200].sum tensor(-566.9743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.7946, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(265586.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1055, 0.0561],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1701, 0.0278],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2417, 0.0034],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0364, 0.1011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0364, 0.1011],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0364, 0.1011]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1899506.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3119.0601, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(76367.9062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1458.0676, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-158.0331, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7698],
        [-0.6759],
        [ 0.7603],
        ...,
        [-7.9330],
        [-7.9438],
        [-7.9655]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-835609.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2720],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(699.9802, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2720],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(699.9802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.3067e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2007e-03],
        [-6.2631e-03,  1.4701e-03, -1.5640e-02,  ..., -8.4348e-03,
         -5.0932e-06, -2.8474e-03],
        [-6.2196e-03,  1.4425e-03, -1.5473e-02,  ..., -8.3563e-03,
         -4.2332e-06, -3.2447e-03],
        ...,
        [-6.0055e-03,  1.3067e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2007e-03],
        [-6.0055e-03,  1.3067e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2007e-03],
        [-6.0055e-03,  1.3067e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2007e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-87074.8281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7781, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.4756, device='cuda:0')



h[100].sum tensor(-0.0391, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(97.4539, device='cuda:0')



h[200].sum tensor(-567.7380, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.6243, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(277149.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1174, 0.0479],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1856, 0.0219],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2478, 0.0013],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0351, 0.1016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0351, 0.1016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0351, 0.1016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1963326.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3086.3669, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(76936.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1429.4036, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-61.1484, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0434],
        [-0.1349],
        [ 0.8970],
        ...,
        [-8.0564],
        [-8.0491],
        [-8.0486]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-865616., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(665.8905, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(665.8905, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-87919.2031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6478, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.8940, device='cuda:0')



h[100].sum tensor(-0.0359, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.7078, device='cuda:0')



h[200].sum tensor(-567.5580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.8407, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(270840.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 7.6555e-02,
         7.3989e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.5909e-02,
         9.4231e-02],
        [0.0000e+00, 5.3693e-05, 0.0000e+00,  ..., 0.0000e+00, 3.5715e-02,
         1.0095e-01],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.5148e-02,
         1.0148e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.5148e-02,
         1.0148e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.5148e-02,
         1.0148e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1928781.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2892.9790, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(78292.3203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1311.2090, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(57.0212, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5367],
        [-5.4575],
        [-6.6338],
        ...,
        [-8.0670],
        [-8.0595],
        [-8.0589]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-909085.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4509],
        [0.4463],
        [0.4287],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(897.0768, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4509],
        [0.4463],
        [0.4287],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(897.0768, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.4282e-03,  1.4647e-03, -1.6277e-02,  ..., -8.7327e-03,
         -7.8448e-06, -1.3445e-03],
        [-7.1841e-03,  1.9162e-03, -1.9192e-02,  ..., -1.0097e-02,
         -2.1874e-05,  5.5634e-03],
        [-6.7737e-03,  1.6711e-03, -1.7609e-02,  ..., -9.3561e-03,
         -1.4257e-05,  1.8128e-03],
        ...,
        [-6.0055e-03,  1.2123e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2071e-03],
        [-6.0055e-03,  1.2123e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2071e-03],
        [-6.0055e-03,  1.2123e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2071e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-86590.8594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.4911, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(41.6198, device='cuda:0')



h[100].sum tensor(-0.0467, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(124.8944, device='cuda:0')



h[200].sum tensor(-568.7233, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(46.9368, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0093],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0065],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0069],
        ...,
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(290525.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3606, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4003, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4193, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0360, 0.1010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0360, 0.1010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0360, 0.1010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2030850.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2887.6333, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(77646.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1293.9988, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(165.8322, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.4709],
        [ 1.5071],
        [ 1.5379],
        ...,
        [-8.0746],
        [-8.0665],
        [-8.0657]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-850852.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 430.0 event: 6450 loss: tensor(452.2371, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(526.1150, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(526.1150, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0012, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-90158.6406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.1581, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.4091, device='cuda:0')



h[100].sum tensor(-0.0269, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(73.2477, device='cuda:0')



h[200].sum tensor(-566.8813, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.5273, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(260664.7969, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0396, 0.0995],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0445, 0.0965],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0583, 0.0877],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0373, 0.1010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0373, 0.1010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0373, 0.1010]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1888071.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2792.0505, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(79902.7500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1270.0504, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(93.6465, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.0515],
        [-5.3080],
        [-3.9487],
        ...,
        [-8.1324],
        [-8.1243],
        [-8.1235]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-947086.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(673.5509, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(673.5509, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-89372.3359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6954, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.2494, device='cuda:0')



h[100].sum tensor(-0.0331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.7743, device='cuda:0')



h[200].sum tensor(-567.6238, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.2415, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(270271.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0398, 0.0997],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0452, 0.0960],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0685, 0.0799],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0394, 0.1000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0394, 0.1000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0394, 0.1000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1938335., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2894.5781, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(78160.4375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1296.4867, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(83.7983, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.5583],
        [-5.2954],
        [-3.3060],
        ...,
        [-8.1189],
        [-8.1103],
        [-8.1093]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-821706., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(571.4802, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(571.4802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6021e-03,  1.3419e-03, -1.6947e-02,  ..., -9.0464e-03,
         -1.0063e-05,  2.1550e-04],
        [-6.0055e-03,  1.0392e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2319e-03],
        [-6.0055e-03,  1.0392e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2319e-03],
        ...,
        [-6.0055e-03,  1.0392e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2319e-03],
        [-6.0055e-03,  1.0392e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2319e-03],
        [-6.0055e-03,  1.0392e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2319e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-90596.7969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3210, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.5138, device='cuda:0')



h[100].sum tensor(-0.0272, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(79.5636, device='cuda:0')



h[200].sum tensor(-567.1064, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.9009, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(263244.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1474, 0.0328],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1400, 0.0349],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1318, 0.0375],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0402, 0.0999],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0402, 0.0999],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0402, 0.0999]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1908139.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2859.5015, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(78790.9375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1292.3558, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(64.7272, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.3842],
        [-0.4946],
        [-0.4585],
        ...,
        [-8.1549],
        [-8.1465],
        [-8.1457]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-842134.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(766.0366, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(766.0366, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-89333.8047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0332, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.5402, device='cuda:0')



h[100].sum tensor(-0.0352, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(106.6505, device='cuda:0')



h[200].sum tensor(-568.0906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.0805, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(276531.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0789, 0.0743],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0451, 0.0969],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0554, 0.0902],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0407, 0.0997],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0407, 0.0997],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0407, 0.0997]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1966377., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2910.4414, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(77849.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1316.1498, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(100.7160, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.3928],
        [-3.8187],
        [-4.1238],
        ...,
        [-8.1929],
        [-8.1846],
        [-8.1838]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-838984.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3223],
        [0.3923],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(654.9940, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3223],
        [0.3923],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(654.9940, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.7565e-03,  1.3646e-03, -1.7543e-02,  ..., -9.3252e-03,
         -1.1882e-05,  1.6356e-03],
        [-6.3771e-03,  1.1765e-03, -1.6080e-02,  ..., -8.6405e-03,
         -5.8787e-06, -1.8322e-03],
        [-6.7565e-03,  1.3646e-03, -1.7543e-02,  ..., -9.3252e-03,
         -1.1882e-05,  1.6356e-03],
        ...,
        [-6.0055e-03,  9.9227e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2281e-03],
        [-6.0055e-03,  9.9227e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2281e-03],
        [-6.0055e-03,  9.9227e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2281e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-90467.0234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.3884, device='cuda:0')



h[100].sum tensor(-0.0290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.1907, device='cuda:0')



h[200].sum tensor(-567.5099, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.2705, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0046],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0073],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0032],
        ...,
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(270450.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5117, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5175, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4506, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0411, 0.0995],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0411, 0.0995],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0411, 0.0995]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1947029.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2894.4690, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(77231.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1275.1886, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(41.8227, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.2948],
        [ 1.3702],
        [ 1.4975],
        ...,
        [-8.1221],
        [-8.1133],
        [-8.1123]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-743946.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(750.8248, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(750.8248, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-89968.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.8345, device='cuda:0')



h[100].sum tensor(-0.0323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(104.5327, device='cuda:0')



h[200].sum tensor(-568.0243, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.2846, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(278009.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0473, 0.0960],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0529, 0.0926],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0583, 0.0892],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0400, 0.1005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0400, 0.1005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0400, 0.1005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1979438.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2927.9512, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(76865.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1293.4014, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(78.1037, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.4028],
        [-6.1851],
        [-5.9896],
        ...,
        [-8.2219],
        [-8.2136],
        [-8.2127]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-823324.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(784.8184, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(784.8184, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-90030.9062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.4116, device='cuda:0')



h[100].sum tensor(-0.0325, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(109.2654, device='cuda:0')



h[200].sum tensor(-568.1783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.0632, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(279365.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0390, 0.1015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0400, 0.1009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0476, 0.0959],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0403, 0.1008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0386, 0.1019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0386, 0.1019]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1990153.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2934.7607, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(77138.1250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1303.3684, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(81.4161, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.4855],
        [-7.0335],
        [-6.1375],
        ...,
        [-7.5475],
        [-7.8862],
        [-8.1240]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-849143.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(625.8523, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(625.8523, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-91578.3750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.0364, device='cuda:0')



h[100].sum tensor(-0.0251, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.1335, device='cuda:0')



h[200].sum tensor(-567.3686, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.7458, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268421.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1029, 0.0606],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0797, 0.0756],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1140, 0.0527],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0373, 0.1033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0373, 0.1033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0373, 0.1033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1946924.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2874.0220, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(78388.0156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1282.7336, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(30.6364, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4509],
        [-1.7090],
        [-0.9724],
        ...,
        [-8.3095],
        [-8.3010],
        [-8.3000]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-838146.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(648.7888, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(648.7888, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-91666.7812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6132, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.1005, device='cuda:0')



h[100].sum tensor(-0.0253, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(90.3268, device='cuda:0')



h[200].sum tensor(-567.5100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.9459, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(267681.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0400, 0.1018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0372, 0.1036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0374, 0.1034],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0368, 0.1039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0368, 0.1039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0368, 0.1039]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1922878.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2858.9524, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(78404.1250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1294.9973, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(80.6099, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.4529],
        [-7.3128],
        [-7.7430],
        ...,
        [-8.4381],
        [-8.4290],
        [-8.4279]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1015066.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2832],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(631.9517, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2832],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(631.9517, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.0844e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2519e-03],
        [-6.2737e-03,  1.1956e-03, -1.5681e-02,  ..., -8.4540e-03,
         -3.6100e-06, -2.7997e-03],
        [-6.6361e-03,  1.3458e-03, -1.7078e-02,  ..., -9.1079e-03,
         -8.4876e-06,  5.1351e-04],
        ...,
        [-6.0055e-03,  1.0844e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2519e-03],
        [-6.0055e-03,  1.0844e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2519e-03],
        [-6.0055e-03,  1.0844e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2519e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-91879.4141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5387, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.3194, device='cuda:0')



h[100].sum tensor(-0.0237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.9827, device='cuda:0')



h[200].sum tensor(-567.4072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.0649, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0005],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0015],
        ...,
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(267365.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1348, 0.0414],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2511, 0.0165],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3625, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0601, 0.0873],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0938, 0.0638],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1490, 0.0370]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1919741., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2861.2837, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(77043.1641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1256.1931, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(48.7712, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.6899],
        [ 0.6008],
        [ 1.2340],
        ...,
        [-5.5612],
        [-3.2978],
        [-1.1065]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-912852.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 440.0 event: 6600 loss: tensor(457.9555, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2571],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(667.1073, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2571],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(667.1073, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2490e-03,  1.1380e-03, -1.5586e-02,  ..., -8.4093e-03,
         -3.1720e-06, -3.0192e-03],
        [-6.0055e-03,  1.0385e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2468e-03],
        [-6.2490e-03,  1.1380e-03, -1.5586e-02,  ..., -8.4093e-03,
         -3.1720e-06, -3.0192e-03],
        ...,
        [-6.0055e-03,  1.0385e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2468e-03],
        [-6.0055e-03,  1.0385e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2468e-03],
        [-6.0055e-03,  1.0385e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2468e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-91496.6094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.9504, device='cuda:0')



h[100].sum tensor(-0.0241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.8772, device='cuda:0')



h[200].sum tensor(-567.5690, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.9043, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        ...,
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(274576.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1732, 0.0256],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2533, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2219, 0.0096],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0382, 0.1015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0382, 0.1015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0382, 0.1015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1953780., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2878.6152, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(74475.0547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1203.7239, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(18.5105, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0632],
        [ 1.0608],
        [ 0.8404],
        ...,
        [-8.1914],
        [-8.1831],
        [-8.1822]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-727394.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(740.3474, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(740.3474, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-90857.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9441, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.3484, device='cuda:0')



h[100].sum tensor(-0.0260, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(103.0740, device='cuda:0')



h[200].sum tensor(-567.9675, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.7364, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(284242.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0402, 0.0989],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0402, 0.0989],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0404, 0.0987],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0398, 0.0993],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0398, 0.0993],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0398, 0.0993]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2010192.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2996.8408, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(70081.0625, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1183.1482, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(42.5115, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.7043],
        [-7.6856],
        [-7.6362],
        ...,
        [-8.1682],
        [-8.1600],
        [-8.1591]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-696520.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(729.8859, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(729.8859, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.1583e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2521e-03],
        [-6.0055e-03,  1.1583e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2521e-03],
        [-6.2638e-03,  1.2593e-03, -1.5643e-02,  ..., -8.4360e-03,
         -3.1518e-06, -2.8882e-03],
        ...,
        [-6.0055e-03,  1.1583e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2521e-03],
        [-6.0055e-03,  1.1583e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2521e-03],
        [-6.0055e-03,  1.1583e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2521e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-91075.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.8630, device='cuda:0')



h[100].sum tensor(-0.0249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(101.6175, device='cuda:0')



h[200].sum tensor(-567.9265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.1890, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(278052., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1522, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1173, 0.0485],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1131, 0.0509],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0403, 0.0987],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0403, 0.0987],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0403, 0.0987]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1951869.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2978.1809, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(70111.8438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1169.4478, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(48.7275, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.0809],
        [-0.8663],
        [-1.5807],
        ...,
        [-8.2251],
        [-8.2174],
        [-8.2167]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-762257., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(854.0763, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(854.0763, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-90281.1562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.3818, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.6248, device='cuda:0')



h[100].sum tensor(-0.0282, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(118.9077, device='cuda:0')



h[200].sum tensor(-568.5724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(44.6869, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(288544.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0390, 0.1004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0390, 0.1004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0392, 0.1002],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0386, 0.1008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0386, 0.1008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0386, 0.1008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2023747.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3035.2861, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(70493.9375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1218.3864, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(58.1338, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.8074],
        [-7.7560],
        [-7.6588],
        ...,
        [-8.2820],
        [-8.2741],
        [-8.2734]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-736524.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2479],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(633.3660, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2479],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(633.3660, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2403e-03,  9.7754e-04, -1.5552e-02,  ..., -8.3937e-03,
         -2.6831e-06, -3.1237e-03],
        [-6.0055e-03,  9.1818e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2714e-03],
        [-6.2403e-03,  9.7754e-04, -1.5552e-02,  ..., -8.3937e-03,
         -2.6831e-06, -3.1237e-03],
        ...,
        [-6.0055e-03,  9.1818e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2714e-03],
        [-6.0055e-03,  9.1818e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2714e-03],
        [-6.0055e-03,  9.1818e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2714e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-92354.6094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.3850, device='cuda:0')



h[100].sum tensor(-0.0201, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(88.1796, device='cuda:0')



h[200].sum tensor(-567.4299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.1389, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(271312.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3010, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3046, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3451, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0374, 0.1045],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0374, 0.1045],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0374, 0.1045]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1942435.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2734.8511, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(74482.7500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1414.9630, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-130.1803, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.8991],
        [ 1.8397],
        [ 1.7578],
        ...,
        [-8.3531],
        [-8.3449],
        [-8.3441]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-743762., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(678.5162, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(678.5162, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-92672.7656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7231, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.4797, device='cuda:0')



h[100].sum tensor(-0.0208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(94.4656, device='cuda:0')



h[200].sum tensor(-567.6620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.5013, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(271921.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0927, 0.0676],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0530, 0.0944],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0401, 0.1029],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0364, 0.1055],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0364, 0.1055],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0364, 0.1055]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1954694.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2866.2456, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(75312.9219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1301.2988, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(117.1519, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.1232],
        [-6.5761],
        [-7.2101],
        ...,
        [-8.5737],
        [-8.5645],
        [-8.5634]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1086111.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(588.3982, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(588.3982, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0011, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-93739.2109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3668, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.2987, device='cuda:0')



h[100].sum tensor(-0.0173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(81.9190, device='cuda:0')



h[200].sum tensor(-567.1697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.7861, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(260403.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0397, 0.1022],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0532, 0.0931],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0911, 0.0668],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0371, 0.1040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0371, 0.1040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0371, 0.1040]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1881921.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2884.8191, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(73678.0703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1238.8051, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(59.3107, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.5588],
        [-5.0198],
        [-2.7506],
        ...,
        [-8.5649],
        [-8.5559],
        [-8.5548]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1041047.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(741.0505, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(741.0505, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-92402.6641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9401, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.3810, device='cuda:0')



h[100].sum tensor(-0.0211, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(103.1719, device='cuda:0')



h[200].sum tensor(-567.9618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.7732, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(279267.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0383, 0.1012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0383, 0.1012],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0385, 0.1010],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0380, 0.1015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0380, 0.1015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0380, 0.1015]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1974519.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3005.0681, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(68459.7891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1151.0891, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-2.9117, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.6542],
        [-7.7543],
        [-7.8187],
        ...,
        [-8.3071],
        [-8.2993],
        [-8.2987]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-734475.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(909.7769, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(909.7769, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-90893.2422, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.6138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(42.2091, device='cuda:0')



h[100].sum tensor(-0.0254, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(126.6626, device='cuda:0')



h[200].sum tensor(-568.8929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(47.6013, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(294296.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0386, 0.0995],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0386, 0.0995],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0388, 0.0993],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0382, 0.0999],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0382, 0.0999],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0382, 0.0999]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(2019225.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3046.5024, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(65846.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1085.5728, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-37.2546, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.4617],
        [-7.4000],
        [-7.2901],
        ...,
        [-8.0813],
        [-8.0688],
        [-8.0653]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-541798.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(685.6897, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(685.6897, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-93185.0781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7161, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.8125, device='cuda:0')



h[100].sum tensor(-0.0181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(95.4643, device='cuda:0')



h[200].sum tensor(-567.6523, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.8766, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(275736.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0704, 0.0759],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0531, 0.0876],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0508, 0.0890],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0350, 0.0999],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0350, 0.0999],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0350, 0.0999]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1921553.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2996.0015, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(65832.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(989.6436, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-40.2147, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.0529],
        [-3.2671],
        [-4.0556],
        ...,
        [-8.1158],
        [-8.1118],
        [-8.1149]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-641379.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 450.0 event: 6750 loss: tensor(465.8778, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4387],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(594.1953, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4387],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(594.1953, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  9.7766e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2064e-03],
        [-6.4210e-03,  1.1015e-03, -1.6249e-02,  ..., -8.7197e-03,
         -3.8907e-06, -1.3769e-03],
        [-6.5934e-03,  1.1528e-03, -1.6913e-02,  ..., -9.0308e-03,
         -5.5049e-06,  2.1195e-04],
        ...,
        [-6.0055e-03,  9.7766e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2064e-03],
        [-6.0055e-03,  9.7766e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2064e-03],
        [-6.0055e-03,  9.7766e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2064e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-94381.4062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5677, device='cuda:0')



h[100].sum tensor(-0.0152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(82.7261, device='cuda:0')



h[200].sum tensor(-567.2021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0894, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0064],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(263328.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2913, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3548, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0312, 0.1021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0312, 0.1021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0312, 0.1021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1848380.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2942.8384, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(68111.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(961.5795, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-26.9223, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.6610],
        [ 1.6268],
        [ 1.5939],
        ...,
        [-8.3016],
        [-8.2941],
        [-8.2935]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-743999.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(614.9224, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(614.9224, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-94751.0469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4706, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.5293, device='cuda:0')



h[100].sum tensor(-0.0152, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(85.6118, device='cuda:0')



h[200].sum tensor(-567.3130, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.1739, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(263956.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0266, 0.1059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0270, 0.1056],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0285, 0.1046],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0263, 0.1062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0263, 0.1062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0263, 0.1062]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1850627., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2850.6584, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(71565.5859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(964.6281, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(0.3803, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.7320],
        [-7.5004],
        [-7.1022],
        ...,
        [-8.4491],
        [-8.4418],
        [-8.4435]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-859412., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.8457],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(805.0355, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.8457],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(805.0355, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.7458e-03,  1.4082e-03, -2.1354e-02,  ..., -1.1110e-02,
         -1.5239e-05,  1.0865e-02],
        [-6.6236e-03,  1.0959e-03, -1.7029e-02,  ..., -9.0853e-03,
         -5.4124e-06,  5.0813e-04],
        [-7.6866e-03,  1.3917e-03, -2.1126e-02,  ..., -1.1004e-02,
         -1.4721e-05,  1.0319e-02],
        ...,
        [-6.0055e-03,  9.2394e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1968e-03],
        [-6.0055e-03,  9.2394e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1968e-03],
        [-6.0055e-03,  9.2394e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1968e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-93588.0938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.3496, device='cuda:0')



h[100].sum tensor(-0.0195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(112.0801, device='cuda:0')



h[200].sum tensor(-568.3336, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.1210, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0111],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0341],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0111],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(281599.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5836, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6775, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5929, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0224, 0.1102],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0224, 0.1102],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0224, 0.1102]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1951264.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2833.9514, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(73725.0312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(990.6482, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(21.1396, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.0589],
        [ 1.0258],
        [ 1.0390],
        ...,
        [-8.5831],
        [-8.5748],
        [-8.5740]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-881477.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(608.5293, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(608.5293, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0009, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0009, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0009, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0009, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0009, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0009, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-95784.5938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4447, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.2327, device='cuda:0')



h[100].sum tensor(-0.0140, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(84.7217, device='cuda:0')



h[200].sum tensor(-567.2773, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.8394, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(259519.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1092, 0.0532],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0455, 0.0940],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0306, 0.1048],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0187, 0.1133],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0187, 0.1133],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0187, 0.1133]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1832759.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2702.7695, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(77887.8438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(977.3722, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-33.4535, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4314],
        [-3.4380],
        [-4.5181],
        ...,
        [-8.7084],
        [-8.7023],
        [-8.7031]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1014083.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2979],
        [0.3430],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(594.2109, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2979],
        [0.3430],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(594.2109, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.9780e-03,  1.1320e-03, -1.8395e-02,  ..., -9.7248e-03,
         -7.9605e-06,  3.7525e-03],
        [-6.2876e-03,  9.7908e-04, -1.5734e-02,  ..., -8.4790e-03,
         -2.3092e-06, -2.6126e-03],
        [-6.3304e-03,  9.8856e-04, -1.5899e-02,  ..., -8.5562e-03,
         -2.6593e-06, -2.2182e-03],
        ...,
        [-6.0055e-03,  9.1659e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2135e-03],
        [-6.0055e-03,  9.1659e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2135e-03],
        [-6.0055e-03,  9.1659e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2135e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-96107.6641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4214, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5684, device='cuda:0')



h[100].sum tensor(-0.0134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(82.7283, device='cuda:0')



h[200].sum tensor(-567.2451, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0903, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0111],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0059],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(259693.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4348, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3297, 0.0016],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1791, 0.0299],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0168, 0.1128],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0168, 0.1128],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0168, 0.1128]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1830329.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2707.4839, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(76207.7109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(953.4819, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(14.2836, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.3583],
        [ 1.1564],
        [ 0.4123],
        ...,
        [-8.7709],
        [-8.7632],
        [-8.7628]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1026430.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2651],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(755.8292, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2651],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(755.8292, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2656e-03,  9.9548e-04, -1.5650e-02,  ..., -8.4394e-03,
         -2.0585e-06, -2.8250e-03],
        [-6.2566e-03,  9.9367e-04, -1.5615e-02,  ..., -8.4231e-03,
         -1.9872e-06, -2.9082e-03],
        [-6.0055e-03,  9.4303e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2223e-03],
        ...,
        [-6.0055e-03,  9.4303e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2223e-03],
        [-6.0055e-03,  9.4303e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2223e-03],
        [-6.0055e-03,  9.4303e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2223e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-94716.7656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.0667, device='cuda:0')



h[100].sum tensor(-0.0164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(105.2294, device='cuda:0')



h[200].sum tensor(-568.0622, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.5464, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(271647.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1766, 0.0107],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1141, 0.0409],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0732, 0.0699],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0195, 0.1079],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0195, 0.1079],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0195, 0.1079]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1853652.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2750.3560, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(70342.2031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(848.1443, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(73.1904, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.4225],
        [-2.1210],
        [-4.0109],
        ...,
        [-8.6330],
        [-8.6246],
        [-8.6238]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-852345.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4675],
        [0.2751],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(675.2899, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4675],
        [0.2751],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(675.2899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.4483e-03,  1.0296e-03, -1.6354e-02,  ..., -8.7690e-03,
         -3.3873e-06, -1.1490e-03],
        [-6.2661e-03,  9.9672e-04, -1.5651e-02,  ..., -8.4402e-03,
         -1.9935e-06, -2.8278e-03],
        [-6.4483e-03,  1.0296e-03, -1.6354e-02,  ..., -8.7690e-03,
         -3.3873e-06, -1.1490e-03],
        ...,
        [-6.0055e-03,  9.4967e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2287e-03],
        [-6.0055e-03,  9.4967e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2287e-03],
        [-6.0055e-03,  9.4967e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2287e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-95272.2656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.3300, device='cuda:0')



h[100].sum tensor(-0.0141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(94.0164, device='cuda:0')



h[200].sum tensor(-567.6327, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.3325, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0000, 0.0015],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0015],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(265405.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2154, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2699, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2849, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0230, 0.1029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0230, 0.1029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0230, 0.1029]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1816499.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2781.2654, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(65235.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(709.8877, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(12.2811, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 2.0429],
        [ 2.0415],
        [ 2.0169],
        ...,
        [-8.4376],
        [-8.4303],
        [-8.4298]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-660524.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(722.9321, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(722.9321, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0009, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0009, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0009, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0009, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0009, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0009, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-94888.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.5404, device='cuda:0')



h[100].sum tensor(-0.0145, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(100.6493, device='cuda:0')



h[200].sum tensor(-567.8708, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.8252, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(274789.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0259, 0.0998],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0259, 0.0998],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0261, 0.0996],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0256, 0.1002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0256, 0.1002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0321, 0.0957]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1865559.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2799.7456, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(62302.1797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(665.3428, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(45.6361, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.7112],
        [-7.6327],
        [-7.4273],
        ...,
        [-8.0118],
        [-7.5043],
        [-6.7467]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-601982.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(621.4728, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(621.4728, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0008, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0008, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0008, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0008, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0008, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0008, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-95616.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5253, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.8332, device='cuda:0')



h[100].sum tensor(-0.0122, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(86.5238, device='cuda:0')



h[200].sum tensor(-567.3887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.5166, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264202.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0282, 0.0991],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0282, 0.0991],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0283, 0.0989],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0278, 0.0995],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0278, 0.0995],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0278, 0.0995]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1782000.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2771.9043, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(63216.9961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(879.1421, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(79.0801, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.6234],
        [-7.6919],
        [-7.7355],
        ...,
        [-8.3384],
        [-8.3316],
        [-8.3314]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-631372.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(625.5175, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(625.5175, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  7.0890e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2331e-03],
        [-6.0055e-03,  7.0890e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2331e-03],
        [-6.5885e-03,  7.9759e-04, -1.6894e-02,  ..., -9.0220e-03,
         -4.0264e-06,  1.4107e-04],
        ...,
        [-6.0055e-03,  7.0890e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2331e-03],
        [-6.0055e-03,  7.0890e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2331e-03],
        [-6.0055e-03,  7.0890e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2331e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-95663.1406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.0209, device='cuda:0')



h[100].sum tensor(-0.0117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.0869, device='cuda:0')



h[200].sum tensor(-567.3775, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.7283, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0000, 0.0001],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0000, 0.0055],
        ...,
        [0.0000, 0.0028, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0028, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(263085.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2708, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2703, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3454, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0286, 0.1008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0286, 0.1008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0286, 0.1008]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1758568.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2722.7148, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(65839.8672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1151.5619, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(167.9731, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.5747],
        [ 1.6507],
        [ 1.6436],
        ...,
        [-8.4056],
        [-8.3986],
        [-8.3982]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-716056.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 460.0 event: 6900 loss: tensor(459.9949, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(552.7987, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(552.7987, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0006, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0006, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0006, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0006, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0006, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0006, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-96299.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.6471, device='cuda:0')



h[100].sum tensor(-0.0100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(76.9627, device='cuda:0')



h[200].sum tensor(-567.0170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.9235, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0024, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258935.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0314, 0.1008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0323, 0.1001],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0361, 0.0974],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0281, 0.1032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0281, 0.1032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0281, 0.1032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1740243., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2625.6233, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(69878.3047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1427.0046, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(218.6400, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.0280],
        [-6.6661],
        [-5.9799],
        ...,
        [-8.4882],
        [-8.4809],
        [-8.4804]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-805411.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(582.2802, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(582.2802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0005, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0005, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0005, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0005, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0005, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0005, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-96247.8594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.0149, device='cuda:0')



h[100].sum tensor(-0.0102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(81.0672, device='cuda:0')



h[200].sum tensor(-567.1656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.4660, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0021, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(259268.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0473, 0.0916],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0711, 0.0749],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1275, 0.0392],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0276, 0.1055],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0276, 0.1055],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0276, 0.1055]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1746639.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2581.7295, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(73300.3594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1550.6556, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(261.0538, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.7622],
        [-4.2172],
        [-2.5108],
        ...,
        [-8.5904],
        [-8.5814],
        [-8.5804]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-912782.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(626.7279, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(626.7279, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0006, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0006, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0006, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0006, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0006, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0006, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-96152.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.0770, device='cuda:0')



h[100].sum tensor(-0.0105, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.2554, device='cuda:0')



h[200].sum tensor(-567.3628, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.7916, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0022, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258385.6719, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0283, 0.1056],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0283, 0.1056],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0285, 0.1055],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0280, 0.1060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0280, 0.1060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0280, 0.1060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1750530.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2572.5659, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(74233.8125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1408.2982, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(243.1745, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.6457],
        [-7.5712],
        [-7.2449],
        ...,
        [-8.6168],
        [-8.6075],
        [-8.6063]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-984100.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(619.0948, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(619.0948, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5942e-03,  8.2838e-04, -1.6915e-02,  ..., -9.0322e-03,
         -3.5433e-06,  2.8314e-04],
        [-6.5942e-03,  8.2838e-04, -1.6915e-02,  ..., -9.0322e-03,
         -3.5433e-06,  2.8314e-04],
        [-6.2079e-03,  7.1620e-04, -1.5427e-02,  ..., -8.3352e-03,
         -1.2182e-06, -3.2973e-03],
        ...,
        [-6.0055e-03,  6.5742e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1733e-03],
        [-6.0055e-03,  6.5742e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1733e-03],
        [-6.0055e-03,  6.5742e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1733e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-96209.5078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4971, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.7229, device='cuda:0')



h[100].sum tensor(-0.0100, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(86.1927, device='cuda:0')



h[200].sum tensor(-567.3496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.3922, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0029, 0.0000,  ..., 0.0000, 0.0000, 0.0003],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0000, 0.0034],
        [0.0000, 0.0033, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        ...,
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(257547.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2951, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3572, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4174, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0292, 0.1049],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0292, 0.1049],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0292, 0.1049]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1746892.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2587.2617, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(71346.1797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(1107.0907, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(145.1741, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.7130],
        [ 1.5920],
        [ 1.5125],
        ...,
        [-8.4947],
        [-8.4857],
        [-8.4845]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-857765.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(602.5038, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(602.5038, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2141e-03,  8.5030e-04, -1.5451e-02,  ..., -8.3464e-03,
         -1.2130e-06, -3.2169e-03],
        [-6.5891e-03,  9.7967e-04, -1.6896e-02,  ..., -9.0230e-03,
         -3.3931e-06,  2.6429e-04],
        [-6.8151e-03,  1.0576e-03, -1.7766e-02,  ..., -9.4308e-03,
         -4.7071e-06,  2.3625e-03],
        ...,
        [-6.0055e-03,  7.7832e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1538e-03],
        [-6.0055e-03,  7.7832e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1538e-03],
        [-6.0055e-03,  7.7832e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1538e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-96436.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.9531, device='cuda:0')



h[100].sum tensor(-0.0094, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(83.8829, device='cuda:0')



h[200].sum tensor(-567.2495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.5241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0035, 0.0000,  ..., 0.0000, 0.0000, 0.0003],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0024],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0003],
        ...,
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(254264.8281, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2246, 0.0082],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2914, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2997, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0311, 0.1032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0311, 0.1032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0311, 0.1032]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1733769.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2607.3252, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(68122.2734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(745.6511, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(49.8761, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 2.0643],
        [ 2.0312],
        [ 1.7027],
        ...,
        [-8.3980],
        [-8.3894],
        [-8.3885]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-777554.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5352],
        [0.0000],
        [0.5889],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(650.2831, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5352],
        [0.0000],
        [0.5889],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(650.2831, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.0365e-03,  1.2986e-03, -1.8620e-02,  ..., -9.8304e-03,
         -5.7904e-06,  4.4461e-03],
        [-7.4658e-03,  1.4664e-03, -2.0274e-02,  ..., -1.0605e-02,
         -8.2014e-06,  8.4373e-03],
        [-6.0055e-03,  8.9540e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1393e-03],
        ...,
        [-6.0055e-03,  8.9540e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1393e-03],
        [-6.0055e-03,  8.9540e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1393e-03],
        [-6.0055e-03,  8.9540e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1393e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-96180.3359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.1699, device='cuda:0')



h[100].sum tensor(-0.0098, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(90.5349, device='cuda:0')



h[200].sum tensor(-567.4999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.0241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0281],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0082],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0106],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258844.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7131, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4986, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3802, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0324, 0.1023],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0324, 0.1023],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0324, 0.1023]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1754791.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2631.9746, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(65623.4844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(716.7142, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(56.2555, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.8518],
        [ 0.9850],
        [ 0.8788],
        ...,
        [-8.4028],
        [-8.3944],
        [-8.3934]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-779108.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2411],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(639.4592, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2411],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(639.4592, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.7060e-03,  1.2683e-03, -1.7346e-02,  ..., -9.2340e-03,
         -3.7998e-06,  1.3895e-03],
        [-6.0055e-03,  9.7339e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1302e-03],
        [-6.4923e-03,  1.1783e-03, -1.6523e-02,  ..., -8.8484e-03,
         -2.6406e-06, -5.9947e-04],
        ...,
        [-6.0055e-03,  9.7339e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1302e-03],
        [-6.0055e-03,  9.7339e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1302e-03],
        [-6.0055e-03,  9.7339e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1302e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-96487.8672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5645, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.6677, device='cuda:0')



h[100].sum tensor(-0.0093, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.0279, device='cuda:0')



h[200].sum tensor(-567.4428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.4577, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0013],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(259747.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3567, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3854, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3769, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0329, 0.1026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0329, 0.1026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0329, 0.1026]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1768555., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2645.1807, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(64284.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(712.0973, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(44.5364, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.7741],
        [ 1.7241],
        [ 1.7027],
        ...,
        [-8.4460],
        [-8.4375],
        [-8.4366]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-786173.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6543],
        [0.4158],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(847.5975, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6543],
        [0.4158],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(847.5975, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.8488e-03,  1.2995e-03, -1.7896e-02,  ..., -9.4916e-03,
         -4.4176e-06,  2.7092e-03],
        [-6.9518e-03,  1.3407e-03, -1.8293e-02,  ..., -9.6775e-03,
         -4.9573e-06,  3.6680e-03],
        [-6.7259e-03,  1.2505e-03, -1.7423e-02,  ..., -9.2699e-03,
         -3.7738e-06,  1.5654e-03],
        ...,
        [-6.0055e-03,  9.6290e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1399e-03],
        [-6.0055e-03,  9.6290e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1399e-03],
        [-6.0055e-03,  9.6290e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1399e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-95200.2266, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.3300, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.3243, device='cuda:0')



h[100].sum tensor(-0.0118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(118.0057, device='cuda:0')



h[200].sum tensor(-568.5008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(44.3479, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0305],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0148],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0040],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(273940.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7444, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5827, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3728, 0.0051],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0326, 0.1046],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0326, 0.1046],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0326, 0.1046]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1833837.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2642.0510, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(63986.1484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(761.2855, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(93.9938, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7961],
        [ 0.8782],
        [ 0.7147],
        ...,
        [-8.5630],
        [-8.5541],
        [-8.5530]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-801469.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(590.0802, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(590.0802, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0009, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0009, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0009, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0009, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0009, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0009, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-97783.2109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3875, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.3767, device='cuda:0')



h[100].sum tensor(-0.0080, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(82.1532, device='cuda:0')



h[200].sum tensor(-567.1982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.8741, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(251213.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0323, 0.1067],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0323, 0.1067],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0325, 0.1065],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0319, 0.1070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0319, 0.1070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0319, 0.1070]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1736998., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2552.8384, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(66175.7109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(777.2256, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(22.1210, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.2212],
        [-8.1890],
        [-8.1124],
        ...,
        [-8.7296],
        [-8.7201],
        [-8.7190]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-868771.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(728.6844, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(728.6844, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  9.5416e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1719e-03],
        [-6.2215e-03,  1.0295e-03, -1.5479e-02,  ..., -8.3597e-03,
         -1.0550e-06, -3.1631e-03],
        [-6.2215e-03,  1.0295e-03, -1.5479e-02,  ..., -8.3597e-03,
         -1.0550e-06, -3.1631e-03],
        ...,
        [-6.0055e-03,  9.5416e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1719e-03],
        [-6.0055e-03,  9.5416e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1719e-03],
        [-6.0055e-03,  9.5416e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1719e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-97037.3359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8990, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.8073, device='cuda:0')



h[100].sum tensor(-0.0095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(101.4502, device='cuda:0')



h[200].sum tensor(-567.9050, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.1262, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(260917.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0981, 0.0635],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1119, 0.0541],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1122, 0.0539],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0314, 0.1088],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0314, 0.1088],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0314, 0.1088]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1785621.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2553.3027, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(66190.8438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(828.6116, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(68.5598, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.3184],
        [-2.7874],
        [-2.4032],
        ...,
        [-8.8769],
        [-8.8674],
        [-8.8663]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-945020.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 470.0 event: 7050 loss: tensor(459.8137, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5723],
        [0.5127],
        [0.5645],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(676.9849, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5723],
        [0.5127],
        [0.5645],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(676.9849, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.4911e-03,  1.1417e-03, -1.6518e-02,  ..., -8.8462e-03,
         -2.2899e-06, -6.7719e-04],
        [-7.0821e-03,  1.3347e-03, -1.8795e-02,  ..., -9.9126e-03,
         -5.0771e-06,  4.8167e-03],
        [-7.0835e-03,  1.3352e-03, -1.8800e-02,  ..., -9.9151e-03,
         -5.0837e-06,  4.8296e-03],
        ...,
        [-6.0055e-03,  9.8318e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1910e-03],
        [-6.0055e-03,  9.8318e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1910e-03],
        [-6.0055e-03,  9.8318e-04, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1910e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-97623.2969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.4087, device='cuda:0')



h[100].sum tensor(-0.0085, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(94.2524, device='cuda:0')



h[200].sum tensor(-567.6500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.4211, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0074],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0100],
        [0.0000, 0.0050, 0.0000,  ..., 0.0000, 0.0000, 0.0107],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(261609.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3376, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4001, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4156, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0324, 0.1088],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0324, 0.1088],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0324, 0.1088]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1802364., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2537.7329, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(64910.7344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(822.0784, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(50.9107, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.7071],
        [ 1.3878],
        [ 1.5406],
        ...,
        [-8.8735],
        [-8.8633],
        [-8.8570]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-938414.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(581.2863, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(581.2863, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0010, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-98449.9531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3563, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.9688, device='cuda:0')



h[100].sum tensor(-0.0070, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(80.9289, device='cuda:0')



h[200].sum tensor(-567.1552, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.4140, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(251677.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0345, 0.1070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0345, 0.1070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0347, 0.1069],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0342, 0.1074],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0342, 0.1074],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0342, 0.1074]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1747220.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2554.4187, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(63193.2422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(780.3018, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-28.7340, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.3471],
        [-8.2760],
        [-8.1524],
        ...,
        [-8.8380],
        [-8.8290],
        [-8.8280]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-869361., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2678],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(504.4647, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2678],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(504.4647, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.1269e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2186e-03],
        [-6.2592e-03,  1.2051e-03, -1.5624e-02,  ..., -8.4277e-03,
         -1.1148e-06, -2.8623e-03],
        [-6.0055e-03,  1.1269e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2186e-03],
        ...,
        [-6.0055e-03,  1.1269e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2186e-03],
        [-6.0055e-03,  1.1269e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2186e-03],
        [-6.0055e-03,  1.1269e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2186e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-99065.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.0629, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.4046, device='cuda:0')



h[100].sum tensor(-0.0059, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(70.2335, device='cuda:0')



h[200].sum tensor(-566.7496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.3946, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0046, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(247650.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1834, 0.0143],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1303, 0.0453],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0943, 0.0653],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0359, 0.1052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0359, 0.1052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0359, 0.1052]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1728364.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2565.6047, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(60999.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(730.3285, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-65.3573, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5121],
        [-1.7101],
        [-3.5710],
        ...,
        [-8.7863],
        [-8.7774],
        [-8.7765]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-847940.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(785.0918, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(785.0918, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.1827e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2275e-03],
        [-6.0055e-03,  1.1827e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2275e-03],
        [-6.2393e-03,  1.2545e-03, -1.5548e-02,  ..., -8.3918e-03,
         -9.9165e-07, -3.0560e-03],
        ...,
        [-6.0055e-03,  1.1827e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2275e-03],
        [-6.0055e-03,  1.1827e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2275e-03],
        [-6.7163e-03,  1.4008e-03, -1.7385e-02,  ..., -9.2526e-03,
         -3.0152e-06,  1.3752e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-96737.6406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.4243, device='cuda:0')



h[100].sum tensor(-0.0087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(109.3035, device='cuda:0')



h[200].sum tensor(-568.1609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.0775, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0000, 0.0014],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0016]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(270986.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0746, 0.0792],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0863, 0.0711],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1005, 0.0612],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0714, 0.0796],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1562, 0.0392],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2647, 0.0132]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1832534.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2676.9229, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(57605.2852, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(735.8489, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-40.3256, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.9109],
        [-3.1698],
        [-1.8889],
        ...,
        [-5.3827],
        [-3.0034],
        [-0.6698]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-752502.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4495],
        [0.0000],
        [0.2793],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(643.4769, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4495],
        [0.0000],
        [0.2793],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(643.4769, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2445e-03,  1.2850e-03, -1.5568e-02,  ..., -8.4013e-03,
         -9.7856e-07, -3.0127e-03],
        [-6.9347e-03,  1.4948e-03, -1.8227e-02,  ..., -9.6467e-03,
         -3.8044e-06,  3.3989e-03],
        [-6.0055e-03,  1.2123e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2330e-03],
        ...,
        [-6.0055e-03,  1.2123e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2330e-03],
        [-6.0055e-03,  1.2123e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2330e-03],
        [-6.0055e-03,  1.2123e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2330e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-97846.7969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.8541, device='cuda:0')



h[100].sum tensor(-0.0070, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.5873, device='cuda:0')



h[200].sum tensor(-567.4816, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.6679, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0049],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0034],
        ...,
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258438.6094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3189, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2679, 0.0029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2515, 0.0075],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0383, 0.1041],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0483, 0.0971],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0834, 0.0724]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1763535.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2618.4287, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(58067.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(712.9216, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-104.1637, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.7975],
        [ 1.9807],
        [ 1.6022],
        ...,
        [-7.6929],
        [-6.2778],
        [-3.8492]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-741548.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(596.5765, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(596.5765, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0013, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-98232.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4125, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.6781, device='cuda:0')



h[100].sum tensor(-0.0062, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(83.0576, device='cuda:0')



h[200].sum tensor(-567.2328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.2140, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(256972.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1029, 0.0606],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0961, 0.0652],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1345, 0.0386],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0381, 0.1046],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0381, 0.1046],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0381, 0.1046]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1765116., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2621.8120, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(57898.4062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(715.4357, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-142.1944, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5699],
        [-0.9173],
        [-0.0564],
        ...,
        [-8.6744],
        [-8.6659],
        [-8.6651]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-713062.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4099],
        [0.4167],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(718.4080, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4099],
        [0.4167],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(718.4080, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3937e-03,  1.5229e-03, -1.6143e-02,  ..., -8.6705e-03,
         -1.4802e-06, -1.6131e-03],
        [-6.4002e-03,  1.5251e-03, -1.6168e-02,  ..., -8.6822e-03,
         -1.5049e-06, -1.5529e-03],
        [-6.3937e-03,  1.5229e-03, -1.6143e-02,  ..., -8.6705e-03,
         -1.4802e-06, -1.6131e-03],
        ...,
        [-6.0055e-03,  1.3916e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2239e-03],
        [-6.0055e-03,  1.3916e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2239e-03],
        [-6.0055e-03,  1.3916e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2239e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-97292.4531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8608, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.3305, device='cuda:0')



h[100].sum tensor(-0.0072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(100.0195, device='cuda:0')



h[200].sum tensor(-567.8524, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.5885, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0007],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0007],
        ...,
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264086.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1549, 0.0306],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2164, 0.0024],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2195, 0.0022],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0357, 0.1056],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0357, 0.1056],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0357, 0.1056]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1790739., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2657.3315, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(57704.3945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(739.6292, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-99.5170, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.4272],
        [ 0.5501],
        [ 1.3923],
        ...,
        [-8.8272],
        [-8.8184],
        [-8.8175]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-815846.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4744],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(575.6266, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4744],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(575.6266, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.4294e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2105e-03],
        [-6.4548e-03,  1.5906e-03, -1.6378e-02,  ..., -8.7807e-03,
         -1.6527e-06, -1.0268e-03],
        [-6.0055e-03,  1.4294e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2105e-03],
        ...,
        [-6.0055e-03,  1.4294e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2105e-03],
        [-6.0055e-03,  1.4294e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2105e-03],
        [-6.0055e-03,  1.4294e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2105e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-98447.2891, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3303, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.7062, device='cuda:0')



h[100].sum tensor(-0.0056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(80.1409, device='cuda:0')



h[200].sum tensor(-567.1192, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.1179, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(253755.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2459, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1699, 0.0162],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2307, 0.0044],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0339, 0.1071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0339, 0.1071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0339, 0.1071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1740108.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2580.6621, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(59631.3359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(736.2776, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-126.9551, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 2.2606],
        [ 2.2595],
        [ 2.2142],
        ...,
        [-8.8733],
        [-8.8644],
        [-8.8635]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-878471.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(567.2323, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(567.2323, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.4294e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2105e-03],
        [-6.0055e-03,  1.4294e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2105e-03],
        [-6.7514e-03,  1.6971e-03, -1.7521e-02,  ..., -9.3160e-03,
         -2.7441e-06,  1.7358e-03],
        ...,
        [-6.0055e-03,  1.4294e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2105e-03],
        [-6.0055e-03,  1.4294e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2105e-03],
        [-6.0055e-03,  1.4294e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2105e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-98528.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2941, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.3167, device='cuda:0')



h[100].sum tensor(-0.0055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(78.9722, device='cuda:0')



h[200].sum tensor(-567.0692, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.6787, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0017],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0036],
        ...,
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(254930., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0732, 0.0790],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1582, 0.0379],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2762, 0.0112],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0339, 0.1071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0339, 0.1071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0339, 0.1071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1745774.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2552.3325, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(60010.8359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(735.2054, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-113.1670, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5581],
        [-0.5360],
        [ 0.8267],
        ...,
        [-8.8733],
        [-8.8644],
        [-8.8635]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-922971., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(673.8251, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(673.8251, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.4294e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2105e-03],
        [-6.0055e-03,  1.4294e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2105e-03],
        [-6.2298e-03,  1.5099e-03, -1.5511e-02,  ..., -8.3747e-03,
         -8.2509e-07, -3.1219e-03],
        ...,
        [-6.0055e-03,  1.4294e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2105e-03],
        [-6.0055e-03,  1.4294e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2105e-03],
        [-6.0055e-03,  1.4294e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2105e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-97643.7500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6904, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.2621, device='cuda:0')



h[100].sum tensor(-0.0065, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.8125, device='cuda:0')



h[200].sum tensor(-567.6168, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.2558, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(261867.0469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0600, 0.0895],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1022, 0.0610],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1168, 0.0511],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0339, 0.1071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0339, 0.1071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0339, 0.1071]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1778993.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2605.8225, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(59209.4414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(746.8206, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-103.4052, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.2175],
        [-4.2253],
        [-3.5654],
        ...,
        [-8.8733],
        [-8.8644],
        [-8.8635]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-871041.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 480.0 event: 7200 loss: tensor(359.5991, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(574.5188, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(574.5188, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-98195.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3163, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.6548, device='cuda:0')



h[100].sum tensor(-0.0053, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(79.9867, device='cuda:0')



h[200].sum tensor(-567.0999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.0599, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(256832.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0353, 0.1066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0327, 0.1084],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0329, 0.1082],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0323, 0.1087],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0323, 0.1087],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0323, 0.1087]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1744456.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2507.6711, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(60859.4961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(980.7937, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-401.1199, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.6419],
        [-7.4629],
        [-7.8561],
        ...,
        [-8.8415],
        [-8.8332],
        [-8.8323]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-839388.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2656],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(813.1508, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2656],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(813.1508, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.4362e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1856e-03],
        [-6.2571e-03,  1.5334e-03, -1.5616e-02,  ..., -8.4239e-03,
         -8.6136e-07, -2.8370e-03],
        [-6.0055e-03,  1.4362e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1856e-03],
        ...,
        [-6.0055e-03,  1.4362e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1856e-03],
        [-6.0055e-03,  1.4362e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1856e-03],
        [-6.0055e-03,  1.4362e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1856e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-96436.0625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.7261, device='cuda:0')



h[100].sum tensor(-0.0072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(113.2099, device='cuda:0')



h[200].sum tensor(-568.3102, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.5456, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(272681.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1804, 0.0169],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1412, 0.0395],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1377, 0.0379],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0308, 0.1101],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0308, 0.1101],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0308, 0.1101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1828161.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2565.6191, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(61121.1523, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(796.9683, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-78.0811, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.6766],
        [ 0.6688],
        [ 0.1833],
        ...,
        [-8.2480],
        [-7.6879],
        [-6.6288]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-885765.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(640.2166, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(640.2166, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-98171.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.7028, device='cuda:0')



h[100].sum tensor(-0.0055, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.1334, device='cuda:0')



h[200].sum tensor(-567.4480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.4974, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258828.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1732, 0.0353],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0704, 0.0826],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0444, 0.1008],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0298, 0.1111],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0298, 0.1111],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0298, 0.1111]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1756794.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2504.4238, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(63118.2812, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(800.2141, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-91.3266, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.8785],
        [-3.3007],
        [-5.5997],
        ...,
        [-8.9708],
        [-8.9611],
        [-8.9599]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1008682.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(578.2625, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(578.2625, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-98770.7188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.8285, device='cuda:0')



h[100].sum tensor(-0.0048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(80.5079, device='cuda:0')



h[200].sum tensor(-567.1494, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.2558, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(255751.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 7.6597e-02,
         7.8150e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.4946e-02,
         1.0060e-01],
        [0.0000e+00, 5.0357e-05, 0.0000e+00,  ..., 0.0000e+00, 3.5990e-02,
         1.0693e-01],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.0714e-02,
         1.1048e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.0714e-02,
         1.1048e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.0714e-02,
         1.1048e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1736981.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2490.1777, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(62661.3555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(774.3790, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-131.6522, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.7794],
        [-5.0040],
        [-6.3825],
        ...,
        [-8.9011],
        [-8.8915],
        [-8.8903]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-938789.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(655.6462, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(655.6462, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-98225.4219, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6262, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.4187, device='cuda:0')



h[100].sum tensor(-0.0052, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.2815, device='cuda:0')



h[200].sum tensor(-567.5281, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.3047, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(267218.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0349, 0.1072],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0676, 0.0844],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1400, 0.0428],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0321, 0.1092],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0321, 0.1092],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0321, 0.1092]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1810014., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2557.3657, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(60644.6328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(765.7798, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-145.4362, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.9243],
        [-3.6880],
        [-1.2368],
        ...,
        [-8.8389],
        [-8.8295],
        [-8.8284]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-807547.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(637.5782, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(637.5782, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-98515.4453, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5544, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.5804, device='cuda:0')



h[100].sum tensor(-0.0049, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(88.7661, device='cuda:0')



h[200].sum tensor(-567.4290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.3593, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(260776.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0360, 0.1062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0517, 0.0957],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0989, 0.0634],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0331, 0.1083],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0331, 0.1083],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0331, 0.1083]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1758578.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2582.5615, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(60110.8281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(754.9701, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.8564, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.4394],
        [-4.9653],
        [-2.7354],
        ...,
        [-8.8613],
        [-8.8519],
        [-8.8508]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-810592.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(757.0585, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(757.0585, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-97635.3672, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0181, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.1237, device='cuda:0')



h[100].sum tensor(-0.0056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(105.4005, device='cuda:0')



h[200].sum tensor(-568.0696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.6107, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(273060.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1634, 0.0421],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0706, 0.0820],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0388, 0.1045],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0333, 0.1086],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0333, 0.1086],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0333, 0.1086]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1818999., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2614.1821, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(59558.0156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(777.7391, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-133.0986, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.3594],
        [-3.5928],
        [-5.6519],
        ...,
        [-8.8748],
        [-8.8653],
        [-8.8642]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-784665.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3928],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(787.4545, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3928],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(787.4545, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2622e-03,  1.4533e-03, -1.5636e-02,  ..., -8.4331e-03,
         -7.0694e-07, -2.7558e-03],
        [-6.3775e-03,  1.5028e-03, -1.6080e-02,  ..., -8.6413e-03,
         -1.0247e-06, -1.6743e-03],
        [-6.0055e-03,  1.3433e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1616e-03],
        ...,
        [-6.0055e-03,  1.3433e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1616e-03],
        [-6.0055e-03,  1.3433e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1616e-03],
        [-6.0055e-03,  1.3433e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1616e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-97650.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1270, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.5339, device='cuda:0')



h[100].sum tensor(-0.0056, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(109.6324, device='cuda:0')



h[200].sum tensor(-568.2202, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.2011, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0037],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(272327.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3069, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1902, 0.0287],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1293, 0.0465],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0328, 0.1098],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0328, 0.1098],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0328, 0.1098]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1806341.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2572.2896, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(61053.9922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(802.9177, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-126.1181, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.8512],
        [ 1.2695],
        [ 0.7219],
        ...,
        [-8.9208],
        [-8.9111],
        [-8.9099]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-823753., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(569.4669, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(569.4669, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.3192e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1656e-03],
        [-6.0055e-03,  1.3192e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1656e-03],
        [-6.2636e-03,  1.4281e-03, -1.5641e-02,  ..., -8.4356e-03,
         -6.8522e-07, -2.7462e-03],
        ...,
        [-6.0055e-03,  1.3192e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1656e-03],
        [-6.0055e-03,  1.3192e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1656e-03],
        [-6.0055e-03,  1.3192e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1656e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-99757.2578, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3239, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.4204, device='cuda:0')



h[100].sum tensor(-0.0039, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(79.2833, device='cuda:0')



h[200].sum tensor(-567.1104, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.7956, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(256912.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0539, 0.0969],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1005, 0.0651],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1677, 0.0322],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0323, 0.1114],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0323, 0.1114],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0323, 0.1114]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1742201., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2464.1021, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(63715.5547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(818.8967, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-150.8772, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-4.6167],
        [-2.2621],
        [-0.1442],
        ...,
        [-9.0142],
        [-9.0042],
        [-9.0030]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-927219.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2871],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(718.2025, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2871],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(718.2025, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.8601e-03,  1.6799e-03, -1.7938e-02,  ..., -9.5121e-03,
         -2.1875e-06,  2.8373e-03],
        [-6.0055e-03,  1.3289e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1739e-03],
        [-6.2774e-03,  1.4406e-03, -1.5694e-02,  ..., -8.4606e-03,
         -6.9605e-07, -2.6248e-03],
        ...,
        [-6.0055e-03,  1.3289e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1739e-03],
        [-6.0055e-03,  1.3289e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1739e-03],
        [-6.0055e-03,  1.3289e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1739e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-98782.5078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8903, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.3210, device='cuda:0')



h[100].sum tensor(-0.0048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(99.9909, device='cuda:0')



h[200].sum tensor(-567.8931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.5777, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0063],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0034],
        [0.0000, 0.0054, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(267991.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4371, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3138, 0.0062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1644, 0.0342],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0324, 0.1124],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0324, 0.1124],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0324, 0.1124]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1794498., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2493.4580, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(63487.7422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(863.6094, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-91.4923, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.5865],
        [ 1.3857],
        [ 0.7524],
        ...,
        [-9.1104],
        [-9.1004],
        [-9.0992]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-948927.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 490.0 event: 7350 loss: tensor(450.1315, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(526.2548, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(526.2548, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0014, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-100603.5938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.1407, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.4156, device='cuda:0')



h[100].sum tensor(-0.0033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(73.2672, device='cuda:0')



h[200].sum tensor(-566.8573, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.5347, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(250429.0156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0337, 0.1113],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0359, 0.1097],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0460, 0.1027],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1636, 0.0350],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0778, 0.0805],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0430, 0.1049]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1712089.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2458.6904, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(63312.5312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(817.5918, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-160.3267, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.7434],
        [-7.1467],
        [-6.0339],
        ...,
        [-1.2593],
        [-4.1389],
        [-6.6426]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-936739.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(701.3899, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2776],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(701.3899, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.4548e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1937e-03],
        [-6.2684e-03,  1.5568e-03, -1.5659e-02,  ..., -8.4444e-03,
         -6.2517e-07, -2.7317e-03],
        [-6.0055e-03,  1.4548e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1937e-03],
        ...,
        [-6.0055e-03,  1.4548e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1937e-03],
        [-6.0055e-03,  1.4548e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1937e-03],
        [-6.0055e-03,  1.4548e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1937e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-99174.7344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.5410, device='cuda:0')



h[100].sum tensor(-0.0043, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(97.6502, device='cuda:0')



h[200].sum tensor(-567.7623, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.6981, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(266108.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1200, 0.0516],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0963, 0.0675],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1039, 0.0618],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0352, 0.1093],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0480, 0.1007],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0668, 0.0881]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1787901.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2518.1814, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(60715.6367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(796.6536, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-126.6219, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-1.5857],
        [-1.2661],
        [-0.5303],
        ...,
        [-8.3594],
        [-7.4512],
        [-6.1234]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-891206.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(696.5654, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(696.5654, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-99125.2656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7743, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.3171, device='cuda:0')



h[100].sum tensor(-0.0041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.9785, device='cuda:0')



h[200].sum tensor(-567.7328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.4456, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0006],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264923.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1221, 0.0524],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0629, 0.0881],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0440, 0.1014],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0378, 0.1060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0378, 0.1060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0378, 0.1060]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1775868.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2590.1912, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(58063.3984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(737.8597, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-188.3527, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-2.5668],
        [-5.0725],
        [-6.7704],
        ...,
        [-8.8806],
        [-8.8713],
        [-8.8702]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-775003.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4255],
        [0.3010],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(708.1216, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4255],
        [0.3010],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(708.1216, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6379e-03,  1.8079e-03, -1.7082e-02,  ..., -9.1111e-03,
         -1.3965e-06,  7.3419e-04],
        [-6.4085e-03,  1.7171e-03, -1.6199e-02,  ..., -8.6972e-03,
         -8.8998e-07, -1.4150e-03],
        [-6.5947e-03,  1.7908e-03, -1.6916e-02,  ..., -9.0331e-03,
         -1.3010e-06,  3.2904e-04],
        ...,
        [-6.0055e-03,  1.5575e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1913e-03],
        [-6.0055e-03,  1.5575e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1913e-03],
        [-6.0055e-03,  1.5575e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1913e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-98883.4844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8480, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.8533, device='cuda:0')



h[100].sum tensor(-0.0040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(98.5874, device='cuda:0')



h[200].sum tensor(-567.8347, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.0503, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0006],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0023],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(271337.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3687, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4125, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4782, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0390, 0.1038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0390, 0.1038],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0390, 0.1038]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1814776.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2629.3594, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(55982.0781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(703.3921, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-203.9639, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.7115],
        [ 1.6843],
        [ 1.6148],
        ...,
        [-8.8093],
        [-8.8004],
        [-8.7995]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-732775.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(657.3640, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(657.3640, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-99376.6562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.4984, device='cuda:0')



h[100].sum tensor(-0.0036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.5207, device='cuda:0')



h[200].sum tensor(-567.5339, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.3945, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264862.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0391, 0.1026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0391, 0.1026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0444, 0.0989],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0387, 0.1030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0387, 0.1030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0387, 0.1030]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1778721.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2616.4373, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(56082.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(683.5233, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-238.8552, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.6833],
        [-7.4208],
        [-6.7330],
        ...,
        [-8.8485],
        [-8.8395],
        [-8.8385]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-741675.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(625.3265, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(625.3265, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-99706.3594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5228, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.0120, device='cuda:0')



h[100].sum tensor(-0.0033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.0603, device='cuda:0')



h[200].sum tensor(-567.3853, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.7183, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(262888.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0514, 0.0942],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0377, 0.1033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0375, 0.1034],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0369, 0.1040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0369, 0.1040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0369, 0.1040]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1770855.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2574.4834, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(57470.3633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(693.8102, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-232.1910, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.4515],
        [-6.4006],
        [-6.2781],
        ...,
        [-8.9600],
        [-8.9500],
        [-8.9321]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-813700.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4756],
        [0.5186],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(651.5790, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4756],
        [0.5186],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(651.5790, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.4559e-03,  1.8689e-03, -1.6382e-02,  ..., -8.7828e-03,
         -8.8944e-07, -9.6008e-04],
        [-6.4966e-03,  1.8861e-03, -1.6538e-02,  ..., -8.8562e-03,
         -9.6980e-07, -5.7824e-04],
        [-6.9776e-03,  2.0903e-03, -1.8390e-02,  ..., -9.7240e-03,
         -1.9195e-06,  3.9344e-03],
        ...,
        [-6.0055e-03,  1.6776e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1864e-03],
        [-6.0055e-03,  1.6776e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1864e-03],
        [-6.0055e-03,  1.6776e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1864e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-99729.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6072, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.2300, device='cuda:0')



h[100].sum tensor(-0.0033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(90.7153, device='cuda:0')



h[200].sum tensor(-567.5020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.0919, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0060],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0084],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264355.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1964, 0.0273],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3271, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4098, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0343, 0.1064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0343, 0.1064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0343, 0.1064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1776883.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2504.0923, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(59773.4414, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(730.8423, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-198.4659, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.5408],
        [ 1.2004],
        [ 1.8694],
        ...,
        [-9.1129],
        [-9.1049],
        [-9.1044]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-893162.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(628.5654, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(628.5654, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-99984.4531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5311, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.1623, device='cuda:0')



h[100].sum tensor(-0.0030, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.5113, device='cuda:0')



h[200].sum tensor(-567.3967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.8877, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(261850.1406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0361, 0.1052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0340, 0.1066],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0396, 0.1028],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0326, 0.1076],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0326, 0.1076],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0326, 0.1076]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1771166.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2484.4253, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(61083.4219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(748.2061, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-218.4949, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.4833],
        [-6.1330],
        [-5.1420],
        ...,
        [-9.2082],
        [-9.2018],
        [-9.1971]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-894851.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2952],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(715.8369, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2952],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(715.8369, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2851e-03,  1.9086e-03, -1.5723e-02,  ..., -8.4744e-03,
         -5.1212e-07, -2.5596e-03],
        [-6.0055e-03,  1.7809e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1853e-03],
        [-6.2851e-03,  1.9086e-03, -1.5723e-02,  ..., -8.4744e-03,
         -5.1212e-07, -2.5596e-03],
        ...,
        [-6.0055e-03,  1.7809e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1853e-03],
        [-6.0055e-03,  1.7809e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1853e-03],
        [-6.0055e-03,  1.7809e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1853e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-99334.8906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8461, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.2112, device='cuda:0')



h[100].sum tensor(-0.0033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(99.6615, device='cuda:0')



h[200].sum tensor(-567.8320, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.4540, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268053.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0835, 0.0740],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1233, 0.0482],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0933, 0.0676],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0314, 0.1084],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0314, 0.1084],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0314, 0.1084]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1798712.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2487.1050, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(61137.0078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(784.1339, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-150.5358, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.8730],
        [-4.8172],
        [-3.8531],
        ...,
        [-9.3683],
        [-9.3595],
        [-9.3588]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1027956.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(642.3165, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(642.3165, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-99759.2969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5948, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.8002, device='cuda:0')



h[100].sum tensor(-0.0029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.4257, device='cuda:0')



h[200].sum tensor(-567.4847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.6072, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(265811.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1420, 0.0394],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2086, 0.0174],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2319, 0.0136],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0321, 0.1065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0321, 0.1065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0321, 0.1065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1792966.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2468.2776, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(60495.5234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(745.3020, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-198.1039, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.4198],
        [ 1.1943],
        [ 1.4213],
        ...,
        [-9.3392],
        [-9.3306],
        [-9.3299]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-975378.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 500.0 event: 7500 loss: tensor(442.8790, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(607.7573, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(607.7573, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-99895.0938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.1969, device='cuda:0')



h[100].sum tensor(-0.0026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(84.6143, device='cuda:0')



h[200].sum tensor(-567.2722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.7990, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264740.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0344, 0.1032],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0344, 0.1032],
        [0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0345, 0.1030],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0340, 0.1036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0340, 0.1036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0340, 0.1036]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1788622.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2514.2451, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(58769.0195, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(688.5674, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-307.7069, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.1887],
        [-8.2128],
        [-8.1798],
        ...,
        [-9.1878],
        [-9.1791],
        [-9.1782]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-773885.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4534],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(784.6996, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4534],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(784.6996, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.6861e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1925e-03],
        [-6.4349e-03,  1.8802e-03, -1.6300e-02,  ..., -8.7448e-03,
         -7.0236e-07, -1.1566e-03],
        [-6.5102e-03,  1.9142e-03, -1.6590e-02,  ..., -8.8807e-03,
         -8.2554e-07, -4.4876e-04],
        ...,
        [-6.0055e-03,  1.6861e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1925e-03],
        [-6.0055e-03,  1.6861e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1925e-03],
        [-6.0055e-03,  1.6861e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1925e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-98250.8125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1249, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.4061, device='cuda:0')



h[100].sum tensor(-0.0033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(109.2488, device='cuda:0')



h[200].sum tensor(-568.2173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.0570, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(278460.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1492, 0.0327],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2317, 0.0127],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3174, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0347, 0.1021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0347, 0.1021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0347, 0.1021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1846029., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2580.5017, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(56542.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(690.9010, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-268.8854, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.0948],
        [ 1.2079],
        [ 1.8779],
        ...,
        [-9.1761],
        [-9.1727],
        [-9.1753]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-726372.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(826.9149, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(826.9149, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-98018.6172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2435, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.3647, device='cuda:0')



h[100].sum tensor(-0.0033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(115.1262, device='cuda:0')



h[200].sum tensor(-568.3812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.2658, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(280728.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0351, 0.1021],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0362, 0.1014],
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0432, 0.0966],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0347, 0.1025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0347, 0.1025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0347, 0.1025]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1853078.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2590.6733, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(56164.6172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(723.5571, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-219.5744, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.8825],
        [-7.4911],
        [-6.6378],
        ...,
        [-9.2756],
        [-9.2675],
        [-9.2670]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-773796.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(605.2921, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(605.2921, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-99709.9922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.0825, device='cuda:0')



h[100].sum tensor(-0.0023, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(84.2711, device='cuda:0')



h[200].sum tensor(-567.2772, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.6700, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264357.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0345, 0.1036],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0345, 0.1036],
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0346, 0.1034],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0341, 0.1040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0341, 0.1040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0341, 0.1040]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1772410.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2272.9775, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(57920.8828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(602.9194, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(1.0591, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.5375],
        [-8.4599],
        [-8.2659],
        ...,
        [-9.3013],
        [-9.2932],
        [-9.2926]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-808660.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(717.2118, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(717.2118, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-98508.8359, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.2750, device='cuda:0')



h[100].sum tensor(-0.0026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(99.8529, device='cuda:0')



h[200].sum tensor(-567.8500, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.5259, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(278191.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0604, 0.0882],
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0434, 0.0991],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0338, 0.1053],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0333, 0.1059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0333, 0.1059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0333, 0.1059]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1824137.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2033.5675, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(58581.0547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(613.6653, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(238.0580, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.5378],
        [-6.4464],
        [-6.6920],
        ...,
        [-9.3101],
        [-9.3019],
        [-9.3014]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-828820.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(743.2458, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(743.2458, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6300e-03,  1.6937e-03, -1.7051e-02,  ..., -9.0969e-03,
         -8.7712e-07,  5.1172e-04],
        [-6.0055e-03,  1.5426e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3179e-03],
        [-6.0055e-03,  1.5426e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3179e-03],
        ...,
        [-6.0055e-03,  1.5426e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3179e-03],
        [-6.0055e-03,  1.5426e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3179e-03],
        [-6.0055e-03,  1.5426e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3179e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-98831.6562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.4829, device='cuda:0')



h[100].sum tensor(-0.0026, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(103.4775, device='cuda:0')



h[200].sum tensor(-567.9634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.8880, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0005],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0005],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(272003., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2078, 0.0125],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1773, 0.0287],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0854, 0.0713],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0323, 0.1079],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0323, 0.1079],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0323, 0.1079]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1804985.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2249.9250, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(59549.3945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(682.6625, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(110.6282, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.0971],
        [-0.8623],
        [-3.2050],
        ...,
        [-9.3826],
        [-9.3462],
        [-9.2730]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-879660.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(512.5269, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(512.5269, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.5825e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3515e-03],
        [-6.0055e-03,  1.5825e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3515e-03],
        [-6.2763e-03,  1.6400e-03, -1.5689e-02,  ..., -8.4586e-03,
         -3.6596e-07, -2.8283e-03],
        ...,
        [-6.0055e-03,  1.5825e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3515e-03],
        [-6.0055e-03,  1.5825e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3515e-03],
        [-6.0055e-03,  1.5825e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3515e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-101097.9766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.1029, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7787, device='cuda:0')



h[100].sum tensor(-0.0017, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(71.3559, device='cuda:0')



h[200].sum tensor(-566.8051, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.8164, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0014],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(251627.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2629, 0.0040],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2891, 0.0009],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3556, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0321, 0.1077],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0321, 0.1077],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0321, 0.1077]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1715963.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2374.2998, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(61543.0703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(753.8318, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-133.6875, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.8530],
        [ 1.8160],
        [ 1.6576],
        ...,
        [-9.4532],
        [-9.4142],
        [-9.3066]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1040669.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(569.4043, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(569.4043, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-100797.0312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3020, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.4175, device='cuda:0')



h[100].sum tensor(-0.0018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(79.2746, device='cuda:0')



h[200].sum tensor(-567.0801, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.7923, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(259205.5469, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0886, 0.0704],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0581, 0.0893],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0663, 0.0833],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0331, 0.1055],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0408, 0.1005],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0574, 0.0896]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1741785.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2326.7500, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(60049.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(475.5035, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-162.0300, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.0790],
        [-4.1420],
        [-4.1182],
        ...,
        [-8.1307],
        [-7.1178],
        [-6.1126]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-901079., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(556.8002, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(556.8002, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2088e-03,  1.6235e-03, -1.5430e-02,  ..., -8.3369e-03,
         -2.5444e-07, -3.4891e-03],
        [-6.0055e-03,  1.5828e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3817e-03],
        [-6.0055e-03,  1.5828e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3817e-03],
        ...,
        [-6.0055e-03,  1.5828e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3817e-03],
        [-6.0055e-03,  1.5828e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3817e-03],
        [-6.0055e-03,  1.5828e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3817e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-100890.8281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2682, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.8327, device='cuda:0')



h[100].sum tensor(-0.0017, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(77.5198, device='cuda:0')



h[200].sum tensor(-567.0334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.1328, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(262379.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1636, 0.0240],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1225, 0.0452],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0709, 0.0783],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0339, 0.1030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0339, 0.1030],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0339, 0.1030]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1742079.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2209.0728, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(58884.3594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(176.8523, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.6614, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-0.1241],
        [-1.6833],
        [-3.4049],
        ...,
        [-9.2245],
        [-9.2153],
        [-9.2144]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-793855.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(573.3396, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(573.3396, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-101236.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.6001, device='cuda:0')



h[100].sum tensor(-0.0017, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(79.8225, device='cuda:0')



h[200].sum tensor(-567.1008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.9982, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(261118.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0327, 0.1026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0327, 0.1026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0329, 0.1024],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0324, 0.1031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0324, 0.1031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0324, 0.1031]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1735080.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2338.1006, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(57776.1758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(363.9277, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-183.5020, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.0311],
        [-7.9622],
        [-7.6549],
        ...,
        [-9.3056],
        [-9.2959],
        [-9.2935]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-882848.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 510.0 event: 7650 loss: tensor(449.8293, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(638.0793, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(638.0793, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.7883e-03,  1.9308e-03, -1.7661e-02,  ..., -9.3826e-03,
         -9.0659e-07,  1.8849e-03],
        [-6.6667e-03,  1.9035e-03, -1.7192e-02,  ..., -9.1630e-03,
         -7.6568e-07,  7.5281e-04],
        [-7.3622e-03,  2.0598e-03, -1.9870e-02,  ..., -1.0418e-02,
         -1.5712e-06,  7.2242e-03],
        ...,
        [-6.0055e-03,  1.7549e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3988e-03],
        [-6.0055e-03,  1.7549e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3988e-03],
        [-6.0055e-03,  1.7549e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3988e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-101174.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5803, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.6037, device='cuda:0')



h[100].sum tensor(-0.0018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(88.8358, device='cuda:0')



h[200].sum tensor(-567.4647, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.3855, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0108],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0091],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0155],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264662.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6497, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6468, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6913, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0304, 0.1035],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0304, 0.1035],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0304, 0.1035]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1755960.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2462.6826, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(58060.2344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(570.6416, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-190.7695, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.1684],
        [ 1.1378],
        [ 1.1344],
        ...,
        [-9.3588],
        [-9.3492],
        [-9.3482]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-913133.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2732],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(766.1799, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2732],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(766.1799, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.4002e-03,  1.8290e-03, -1.6166e-02,  ..., -8.6822e-03,
         -4.3967e-07, -1.7262e-03],
        [-6.2642e-03,  1.7971e-03, -1.5643e-02,  ..., -8.4369e-03,
         -2.8822e-07, -2.9916e-03],
        [-6.2338e-03,  1.7899e-03, -1.5526e-02,  ..., -8.3818e-03,
         -2.5425e-07, -3.2754e-03],
        ...,
        [-6.0055e-03,  1.7364e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3998e-03],
        [-6.0055e-03,  1.7364e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3998e-03],
        [-6.0055e-03,  1.7364e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3998e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-100715.2031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.5469, device='cuda:0')



h[100].sum tensor(-0.0021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(106.6705, device='cuda:0')



h[200].sum tensor(-568.0792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.0880, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0044],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(276154.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2809, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2862, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3447, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0284, 0.1033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0284, 0.1033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0284, 0.1033]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1823754.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2491.3418, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(57810.4297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(558.4927, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-206.1671, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 2.2362],
        [ 2.2742],
        [ 2.2501],
        ...,
        [-9.3078],
        [-9.2983],
        [-9.2972]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-822462.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4006],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(800.6370, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4006],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(800.6370, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.7455e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4108e-03],
        [-6.3849e-03,  1.8304e-03, -1.6108e-02,  ..., -8.6547e-03,
         -4.0650e-07, -1.8802e-03],
        [-6.5209e-03,  1.8608e-03, -1.6631e-02,  ..., -8.9001e-03,
         -5.5220e-07, -6.1475e-04],
        ...,
        [-6.0055e-03,  1.7455e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4108e-03],
        [-6.0055e-03,  1.7455e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4108e-03],
        [-6.0055e-03,  1.7455e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4108e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-100787.2969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.1455, device='cuda:0')



h[100].sum tensor(-0.0021, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(111.4677, device='cuda:0')



h[200].sum tensor(-568.2848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.8909, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0037],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(280010.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2641, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2521, 0.0025],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2719, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0270, 0.1042],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0270, 0.1042],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0270, 0.1042]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1817637., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2389.7986, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(58243.1172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(657.1301, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(17.1562, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 2.4948],
        [ 2.4655],
        [ 2.4238],
        ...,
        [-9.3843],
        [-9.3747],
        [-9.3737]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-841443.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(583.8279, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(583.8279, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-103045.6641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3700, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.0867, device='cuda:0')



h[100].sum tensor(-0.0015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(81.2827, device='cuda:0')



h[200].sum tensor(-567.1741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.5470, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(263533.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0267, 0.1058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0267, 0.1058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0268, 0.1056],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0264, 0.1062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0264, 0.1062],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0264, 0.1062]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1732644.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2237.3945, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(59719.6875, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(767.4352, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(212.9712, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.7907],
        [-8.7636],
        [-8.7060],
        ...,
        [-9.5594],
        [-9.5502],
        [-9.5493]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-970258., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(723.6144, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(723.6144, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2373e-03,  1.7562e-03, -1.5539e-02,  ..., -8.3883e-03,
         -2.2963e-07, -3.3175e-03],
        [-6.0055e-03,  1.7258e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4677e-03],
        [-6.0055e-03,  1.7258e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4677e-03],
        ...,
        [-6.0055e-03,  1.7258e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4677e-03],
        [-6.0055e-03,  1.7258e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4677e-03],
        [-6.0055e-03,  1.7258e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4677e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-102705.7578, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.5721, device='cuda:0')



h[100].sum tensor(-0.0018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(100.7443, device='cuda:0')



h[200].sum tensor(-567.8673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.8609, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(271422.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1098, 0.0542],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0721, 0.0776],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0594, 0.0852],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0254, 0.1072],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0254, 0.1072],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0254, 0.1072]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1784689.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2353.0947, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(59822.3008, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(708.1749, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(58.3202, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-3.5589],
        [-3.9407],
        [-3.3201],
        ...,
        [-9.6312],
        [-9.6227],
        [-9.6221]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-968147.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(537.1479, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(537.1479, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5166e-03,  1.7282e-03, -1.6614e-02,  ..., -8.8922e-03,
         -4.8679e-07, -7.4345e-04],
        [-6.0055e-03,  1.6760e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4815e-03],
        [-6.2415e-03,  1.7001e-03, -1.5555e-02,  ..., -8.3958e-03,
         -2.2477e-07, -3.2938e-03],
        ...,
        [-6.0055e-03,  1.6760e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4815e-03],
        [-6.0055e-03,  1.6760e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4815e-03],
        [-6.0055e-03,  1.6760e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4815e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-105093.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.1848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.9210, device='cuda:0')



h[100].sum tensor(-0.0012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(74.7838, device='cuda:0')



h[200].sum tensor(-566.9182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.1046, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 6.8378e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.0497e-05],
        [0.0000e+00, 6.8212e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.7639e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 6.7042e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.7042e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.7042e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(255675.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3005, 0.0018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2688, 0.0027],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2082, 0.0101],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0254, 0.1068],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0254, 0.1068],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0254, 0.1068]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1717605.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2371.9253, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(61229.0078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(578.9843, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-201.4797, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 2.1655],
        [ 2.1206],
        [ 1.8954],
        ...,
        [-9.5605],
        [-9.5854],
        [-9.5905]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-962292.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(744.9062, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(744.9062, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6918e-03,  1.7536e-03, -1.7289e-02,  ..., -9.2083e-03,
         -6.2838e-07,  8.6423e-04],
        [-6.0055e-03,  1.6996e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4947e-03],
        [-6.0055e-03,  1.6996e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4947e-03],
        ...,
        [-6.0055e-03,  1.6996e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4947e-03],
        [-6.0055e-03,  1.6996e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4947e-03],
        [-6.0055e-03,  1.6996e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4947e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-104140.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.5599, device='cuda:0')



h[100].sum tensor(-0.0017, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(103.7087, device='cuda:0')



h[200].sum tensor(-567.9876, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.9749, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0009],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(272942.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1398, 0.0379],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1143, 0.0514],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0544, 0.0873],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0265, 0.1064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0265, 0.1064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0265, 0.1064]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1800913.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2465.4673, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(58939.8203, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(599.6267, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-173.3134, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.1364],
        [-1.1946],
        [-2.4533],
        ...,
        [-9.6135],
        [-9.6054],
        [-9.6052]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-893491.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(650.2198, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(650.2198, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2633e-03,  1.7341e-03, -1.5639e-02,  ..., -8.4352e-03,
         -2.2692e-07, -3.1185e-03],
        [-6.0055e-03,  1.7196e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5060e-03],
        [-6.0055e-03,  1.7196e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5060e-03],
        ...,
        [-6.0055e-03,  1.7196e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5060e-03],
        [-6.0055e-03,  1.7196e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5060e-03],
        [-6.0055e-03,  1.7196e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5060e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-105658.3125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6173, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.1669, device='cuda:0')



h[100].sum tensor(-0.0014, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(90.5261, device='cuda:0')



h[200].sum tensor(-567.5157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.0207, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 6.9263e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         1.5158e-05],
        [0.0000e+00, 6.8928e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.8783e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 6.8783e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.8783e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.8783e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(265455.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1908, 0.0223],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1165, 0.0498],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0621, 0.0841],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0279, 0.1065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0279, 0.1065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0279, 0.1065]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1769953.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2472.2727, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(58623.7188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(612.3643, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-191.5621, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.5715],
        [-1.6038],
        [-4.1128],
        ...,
        [-9.6545],
        [-9.6455],
        [-9.6446]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-927504.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(641.1031, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(641.1031, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        ...,
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-106378.2188, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5782, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.7440, device='cuda:0')



h[100].sum tensor(-0.0013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.2568, device='cuda:0')



h[200].sum tensor(-567.4618, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.5437, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(266335.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0375, 0.1004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0326, 0.1035],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0305, 0.1048],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0294, 0.1058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0294, 0.1058],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0294, 0.1058]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1771985., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2464.4270, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(57641.2500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(566.8407, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-212.9293, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.8331],
        [-6.9813],
        [-7.6550],
        ...,
        [-9.5597],
        [-9.5503],
        [-9.5494]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-862916.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5352],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(635.9329, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5352],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(635.9329, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.7473e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5101e-03],
        [-6.9009e-03,  1.7835e-03, -1.8093e-02,  ..., -9.5857e-03,
         -7.2794e-07,  2.7846e-03],
        [-7.3010e-03,  1.7996e-03, -1.9633e-02,  ..., -1.0308e-02,
         -1.0532e-06,  6.4909e-03],
        ...,
        [-6.0055e-03,  1.7473e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5101e-03],
        [-6.0055e-03,  1.7473e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5101e-03],
        [-6.0055e-03,  1.7473e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5101e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-106975.5781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5632, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.5041, device='cuda:0')



h[100].sum tensor(-0.0013, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(88.5370, device='cuda:0')



h[200].sum tensor(-567.4412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.2732, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0032],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0138],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0333],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(265922.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3920, 0.0128],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6610, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.9353, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0290, 0.1059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0290, 0.1059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0290, 0.1059]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1762685.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2469.6931, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(56926.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(537.7047, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-216.7436, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.9852],
        [ 1.1828],
        [ 1.1802],
        ...,
        [-9.4577],
        [-9.4515],
        [-9.4554]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-850657.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 520.0 event: 7800 loss: tensor(448.8202, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.4253],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(859.4908, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3232],
        [0.4253],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(859.4908, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3116e-03,  1.8092e-03, -1.5825e-02,  ..., -8.5224e-03,
         -2.3917e-07, -2.6630e-03],
        [-6.4083e-03,  1.8143e-03, -1.6197e-02,  ..., -8.6968e-03,
         -3.1467e-07, -1.7667e-03],
        [-6.3116e-03,  1.8092e-03, -1.5825e-02,  ..., -8.5224e-03,
         -2.3917e-07, -2.6630e-03],
        ...,
        [-6.0055e-03,  1.7931e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5020e-03],
        [-6.0055e-03,  1.7931e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5020e-03],
        [-6.0055e-03,  1.7931e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5020e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-105733.5078, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.3959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(39.8760, device='cuda:0')



h[100].sum tensor(-0.0016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(119.6616, device='cuda:0')



h[200].sum tensor(-568.5917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(44.9702, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(281205.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1741, 0.0229],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2232, 0.0039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2220, 0.0046],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0271, 0.1071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0271, 0.1071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0371, 0.1005]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1829982.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2555.1582, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(55402.1055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(555.1027, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-171.7346, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 0.9318],
        [ 1.9501],
        [ 1.7233],
        ...,
        [-9.2291],
        [-8.3342],
        [-6.6380]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-830174.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(674.7272, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(674.7272, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.8055e-03,  1.8282e-03, -1.7726e-02,  ..., -9.4136e-03,
         -6.0053e-07,  1.9531e-03],
        [-6.0055e-03,  1.7718e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4805e-03],
        [-6.5195e-03,  1.8081e-03, -1.6625e-02,  ..., -8.8974e-03,
         -3.8581e-07, -7.0482e-04],
        ...,
        [-6.0055e-03,  1.7718e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4805e-03],
        [-6.0055e-03,  1.7718e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4805e-03],
        [-6.0055e-03,  1.7718e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4805e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-107852.4297, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7015, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.3039, device='cuda:0')



h[100].sum tensor(-0.0012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.9381, device='cuda:0')



h[200].sum tensor(-567.6322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.3030, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0034],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0042],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(266849.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4174, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3637, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3643, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0249, 0.1090],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0249, 0.1090],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0249, 0.1090]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1758900.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2456.2632, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(57497.7578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(544.4316, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-179.8240, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.5673],
        [ 1.6583],
        [ 1.6967],
        ...,
        [-9.6794],
        [-9.6695],
        [-9.6684]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-921147.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(639.1042, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(639.1042, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        ...,
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-108565.9062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5840, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.6512, device='cuda:0')



h[100].sum tensor(-0.0011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(88.9785, device='cuda:0')



h[200].sum tensor(-567.4698, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.4392, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(266327.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0416, 0.0980],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0321, 0.1039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0366, 0.1006],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0236, 0.1098],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0236, 0.1098],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0236, 0.1098]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1760526.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2453.5315, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(57877.0469, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(528.0244, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-201.6011, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.1166],
        [-6.7395],
        [-6.3398],
        ...,
        [-9.6922],
        [-9.6818],
        [-9.6805]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-873275.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(671.5453, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(671.5453, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-108795.6484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.1563, device='cuda:0')



h[100].sum tensor(-0.0011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.4951, device='cuda:0')



h[200].sum tensor(-567.6309, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.1365, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(269170.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0298, 0.1065],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0252, 0.1096],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0231, 0.1109],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0227, 0.1114],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0227, 0.1114],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0227, 0.1114]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1774906., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2425.8357, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(57426.8945, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(536.4872, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-160.6049, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-6.3652],
        [-6.6249],
        [-6.2260],
        ...,
        [-9.8406],
        [-9.8305],
        [-9.8294]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-987331.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2732],
        [0.2839],
        [0.3171],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(665.2507, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2732],
        [0.2839],
        [0.3171],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(665.2507, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.9259e-03,  1.7884e-03, -1.8189e-02,  ..., -9.6308e-03,
         -6.1240e-07,  3.1745e-03],
        [-6.7608e-03,  1.7662e-03, -1.7554e-02,  ..., -9.3328e-03,
         -5.0253e-07,  1.6322e-03],
        [-6.7568e-03,  1.7657e-03, -1.7538e-02,  ..., -9.3257e-03,
         -4.9990e-07,  1.5953e-03],
        ...,
        [-6.0055e-03,  1.6645e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4222e-03],
        [-6.0055e-03,  1.6645e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4222e-03],
        [-6.0055e-03,  1.6645e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4222e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109146.2656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.8643, device='cuda:0')



h[100].sum tensor(-0.0011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.6187, device='cuda:0')



h[200].sum tensor(-567.5942, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.8072, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0125],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0132],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0146],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268488.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5975, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6378, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6620, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0235, 0.1110],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0235, 0.1110],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0235, 0.1110]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1763444.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2447.9131, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(55742.0234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(511.6785, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-178.8182, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.2972],
        [ 1.2183],
        [ 1.1796],
        ...,
        [-9.8530],
        [-9.8430],
        [-9.8420]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-933158.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6079],
        [0.6045],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(619.9705, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6079],
        [0.6045],
        [0.6064],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(619.9705, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.1430e-03,  1.9962e-03, -2.2873e-02,  ..., -1.1827e-02,
         -1.3659e-06,  1.4605e-02],
        [-7.6495e-03,  1.9176e-03, -2.0974e-02,  ..., -1.0937e-02,
         -1.0505e-06,  9.9876e-03],
        [-7.1806e-03,  1.8428e-03, -1.9169e-02,  ..., -1.0090e-02,
         -7.5090e-07,  5.5994e-03],
        ...,
        [-6.0055e-03,  1.6555e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3971e-03],
        [-6.0055e-03,  1.6555e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3971e-03],
        [-6.0055e-03,  1.6555e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3971e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109615.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.7635, device='cuda:0')



h[100].sum tensor(-0.0009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(86.3146, device='cuda:0')



h[200].sum tensor(-567.3442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.4380, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0290],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0346],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0207],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(266135.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6773, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7146, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6296, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0250, 0.1087],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0250, 0.1087],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0250, 0.1087]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1752961.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2495.5181, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(52909.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(424.7385, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-257.1498, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.3742],
        [ 1.3418],
        [ 1.3880],
        ...,
        [-9.6798],
        [-9.6701],
        [-9.6692]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-761141.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(575.0438, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(575.0438, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-110142.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3377, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.6791, device='cuda:0')



h[100].sum tensor(-0.0008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(80.0598, device='cuda:0')



h[200].sum tensor(-567.1295, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.0874, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264776.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0266, 0.1077],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0266, 0.1077],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0267, 0.1075],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0263, 0.1081],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0263, 0.1081],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0263, 0.1081]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1739142.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2465.0164, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(51385.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(381.0326, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-261.8456, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.3650],
        [-8.3867],
        [-8.2587],
        ...,
        [-9.6241],
        [-9.6196],
        [-9.6204]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-754380.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(599.7601, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(599.7601, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-110267.9062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4185, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.8258, device='cuda:0')



h[100].sum tensor(-0.0008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(83.5009, device='cuda:0')



h[200].sum tensor(-567.2411, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.3806, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(266225.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0300, 0.1071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0430, 0.0988],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0790, 0.0756],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0267, 0.1095],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0267, 0.1095],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0267, 0.1095]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1743768.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2455.8638, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(50671.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(384.8754, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-247.7922, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.9541],
        [-6.9914],
        [-5.2967],
        ...,
        [-9.7290],
        [-9.7192],
        [-9.7182]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-775011.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(632.7098, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(632.7098, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-110333.1562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5449, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.3545, device='cuda:0')



h[100].sum tensor(-0.0009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(88.0883, device='cuda:0')



h[200].sum tensor(-567.4158, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.1046, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268071.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0265, 0.1114],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0265, 0.1114],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0267, 0.1112],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0262, 0.1118],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0262, 0.1118],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0262, 0.1118]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1758694.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2446.1548, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(51157.6406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(419.2944, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-230.2556, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.5585],
        [-8.6778],
        [-8.7421],
        ...,
        [-9.8536],
        [-9.8435],
        [-9.8425]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-818749.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(480.3881, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(480.3881, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111882.6562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-613.9783, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.2876, device='cuda:0')



h[100].sum tensor(-0.0006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(66.8814, device='cuda:0')



h[200].sum tensor(-566.6328, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.1348, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(256573.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0261, 0.1133],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0265, 0.1130],
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0280, 0.1120],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0258, 0.1137],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0258, 0.1137],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0258, 0.1137]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1710719.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2382.2256, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(52975.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(441.8276, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-238.7869, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-8.6462],
        [-8.3654],
        [-7.8810],
        ...,
        [-9.0995],
        [-9.6152],
        [-9.8545]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-892503.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 530.0 event: 7950 loss: tensor(448.0076, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.5869],
        [0.6304],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(703.5680, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.5869],
        [0.6304],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(703.5680, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.8932e-03,  1.7809e-03, -1.8063e-02,  ..., -9.5718e-03,
         -4.6267e-07,  3.0511e-03],
        [-6.5614e-03,  1.7137e-03, -1.6786e-02,  ..., -8.9730e-03,
         -2.8973e-07, -7.6916e-05],
        [-6.6025e-03,  1.7221e-03, -1.6944e-02,  ..., -9.0473e-03,
         -3.1118e-07,  3.1109e-04],
        ...,
        [-6.0055e-03,  1.6011e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3172e-03],
        [-6.0055e-03,  1.6011e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3172e-03],
        [-6.0055e-03,  1.6011e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3172e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-110332.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.6420, device='cuda:0')



h[100].sum tensor(-0.0009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(97.9534, device='cuda:0')



h[200].sum tensor(-567.7693, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.8120, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0117],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0160],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0056],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(272169.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5013, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5293, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4689, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0255, 0.1150],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0255, 0.1150],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0255, 0.1150]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1785953.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2439.5493, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(53174.5391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(499.8774, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-182.9500, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.4377],
        [  1.4133],
        [  1.4064],
        ...,
        [-10.0359],
        [-10.0257],
        [-10.0247]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-877899.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(715.4167, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(715.4167, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3940e-03,  1.6913e-03, -1.6142e-02,  ..., -8.6709e-03,
         -1.9432e-07, -1.6393e-03],
        [-6.0055e-03,  1.6073e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3061e-03],
        [-6.0055e-03,  1.6073e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3061e-03],
        ...,
        [-6.0055e-03,  1.6073e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3061e-03],
        [-6.0055e-03,  1.6073e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3061e-03],
        [-6.0055e-03,  1.6073e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3061e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-110365.4141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8521, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.1917, device='cuda:0')



h[100].sum tensor(-0.0008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(99.6030, device='cuda:0')



h[200].sum tensor(-567.8402, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.4320, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(272207.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2122, 0.0213],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1613, 0.0315],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1284, 0.0513],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0260, 0.1149],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0260, 0.1149],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0260, 0.1149]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1784968.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2462.0154, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(53646.2617, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(509.3591, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-181.5729, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.5087],
        [  1.2147],
        [  0.8705],
        ...,
        [-10.0746],
        [-10.0646],
        [-10.0636]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-883205.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2462],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(765.1541, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2462],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(765.1541, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5475e-03,  1.7219e-03, -1.6732e-02,  ..., -8.9480e-03,
         -2.6017e-07, -1.7206e-04],
        [-6.2387e-03,  1.6514e-03, -1.5544e-02,  ..., -8.3908e-03,
         -1.1194e-07, -3.0907e-03],
        [-6.0055e-03,  1.5981e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2948e-03],
        ...,
        [-6.0055e-03,  1.5981e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2948e-03],
        [-6.0055e-03,  1.5981e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2948e-03],
        [-6.0055e-03,  1.5981e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2948e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-110090.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.4993, device='cuda:0')



h[100].sum tensor(-0.0009, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(106.5276, device='cuda:0')



h[200].sum tensor(-568.0733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.0343, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0025],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(274654.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2736, 0.0075],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1594, 0.0380],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0854, 0.0746],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0269, 0.1142],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0269, 0.1142],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0269, 0.1142]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1793711.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2450.2451, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(54078.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(499.4657, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-187.5282, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.8379],
        [ -1.2854],
        [ -3.7450],
        ...,
        [-10.0778],
        [-10.0681],
        [-10.0672]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-861669.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4011],
        [0.3823],
        [0.4502],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(593.4536, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4011],
        [0.3823],
        [0.4502],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(593.4536, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.8046e-03,  1.7406e-03, -1.7722e-02,  ..., -9.4120e-03,
         -3.6807e-07,  2.2804e-03],
        [-6.8118e-03,  1.7423e-03, -1.7749e-02,  ..., -9.4249e-03,
         -3.7137e-07,  2.3482e-03],
        [-6.3676e-03,  1.6385e-03, -1.6040e-02,  ..., -8.6234e-03,
         -1.6678e-07, -1.8560e-03],
        ...,
        [-6.0055e-03,  1.5538e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2832e-03],
        [-6.0055e-03,  1.5538e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2832e-03],
        [-6.0055e-03,  1.5538e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2832e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111585.3594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3815, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5333, device='cuda:0')



h[100].sum tensor(-0.0006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(82.6229, device='cuda:0')



h[200].sum tensor(-567.1900, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0506, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0038],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0035],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0032],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(260651.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.1395e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.1681e-01,
         0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 2.8071e-01,
         2.1607e-05],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 2.7353e-02,
         1.1391e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 2.7353e-02,
         1.1391e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 2.7353e-02,
         1.1391e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1736953.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2428.0718, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(55266.7109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(487.8912, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-251.3875, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.1322],
        [  2.3802],
        [  2.1901],
        ...,
        [-10.1086],
        [-10.0328],
        [ -9.7309]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-849900.4375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4116],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(803.1136, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4116],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(803.1136, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.7865e-03,  1.6619e-03, -1.7652e-02,  ..., -9.3793e-03,
         -3.4513e-07,  2.1067e-03],
        [-6.3954e-03,  1.5787e-03, -1.6147e-02,  ..., -8.6734e-03,
         -1.7227e-07, -1.5974e-03],
        [-6.4150e-03,  1.5829e-03, -1.6223e-02,  ..., -8.7089e-03,
         -1.8095e-07, -1.4113e-03],
        ...,
        [-6.0055e-03,  1.4957e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2889e-03],
        [-6.0055e-03,  1.4957e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2889e-03],
        [-6.0055e-03,  1.4957e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2889e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-109982.4766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.2604, device='cuda:0')



h[100].sum tensor(-0.0008, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(111.8125, device='cuda:0')



h[200].sum tensor(-568.3025, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.0204, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0121],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0047],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(277707.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5412, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4757, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4546, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0265, 0.1153],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0265, 0.1153],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0265, 0.1153]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1820348.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2479.5308, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(55358.2188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(560.8998, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-175.0369, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.2119],
        [  1.1790],
        [  1.1387],
        ...,
        [-10.2799],
        [-10.2707],
        [-10.2568]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-917731.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(560.9187, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(560.9187, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.5646e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2905e-03],
        [-6.2205e-03,  1.6085e-03, -1.5474e-02,  ..., -8.3580e-03,
         -9.1153e-08, -3.2530e-03],
        [-6.2205e-03,  1.6085e-03, -1.5474e-02,  ..., -8.3580e-03,
         -9.1153e-08, -3.2530e-03],
        ...,
        [-6.0055e-03,  1.5646e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2905e-03],
        [-6.0055e-03,  1.5646e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2905e-03],
        [-6.0055e-03,  1.5646e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2905e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111964.1641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2837, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0238, device='cuda:0')



h[100].sum tensor(-0.0006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(78.0932, device='cuda:0')



h[200].sum tensor(-567.0548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.3483, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(259495.8906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1435, 0.0417],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1724, 0.0241],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1551, 0.0312],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0264, 0.1149],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0264, 0.1149],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0264, 0.1149]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1727453.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2393.4185, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(55729.4961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(486.0624, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-228.2833, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-5.0049e-01],
        [ 5.7990e-01],
        [ 4.0818e-03],
        ...,
        [-1.0216e+01],
        [-1.0208e+01],
        [-1.0208e+01]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-932491., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(587.1412, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(587.1412, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111875.6562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3852, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.2404, device='cuda:0')



h[100].sum tensor(-0.0006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(81.7440, device='cuda:0')



h[200].sum tensor(-567.1951, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.7204, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(262193.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0305, 0.1116],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0374, 0.1069],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0592, 0.0922],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0257, 0.1148],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0257, 0.1148],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0257, 0.1148]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1735505.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2418.5518, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(54585.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(462.1333, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-213.9821, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.6573],
        [ -6.6002],
        [ -4.9948],
        ...,
        [-10.2060],
        [-10.1983],
        [-10.1982]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-927159.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6479],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(664.1135, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6479],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(664.1135, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.6363e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2935e-03],
        [-6.6192e-03,  1.7565e-03, -1.7008e-02,  ..., -9.0773e-03,
         -2.4951e-07,  5.2479e-04],
        [-6.0055e-03,  1.6363e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2935e-03],
        ...,
        [-6.0055e-03,  1.6363e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2935e-03],
        [-6.0055e-03,  1.6363e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2935e-03],
        [-6.0055e-03,  1.6363e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2935e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111275.5781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.8115, device='cuda:0')



h[100].sum tensor(-0.0006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.4604, device='cuda:0')



h[200].sum tensor(-567.5707, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.7477, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0050],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0005],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(266448.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2855, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1718, 0.0312],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1213, 0.0556],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0257, 0.1148],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0257, 0.1148],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0257, 0.1148]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1748997.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2429.9744, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(54333.9688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(467.6409, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-202.8289, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.9048],
        [  0.5873],
        [ -1.6252],
        ...,
        [-10.2060],
        [-10.1983],
        [-10.1982]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-907318.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(654.2939, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(654.2939, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111449.1562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6182, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.3559, device='cuda:0')



h[100].sum tensor(-0.0006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.0933, device='cuda:0')



h[200].sum tensor(-567.5170, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.2339, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(266623.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0278, 0.1128],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0447, 0.1019],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0868, 0.0747],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0248, 0.1150],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0248, 0.1150],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0248, 0.1150]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1740870.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2421.3062, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(53833.7188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(452.0768, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-171.5729, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.5915],
        [ -6.0072],
        [ -3.6256],
        ...,
        [-10.2037],
        [-10.1961],
        [-10.1960]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-950140.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2446],
        [0.2413],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(620.4393, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2446],
        [0.2413],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(620.4393, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.0535e-03,  1.9087e-03, -1.8679e-02,  ..., -9.8611e-03,
         -4.0867e-07,  4.6516e-03],
        [-6.7691e-03,  1.8538e-03, -1.7585e-02,  ..., -9.3479e-03,
         -2.9776e-07,  1.9527e-03],
        [-6.4819e-03,  1.7983e-03, -1.6480e-02,  ..., -8.8296e-03,
         -1.8575e-07, -7.7288e-04],
        ...,
        [-6.0055e-03,  1.7064e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2930e-03],
        [-6.0055e-03,  1.7064e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2930e-03],
        [-6.0055e-03,  1.7064e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2930e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111699.4844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5048, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.7853, device='cuda:0')



h[100].sum tensor(-0.0006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(86.3799, device='cuda:0')



h[200].sum tensor(-567.3604, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.4626, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0032],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0053],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0025],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264689.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4188, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3911, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2988, 0.0054],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0248, 0.1150],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0248, 0.1150],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0248, 0.1150]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1738477.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2441.1882, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(53789.6367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(451.9086, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-187.7466, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.1684],
        [  2.2030],
        [  2.1727],
        ...,
        [-10.2037],
        [-10.1961],
        [-10.1960]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-936645.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 540.0 event: 8100 loss: tensor(364.9723, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2759],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(603.2545, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2759],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(603.2545, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2668e-03,  1.7519e-03, -1.5652e-02,  ..., -8.4414e-03,
         -9.7699e-08, -2.8120e-03],
        [-6.0055e-03,  1.7039e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2931e-03],
        [-6.5262e-03,  1.7996e-03, -1.6650e-02,  ..., -8.9095e-03,
         -1.9470e-07, -3.4884e-04],
        ...,
        [-6.0055e-03,  1.7039e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2931e-03],
        [-6.0055e-03,  1.7039e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2931e-03],
        [-6.0055e-03,  1.7039e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2931e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111950.5938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4250, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.9880, device='cuda:0')



h[100].sum tensor(-0.0005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(83.9874, device='cuda:0')



h[200].sum tensor(-567.2501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.5634, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(265179.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1178, 0.0530],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2156, 0.0140],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2638, 0.0092],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0238, 0.1151],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0238, 0.1151],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0238, 0.1151]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1740453., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2414.3896, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(53676.2500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(426.4675, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-181.6359, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.7723],
        [  1.2554],
        [  2.2885],
        ...,
        [-10.1836],
        [-10.1761],
        [-10.1760]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-933436.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3740],
        [0.4832],
        [0.4106],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(593.3538, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3740],
        [0.4832],
        [0.4106],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(593.3538, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.0080e-03,  1.8330e-03, -1.8504e-02,  ..., -9.7789e-03,
         -3.5940e-07,  4.2249e-03],
        [-6.9399e-03,  1.8218e-03, -1.8242e-02,  ..., -9.6561e-03,
         -3.3500e-07,  3.5785e-03],
        [-6.9521e-03,  1.8238e-03, -1.8289e-02,  ..., -9.6780e-03,
         -3.3936e-07,  3.6941e-03],
        ...,
        [-6.0055e-03,  1.6669e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2972e-03],
        [-6.0055e-03,  1.6669e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2972e-03],
        [-6.0055e-03,  1.6669e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2972e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112034.0781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3847, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5286, device='cuda:0')



h[100].sum tensor(-0.0005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(82.6090, device='cuda:0')



h[200].sum tensor(-567.1943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0454, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0197],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0196],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0165],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(263512.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6053, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6185, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5905, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0247, 0.1140],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0281, 0.1117],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0304, 0.1101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1724812.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2416.5027, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(53015.9844, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(393.7834, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-199.3661, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.2945],
        [ 1.2577],
        [ 1.2951],
        ...,
        [-8.9993],
        [-8.5963],
        [-8.4779]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-894050.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(701.5530, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(701.5530, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111269.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.5485, device='cuda:0')



h[100].sum tensor(-0.0006, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(97.6729, device='cuda:0')



h[200].sum tensor(-567.7501, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.7066, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(271254.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0239, 0.1153],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0239, 0.1153],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0240, 0.1151],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0236, 0.1157],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0236, 0.1157],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0236, 0.1157]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1760523., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2495.9819, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(51992.9922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(422.9272, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-164.1878, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.8907],
        [ -8.2414],
        [ -8.4009],
        ...,
        [-10.0739],
        [ -9.6334],
        [ -8.6867]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-896778., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(587.9990, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(587.9990, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112209.1016, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.2802, device='cuda:0')



h[100].sum tensor(-0.0004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(81.8634, device='cuda:0')



h[200].sum tensor(-567.1917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.7652, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264559.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0660, 0.0878],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0409, 0.1039],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0620, 0.0901],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0241, 0.1152],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0241, 0.1152],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0241, 0.1152]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1732801.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2489.7671, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(51899.2266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(408.7344, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-181.3082, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -2.8949],
        [ -4.0430],
        [ -3.2569],
        ...,
        [-10.2308],
        [-10.2234],
        [-10.2235]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-902870.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(662.9185, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(662.9185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2451e-03,  1.5933e-03, -1.5568e-02,  ..., -8.4022e-03,
         -7.5646e-08, -3.0525e-03],
        [-6.0055e-03,  1.5701e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3275e-03],
        [-6.0055e-03,  1.5701e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3275e-03],
        ...,
        [-6.0055e-03,  1.5701e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3275e-03],
        [-6.0055e-03,  1.5701e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3275e-03],
        [-6.0055e-03,  1.5701e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3275e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111662.9844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6555, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.7561, device='cuda:0')



h[100].sum tensor(-0.0005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.2940, device='cuda:0')



h[200].sum tensor(-567.5687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.6852, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(270446.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1267, 0.0509],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0908, 0.0720],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0516, 0.0971],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0241, 0.1149],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0241, 0.1149],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0241, 0.1149]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1758663.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2517.3120, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(51488.8359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(402.8818, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-187.4637, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.7364],
        [ -2.7509],
        [ -4.5976],
        ...,
        [-10.2236],
        [-10.2165],
        [-10.2166]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-856067.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4956],
        [0.3542],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(600.4810, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4956],
        [0.3542],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(600.4810, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.7285e-03,  1.5847e-03, -1.7428e-02,  ..., -9.2747e-03,
         -2.1881e-07,  1.5437e-03],
        [-6.3410e-03,  1.5514e-03, -1.5938e-02,  ..., -8.5754e-03,
         -1.0153e-07, -2.1388e-03],
        [-6.7785e-03,  1.5890e-03, -1.7620e-02,  ..., -9.3648e-03,
         -2.3393e-07,  2.0183e-03],
        ...,
        [-6.0055e-03,  1.5226e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3270e-03],
        [-6.0055e-03,  1.5226e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3270e-03],
        [-6.0055e-03,  1.5226e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3270e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112242.0703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4290, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.8593, device='cuda:0')



h[100].sum tensor(-0.0004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(83.6012, device='cuda:0')



h[200].sum tensor(-567.2557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.4183, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0045],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0009],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(263940.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3079, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3244, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2726, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0239, 0.1149],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0239, 0.1149],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0239, 0.1149]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1722957.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2495.8843, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(52128.4570, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(387.2615, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-228.2031, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.6014],
        [  2.5766],
        [  2.3061],
        ...,
        [-10.2392],
        [-10.2284],
        [-10.1585]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-841361.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(498.6869, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(498.6869, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113073.6406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.0527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.1366, device='cuda:0')



h[100].sum tensor(-0.0003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(69.4291, device='cuda:0')



h[200].sum tensor(-566.7356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.0923, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(257604.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0285, 0.1117],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0506, 0.0975],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1123, 0.0574],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0239, 0.1149],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0239, 0.1149],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0239, 0.1149]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1698198.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2467.1531, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(52536.7344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(379.7363, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-243.2119, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.1723],
        [ -5.0513],
        [ -2.1423],
        ...,
        [-10.2534],
        [-10.2463],
        [-10.2465]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-868274.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(588.6569, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(588.6569, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.5325e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3296e-03],
        [-6.0055e-03,  1.5325e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3296e-03],
        [-6.5164e-03,  1.5741e-03, -1.6612e-02,  ..., -8.8919e-03,
         -1.4816e-07, -4.7239e-04],
        ...,
        [-6.0055e-03,  1.5325e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3296e-03],
        [-6.0055e-03,  1.5325e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3296e-03],
        [-6.0055e-03,  1.5325e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3296e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112431.8516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3820, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.3107, device='cuda:0')



h[100].sum tensor(-0.0004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(81.9550, device='cuda:0')



h[200].sum tensor(-567.1906, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.7997, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0061, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(263810.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0680, 0.0862],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1440, 0.0396],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1879, 0.0200],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0239, 0.1148],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0239, 0.1148],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0239, 0.1148]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1727515.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2496.4912, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(52062.5859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(383.6647, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-228.3237, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -3.6028],
        [ -0.8416],
        [  1.0771],
        ...,
        [-10.3333],
        [-10.3260],
        [-10.3261]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-880685.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(700.0138, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(700.0138, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111537.0781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.4771, device='cuda:0')



h[100].sum tensor(-0.0004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(97.4586, device='cuda:0')



h[200].sum tensor(-567.7738, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.6261, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(272675.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0595, 0.0913],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0321, 0.1090],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0265, 0.1125],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0240, 0.1145],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0240, 0.1145],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0240, 0.1145]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1775163.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2547.3108, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(51398.2305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(390.7410, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-216.2634, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -3.8108],
        [ -6.1724],
        [ -7.5716],
        ...,
        [-10.4079],
        [-10.4007],
        [-10.4008]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-865675.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(590.5974, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(590.5974, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.8597e-03,  1.6447e-03, -1.7932e-02,  ..., -9.5113e-03,
         -2.2733e-07,  2.8116e-03],
        [-6.5919e-03,  1.6182e-03, -1.6903e-02,  ..., -9.0281e-03,
         -1.5607e-07,  2.6175e-04],
        [-6.5417e-03,  1.6132e-03, -1.6709e-02,  ..., -8.9375e-03,
         -1.4270e-07, -2.1649e-04],
        ...,
        [-6.0055e-03,  1.5601e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3225e-03],
        [-6.0055e-03,  1.5601e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3225e-03],
        [-6.0055e-03,  1.5601e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3225e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112468.1484, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3965, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.4007, device='cuda:0')



h[100].sum tensor(-0.0004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(82.2252, device='cuda:0')



h[200].sum tensor(-567.2107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.9012, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0003],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0028],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0003],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(265402.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3130, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3395, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2759, 0.0039],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0235, 0.1145],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0235, 0.1145],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0235, 0.1145]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1747919.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2499.6445, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(52487.3984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(378.8545, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-250.2820, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.5157],
        [  2.5432],
        [  1.7247],
        ...,
        [-10.4523],
        [-10.4449],
        [-10.4450]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-900720.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 550.0 event: 8250 loss: tensor(394.0351, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2654],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(593.8440, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2654],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(593.8440, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.5519e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3170e-03],
        [-6.2568e-03,  1.5791e-03, -1.5614e-02,  ..., -8.4235e-03,
         -6.4075e-08, -2.9211e-03],
        [-6.0055e-03,  1.5519e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3170e-03],
        ...,
        [-6.0055e-03,  1.5519e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3170e-03],
        [-6.0055e-03,  1.5519e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3170e-03],
        [-6.0055e-03,  1.5519e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3170e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112461.9766, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3943, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5514, device='cuda:0')



h[100].sum tensor(-0.0003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(82.6772, device='cuda:0')



h[200].sum tensor(-567.2076, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0711, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0017],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0023],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0015],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(265783.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4288, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3918, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3470, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0234, 0.1140],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0234, 0.1140],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0234, 0.1140]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1754419.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2493.3491, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(53191.7109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(381.5616, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-270.2242, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.3183],
        [  1.3713],
        [  1.3983],
        ...,
        [-10.4482],
        [-10.4409],
        [-10.4410]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-877814.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(631.7926, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(631.7926, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112104.2812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5496, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.3120, device='cuda:0')



h[100].sum tensor(-0.0003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.9606, device='cuda:0')



h[200].sum tensor(-567.4222, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.0566, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 6.2960e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         8.9184e-05],
        [0.0000e+00, 6.3573e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         1.7837e-04],
        [0.0000e+00, 6.2960e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         8.9184e-05],
        ...,
        [0.0000e+00, 6.2346e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.2346e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.2346e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(269174.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1325, 0.0487],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1760, 0.0257],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1324, 0.0487],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0227, 0.1144],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0227, 0.1144],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0227, 0.1144]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1768749.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2502.5410, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(53945.4531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(426.0524, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-222.5675, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.7875],
        [  0.9665],
        [ -0.2807],
        ...,
        [-10.5272],
        [-10.5197],
        [-10.5197]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-962472.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(662.1312, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(662.1312, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111868.2500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6533, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.7196, device='cuda:0')



h[100].sum tensor(-0.0003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.1844, device='cuda:0')



h[200].sum tensor(-567.5657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.6440, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(269286.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1475, 0.0441],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1005, 0.0652],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1047, 0.0629],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0232, 0.1146],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0232, 0.1146],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0232, 0.1146]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1767513.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2525.3906, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(53752.2383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(447.8154, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-227.0422, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.0416],
        [  1.8640],
        [  1.6845],
        ...,
        [-10.5262],
        [-10.5191],
        [-10.5198]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-938753.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2837],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(667.3877, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2837],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(667.3877, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.5138e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3089e-03],
        [-6.5026e-03,  1.5538e-03, -1.6559e-02,  ..., -8.8670e-03,
         -1.1128e-07, -5.5889e-04],
        [-6.2339e-03,  1.5322e-03, -1.5526e-02,  ..., -8.3822e-03,
         -5.1138e-08, -3.1262e-03],
        ...,
        [-6.0055e-03,  1.5138e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3089e-03],
        [-6.0055e-03,  1.5138e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3089e-03],
        [-6.0055e-03,  1.5138e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3089e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111818.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6609, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.9634, device='cuda:0')



h[100].sum tensor(-0.0003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.9162, device='cuda:0')



h[200].sum tensor(-567.5761, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.9190, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 6.1102e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.1187e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.1920e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         4.4927e-05],
        ...,
        [0.0000e+00, 6.0552e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.0552e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.0552e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(272217.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1529, 0.0362],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1952, 0.0195],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2393, 0.0065],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0243, 0.1143],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0243, 0.1143],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0243, 0.1143]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1795714.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2547.2812, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(53435.6328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(459.3319, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-228.3502, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.0900],
        [  1.5594],
        [  1.9631],
        ...,
        [-10.4510],
        [-10.4406],
        [-10.4363]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-923884.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(541.2347, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(541.2347, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112711.6562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.1106, device='cuda:0')



h[100].sum tensor(-0.0003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(75.3527, device='cuda:0')



h[200].sum tensor(-566.9448, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.3184, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(260933.3594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0255, 0.1134],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0255, 0.1134],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0256, 0.1132],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0252, 0.1138],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0252, 0.1138],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0252, 0.1138]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1723645.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2504.0491, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(53970.1953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(449.9282, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-252.2163, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.0835],
        [ -8.8623],
        [ -8.4320],
        ...,
        [-10.5142],
        [-10.5072],
        [-10.5075]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-976877.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2849],
        ...,
        [0.0000],
        [0.3513],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(572.4608, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2849],
        ...,
        [0.0000],
        [0.3513],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(572.4608, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.4772e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2947e-03],
        [-6.2753e-03,  1.4980e-03, -1.5685e-02,  ..., -8.4569e-03,
         -5.5354e-08, -2.7107e-03],
        [-6.2094e-03,  1.4929e-03, -1.5431e-02,  ..., -8.3379e-03,
         -4.1827e-08, -3.3422e-03],
        ...,
        [-6.5627e-03,  1.5202e-03, -1.6790e-02,  ..., -8.9754e-03,
         -1.1430e-07,  4.1122e-05],
        [-6.2300e-03,  1.4945e-03, -1.5510e-02,  ..., -8.3750e-03,
         -4.6049e-08, -3.1451e-03],
        [-6.3382e-03,  1.5029e-03, -1.5927e-02,  ..., -8.5704e-03,
         -6.8256e-08, -2.1084e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112288.6016, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.5593, device='cuda:0')



h[100].sum tensor(-0.0003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(79.7002, device='cuda:0')



h[200].sum tensor(-567.1166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.9522, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 5.9297e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 5.9544e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.0282e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 5.9612e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 6.0335e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         4.1122e-05],
        [0.0000e+00, 5.9612e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264969.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1669, 0.0243],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1868, 0.0192],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2243, 0.0091],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1843, 0.0211],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2247, 0.0078],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1638, 0.0360]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1745958.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2578.0249, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(52828.3438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(437.2833, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-282.9007, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 2.7779],
        [ 2.7663],
        [ 2.5821],
        ...,
        [ 0.7737],
        [ 1.5684],
        [-0.2324]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-887541.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4077],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(585.3773, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4077],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(585.3773, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3917e-03,  1.4927e-03, -1.6132e-02,  ..., -8.6668e-03,
         -7.5814e-08, -1.5858e-03],
        [-6.0055e-03,  1.4618e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2873e-03],
        [-6.6754e-03,  1.5154e-03, -1.7223e-02,  ..., -9.1787e-03,
         -1.3152e-07,  1.1339e-03],
        ...,
        [-6.0055e-03,  1.4618e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2873e-03],
        [-6.0055e-03,  1.4618e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2873e-03],
        [-6.0055e-03,  1.4618e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2873e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112115.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.1586, device='cuda:0')



h[100].sum tensor(-0.0003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(81.4984, device='cuda:0')



h[200].sum tensor(-567.1691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.6281, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0059, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268356.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1235, 0.0531],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2301, 0.0125],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2742, 0.0097],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0275, 0.1118],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0275, 0.1118],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0275, 0.1118]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1769639.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2607.0894, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(52184.8633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(429.8500, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-295.8855, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.6103],
        [  1.4770],
        [  2.4065],
        ...,
        [-10.4775],
        [-10.4690],
        [-10.4687]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-842220.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(574.8933, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(574.8933, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0015, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112118.1719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.6722, device='cuda:0')



h[100].sum tensor(-0.0002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(80.0388, device='cuda:0')



h[200].sum tensor(-567.1252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.0795, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0060, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(266103.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0269, 0.1123],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0300, 0.1103],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0431, 0.1020],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0266, 0.1127],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0266, 0.1127],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0266, 0.1127]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1745420.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2581.2998, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(51946.7578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(427.3821, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-278.7676, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.7975],
        [ -6.7606],
        [ -5.0363],
        ...,
        [-10.6307],
        [-10.6222],
        [-10.6219]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-923420.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2401],
        [0.2761],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(508.7311, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2401],
        [0.2761],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(508.7311, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.0176e-03,  1.7420e-03, -1.8539e-02,  ..., -9.7962e-03,
         -1.8194e-07,  4.4292e-03],
        [-6.5551e-03,  1.7024e-03, -1.6761e-02,  ..., -8.9617e-03,
         -9.8805e-08, -9.6271e-06],
        [-6.5557e-03,  1.7024e-03, -1.6763e-02,  ..., -8.9628e-03,
         -9.8910e-08, -4.0201e-06],
        ...,
        [-6.0055e-03,  1.6554e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2852e-03],
        [-6.0055e-03,  1.6554e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2852e-03],
        [-6.0055e-03,  1.6554e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2852e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112647.1562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.0800, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.6026, device='cuda:0')



h[100].sum tensor(-0.0002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(70.8275, device='cuda:0')



h[200].sum tensor(-566.7733, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.6178, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0052],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0008],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(261290.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3991, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3771, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3009, 0.0019],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0263, 0.1135],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0263, 0.1135],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0263, 0.1135]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1721225., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2548.8174, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(50034.6250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(349.2993, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-318.6292, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.4757],
        [  2.5114],
        [  2.2799],
        ...,
        [-10.5975],
        [-10.5878],
        [-10.5867]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-882150.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2524],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(718.6469, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2524],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(718.6469, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.7845e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2862e-03],
        [-6.2446e-03,  1.8058e-03, -1.5566e-02,  ..., -8.4014e-03,
         -4.1119e-08, -2.9901e-03],
        [-6.2536e-03,  1.8066e-03, -1.5601e-02,  ..., -8.4176e-03,
         -4.2665e-08, -2.9037e-03],
        ...,
        [-6.0055e-03,  1.7845e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2862e-03],
        [-6.0055e-03,  1.7845e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2862e-03],
        [-6.0055e-03,  1.7845e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2862e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-110970.0391, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.3416, device='cuda:0')



h[100].sum tensor(-0.0003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(100.0527, device='cuda:0')



h[200].sum tensor(-567.8601, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.6010, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 7.1594e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.2091e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         1.2737e-05],
        [0.0000e+00, 7.2825e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         1.2737e-05],
        ...,
        [0.0000e+00, 7.1381e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.1381e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.1381e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(277386.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1185, 0.0572],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1710, 0.0348],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2291, 0.0078],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0260, 0.1149],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0260, 0.1149],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0260, 0.1149]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1795045.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2593.3191, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(47235.7383, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(317.1768, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-304.5087, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.3374],
        [  2.4239],
        [  2.3413],
        ...,
        [-10.6778],
        [-10.6686],
        [-10.6681]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-797863.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 560.0 event: 8400 loss: tensor(442.2625, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8057],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(617.1600, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.8057],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(617.1600, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.9767e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3001e-03],
        [-6.7685e-03,  2.0421e-03, -1.7581e-02,  ..., -9.3469e-03,
         -1.2553e-07,  2.0267e-03],
        [-6.0055e-03,  1.9767e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3001e-03],
        ...,
        [-6.0055e-03,  1.9767e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3001e-03],
        [-6.0055e-03,  1.9767e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3001e-03],
        [-6.0055e-03,  1.9767e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3001e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111995.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4893, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.6331, device='cuda:0')



h[100].sum tensor(-0.0002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(85.9234, device='cuda:0')



h[200].sum tensor(-567.3388, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.2910, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0094],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0007],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(267520.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3189, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1894, 0.0298],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1667, 0.0323],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0248, 0.1168],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0248, 0.1168],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0248, 0.1168]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1752051.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2499.9561, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(47689.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(275.4825, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-305.2182, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.2853],
        [  2.2894],
        [  2.2814],
        ...,
        [-10.9786],
        [-10.9703],
        [-10.9702]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-962015.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(659.2135, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(659.2135, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111868.6172, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6476, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.5842, device='cuda:0')



h[100].sum tensor(-0.0002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.7782, device='cuda:0')



h[200].sum tensor(-567.5577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.4913, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(271620.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0426, 0.1068],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0273, 0.1159],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0465, 0.1040],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0244, 0.1179],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0244, 0.1179],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0244, 0.1179]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1781350.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2486.3916, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(47314.7266, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(252.6316, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-298.1812, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.2229],
        [ -6.9494],
        [ -5.9938],
        ...,
        [-11.1946],
        [-11.1860],
        [-11.1859]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-990716.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2974],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(783.8651, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2974],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(783.8651, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.4150e-03,  2.1274e-03, -1.6222e-02,  ..., -8.7089e-03,
         -6.1616e-08, -1.3977e-03],
        [-6.5107e-03,  2.1330e-03, -1.6589e-02,  ..., -8.8815e-03,
         -7.6010e-08, -4.7956e-04],
        [-6.8706e-03,  2.1540e-03, -1.7973e-02,  ..., -9.5309e-03,
         -1.3016e-07,  2.9745e-03],
        ...,
        [-6.0055e-03,  2.1035e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3279e-03],
        [-6.0055e-03,  2.1035e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3279e-03],
        [-6.0055e-03,  2.1035e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3279e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-110978.8281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1356, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.3674, device='cuda:0')



h[100].sum tensor(-0.0003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(109.1327, device='cuda:0')



h[200].sum tensor(-568.2319, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.0133, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 8.5169e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         8.8062e-05],
        [0.0000e+00, 8.5228e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         2.9745e-03],
        [0.0000e+00, 8.5090e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         1.6873e-03],
        ...,
        [0.0000e+00, 8.4139e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.4139e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.4139e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(280582.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2840, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3147, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3123, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0251, 0.1177],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0251, 0.1177],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0251, 0.1177]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1822751.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2498.3464, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(46751.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(225.9855, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-309.9234, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.8374],
        [  2.7815],
        [  2.7527],
        ...,
        [-11.2531],
        [-11.2445],
        [-11.2444]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-900389.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(537.6055, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(537.6055, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113147.1562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.1987, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.9422, device='cuda:0')



h[100].sum tensor(-0.0002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(74.8475, device='cuda:0')



h[200].sum tensor(-566.9375, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.1285, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(261955.2031, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0463, 0.1052],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0294, 0.1152],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0257, 0.1173],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0393, 0.1097],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0285, 0.1160],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0253, 0.1178]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1744107.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2461.8889, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(47263.9922, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(178.8663, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-374.9506, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.7082],
        [ -8.2295],
        [ -9.0598],
        ...,
        [ -9.8420],
        [-10.4355],
        [-10.4932]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-938400.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(573.1383, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(573.1383, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  2.0974e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3750e-03],
        [-6.4915e-03,  2.0938e-03, -1.6516e-02,  ..., -8.8469e-03,
         -6.6839e-08, -7.1915e-04],
        [-6.4915e-03,  2.0938e-03, -1.6516e-02,  ..., -8.8469e-03,
         -6.6839e-08, -7.1915e-04],
        ...,
        [-6.0055e-03,  2.0974e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3750e-03],
        [-6.0055e-03,  2.0974e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3750e-03],
        [-6.0055e-03,  2.0974e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3750e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112913.7266, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.5907, device='cuda:0')



h[100].sum tensor(-0.0002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(79.7945, device='cuda:0')



h[200].sum tensor(-567.1144, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.9877, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264423.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4089, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3119, 0.0004],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2396, 0.0163],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0240, 0.1186],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0240, 0.1186],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0240, 0.1186]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1748039.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2507.1343, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(46741.9258, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(174.7313, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-328.4390, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.6234],
        [  1.7881],
        [  1.9787],
        ...,
        [-11.4612],
        [-11.4521],
        [-11.4519]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1000379.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(568.9377, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(568.9377, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112915.8047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3091, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.3958, device='cuda:0')



h[100].sum tensor(-0.0002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(79.2097, device='cuda:0')



h[200].sum tensor(-567.0899, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.7679, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(265428.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0299, 0.1133],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0464, 0.1031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0881, 0.0774],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0235, 0.1175],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0235, 0.1175],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0235, 0.1175]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1738242., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2502.3057, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(46846.3047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(156.8455, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-329.4296, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.5119],
        [ -7.3708],
        [ -5.6626],
        ...,
        [-11.2471],
        [-11.2884],
        [-11.3169]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-974111.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4319],
        [0.2874],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(702.0966, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4319],
        [0.2874],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(702.0966, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.4145e-03,  1.9311e-03, -1.6220e-02,  ..., -8.7081e-03,
         -5.1397e-08, -1.4994e-03],
        [-6.5001e-03,  1.9246e-03, -1.6548e-02,  ..., -8.8624e-03,
         -6.2142e-08, -6.8116e-04],
        [-6.6369e-03,  1.9142e-03, -1.7075e-02,  ..., -9.1094e-03,
         -7.9342e-08,  6.2863e-04],
        ...,
        [-6.0055e-03,  1.9621e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4133e-03],
        [-6.0055e-03,  1.9621e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4133e-03],
        [-6.0055e-03,  1.9621e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4133e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111787.7266, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8108, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.5737, device='cuda:0')



h[100].sum tensor(-0.0002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(97.7485, device='cuda:0')



h[200].sum tensor(-567.7832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.7350, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0086],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0099],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0045],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(276196.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5264, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4882, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3944, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0236, 0.1159],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0236, 0.1159],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0236, 0.1159]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1785855.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2586.1196, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(46035.5273, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(155.9878, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-354.1985, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.6449],
        [  1.7537],
        [  1.6640],
        ...,
        [-11.2148],
        [-11.2070],
        [-11.2071]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-829109.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(544.4449, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(544.4449, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113178.7109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2203, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.2595, device='cuda:0')



h[100].sum tensor(-0.0001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(75.7997, device='cuda:0')



h[200].sum tensor(-566.9673, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.4864, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264407.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0244, 0.1150],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0244, 0.1150],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0245, 0.1148],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0241, 0.1154],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0241, 0.1154],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0241, 0.1154]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1727138.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2562.8850, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(46482.7148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(160.3575, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-341.1213, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.8613],
        [ -9.7563],
        [ -9.5620],
        ...,
        [-11.2137],
        [-11.2058],
        [-11.2064]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-909481.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(611.9121, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(611.9121, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112724.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4692, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.3896, device='cuda:0')



h[100].sum tensor(-0.0002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(85.1927, device='cuda:0')



h[200].sum tensor(-567.3110, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.0164, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(270241.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0245, 0.1148],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0245, 0.1148],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0246, 0.1146],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0242, 0.1152],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0242, 0.1152],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0242, 0.1152]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1753980.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2590.8418, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(46611.0352, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(196.2722, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-302.1829, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.6364],
        [ -9.7100],
        [ -9.6577],
        ...,
        [-11.2374],
        [-11.2297],
        [-11.2284]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-912363.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(870.8616, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(870.8616, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2479e-03,  1.8279e-03, -1.5579e-02,  ..., -8.4074e-03,
         -2.6568e-08, -3.1125e-03],
        [-6.2479e-03,  1.8279e-03, -1.5579e-02,  ..., -8.4074e-03,
         -2.6568e-08, -3.1125e-03],
        [-6.0055e-03,  1.8567e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4341e-03],
        ...,
        [-6.0055e-03,  1.8567e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4341e-03],
        [-6.0055e-03,  1.8567e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4341e-03],
        [-6.0055e-03,  1.8567e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4341e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-110644.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.4213, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(40.4036, device='cuda:0')



h[100].sum tensor(-0.0002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(121.2446, device='cuda:0')



h[200].sum tensor(-568.6268, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(45.5651, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(294844.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1511, 0.0365],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1642, 0.0293],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1393, 0.0448],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0235, 0.1148],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0235, 0.1148],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0235, 0.1148]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1889645.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2650.4485, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(46458.9453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(248.7135, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-255.8322, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.3698],
        [  0.4381],
        [ -0.0505],
        ...,
        [-11.1398],
        [-11.1322],
        [-11.1323]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-799876.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 570.0 event: 8550 loss: tensor(440.6045, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(616.0559, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(616.0559, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.8240e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4225e-03],
        [-6.5020e-03,  1.7708e-03, -1.6556e-02,  ..., -8.8659e-03,
         -5.1979e-08, -6.6014e-04],
        [-6.5020e-03,  1.7708e-03, -1.6556e-02,  ..., -8.8659e-03,
         -5.1979e-08, -6.6014e-04],
        ...,
        [-6.0055e-03,  1.8240e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4225e-03],
        [-6.0055e-03,  1.8240e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4225e-03],
        [-6.0055e-03,  1.8240e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4225e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112765.4375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4832, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.5819, device='cuda:0')



h[100].sum tensor(-0.0001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(85.7696, device='cuda:0')



h[200].sum tensor(-567.3304, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.2332, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(272770.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1391, 0.0453],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1639, 0.0305],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1654, 0.0295],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0227, 0.1150],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0227, 0.1150],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0227, 0.1150]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1771021.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2552.0300, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(49202.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(253.3982, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-292.0807, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.9672],
        [  0.3909],
        [  1.4567],
        ...,
        [-11.1254],
        [-11.1182],
        [-11.1185]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-891253.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(750.5299, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(750.5299, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-111851.3047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0033, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.8208, device='cuda:0')



h[100].sum tensor(-0.0002, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(104.4916, device='cuda:0')



h[200].sum tensor(-568.0491, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.2692, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(282447.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0338, 0.1084],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0259, 0.1129],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0222, 0.1149],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0218, 0.1155],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0218, 0.1155],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0218, 0.1155]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1815130.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2604.3872, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(49579.8438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(309.5734, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-253.1536, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -5.9050],
        [ -7.4619],
        [ -8.3409],
        ...,
        [-11.2098],
        [-11.2023],
        [-11.2026]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-893695.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(524.6238, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(524.6238, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113757.2656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.1420, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.3399, device='cuda:0')



h[100].sum tensor(-0.0001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(73.0401, device='cuda:0')



h[200].sum tensor(-566.8590, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.4493, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(262211.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0221, 0.1151],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0231, 0.1145],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0257, 0.1129],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0218, 0.1155],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0218, 0.1155],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0218, 0.1155]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1712640.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2546.4734, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(50677.8984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(278.4681, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-300.6625, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.3810],
        [ -9.0212],
        [ -8.3894],
        ...,
        [-11.2098],
        [-11.2023],
        [-11.2026]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-938404.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2808],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(610.9866, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2808],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(610.9866, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2714e-03,  1.7939e-03, -1.5669e-02,  ..., -8.4498e-03,
         -2.5385e-08, -2.8328e-03],
        [-6.2194e-03,  1.7975e-03, -1.5469e-02,  ..., -8.3560e-03,
         -2.0420e-08, -3.3332e-03],
        [-6.4853e-03,  1.7792e-03, -1.6492e-02,  ..., -8.8358e-03,
         -4.5805e-08, -7.7431e-04],
        ...,
        [-6.0055e-03,  1.8123e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3917e-03],
        [-6.0055e-03,  1.8123e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3917e-03],
        [-6.0055e-03,  1.8123e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3917e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113185.7656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.3467, device='cuda:0')



h[100].sum tensor(-0.0001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(85.0639, device='cuda:0')



h[200].sum tensor(-567.3087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.9680, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(269375.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1647, 0.0373],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2251, 0.0080],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1793, 0.0269],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0210, 0.1159],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0210, 0.1159],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0210, 0.1159]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1751410.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2577.1150, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(51071.0312, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(328.9663, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-266.0396, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.7861],
        [  1.6471],
        [  0.7795],
        ...,
        [-11.2905],
        [-11.2824],
        [-11.2823]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-978422.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3027],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(447.5969, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3027],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(447.5969, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2652e-03,  1.7879e-03, -1.5645e-02,  ..., -8.4385e-03,
         -2.3667e-08, -2.8666e-03],
        [-6.2922e-03,  1.7865e-03, -1.5749e-02,  ..., -8.4873e-03,
         -2.6132e-08, -2.6057e-03],
        [-6.0055e-03,  1.8014e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3704e-03],
        ...,
        [-6.0055e-03,  1.8014e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3704e-03],
        [-6.0055e-03,  1.8014e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3704e-03],
        [-6.0055e-03,  1.8014e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3704e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-114553.5000, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-613.8680, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(20.7662, device='cuda:0')



h[100].sum tensor(-8.6043e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(62.3161, device='cuda:0')



h[200].sum tensor(-566.4804, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(23.4191, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0016],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258527.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2051, 0.0171],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1251, 0.0537],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0837, 0.0785],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0207, 0.1159],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0207, 0.1159],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0207, 0.1159]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1707690.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2506.4570, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(51971.1797, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(329.5415, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-293.5798, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.1379],
        [ -0.7411],
        [ -2.3862],
        ...,
        [-11.3079],
        [-11.3001],
        [-11.3003]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1016639.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(668.4331, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(668.4331, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112810.5000, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6868, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.0119, device='cuda:0')



h[100].sum tensor(-0.0001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.0618, device='cuda:0')



h[200].sum tensor(-567.6118, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.9737, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(273733.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0256, 0.1131],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0257, 0.1131],
        [0.0000, 0.0001, 0.0000,  ..., 0.0000, 0.0262, 0.1127],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0210, 0.1160],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0210, 0.1160],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0210, 0.1160]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1778293.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2570.6851, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(50586.8477, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(363.4824, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-286.4494, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.0669],
        [ -8.9413],
        [ -8.6143],
        ...,
        [-11.3115],
        [-11.3037],
        [-11.3039]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-875044.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(748.6191, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(748.6191, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112322.7812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9731, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.7322, device='cuda:0')



h[100].sum tensor(-0.0001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(104.2256, device='cuda:0')



h[200].sum tensor(-568.0074, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.1692, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(283330.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0258, 0.1136],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0437, 0.1033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0905, 0.0760],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0216, 0.1163],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0216, 0.1163],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0216, 0.1163]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1857079.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2612.2305, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(49233.4883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(403.9606, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-257.5266, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.5354],
        [ -6.9378],
        [ -4.2959],
        ...,
        [-11.3743],
        [-11.3662],
        [-11.3664]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-897140.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(589.9164, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(589.9164, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113731.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.3691, device='cuda:0')



h[100].sum tensor(-9.7488e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(82.1304, device='cuda:0')



h[200].sum tensor(-567.2123, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.8656, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(267936.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0848, 0.0806],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0438, 0.1044],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0299, 0.1124],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0222, 0.1173],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0222, 0.1173],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0222, 0.1173]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1786606.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2548.6719, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(49323.4453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(396.0739, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-282.9407, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -3.6299],
        [ -6.2162],
        [ -7.8292],
        ...,
        [-11.4184],
        [-11.4101],
        [-11.4101]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-962103.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(609.8967, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(609.8967, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.8153e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3563e-03],
        [-6.0055e-03,  1.8153e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3563e-03],
        [-6.4560e-03,  1.7981e-03, -1.6378e-02,  ..., -8.7828e-03,
         -3.4057e-08, -9.9534e-04],
        ...,
        [-6.0055e-03,  1.8153e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3563e-03],
        [-6.0055e-03,  1.8153e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3563e-03],
        [-6.0055e-03,  1.8153e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3563e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113639.1562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.2961, device='cuda:0')



h[100].sum tensor(-9.5811e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(84.9121, device='cuda:0')



h[200].sum tensor(-567.3156, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.9110, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0030],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(270932.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0707, 0.0903],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1591, 0.0420],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2466, 0.0158],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0238, 0.1175],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0238, 0.1175],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0238, 0.1175]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1815359.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2557.4702, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(47971.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(390.6322, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-276.2769, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.7354],
        [ -1.5801],
        [  1.0987],
        ...,
        [-11.3241],
        [-11.3162],
        [-11.3163]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-929657.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(722.5938, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6211],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(722.5938, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5244e-03,  1.7954e-03, -1.6642e-02,  ..., -8.9063e-03,
         -3.9232e-08, -3.3274e-04],
        [-6.5937e-03,  1.7928e-03, -1.6908e-02,  ..., -9.0314e-03,
         -4.4474e-08,  3.3846e-04],
        [-6.0055e-03,  1.8153e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3563e-03],
        ...,
        [-6.0055e-03,  1.8153e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3563e-03],
        [-6.0055e-03,  1.8153e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3563e-03],
        [-6.0055e-03,  1.8153e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3563e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112757.3594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.5247, device='cuda:0')



h[100].sum tensor(-0.0001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(100.6022, device='cuda:0')



h[200].sum tensor(-567.8655, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.8075, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0078],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0044],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(278957.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5332, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4338, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4128, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0238, 0.1175],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0238, 0.1175],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0238, 0.1175]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1852194.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2590.9468, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(47347.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(401.7964, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-257.1161, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.1626],
        [  1.3338],
        [  1.4286],
        ...,
        [-11.3241],
        [-11.3162],
        [-11.3163]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-906642.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 580.0 event: 8700 loss: tensor(344.8773, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2590],
        [0.9194],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(706.8558, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2590],
        [0.9194],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(706.8558, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2508e-03,  1.8112e-03, -1.5590e-02,  ..., -8.4127e-03,
         -1.7694e-08, -3.0004e-03],
        [-6.8763e-03,  1.7777e-03, -1.7994e-02,  ..., -9.5413e-03,
         -6.2806e-08,  3.0534e-03],
        [-7.0629e-03,  1.7677e-03, -1.8712e-02,  ..., -9.8780e-03,
         -7.6264e-08,  4.8595e-03],
        ...,
        [-6.0055e-03,  1.8243e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3749e-03],
        [-6.0055e-03,  1.8243e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3749e-03],
        [-6.0055e-03,  1.8243e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3749e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112976.8984, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8227, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.7945, device='cuda:0')



h[100].sum tensor(-0.0001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(98.4111, device='cuda:0')



h[200].sum tensor(-567.7997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.9840, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0031],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0083],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0175],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(275335.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2017, 0.0370],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3250, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4145, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0247, 0.1178],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0247, 0.1178],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0247, 0.1178]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1827040.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2592.9075, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(46880.2188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(396.7760, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-261.2148, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.3072],
        [  0.8190],
        [  1.6657],
        ...,
        [-11.2774],
        [-11.2680],
        [-11.2679]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-900636., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2805],
        [0.3481],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(730.4985, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2805],
        [0.3481],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(730.4985, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.7396e-03,  1.8216e-03, -1.7469e-02,  ..., -9.2947e-03,
         -5.0504e-08,  1.6957e-03],
        [-6.3352e-03,  1.8493e-03, -1.5914e-02,  ..., -8.5650e-03,
         -2.2683e-08, -2.2154e-03],
        [-6.2712e-03,  1.8537e-03, -1.5668e-02,  ..., -8.4494e-03,
         -1.8277e-08, -2.8348e-03],
        ...,
        [-6.0055e-03,  1.8719e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4042e-03],
        [-6.0055e-03,  1.8719e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4042e-03],
        [-6.0055e-03,  1.8719e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4042e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112890.0547, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9209, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.8914, device='cuda:0')



h[100].sum tensor(-0.0001, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(101.7028, device='cuda:0')



h[200].sum tensor(-567.9354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.2211, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0017],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(279884.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2146, 0.0126],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2226, 0.0083],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1872, 0.0262],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0245, 0.1179],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0245, 0.1179],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0245, 0.1179]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1853086.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2592.9106, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(46469.2734, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(400.6067, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-209.0808, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.3791],
        [  2.0782],
        [  0.9632],
        ...,
        [-11.3434],
        [-11.3327],
        [-11.3318]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1008667.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(657.0529, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(657.0529, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5826e-03,  1.7273e-03, -1.6865e-02,  ..., -9.0114e-03,
         -3.7863e-08,  1.4424e-04],
        [-6.0055e-03,  1.7892e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4314e-03],
        [-6.3535e-03,  1.7518e-03, -1.5984e-02,  ..., -8.5979e-03,
         -2.2830e-08, -2.0695e-03],
        ...,
        [-6.0055e-03,  1.7892e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4314e-03],
        [-6.0055e-03,  1.7892e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4314e-03],
        [-6.0055e-03,  1.7892e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4314e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113459., device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6412, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.4839, device='cuda:0')



h[100].sum tensor(-8.8282e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.4774, device='cuda:0')



h[200].sum tensor(-567.5490, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.3783, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0001],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0007],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(270635.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2942, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3149, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3426, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0279, 0.1169],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0279, 0.1169],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0279, 0.1169]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1790434.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2609.6938, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(45385.9141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(348.8242, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-278.5521, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.5547],
        [  2.5289],
        [  2.4424],
        ...,
        [-11.1394],
        [-11.1281],
        [-11.1265]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-864875.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(741.5101, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(741.5101, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        ...,
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112677.9609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9681, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.4023, device='cuda:0')



h[100].sum tensor(-9.5070e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(103.2358, device='cuda:0')



h[200].sum tensor(-568.0005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.7972, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(279309.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0445, 0.1079],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0299, 0.1163],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0300, 0.1161],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0296, 0.1167],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0296, 0.1167],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0296, 0.1167]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1825468.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2641.6165, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(44238.6953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(334.1243, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-258.3075, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.9377],
        [ -8.1525],
        [ -8.3916],
        ...,
        [-11.1288],
        [-11.1186],
        [-11.1178]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-790949.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(644.2640, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(644.2640, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        ...,
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113521.5625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5908, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.8906, device='cuda:0')



h[100].sum tensor(-7.7957e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.6969, device='cuda:0')



h[200].sum tensor(-567.4792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.7091, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(269095.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0835, 0.0866],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1359, 0.0568],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1817, 0.0325],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0285, 0.1186],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0285, 0.1186],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0285, 0.1186]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1755901., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2574.5376, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(45325.5781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(330.0083, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-234.6415, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -2.1176],
        [  0.0428],
        [  1.7340],
        ...,
        [-11.3413],
        [-11.3302],
        [-11.3293]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-918906., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(834.2002, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(834.2002, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2494e-03,  1.7124e-03, -1.5584e-02,  ..., -8.4101e-03,
         -1.3865e-08, -3.1464e-03],
        [-6.2494e-03,  1.7124e-03, -1.5584e-02,  ..., -8.4101e-03,
         -1.3865e-08, -3.1464e-03],
        [-6.0055e-03,  1.7511e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4978e-03],
        ...,
        [-6.0055e-03,  1.7511e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4978e-03],
        [-6.0055e-03,  1.7511e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4978e-03],
        [-6.0055e-03,  1.7511e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4978e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112006.6797, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2929, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(38.7027, device='cuda:0')



h[100].sum tensor(-9.5746e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(116.1405, device='cuda:0')



h[200].sum tensor(-568.4495, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(43.6469, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(283544.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1566, 0.0460],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1697, 0.0393],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1399, 0.0568],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0280, 0.1209],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0280, 0.1209],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0280, 0.1209]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1832746.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2622.2715, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(45156.5938, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(374.2534, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-203.5121, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.4448],
        [  0.0132],
        [ -1.4577],
        ...,
        [-11.5212],
        [-11.5095],
        [-11.5084]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-872417., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4688],
        [0.3921],
        [0.3428],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(525.5649, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4688],
        [0.3921],
        [0.3428],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(525.5649, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.3363e-03,  1.5541e-03, -1.9762e-02,  ..., -1.0371e-02,
         -7.2090e-08,  7.3231e-03],
        [-7.0983e-03,  1.5891e-03, -1.8847e-02,  ..., -9.9418e-03,
         -5.9194e-08,  5.0281e-03],
        [-6.7991e-03,  1.6331e-03, -1.7697e-02,  ..., -9.4019e-03,
         -4.2987e-08,  2.1438e-03],
        ...,
        [-6.0055e-03,  1.7499e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5065e-03],
        [-6.0055e-03,  1.7499e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5065e-03],
        [-6.0055e-03,  1.7499e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5065e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-114575.5234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.1409, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.3836, device='cuda:0')



h[100].sum tensor(-5.7005e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(73.1711, device='cuda:0')



h[200].sum tensor(-566.8574, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.4986, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0202],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0239],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0141],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258998.4531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6303, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6451, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5250, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0273, 0.1233],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0273, 0.1233],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0273, 0.1233]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1718576.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2529.4526, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(47755.0234, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(372.4681, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-264.1082, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.4146],
        [  1.4327],
        [  1.4344],
        ...,
        [-11.7143],
        [-11.7027],
        [-11.7017]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-981733.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(769.0875, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(769.0875, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2830e-03,  1.7337e-03, -1.5713e-02,  ..., -8.4707e-03,
         -1.4322e-08, -2.8371e-03],
        [-6.0055e-03,  1.7690e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5126e-03],
        [-6.0055e-03,  1.7690e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5126e-03],
        ...,
        [-6.0055e-03,  1.7690e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5126e-03],
        [-6.0055e-03,  1.7690e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5126e-03],
        [-6.0055e-03,  1.7690e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5126e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112430.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0575, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.6818, device='cuda:0')



h[100].sum tensor(-7.9650e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(107.0753, device='cuda:0')



h[200].sum tensor(-568.1241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.2401, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(276722.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1210, 0.0706],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0816, 0.0930],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0576, 0.1061],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0272, 0.1234],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0272, 0.1234],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0272, 0.1234]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1785030., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2581.7524, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(47699.8398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(395.6797, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-234.4402, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -2.2947],
        [ -4.0920],
        [ -4.5316],
        ...,
        [-11.7569],
        [-11.7457],
        [-11.7448]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-902837.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3242],
        [0.3210],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(652.1185, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3242],
        [0.3210],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(652.1185, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.8994e-03,  1.7395e-03, -1.8082e-02,  ..., -9.5830e-03,
         -4.3955e-08,  3.0843e-03],
        [-6.5053e-03,  1.7830e-03, -1.6568e-02,  ..., -8.8719e-03,
         -2.4578e-08, -7.1341e-04],
        [-6.5084e-03,  1.7827e-03, -1.6579e-02,  ..., -8.8773e-03,
         -2.4726e-08, -6.8444e-04],
        ...,
        [-6.0055e-03,  1.8382e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5304e-03],
        [-6.0055e-03,  1.8382e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5304e-03],
        [-6.0055e-03,  1.8382e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5304e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113250.2500, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.2550, device='cuda:0')



h[100].sum tensor(-6.4152e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(90.7904, device='cuda:0')



h[200].sum tensor(-567.5284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.1201, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0041],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0118],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(270187.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4078, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4784, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6227, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0260, 0.1237],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0260, 0.1237],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0260, 0.1237]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1763643.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2564.4932, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(48800.3359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(386.5223, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-240.4344, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.4041],
        [  1.2672],
        [  1.1142],
        ...,
        [-11.9086],
        [-11.8992],
        [-11.8991]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-969558.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.5098],
        [0.5044],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(665.3451, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.5098],
        [0.5044],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(665.3451, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.2777e-03,  1.7870e-03, -1.9536e-02,  ..., -1.0266e-02,
         -5.9582e-08,  6.7142e-03],
        [-6.9144e-03,  1.8224e-03, -1.8140e-02,  ..., -9.6101e-03,
         -4.2570e-08,  3.2146e-03],
        [-6.4883e-03,  1.8640e-03, -1.6502e-02,  ..., -8.8412e-03,
         -2.2612e-08, -8.9090e-04],
        ...,
        [-6.0055e-03,  1.9111e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5425e-03],
        [-6.0055e-03,  1.9111e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5425e-03],
        [-6.0055e-03,  1.9111e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5425e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113039.5000, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.8687, device='cuda:0')



h[100].sum tensor(-6.1399e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.6319, device='cuda:0')



h[200].sum tensor(-567.5593, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.8121, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0103],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0132],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0173],
        ...,
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(270732.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5598, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5307, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5402, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0252, 0.1226],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0252, 0.1226],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0252, 0.1226]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1766418., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2555.5430, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(49455.1172, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(379.3843, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-225.0518, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.3110],
        [  1.3776],
        [  1.4416],
        ...,
        [-11.9543],
        [-11.9449],
        [-11.9448]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1010546., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 590.0 event: 8850 loss: tensor(451.0380, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(603.7852, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(603.7852, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.9421e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5527e-03],
        [-6.0055e-03,  1.9421e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5527e-03],
        [-6.5008e-03,  1.8934e-03, -1.6550e-02,  ..., -8.8637e-03,
         -2.2091e-08, -7.8205e-04],
        ...,
        [-6.0055e-03,  1.9421e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5527e-03],
        [-6.0055e-03,  1.9421e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5527e-03],
        [-6.0055e-03,  1.9421e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5527e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113371.4141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4331, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.0126, device='cuda:0')



h[100].sum tensor(-5.3087e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(84.0613, device='cuda:0')



h[200].sum tensor(-567.2614, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.5912, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(265421.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0723, 0.0952],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1237, 0.0671],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2324, 0.0371],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0258, 0.1207],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0258, 0.1207],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0258, 0.1207]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1742204., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2555.5503, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(49779.6367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(357.3922, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-276.8245, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.0111],
        [ -0.4288],
        [  0.4891],
        ...,
        [-11.9164],
        [-11.9074],
        [-11.9074]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-947533.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(535.5525, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(535.5525, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        ...,
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113811.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.1907, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.8469, device='cuda:0')



h[100].sum tensor(-4.4853e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(74.5616, device='cuda:0')



h[200].sum tensor(-566.9263, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.0211, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(262875.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0988, 0.0799],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0515, 0.1050],
        [0.0000, 0.0010, 0.0000,  ..., 0.0000, 0.0354, 0.1136],
        ...,
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0265, 0.1186],
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0265, 0.1186],
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0265, 0.1186]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1742257.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2548.5142, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(49984.3164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(343.4214, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-302.2784, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -2.8511],
        [ -5.7752],
        [ -7.8056],
        ...,
        [-11.8977],
        [-11.8890],
        [-11.8890]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-930164.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6694],
        [0.5161],
        [0.5010],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(640.5792, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6694],
        [0.5161],
        [0.5010],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(640.5792, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.8175e-03,  1.7973e-03, -2.1610e-02,  ..., -1.1240e-02,
         -7.3257e-08,  1.1843e-02],
        [-8.0357e-03,  1.7708e-03, -2.2449e-02,  ..., -1.1633e-02,
         -8.2079e-08,  1.3943e-02],
        [-7.4650e-03,  1.8401e-03, -2.0255e-02,  ..., -1.0604e-02,
         -5.9005e-08,  8.4517e-03],
        ...,
        [-6.0055e-03,  2.0174e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5894e-03],
        [-6.0055e-03,  2.0174e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5894e-03],
        [-6.0055e-03,  2.0174e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5894e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112983.0938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5591, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.7196, device='cuda:0')



h[100].sum tensor(-5.0380e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.1839, device='cuda:0')



h[200].sum tensor(-567.4355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.5163, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0350],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0344],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0251],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(269230.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7703, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7778, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6622, 0.0000],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0255, 0.1179],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0255, 0.1179],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0255, 0.1179]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1785782.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2553.9800, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(50247.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(361.8600, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-251.9667, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.2625],
        [  1.2548],
        [  1.4016],
        ...,
        [-12.0136],
        [-12.0047],
        [-12.0047]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1017358.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(539.8663, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(539.8663, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5277e-03,  2.0131e-03, -1.6654e-02,  ..., -8.9123e-03,
         -2.0095e-08, -5.7791e-04],
        [-6.0055e-03,  2.0851e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6011e-03],
        [-6.0055e-03,  2.0851e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6011e-03],
        ...,
        [-6.0055e-03,  2.0851e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6011e-03],
        [-6.0055e-03,  2.0851e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6011e-03],
        [-6.0055e-03,  2.0851e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6011e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113724.0234, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2036, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.0471, device='cuda:0')



h[100].sum tensor(-4.0531e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(75.1622, device='cuda:0')



h[200].sum tensor(-566.9440, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.2468, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(263351.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2059, 0.0251],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1798, 0.0381],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0980, 0.0768],
        ...,
        [0.0000, 0.0019, 0.0000,  ..., 0.0000, 0.0259, 0.1164],
        [0.0000, 0.0019, 0.0000,  ..., 0.0000, 0.0259, 0.1164],
        [0.0000, 0.0019, 0.0000,  ..., 0.0000, 0.0259, 0.1164]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1742474.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2538.6450, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(49624.0898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(315.1185, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-294.7313, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.2820],
        [  1.5246],
        [  0.3637],
        ...,
        [-11.9333],
        [-11.9253],
        [-11.9261]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-958421.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(584.4961, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(584.4961, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        ...,
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113355.2969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.1177, device='cuda:0')



h[100].sum tensor(-4.1581e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(81.3758, device='cuda:0')



h[200].sum tensor(-567.1740, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.5820, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(267763.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0267, 0.1149],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0267, 0.1149],
        [0.0000, 0.0024, 0.0000,  ..., 0.0000, 0.0434, 0.1054],
        ...,
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0264, 0.1153],
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0264, 0.1153],
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0264, 0.1153]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1763031.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2572.6416, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(48650.7031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(299.2480, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-307.9958, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.0738],
        [ -8.1233],
        [ -6.2456],
        ...,
        [-11.9339],
        [-11.9256],
        [-11.9258]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-908605.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(960.0989, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(960.0989, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.1933e-03,  2.1783e-03, -1.5368e-02,  ..., -8.3088e-03,
         -6.5415e-09, -3.8149e-03],
        [-6.0055e-03,  2.2103e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6211e-03],
        [-6.0055e-03,  2.2103e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6211e-03],
        ...,
        [-6.0055e-03,  2.2103e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6211e-03],
        [-6.0055e-03,  2.2103e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6211e-03],
        [-6.0055e-03,  2.2103e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6211e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-110405.9062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.7252, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(44.5437, device='cuda:0')



h[100].sum tensor(-6.3984e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(133.6686, device='cuda:0')



h[200].sum tensor(-569.0469, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(50.2342, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(303046.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1087, 0.0675],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0973, 0.0741],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0611, 0.0950],
        ...,
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0253, 0.1154],
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0253, 0.1154],
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0253, 0.1154]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1951014., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2602.2939, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(47391.9297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(351.9452, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.6442, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -3.4775],
        [ -4.3350],
        [ -5.5403],
        ...,
        [-12.1356],
        [-12.1267],
        [-12.1267]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-971312.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(562.7744, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2830],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(562.7744, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  2.2336e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6207e-03],
        [-6.2735e-03,  2.1886e-03, -1.5677e-02,  ..., -8.4535e-03,
         -8.8818e-09, -3.0399e-03],
        [-6.0055e-03,  2.2336e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6207e-03],
        ...,
        [-6.0055e-03,  2.2336e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6207e-03],
        [-6.0055e-03,  2.2336e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6207e-03],
        [-6.0055e-03,  2.2336e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6207e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113759.9062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2741, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.1099, device='cuda:0')



h[100].sum tensor(-3.5628e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(78.3516, device='cuda:0')



h[200].sum tensor(-567.0416, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.4454, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0010],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(266688., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2443, 0.0136],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1640, 0.0437],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1190, 0.0619],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0252, 0.1144],
        [0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0252, 0.1144],
        [0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0252, 0.1144]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1762476.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2522.1746, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(47968.2422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(260.4742, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-313.8024, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.4701],
        [  1.8011],
        [  1.1253],
        ...,
        [-12.1265],
        [-12.1177],
        [-12.1179]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-992899., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(605.7002, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(605.7002, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        ...,
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113472.4297, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4368, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.1014, device='cuda:0')



h[100].sum tensor(-3.6366e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(84.3279, device='cuda:0')



h[200].sum tensor(-567.2664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.6914, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(269748.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0010, 0.0000,  ..., 0.0000, 0.0527, 0.0961],
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0406, 0.1034],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0318, 0.1085],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0260, 0.1122],
        [0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0260, 0.1122],
        [0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0260, 0.1122]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1779976.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2548.7930, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(46604.2188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(211.2186, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-347.5110, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.1709],
        [ -8.6718],
        [ -9.1355],
        ...,
        [-12.0653],
        [-12.0570],
        [-12.0573]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-907274.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2983],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(678.4443, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2983],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(678.4443, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  2.2934e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6177e-03],
        [-6.2881e-03,  2.2555e-03, -1.5732e-02,  ..., -8.4798e-03,
         -8.4689e-09, -2.8904e-03],
        [-6.0055e-03,  2.2934e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6177e-03],
        ...,
        [-6.0055e-03,  2.2934e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6177e-03],
        [-6.0055e-03,  2.2934e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6177e-03],
        [-6.0055e-03,  2.2934e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6177e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112892.8281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7411, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.4764, device='cuda:0')



h[100].sum tensor(-3.9092e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(94.4556, device='cuda:0')



h[200].sum tensor(-567.6869, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.4975, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(278248.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1173, 0.0604],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0798, 0.0809],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0680, 0.0876],
        ...,
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0264, 0.1108],
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0264, 0.1108],
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0264, 0.1108]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1814911.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2540.3369, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(45690.2422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(198.1490, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-316.5419, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.1261],
        [ -7.6031],
        [ -8.2771],
        ...,
        [-12.0999],
        [-12.0917],
        [-12.0920]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-964329.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(735.0464, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2455],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(735.0464, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2380e-03,  2.2533e-03, -1.5540e-02,  ..., -8.3895e-03,
         -6.6248e-09, -3.3707e-03],
        [-6.2568e-03,  2.2509e-03, -1.5612e-02,  ..., -8.4234e-03,
         -7.1602e-09, -3.1892e-03],
        [-6.4893e-03,  2.2221e-03, -1.6506e-02,  ..., -8.8429e-03,
         -1.3785e-08, -9.4353e-04],
        ...,
        [-6.0055e-03,  2.2821e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6164e-03],
        [-6.0055e-03,  2.2821e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6164e-03],
        [-6.0055e-03,  2.2821e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6164e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112518.7109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9261, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.1024, device='cuda:0')



h[100].sum tensor(-3.9670e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(102.3359, device='cuda:0')



h[200].sum tensor(-567.9426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.4590, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(282833.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1829, 0.0326],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2428, 0.0095],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2055, 0.0223],
        ...,
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0267, 0.1095],
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0267, 0.1095],
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0267, 0.1095]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1840523.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2578.4373, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(44486.4180, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(159.4501, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-335.0447, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.0968],
        [  2.1951],
        [  2.2571],
        ...,
        [-11.7711],
        [-11.7324],
        [-11.7629]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-871382.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 600.0 event: 9000 loss: tensor(436.2625, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(593.8207, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(593.8207, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0022, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0022, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0022, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        ...,
        [-0.0060,  0.0022, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0022, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0022, -0.0146,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113803.7031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3809, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.5503, device='cuda:0')



h[100].sum tensor(-3.0005e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(82.6740, device='cuda:0')



h[200].sum tensor(-567.1890, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.0698, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(271160.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0267, 0.1088],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0267, 0.1088],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0295, 0.1071],
        ...,
        [0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0265, 0.1092],
        [0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0265, 0.1092],
        [0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0265, 0.1092]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1787334.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2535.0811, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(45274.5703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.4862, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-373.0690, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.9249],
        [ -9.6388],
        [ -9.0236],
        ...,
        [-11.8794],
        [-11.8724],
        [-11.8731]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-841935.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(698.7106, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(698.7106, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  2.2268e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6265e-03],
        [-6.0055e-03,  2.2268e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6265e-03],
        [-6.2797e-03,  2.1889e-03, -1.5700e-02,  ..., -8.4648e-03,
         -7.0581e-09, -2.9761e-03],
        ...,
        [-6.0055e-03,  2.2268e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6265e-03],
        [-6.0055e-03,  2.2268e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6265e-03],
        [-6.0055e-03,  2.2268e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6265e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113058.9297, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7805, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.4166, device='cuda:0')



h[100].sum tensor(-3.3567e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(97.2771, device='cuda:0')



h[200].sum tensor(-567.7414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.5579, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(280331.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0400, 0.1018],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0770, 0.0813],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1016, 0.0673],
        ...,
        [0.0000, 0.0024, 0.0000,  ..., 0.0000, 0.0243, 0.1109],
        [0.0000, 0.0024, 0.0000,  ..., 0.0000, 0.0243, 0.1109],
        [0.0000, 0.0024, 0.0000,  ..., 0.0000, 0.0243, 0.1109]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1830589.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2556.2981, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(44901.4453, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(157.8672, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-319.7597, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.0336],
        [ -4.5902],
        [ -1.9906],
        ...,
        [-12.0497],
        [-12.0419],
        [-12.0424]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-919407., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2598],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(750.8954, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2598],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(750.8954, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2642e-03,  2.1232e-03, -1.5641e-02,  ..., -8.4367e-03,
         -6.3248e-09, -3.1343e-03],
        [-6.5107e-03,  2.0848e-03, -1.6588e-02,  ..., -8.8817e-03,
         -1.2355e-08, -7.5090e-04],
        [-6.2647e-03,  2.1231e-03, -1.5643e-02,  ..., -8.4377e-03,
         -6.3386e-09, -3.1289e-03],
        ...,
        [-6.0055e-03,  2.1635e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6343e-03],
        [-6.0055e-03,  2.1635e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6343e-03],
        [-6.0055e-03,  2.1635e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6343e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-112797.4375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9774, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.8378, device='cuda:0')



h[100].sum tensor(-3.4144e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(104.5425, device='cuda:0')



h[200].sum tensor(-568.0134, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.2883, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(281684.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3102, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2819, 0.0037],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2440, 0.0132],
        ...,
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0227, 0.1125],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0227, 0.1125],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0227, 0.1125]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1831099.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2523.5112, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(45770.8438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(194.1538, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-272.7698, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.4227],
        [  2.3202],
        [  1.8061],
        ...,
        [-12.1889],
        [-12.1701],
        [-12.1619]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1038116.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(585.5122, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(585.5122, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  2.2003e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6394e-03],
        [-6.0055e-03,  2.2003e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6394e-03],
        [-6.2457e-03,  2.1614e-03, -1.5570e-02,  ..., -8.4035e-03,
         -5.5804e-09, -3.3181e-03],
        ...,
        [-6.0055e-03,  2.2003e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6394e-03],
        [-6.0055e-03,  2.2003e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6394e-03],
        [-6.0055e-03,  2.2003e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6394e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-114218.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3580, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.1648, device='cuda:0')



h[100].sum tensor(-2.5061e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(81.5172, device='cuda:0')



h[200].sum tensor(-567.1576, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.6351, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(271422.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0812, 0.0793],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1215, 0.0568],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1808, 0.0329],
        ...,
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0221, 0.1124],
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0221, 0.1124],
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0221, 0.1124]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1797200.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2480.7085, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(46061.3516, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(170.8275, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-345.4913, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.4973],
        [  1.3536],
        [  2.1388],
        ...,
        [-12.0807],
        [-12.0728],
        [-12.0733]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-940066.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2844],
        [0.5166],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(663.1322, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2844],
        [0.5166],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(663.1322, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.9228e-03,  2.1068e-03, -1.8171e-02,  ..., -9.6253e-03,
         -2.0236e-08,  3.2115e-03],
        [-6.7055e-03,  2.1429e-03, -1.7336e-02,  ..., -9.2330e-03,
         -1.5441e-08,  1.1123e-03],
        [-6.6499e-03,  2.1522e-03, -1.7123e-02,  ..., -9.1328e-03,
         -1.4216e-08,  5.7590e-04],
        ...,
        [-6.0055e-03,  2.2592e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6479e-03],
        [-6.0055e-03,  2.2592e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6479e-03],
        [-6.0055e-03,  2.2592e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6479e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113643.5781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6594, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.7660, device='cuda:0')



h[100].sum tensor(-2.6977e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.3238, device='cuda:0')



h[200].sum tensor(-567.5740, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.6963, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0077],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0111],
        ...,
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(274726.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4004, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4557, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4880, 0.0000],
        ...,
        [0.0000, 0.0021, 0.0000,  ..., 0.0000, 0.0219, 0.1111],
        [0.0000, 0.0021, 0.0000,  ..., 0.0000, 0.0219, 0.1111],
        [0.0000, 0.0021, 0.0000,  ..., 0.0000, 0.0219, 0.1111]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1804765.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2486.1892, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(46564.7109, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(165.2651, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-357.5281, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.3173],
        [  2.0598],
        [  1.8799],
        ...,
        [-12.0421],
        [-12.0344],
        [-12.0349]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-891486.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6611],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(654.4628, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6611],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(654.4628, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.0546e-03,  2.1232e-03, -1.8677e-02,  ..., -9.8630e-03,
         -2.1973e-08,  4.4704e-03],
        [-6.0055e-03,  2.3029e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6571e-03],
        [-6.6317e-03,  2.1956e-03, -1.7052e-02,  ..., -9.0999e-03,
         -1.3115e-08,  3.8774e-04],
        ...,
        [-6.4673e-03,  2.2238e-03, -1.6421e-02,  ..., -8.8032e-03,
         -9.6717e-09, -1.1994e-03],
        [-6.0055e-03,  2.3029e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6571e-03],
        [-6.0055e-03,  2.3029e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6571e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113848.1328, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.3638, device='cuda:0')



h[100].sum tensor(-2.5218e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.1168, device='cuda:0')



h[200].sum tensor(-567.5403, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.2427, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0109],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0075],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(274789.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4686, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3636, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1890, 0.0272],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1019, 0.0672],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0836, 0.0767],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0412, 0.0999]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1809147.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2464.8018, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(47876.2344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(182.4668, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-345.1046, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 1.7244],
        [ 2.1700],
        [ 2.4625],
        ...,
        [-5.6866],
        [-6.9094],
        [-7.7853]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-959428.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4663],
        [0.4819],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(714.9203, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4663],
        [0.4819],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(714.9203, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.7098e-03,  1.9723e-03, -2.1194e-02,  ..., -1.1045e-02,
         -3.3886e-08,  1.0785e-02],
        [-6.8019e-03,  2.1472e-03, -1.7706e-02,  ..., -9.4070e-03,
         -1.5835e-08,  2.0240e-03],
        [-7.6230e-03,  1.9890e-03, -2.0861e-02,  ..., -1.0889e-02,
         -3.2160e-08,  9.9478e-03],
        ...,
        [-6.0055e-03,  2.3006e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6615e-03],
        [-6.0055e-03,  2.3006e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6615e-03],
        [-6.0055e-03,  2.3006e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6615e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113602.8828, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8442, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.1687, device='cuda:0')



h[100].sum tensor(-2.5849e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(99.5339, device='cuda:0')



h[200].sum tensor(-567.8295, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.4060, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0264],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0300],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0120],
        ...,
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(276545.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8056, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7554, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6161, 0.0000],
        ...,
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0216, 0.1107],
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0216, 0.1107],
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0216, 0.1107]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1819756.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2438.7383, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(49616.2891, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(214.8492, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-360.6985, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.0068],
        [  1.0412],
        [  1.1154],
        ...,
        [-12.1669],
        [-12.1592],
        [-12.1601]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-922828.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(562.5480, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(562.5480, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0057],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0057],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0057],
        ...,
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0057],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0057],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0057]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-114911.7578, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2843, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0994, device='cuda:0')



h[100].sum tensor(-1.9222e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(78.3201, device='cuda:0')



h[200].sum tensor(-567.0557, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.4336, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(267429.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0277, 0.1078],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0229, 0.1106],
        [0.0000, 0.0021, 0.0000,  ..., 0.0000, 0.0207, 0.1117],
        ...,
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0197, 0.1127],
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0197, 0.1127],
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0197, 0.1127]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1791417.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2586.3665, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(52392.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(280.6570, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-570.4031, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.0451],
        [ -8.9462],
        [ -9.5742],
        ...,
        [-12.3944],
        [-12.3855],
        [-12.3857]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1002309.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2452],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(721.4838, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2452],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(721.4838, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.8176e-03,  2.0500e-03, -1.7766e-02,  ..., -9.4353e-03,
         -1.4540e-08,  2.1606e-03],
        [-6.5192e-03,  2.1262e-03, -1.6620e-02,  ..., -8.8969e-03,
         -9.1981e-09, -7.1664e-04],
        [-6.8397e-03,  2.0444e-03, -1.7851e-02,  ..., -9.4752e-03,
         -1.4936e-08,  2.3738e-03],
        ...,
        [-6.0055e-03,  2.2572e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6708e-03],
        [-6.0055e-03,  2.2572e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6708e-03],
        [-6.0055e-03,  2.2572e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6708e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113585.4375, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8729, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.4732, device='cuda:0')



h[100].sum tensor(-2.3254e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(100.4477, device='cuda:0')



h[200].sum tensor(-567.8689, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.7494, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0095],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0095],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0039],
        ...,
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(281572.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5695, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5822, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5659, 0.0000],
        ...,
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0154, 0.1156],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0154, 0.1156],
        [0.0000, 0.0008, 0.0000,  ..., 0.0000, 0.0154, 0.1156]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1850300.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2834.7727, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(53408.8438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(379.7665, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-706.4496, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.3762],
        [  1.3188],
        [  1.3421],
        ...,
        [-12.7115],
        [-12.7015],
        [-12.7013]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1091164.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(787.6448, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(787.6448, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0057],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0057],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0057],
        ...,
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0057],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0057],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0057]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-113339.2656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1180, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.5427, device='cuda:0')



h[100].sum tensor(-2.3939e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(109.6589, device='cuda:0')



h[200].sum tensor(-568.2078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.2111, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(282036.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 6.5749e-02,
         8.7173e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 5.9164e-02,
         9.0925e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 4.3770e-02,
         9.9493e-02],
        ...,
        [0.0000e+00, 3.7357e-05, 0.0000e+00,  ..., 0.0000e+00, 1.3124e-02,
         1.1678e-01],
        [0.0000e+00, 3.7357e-05, 0.0000e+00,  ..., 0.0000e+00, 1.3124e-02,
         1.1678e-01],
        [0.0000e+00, 3.7357e-05, 0.0000e+00,  ..., 0.0000e+00, 1.3124e-02,
         1.1678e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1856888.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2569.5596, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(52450.0547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(315.8376, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-516.9906, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -2.4645],
        [ -3.9029],
        [ -5.3698],
        ...,
        [-12.7635],
        [-12.7533],
        [-12.7531]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1081750.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 610.0 event: 9150 loss: tensor(439.2103, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(653.9934, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(653.9934, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  2.4154e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6665e-03],
        [-6.0055e-03,  2.4154e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6665e-03],
        [-6.2653e-03,  2.3396e-03, -1.5645e-02,  ..., -8.4387e-03,
         -4.1849e-09, -3.1597e-03],
        ...,
        [-6.0055e-03,  2.4154e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6665e-03],
        [-6.0055e-03,  2.4154e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6665e-03],
        [-6.0055e-03,  2.4154e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6665e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-114835.0312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6179, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.3420, device='cuda:0')



h[100].sum tensor(-1.8695e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.0514, device='cuda:0')



h[200].sum tensor(-567.5167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.2182, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(272227.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.5655e-01,
         3.6840e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.5482e-01,
         3.6333e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.8933e-01,
         2.1846e-02],
        ...,
        [0.0000e+00, 6.5710e-05, 0.0000e+00,  ..., 0.0000e+00, 1.3204e-02,
         1.1626e-01],
        [0.0000e+00, 6.5710e-05, 0.0000e+00,  ..., 0.0000e+00, 1.3204e-02,
         1.1626e-01],
        [0.0000e+00, 6.5710e-05, 0.0000e+00,  ..., 0.0000e+00, 1.3204e-02,
         1.1626e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1826422., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2293.0764, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(51001.5742, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(203.8601, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-387.4094, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.5682],
        [  2.4496],
        [  2.2641],
        ...,
        [-11.4102],
        [-12.2726],
        [-12.5417]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-999562., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(609.0004, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(609.0004, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5128e-03,  2.2736e-03, -1.6595e-02,  ..., -8.8853e-03,
         -7.7485e-09, -7.6258e-04],
        [-6.0055e-03,  2.4300e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6615e-03],
        [-6.8467e-03,  2.1707e-03, -1.7878e-02,  ..., -9.4879e-03,
         -1.2849e-08,  2.4622e-03],
        ...,
        [-6.0055e-03,  2.4300e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6615e-03],
        [-6.0055e-03,  2.4300e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6615e-03],
        [-6.0055e-03,  2.4300e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6615e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-115507.7656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4587, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.2545, device='cuda:0')



h[100].sum tensor(-1.6454e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(84.7873, device='cuda:0')



h[200].sum tensor(-567.2966, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.8641, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0025],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0098],
        ...,
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(267827.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2148, 0.0111],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3534, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4973, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0139, 0.1150],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0139, 0.1150],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0139, 0.1150]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1782638.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2306.8257, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(48847.4609, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(160.4667, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-374.1085, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.6774],
        [  2.2031],
        [  1.7515],
        ...,
        [-12.4648],
        [-12.4560],
        [-12.4562]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1048683.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6382],
        [0.7036],
        [0.6753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(551.8673, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6382],
        [0.7036],
        [0.6753],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(551.8673, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.3152e-03,  2.0383e-03, -1.9677e-02,  ..., -1.0333e-02,
         -1.8964e-08,  7.0351e-03],
        [-7.2495e-03,  2.0582e-03, -1.9425e-02,  ..., -1.0215e-02,
         -1.8013e-08,  6.3997e-03],
        [-7.2555e-03,  2.0564e-03, -1.9448e-02,  ..., -1.0226e-02,
         -1.8100e-08,  6.4579e-03],
        ...,
        [-6.0055e-03,  2.4351e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6380e-03],
        [-6.0055e-03,  2.4351e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6380e-03],
        [-6.0055e-03,  2.4351e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6380e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-116151.1719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2400, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.6039, device='cuda:0')



h[100].sum tensor(-1.3997e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(76.8331, device='cuda:0')



h[200].sum tensor(-566.9945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.8748, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0300],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0372],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0267],
        ...,
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264977.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7583, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8194, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7542, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0161, 0.1126],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0161, 0.1126],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0161, 0.1126]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1764986.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2387.4614, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(45931.3359, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(97.5916, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-417.7583, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.2850],
        [  1.2398],
        [  1.2690],
        ...,
        [-12.1348],
        [-12.1246],
        [-12.1243]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-905534.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(614.5498, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(614.5498, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        ...,
        [-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-115870.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4701, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.5120, device='cuda:0')



h[100].sum tensor(-1.4666e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(85.5600, device='cuda:0')



h[200].sum tensor(-567.3124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.1544, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(271523.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0174, 0.1116],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0174, 0.1116],
        [0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0176, 0.1114],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0172, 0.1121],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0172, 0.1121],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0172, 0.1121]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1785427.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2400.6489, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(44550.5547, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(87.6970, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-404.7147, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.8048],
        [ -9.8801],
        [ -9.8975],
        ...,
        [-11.9263],
        [-11.9163],
        [-11.9159]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-827163.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(668.0437, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(668.0437, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        ...,
        [-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-115667.9922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6910, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.9939, device='cuda:0')



h[100].sum tensor(-1.5147e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.0076, device='cuda:0')



h[200].sum tensor(-567.6177, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.9533, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(277237.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0162, 0.1129],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0162, 0.1129],
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0163, 0.1127],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0160, 0.1134],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0160, 0.1134],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0160, 0.1134]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1814534., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2448.8354, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(43635.3281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(110.3302, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-389.7418, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.7288],
        [ -9.2856],
        [ -8.4922],
        ...,
        [-12.0205],
        [-12.0105],
        [-12.0102]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-851375.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(599.5914, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(599.5914, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  2.4792e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5776e-03],
        [-6.0055e-03,  2.4792e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5776e-03],
        [-6.2170e-03,  2.4216e-03, -1.5459e-02,  ..., -8.3515e-03,
         -2.6043e-09, -3.5206e-03],
        ...,
        [-6.0055e-03,  2.4792e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5776e-03],
        [-6.0055e-03,  2.4792e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5776e-03],
        [-6.0055e-03,  2.4792e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5776e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-116663.8750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4096, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.8180, device='cuda:0')



h[100].sum tensor(-1.2639e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(83.4774, device='cuda:0')



h[200].sum tensor(-567.2288, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.3718, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268407.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0543, 0.0920],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0797, 0.0759],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0937, 0.0675],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0140, 0.1160],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0140, 0.1160],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0140, 0.1160]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1765769., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2407.4414, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(44321.3398, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(135.4393, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-386.7707, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -3.9392],
        [ -4.3208],
        [ -3.6917],
        ...,
        [-12.2821],
        [-12.2721],
        [-12.2718]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-973098.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(601.5392, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(601.5392, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2504e-03,  2.4311e-03, -1.5587e-02,  ..., -8.4118e-03,
         -2.8561e-09, -3.1702e-03],
        [-6.0055e-03,  2.4939e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5567e-03],
        [-6.2043e-03,  2.4429e-03, -1.5410e-02,  ..., -8.3287e-03,
         -2.3188e-09, -3.6192e-03],
        ...,
        [-6.0055e-03,  2.4939e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5567e-03],
        [-6.0055e-03,  2.4939e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5567e-03],
        [-6.0055e-03,  2.4939e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5567e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-116971.3750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.9084, device='cuda:0')



h[100].sum tensor(-1.1996e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(83.7486, device='cuda:0')



h[200].sum tensor(-567.2571, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.4737, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268687.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1050, 0.0646],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1475, 0.0406],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1603, 0.0332],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0126, 0.1186],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0126, 0.1186],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0126, 0.1186]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1775178.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2383.1873, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(45471.6055, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(169.9811, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-388.4892, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.0922],
        [  1.2854],
        [  1.9254],
        ...,
        [-12.5147],
        [-12.5063],
        [-12.5067]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1016115.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(598.3425, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(598.3425, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  2.4701e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5388e-03],
        [-6.0055e-03,  2.4701e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5388e-03],
        [-6.2837e-03,  2.3981e-03, -1.5715e-02,  ..., -8.4719e-03,
         -3.0717e-09, -2.8234e-03],
        ...,
        [-6.0055e-03,  2.4701e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5388e-03],
        [-6.0055e-03,  2.4701e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5388e-03],
        [-6.0055e-03,  2.4701e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5388e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-117294.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4120, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.7601, device='cuda:0')



h[100].sum tensor(-1.1185e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(83.3035, device='cuda:0')



h[200].sum tensor(-567.2322, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.3064, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(267459.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0289, 0.1109],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0534, 0.0973],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0727, 0.0858],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0363, 0.1070],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0522, 0.0981],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0545, 0.0968]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1771814., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2369.3291, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(46445.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(198.2408, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-408.6088, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-7.9597],
        [-6.7543],
        [-4.6465],
        ...,
        [-6.7461],
        [-5.2314],
        [-5.0276]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-985485.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2512],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(704.8788, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2512],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(704.8788, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  2.4704e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5242e-03],
        [-6.2434e-03,  2.4092e-03, -1.5561e-02,  ..., -8.3993e-03,
         -2.4869e-09, -3.1981e-03],
        [-6.0055e-03,  2.4704e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5242e-03],
        ...,
        [-6.0055e-03,  2.4704e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5242e-03],
        [-6.0055e-03,  2.4704e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5242e-03],
        [-6.0055e-03,  2.4704e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5242e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-116545.5781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8329, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.7028, device='cuda:0')



h[100].sum tensor(-1.2509e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(98.1359, device='cuda:0')



h[200].sum tensor(-567.8137, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.8806, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(276881.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 6.7354e-02,
         8.9618e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 7.4076e-02,
         8.5234e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.0511e-01,
         6.6955e-02],
        ...,
        [0.0000e+00, 2.8782e-05, 0.0000e+00,  ..., 0.0000e+00, 1.5825e-02,
         1.1929e-01],
        [0.0000e+00, 1.4391e-05, 0.0000e+00,  ..., 0.0000e+00, 4.0130e-02,
         1.0578e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 9.6968e-02,
         7.4567e-02]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1821474.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2410.6548, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(46542.2422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(233.6206, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-385.0674, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.0425],
        [ -4.2157],
        [ -4.5873],
        ...,
        [-10.3211],
        [ -7.5459],
        [ -3.5708]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-967018.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3191],
        [0.6729],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(568.2058, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3191],
        [0.6729],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(568.2058, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.9064e-03,  2.2354e-03, -1.8106e-02,  ..., -9.5955e-03,
         -8.9098e-09,  3.2916e-03],
        [-6.3077e-03,  2.3903e-03, -1.5807e-02,  ..., -8.5153e-03,
         -2.9890e-09, -2.5658e-03],
        [-6.6428e-03,  2.3036e-03, -1.7094e-02,  ..., -9.1199e-03,
         -6.3027e-09,  7.1244e-04],
        ...,
        [-6.0055e-03,  2.4685e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5229e-03],
        [-6.0055e-03,  2.4685e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5229e-03],
        [-6.0055e-03,  2.4685e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5229e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-117931., device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.3619, device='cuda:0')



h[100].sum tensor(-9.3878e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(79.1078, device='cuda:0')



h[200].sum tensor(-567.0830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.7296, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0061],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0060],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(262140.1094, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3238, 0.0010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2900, 0.0045],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1683, 0.0398],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0118, 0.1215],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0118, 0.1215],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0118, 0.1215]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1741515.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2370.6426, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(48142.2422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(251.9475, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-371.0967, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.7025],
        [  2.1287],
        [  0.5968],
        ...,
        [-12.9984],
        [-12.9887],
        [-12.9887]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1116733.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 620.0 event: 9300 loss: tensor(426.3983, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(649.3215, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(649.3215, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  2.4362e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5059e-03],
        [-6.0055e-03,  2.4362e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5059e-03],
        [-6.7778e-03,  2.2372e-03, -1.7613e-02,  ..., -9.3636e-03,
         -7.2257e-09,  2.0633e-03],
        ...,
        [-6.0055e-03,  2.4362e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5059e-03],
        [-6.0055e-03,  2.4362e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5059e-03],
        [-6.0055e-03,  2.4362e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5059e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-117228.8906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.1252, device='cuda:0')



h[100].sum tensor(-1.0154e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(90.4010, device='cuda:0')



h[200].sum tensor(-567.5253, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.9737, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0007],
        ...,
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(270564.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1556, 0.0452],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1562, 0.0450],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1539, 0.0459],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0142, 0.1201],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0142, 0.1201],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0142, 0.1201]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1781263.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2427.5881, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(47725.9883, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(251.1396, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-381.3500, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  3.1417],
        [  3.1292],
        [  2.9110],
        ...,
        [-12.9188],
        [-12.9097],
        [-12.9099]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-982284.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(572.2815, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(572.2815, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        ...,
        [-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-117797.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3246, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.5510, device='cuda:0')



h[100].sum tensor(-8.3499e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(79.6752, device='cuda:0')



h[200].sum tensor(-567.1115, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.9429, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0023],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(263508.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.2587e-01,
         1.6982e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.9723e-01,
         2.7495e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.7633e-01,
         3.4704e-02],
        ...,
        [0.0000e+00, 1.4200e-04, 0.0000e+00,  ..., 0.0000e+00, 1.5342e-02,
         1.1893e-01],
        [0.0000e+00, 1.4200e-04, 0.0000e+00,  ..., 0.0000e+00, 1.5342e-02,
         1.1893e-01],
        [0.0000e+00, 1.4200e-04, 0.0000e+00,  ..., 0.0000e+00, 1.5342e-02,
         1.1893e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1739075.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2444.0916, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(48302.7500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(232.0931, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-416.6761, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.5055],
        [  1.7060],
        [  1.7487],
        ...,
        [-12.8112],
        [-12.8060],
        [-12.8098]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-907966.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(780.9530, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(780.9530, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        ...,
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-116037.3906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0836, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.2323, device='cuda:0')



h[100].sum tensor(-1.0630e-05, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(108.7272, device='cuda:0')



h[200].sum tensor(-568.1602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.8610, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(287571., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0116, 0.1202],
        [0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0116, 0.1202],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0122, 0.1197],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0115, 0.1206],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0115, 0.1206],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0115, 0.1206]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1875904.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2527.4985, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(47516.6406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(294.6294, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-289.7057, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.8832],
        [-10.5349],
        [ -9.8980],
        ...,
        [-13.0092],
        [-13.0000],
        [-13.0002]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1012548.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(665.0172, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(665.0172, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2399e-03,  2.0734e-03, -1.5547e-02,  ..., -8.3928e-03,
         -1.8530e-09, -3.2123e-03],
        [-6.0055e-03,  2.1575e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5135e-03],
        [-6.0055e-03,  2.1575e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5135e-03],
        ...,
        [-6.0055e-03,  2.1575e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5135e-03],
        [-6.0055e-03,  2.1575e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5135e-03],
        [-6.0055e-03,  2.1575e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5135e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-116797.8281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.8534, device='cuda:0')



h[100].sum tensor(-8.5961e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.5862, device='cuda:0')



h[200].sum tensor(-567.6133, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.7950, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0001],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(273019.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.6190e-01,
         4.3609e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 9.3040e-02,
         7.5510e-02],
        [0.0000e+00, 3.8706e-04, 0.0000e+00,  ..., 0.0000e+00, 4.2522e-02,
         1.0244e-01],
        ...,
        [0.0000e+00, 1.3013e-04, 0.0000e+00,  ..., 0.0000e+00, 8.9229e-03,
         1.2105e-01],
        [0.0000e+00, 1.3013e-04, 0.0000e+00,  ..., 0.0000e+00, 8.9229e-03,
         1.2105e-01],
        [0.0000e+00, 1.3013e-04, 0.0000e+00,  ..., 0.0000e+00, 8.9229e-03,
         1.2105e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1763123.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2543.2207, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(48549.3164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(280.1428, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-302.9631, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.2658],
        [ -2.6057],
        [ -5.7379],
        ...,
        [-12.5417],
        [-12.4076],
        [-12.3964]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1006328.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.9604],
        [0.8120],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(761.0405, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.9604],
        [0.8120],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(761.0405, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.7746e-03,  1.7176e-03, -1.7600e-02,  ..., -9.3577e-03,
         -5.7460e-09,  2.0275e-03],
        [-6.9151e-03,  1.6589e-03, -1.8140e-02,  ..., -9.6114e-03,
         -6.7964e-09,  3.4081e-03],
        [-7.6245e-03,  1.3624e-03, -2.0864e-02,  ..., -1.0891e-02,
         -1.2097e-08,  1.0374e-02],
        ...,
        [-6.0055e-03,  2.0390e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5247e-03],
        [-6.0055e-03,  2.0390e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5247e-03],
        [-6.0055e-03,  2.0390e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5247e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-115689.3125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.3084, device='cuda:0')



h[100].sum tensor(-9.1156e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(105.9549, device='cuda:0')



h[200].sum tensor(-568.0663, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.8191, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0165],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0210],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0053],
        ...,
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(286040.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4020, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4622, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4291, 0.0000],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0000, 0.0076, 0.1197],
        [0.0000, 0.0009, 0.0000,  ..., 0.0000, 0.0076, 0.1197],
        [0.0000, 0.0009, 0.0000,  ..., 0.0000, 0.0076, 0.1197]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1812711.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2556.4546, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(48419.4961, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(307.9183, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-127.0772, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.9646],
        [  1.7858],
        [  1.7738],
        ...,
        [-12.7489],
        [-12.7651],
        [-12.7674]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-942076.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(559.9310, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(559.9310, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        ...,
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-117534.8281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.9780, device='cuda:0')



h[100].sum tensor(-6.2533e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(77.9557, device='cuda:0')



h[200].sum tensor(-567.0265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.2967, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(263891.7188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0131, 0.1147],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0312, 0.1051],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0531, 0.0932],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0060, 0.1188],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0060, 0.1188],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0060, 0.1188]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1690332.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2561.0186, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(48610.2969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(252.2856, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-224.9485, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.6859],
        [ -4.9076],
        [ -2.6213],
        ...,
        [-12.7501],
        [-12.7425],
        [-12.7432]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1084732.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(779.1466, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(779.1466, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        ...,
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-115656.3750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.1485, device='cuda:0')



h[100].sum tensor(-8.2007e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(108.4757, device='cuda:0')



h[200].sum tensor(-568.1697, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.7664, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(285247.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0284, 0.1029],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0571, 0.0866],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1182, 0.0521],
        ...,
        [0.0000, 0.0009, 0.0000,  ..., 0.0000, 0.0067, 0.1153],
        [0.0000, 0.0009, 0.0000,  ..., 0.0000, 0.0067, 0.1153],
        [0.0000, 0.0009, 0.0000,  ..., 0.0000, 0.0067, 0.1153]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1783066.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2669.3281, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(45801.5586, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(222.8322, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-149.1888, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.6817],
        [ -4.3156],
        [ -1.8686],
        ...,
        [-12.4645],
        [-12.4583],
        [-12.4595]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-966541.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.9287],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(737.1028, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.9287],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(737.1028, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.9787e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5166e-03],
        [-6.8851e-03,  1.5690e-03, -1.8024e-02,  ..., -9.5572e-03,
         -5.5340e-09,  3.1482e-03],
        [-6.0055e-03,  1.9787e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5166e-03],
        ...,
        [-6.0055e-03,  1.9787e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5166e-03],
        [-6.0055e-03,  1.9787e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5166e-03],
        [-6.0055e-03,  1.9787e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5166e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-115779.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9664, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.1979, device='cuda:0')



h[100].sum tensor(-7.3409e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(102.6222, device='cuda:0')



h[200].sum tensor(-567.9982, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.5666, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0094],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0016],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0031],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(285487.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2926, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2462, 0.0111],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2581, 0.0123],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0158, 0.1064],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0131, 0.1081],
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0091, 0.1105]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1775155.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2764.7305, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(43749.4062, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(153.2489, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-154.6455, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.9926],
        [  2.8667],
        [  2.2480],
        ...,
        [-10.7098],
        [-10.8685],
        [-11.2387]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-828441.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3777],
        [0.3833],
        [0.3801],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(735.7069, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3777],
        [0.3833],
        [0.3801],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(735.7069, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3685e-03,  1.8178e-03, -1.6041e-02,  ..., -8.6250e-03,
         -2.1555e-09, -1.9467e-03],
        [-6.7232e-03,  1.6501e-03, -1.7403e-02,  ..., -9.2651e-03,
         -4.2615e-09,  1.5462e-03],
        [-6.7274e-03,  1.6481e-03, -1.7418e-02,  ..., -9.2726e-03,
         -4.2862e-09,  1.5872e-03],
        ...,
        [-6.0055e-03,  1.9894e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5215e-03],
        [-6.0055e-03,  1.9894e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5215e-03],
        [-6.0055e-03,  1.9894e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5215e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-116037.2656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9265, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.1331, device='cuda:0')



h[100].sum tensor(-6.7634e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(102.4279, device='cuda:0')



h[200].sum tensor(-567.9432, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.4936, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0018],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0022],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0021],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(280482., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2272, 0.0116],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2702, 0.0015],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2695, 0.0016],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0078, 0.1096],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0078, 0.1096],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0078, 0.1096]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1732448., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2820.5945, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41557.3750, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(112.3463, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-139.7750, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.5191],
        [  2.9699],
        [  2.7325],
        ...,
        [-11.9155],
        [-11.8814],
        [-11.8399]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-836009.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(643.6998, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(643.6998, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        ...,
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-116938.6406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5981, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.8644, device='cuda:0')



h[100].sum tensor(-5.5570e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.6183, device='cuda:0')



h[200].sum tensor(-567.4894, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.6796, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0041],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0007],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(273943.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2764, 0.0235],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1321, 0.0420],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1534, 0.0298],
        ...,
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0054, 0.1101],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0054, 0.1101],
        [0.0000, 0.0015, 0.0000,  ..., 0.0000, 0.0054, 0.1101]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1710064., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2826.7222, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(40867.2188, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(94.4177, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-153.8985, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.9679],
        [  2.4392],
        [  2.7908],
        ...,
        [-12.0620],
        [-12.0616],
        [-12.0685]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-887599.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 630.0 event: 9450 loss: tensor(433.8738, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(622.9886, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(622.9886, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        ...,
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-117290.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5086, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.9035, device='cuda:0')



h[100].sum tensor(-4.9942e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(86.7348, device='cuda:0')



h[200].sum tensor(-567.3656, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.5960, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(271458.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0023, 0.0000,  ..., 0.0000, 0.0027, 0.1112],
        [0.0000, 0.0023, 0.0000,  ..., 0.0000, 0.0027, 0.1112],
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0027, 0.1110],
        ...,
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0026, 0.1117],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0026, 0.1117],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0026, 0.1117]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1696211., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2803.2200, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41659.8047, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(97.0249, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-158.5723, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.0321],
        [ -9.9080],
        [ -9.6755],
        ...,
        [-12.2421],
        [-12.2374],
        [-12.2391]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-938893.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(618.4819, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(618.4819, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        ...,
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-117359.5312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4891, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.6944, device='cuda:0')



h[100].sum tensor(-4.6205e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(86.1074, device='cuda:0')



h[200].sum tensor(-567.3388, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.3602, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(271975.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0013, 0.1120],
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0013, 0.1120],
        [0.0000, 0.0029, 0.0000,  ..., 0.0000, 0.0013, 0.1118],
        ...,
        [0.0000, 0.0019, 0.0000,  ..., 0.0000, 0.0012, 0.1125],
        [0.0000, 0.0019, 0.0000,  ..., 0.0000, 0.0012, 0.1125],
        [0.0000, 0.0019, 0.0000,  ..., 0.0000, 0.0012, 0.1125]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1698644.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2807.7271, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41654.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(91.4269, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-173.6677, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.1664],
        [-10.1243],
        [-10.0665],
        ...,
        [-12.3639],
        [-12.3588],
        [-12.3603]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-944208.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(696.3260, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(696.3260, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        ...,
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-116717.2969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7792, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.3060, device='cuda:0')



h[100].sum tensor(-5.2060e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.9451, device='cuda:0')



h[200].sum tensor(-567.7394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.4331, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(279385., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0013, 0.1120],
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0013, 0.1120],
        [0.0000, 0.0029, 0.0000,  ..., 0.0000, 0.0013, 0.1118],
        ...,
        [0.0000, 0.0019, 0.0000,  ..., 0.0000, 0.0012, 0.1125],
        [0.0000, 0.0019, 0.0000,  ..., 0.0000, 0.0012, 0.1125],
        [0.0000, 0.0019, 0.0000,  ..., 0.0000, 0.0012, 0.1125]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1737590.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2816.4536, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41337.7305, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(101.8451, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-156.2365, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.1714],
        [-10.0933],
        [ -9.9764],
        ...,
        [-12.3586],
        [-12.3499],
        [-12.3482]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-928875.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3047],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(674.4069, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3047],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(674.4069, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5252e-03,  1.7660e-03, -1.6642e-02,  ..., -8.9077e-03,
         -2.4391e-09, -3.3502e-04],
        [-6.0055e-03,  1.9949e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4703e-03],
        [-6.2941e-03,  1.8678e-03, -1.5755e-02,  ..., -8.4907e-03,
         -1.3545e-09, -2.6186e-03],
        ...,
        [-6.0055e-03,  1.9949e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4703e-03],
        [-6.0055e-03,  1.9949e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4703e-03],
        [-6.0055e-03,  1.9949e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4703e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-116926.5938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7034, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.2891, device='cuda:0')



h[100].sum tensor(-4.7113e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.8935, device='cuda:0')



h[200].sum tensor(-567.6348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.2863, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0023],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0025],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0005],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(276671.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3784, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3446, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2782, 0.0016],
        ...,
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.1129],
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.1129],
        [0.0000, 0.0013, 0.0000,  ..., 0.0000, 0.0009, 0.1129]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1725438.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2809.3892, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39909.6484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(97.4613, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-141.4061, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.7735],
        [  2.1068],
        [  2.2617],
        ...,
        [-12.4016],
        [-12.5271],
        [-12.5514]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1011013.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(1025.1824, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(1025.1824, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.9880e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4342e-03],
        [-6.0055e-03,  1.9880e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4342e-03],
        [-6.2730e-03,  1.8757e-03, -1.5674e-02,  ..., -8.4527e-03,
         -1.1830e-09, -2.7846e-03],
        ...,
        [-6.0055e-03,  1.9880e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4342e-03],
        [-6.0055e-03,  1.9880e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4342e-03],
        [-6.0055e-03,  1.9880e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4342e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-114058.1094, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.9528, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(47.5633, device='cuda:0')



h[100].sum tensor(-6.5812e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(142.7298, device='cuda:0')



h[200].sum tensor(-569.3611, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(53.6395, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0021]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(306012.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0355, 0.0930],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0840, 0.0672],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1522, 0.0377],
        ...,
        [0.0000, 0.0022, 0.0000,  ..., 0.0000, 0.0084, 0.1088],
        [0.0000, 0.0011, 0.0000,  ..., 0.0000, 0.0309, 0.0972],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0948, 0.0643]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1854851.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2853.4404, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(38940.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(125.7561, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-101.4562, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.0706],
        [ -1.3717],
        [  0.8717],
        ...,
        [-10.2974],
        [ -8.0574],
        [ -5.0583]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-923204., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(775.4427, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(775.4427, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-115717.0938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.9766, device='cuda:0')



h[100].sum tensor(-4.6621e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(107.9601, device='cuda:0')



h[200].sum tensor(-568.1218, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.5726, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(292352.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0038, 0.1114],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0038, 0.1114],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0038, 0.1112],
        ...,
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0036, 0.1119],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0036, 0.1119],
        [0.0000, 0.0032, 0.0000,  ..., 0.0000, 0.0036, 0.1119]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1792835.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2987.4832, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39613.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(168.6511, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-146.7099, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.2949],
        [-10.2156],
        [-10.0934],
        ...,
        [-12.5476],
        [-12.5421],
        [-12.5436]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-881937.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(540.0774, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(540.0774, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-117932.7344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2141, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.0569, device='cuda:0')



h[100].sum tensor(-3.0580e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(75.1916, device='cuda:0')



h[200].sum tensor(-566.9585, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.2579, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(267003.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0052, 0.1126],
        [0.0000, 0.0040, 0.0000,  ..., 0.0000, 0.0052, 0.1126],
        [0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0053, 0.1123],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0051, 0.1131],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0051, 0.1131],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0051, 0.1131]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1680140.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2830.4829, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39807.1719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(103.7075, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-242.4478, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.5908],
        [-10.5174],
        [-10.4150],
        ...,
        [-12.7742],
        [-12.7745],
        [-12.7798]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-938605.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(485.4911, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(485.4911, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2166e-03,  1.9199e-03, -1.5457e-02,  ..., -8.3508e-03,
         -7.7866e-10, -3.2967e-03],
        [-6.2166e-03,  1.9199e-03, -1.5457e-02,  ..., -8.3508e-03,
         -7.7866e-10, -3.2967e-03],
        [-6.0055e-03,  2.0130e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3880e-03],
        ...,
        [-6.0055e-03,  2.0130e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3880e-03],
        [-6.0055e-03,  2.0130e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3880e-03],
        [-6.0055e-03,  2.0130e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3880e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-119037.1406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-613.9997, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(22.5243, device='cuda:0')



h[100].sum tensor(-2.5399e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(67.5919, device='cuda:0')



h[200].sum tensor(-566.6625, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(25.4018, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(261702.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0810, 0.0695],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0752, 0.0729],
        [0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0611, 0.0812],
        ...,
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0064, 0.1150],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0064, 0.1150],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0064, 0.1150]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1666564.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2776.6345, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(40367.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(141.8149, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-213.0964, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.1757],
        [ -6.4853],
        [ -7.1102],
        ...,
        [-13.0867],
        [-13.0793],
        [-13.0802]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1050013.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(646.3888, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.6245],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(646.3888, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5017e-03,  1.8154e-03, -1.6551e-02,  ..., -8.8653e-03,
         -1.7217e-09, -4.4864e-04],
        [-7.6002e-03,  1.3170e-03, -2.0768e-02,  ..., -1.0848e-02,
         -5.5339e-09,  1.0439e-02],
        [-6.9748e-03,  1.6007e-03, -1.8368e-02,  ..., -9.7190e-03,
         -3.3635e-09,  4.2404e-03],
        ...,
        [-6.0055e-03,  2.0404e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3660e-03],
        [-6.0055e-03,  2.0404e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3660e-03],
        [-6.0055e-03,  2.0404e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3660e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-118109.4688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5975, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.9892, device='cuda:0')



h[100].sum tensor(-3.1429e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.9927, device='cuda:0')



h[200].sum tensor(-567.4884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.8203, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0132],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0181],
        [0.0000, 0.0056, 0.0000,  ..., 0.0000, 0.0000, 0.0340],
        ...,
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(273246.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5492, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6970, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8063, 0.0000],
        ...,
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0076, 0.1156],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0076, 0.1156],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0076, 0.1156]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1721683.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2818.6770, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39581.1602, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.9131, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-222.8184, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.4277],
        [  1.2955],
        [  1.2488],
        ...,
        [-13.0874],
        [-13.0800],
        [-13.0809]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-944671.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(697.0507, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(697.0507, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.7662e-03,  1.7322e-03, -1.7567e-02,  ..., -9.3426e-03,
         -2.4819e-09,  2.2186e-03],
        [-6.3565e-03,  1.9119e-03, -1.5994e-02,  ..., -8.6033e-03,
         -1.1452e-09, -1.8480e-03],
        [-6.4541e-03,  1.8691e-03, -1.6369e-02,  ..., -8.7794e-03,
         -1.4636e-09, -8.7911e-04],
        ...,
        [-6.0055e-03,  2.0658e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3317e-03],
        [-6.0055e-03,  2.0658e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3317e-03],
        [-6.0055e-03,  2.0658e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3317e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-118089.7578, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7811, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.3396, device='cuda:0')



h[100].sum tensor(-3.1397e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(97.0460, device='cuda:0')



h[200].sum tensor(-567.7421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.4710, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0022],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(276295.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.5791e-01,
         2.5738e-04],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 2.9254e-01,
         2.6250e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 2.0371e-01,
         1.9191e-02],
        ...,
        [0.0000e+00, 4.6317e-03, 0.0000e+00,  ..., 0.0000e+00, 8.5625e-03,
         1.1612e-01],
        [0.0000e+00, 4.6317e-03, 0.0000e+00,  ..., 0.0000e+00, 8.5625e-03,
         1.1612e-01],
        [0.0000e+00, 4.6317e-03, 0.0000e+00,  ..., 0.0000e+00, 8.5625e-03,
         1.1612e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1729909.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2804.2200, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(40278.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(119.6403, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-223.7064, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.2424],
        [  2.6274],
        [  2.8365],
        ...,
        [-13.0434],
        [-13.0362],
        [-13.0372]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-880398.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 640.0 event: 9600 loss: tensor(406.7328, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(640.7275, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(640.7275, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-119105.7344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5742, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.7265, device='cuda:0')



h[100].sum tensor(-2.6771e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.2045, device='cuda:0')



h[200].sum tensor(-567.4562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.5241, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(270409.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1594, 0.0423],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1058, 0.0667],
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0606, 0.0901],
        ...,
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0096, 0.1182],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0096, 0.1182],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0096, 0.1182]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1710360.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2782.7295, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(40691.0391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(133.3901, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-230.6807, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.7515],
        [  1.3340],
        [  0.9266],
        ...,
        [-13.2591],
        [-13.2513],
        [-13.2525]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-929084.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2678],
        [0.4895],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(691.1582, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2678],
        [0.4895],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(691.1582, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2592e-03,  2.0974e-03, -1.5620e-02,  ..., -8.4277e-03,
         -7.3079e-10, -2.7474e-03],
        [-6.7015e-03,  1.9478e-03, -1.7318e-02,  ..., -9.2259e-03,
         -2.0053e-09,  1.6640e-03],
        [-6.4916e-03,  2.0188e-03, -1.6512e-02,  ..., -8.8471e-03,
         -1.4004e-09, -4.2973e-04],
        ...,
        [-6.0055e-03,  2.1831e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2770e-03],
        [-6.0055e-03,  2.1831e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2770e-03],
        [-6.0055e-03,  2.1831e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2770e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-119306.8984, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7548, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.0663, device='cuda:0')



h[100].sum tensor(-2.6680e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.2257, device='cuda:0')



h[200].sum tensor(-567.7058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.1627, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0069],
        ...,
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(271080.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2748, 0.0068],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3138, 0.0033],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3387, 0.0000],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0102, 0.1209],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0102, 0.1209],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0102, 0.1209]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1716006.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2730.6914, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(40999.0859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(172.0002, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-179.6725, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.9911],
        [  3.0050],
        [  2.8984],
        ...,
        [-13.6042],
        [-13.5947],
        [-13.5950]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1046259.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(650.9526, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(650.9526, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0022, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0022, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0022, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0022, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0022, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0022, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-119908.5781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6158, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.2009, device='cuda:0')



h[100].sum tensor(-2.3340e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(90.6281, device='cuda:0')



h[200].sum tensor(-567.5138, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.0591, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(272272.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0099, 0.1215],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0099, 0.1215],
        [0.0000, 0.0049, 0.0000,  ..., 0.0000, 0.0099, 0.1213],
        ...,
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0097, 0.1219],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0097, 0.1219],
        [0.0000, 0.0039, 0.0000,  ..., 0.0000, 0.0097, 0.1219]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1739039.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2685.6958, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41792.7969, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(157.4600, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-186.3551, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.6741],
        [-11.5406],
        [-11.3072],
        ...,
        [-13.6247],
        [-13.6144],
        [-13.6144]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1047304.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5044],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(579.1711, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.5044],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(579.1711, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3156e-03,  2.0389e-03, -1.5837e-02,  ..., -8.5295e-03,
         -7.8741e-10, -2.0626e-03],
        [-6.4832e-03,  2.0013e-03, -1.6480e-02,  ..., -8.8320e-03,
         -1.2131e-09, -3.7801e-04],
        [-6.5239e-03,  1.9921e-03, -1.6636e-02,  ..., -8.9054e-03,
         -1.3165e-09,  3.0932e-05],
        ...,
        [-6.0055e-03,  2.1085e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1784e-03],
        [-6.0055e-03,  2.1085e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1784e-03],
        [-6.0055e-03,  2.1085e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1784e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-120709.8516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3383, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.8706, device='cuda:0')



h[100].sum tensor(-1.9090e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(80.6344, device='cuda:0')



h[200].sum tensor(-567.1302, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.3033, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 8.2701e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.1606e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         3.0932e-05],
        [0.0000e+00, 7.8924e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         5.1700e-03],
        ...,
        [0.0000e+00, 8.4342e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.4342e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 8.4342e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(263362.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1504, 0.0513],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2265, 0.0237],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3194, 0.0062],
        ...,
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0091, 0.1224],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0091, 0.1224],
        [0.0000, 0.0042, 0.0000,  ..., 0.0000, 0.0091, 0.1224]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1692574., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2701.9487, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42298.4375, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(118.1165, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-276.6055, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.7535],
        [  2.2869],
        [  2.7109],
        ...,
        [-13.5355],
        [-13.5237],
        [-13.5231]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-932979.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.6533],
        [0.6255],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(640.5796, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.6533],
        [0.6255],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(640.5796, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6243e-03,  1.9262e-03, -1.7022e-02,  ..., -9.0865e-03,
         -1.4742e-09,  1.0746e-03],
        [-6.5979e-03,  1.9312e-03, -1.6920e-02,  ..., -9.0389e-03,
         -1.4114e-09,  8.0911e-04],
        [-6.8591e-03,  1.8811e-03, -1.7923e-02,  ..., -9.5103e-03,
         -2.0337e-09,  3.4402e-03],
        ...,
        [-6.0055e-03,  2.0449e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1578e-03],
        [-6.0055e-03,  2.0449e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1578e-03],
        [-6.0055e-03,  2.0449e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1578e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-120517.0703, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5658, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.7197, device='cuda:0')



h[100].sum tensor(-1.9497e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.1839, device='cuda:0')



h[200].sum tensor(-567.4446, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.5163, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0016],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0129],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0137],
        ...,
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(267270.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2945, 0.0078],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3858, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3910, 0.0000],
        ...,
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0090, 0.1239],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0090, 0.1239],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0090, 0.1239]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1713963., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2659.8611, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41781.0664, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(136.7327, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-258.6869, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.6667],
        [  2.4832],
        [  2.3410],
        ...,
        [-13.6897],
        [-13.6767],
        [-13.6757]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-990443.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6323],
        [0.0000],
        [0.5942],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(544.2288, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6323],
        [0.0000],
        [0.5942],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(544.2288, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.9692e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1613e-03],
        [-7.1672e-03,  1.7565e-03, -1.9105e-02,  ..., -1.0066e-02,
         -2.5953e-09,  6.5507e-03],
        [-6.0055e-03,  1.9692e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1613e-03],
        ...,
        [-6.0055e-03,  1.9692e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1613e-03],
        [-6.0055e-03,  1.9692e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1613e-03],
        [-6.0055e-03,  1.9692e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.1613e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-121615.9844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2225, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.2495, device='cuda:0')



h[100].sum tensor(-1.5365e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(75.7696, device='cuda:0')



h[200].sum tensor(-566.9703, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.4751, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0099],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0123],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258163.5781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2976, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2326, 0.0217],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3192, 0.0000],
        ...,
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0088, 0.1258],
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0088, 0.1258],
        [0.0000, 0.0027, 0.0000,  ..., 0.0000, 0.0088, 0.1258]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1671513.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2621.7610, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(42330.2031, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(229.1797, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-210.6564, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.9487],
        [  2.8679],
        [  2.7415],
        ...,
        [-13.8851],
        [-13.8713],
        [-13.8701]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1069801.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(511.0820, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(511.0820, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-121896.7109, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.0901, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.7116, device='cuda:0')



h[100].sum tensor(-1.3220e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(71.1548, device='cuda:0')



h[200].sum tensor(-566.7874, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.7408, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(259809.7812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0092, 0.1254],
        [0.0000, 0.0030, 0.0000,  ..., 0.0000, 0.0092, 0.1254],
        [0.0000, 0.0034, 0.0000,  ..., 0.0000, 0.0092, 0.1252],
        ...,
        [0.0000, 0.0024, 0.0000,  ..., 0.0000, 0.0090, 0.1258],
        [0.0000, 0.0024, 0.0000,  ..., 0.0000, 0.0090, 0.1258],
        [0.0000, 0.0024, 0.0000,  ..., 0.0000, 0.0090, 0.1258]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1681447., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2771.0347, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41488.0859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(459.7709, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-57.4748, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.6508],
        [-11.2363],
        [-11.4971],
        ...,
        [-13.7509],
        [-13.7369],
        [-13.7355]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1038938.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(585.2054, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(585.2054, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-121499.7344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3877, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.1506, device='cuda:0')



h[100].sum tensor(-1.4070e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(81.4745, device='cuda:0')



h[200].sum tensor(-567.1986, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.6191, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(265083.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0020, 0.0000,  ..., 0.0000, 0.0129, 0.1221],
        [0.0000, 0.0010, 0.0000,  ..., 0.0000, 0.0283, 0.1143],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0664, 0.0941],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0103, 0.1239],
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0103, 0.1239],
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0103, 0.1239]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1709142.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2629.5083, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39356.3320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(147.3938, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-251.0228, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.6422],
        [ -7.9890],
        [ -5.5130],
        ...,
        [-13.4874],
        [-13.4737],
        [-13.4724]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-978684.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(950.9340, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(950.9340, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        ...,
        [-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0052],
        [-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0052]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-118849.6719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.6923, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(44.1185, device='cuda:0')



h[100].sum tensor(-2.0624e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(132.3926, device='cuda:0')



h[200].sum tensor(-569.0012, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(49.7547, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(294709.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0119, 0.1211],
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0149, 0.1196],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0307, 0.1116],
        ...,
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0117, 0.1215],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0117, 0.1215],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0117, 0.1215]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1857302.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2684.8640, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(36121.1484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(73.0554, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-254.6641, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.3712],
        [ -9.2343],
        [ -7.2202],
        ...,
        [-13.2122],
        [-13.1199],
        [-12.7362]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-884125.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(589.1174, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(589.1174, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5484e-03,  1.7807e-03, -1.6730e-02,  ..., -8.9496e-03,
         -9.3309e-10,  2.5472e-04],
        [-6.0055e-03,  1.9033e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2234e-03],
        [-6.5789e-03,  1.7738e-03, -1.6847e-02,  ..., -9.0047e-03,
         -9.8554e-10,  5.6269e-04],
        ...,
        [-6.0055e-03,  1.9033e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2234e-03],
        [-6.0055e-03,  1.9033e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2234e-03],
        [-6.0055e-03,  1.9033e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2234e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-122102.3906, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3944, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.3321, device='cuda:0')



h[100].sum tensor(-1.1876e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(82.0192, device='cuda:0')



h[200].sum tensor(-567.2078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.8237, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0008],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0035],
        ...,
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(265216.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 2.8628e-01,
         1.2271e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 2.6277e-01,
         4.2111e-03],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 3.0684e-01,
         1.3952e-02],
        ...,
        [0.0000e+00, 4.1511e-05, 0.0000e+00,  ..., 0.0000e+00, 1.2573e-02,
         1.2054e-01],
        [0.0000e+00, 4.1511e-05, 0.0000e+00,  ..., 0.0000e+00, 1.2573e-02,
         1.2054e-01],
        [0.0000e+00, 4.1511e-05, 0.0000e+00,  ..., 0.0000e+00, 1.2573e-02,
         1.2054e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1710519., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2643.7134, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(36073.3672, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(18.4891, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-333.9912, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.1830],
        [  2.1338],
        [  1.9672],
        ...,
        [-13.1140],
        [-13.1020],
        [-13.1012]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-906714., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 650.0 event: 9750 loss: tensor(436.0814, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(664.8849, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(664.8849, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.8945e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2592e-03],
        [-6.0055e-03,  1.8945e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2592e-03],
        [-6.0055e-03,  1.8945e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2592e-03],
        ...,
        [-6.2578e-03,  1.8273e-03, -1.5615e-02,  ..., -8.4252e-03,
         -4.0548e-10, -2.7168e-03],
        [-6.0055e-03,  1.8945e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2592e-03],
        [-6.0055e-03,  1.8945e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.2592e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-121890.3750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.8473, device='cuda:0')



h[100].sum tensor(-1.2216e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.5678, device='cuda:0')



h[200].sum tensor(-567.5841, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.7880, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(271047.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0130, 0.1199],
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0289, 0.1122],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0607, 0.0972],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0606, 0.0940],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0498, 0.1002],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0244, 0.1140]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1745740.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2665.6240, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(35365.3555, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(33.4551, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-311.9542, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -9.5321],
        [ -7.6542],
        [ -4.6299],
        ...,
        [-10.0938],
        [-10.8233],
        [-11.7494]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-898397.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(631.4736, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(631.4736, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        ...,
        [-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0053],
        [-0.0060,  0.0019, -0.0146,  ..., -0.0080,  0.0000, -0.0053]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-122534.2031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5471, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.2972, device='cuda:0')



h[100].sum tensor(-1.0616e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.9161, device='cuda:0')



h[200].sum tensor(-567.4188, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.0399, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268685.4375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0123, 0.1207],
        [0.0000, 0.0004, 0.0000,  ..., 0.0000, 0.0123, 0.1207],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0129, 0.1202],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0121, 0.1212],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0121, 0.1212],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0121, 0.1212]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1736777.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2630.9077, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(35817.3906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(55.0089, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-288.5558, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.5000],
        [-10.3343],
        [ -9.7645],
        ...,
        [-13.1636],
        [-13.1519],
        [-13.1513]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-969912.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4404],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(685.9840, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4404],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(685.9840, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3512e-03,  1.7829e-03, -1.5973e-02,  ..., -8.5937e-03,
         -5.1935e-10, -1.8253e-03],
        [-6.4226e-03,  1.7605e-03, -1.6247e-02,  ..., -8.7227e-03,
         -6.2669e-10, -1.1065e-03],
        [-6.4865e-03,  1.7404e-03, -1.6492e-02,  ..., -8.8378e-03,
         -7.2257e-10, -4.6451e-04],
        ...,
        [-6.0055e-03,  1.8914e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3028e-03],
        [-6.0055e-03,  1.8914e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3028e-03],
        [-6.0055e-03,  1.8914e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3028e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-122102.1250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7421, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.8262, device='cuda:0')



h[100].sum tensor(-1.1499e-06, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(95.5053, device='cuda:0')



h[200].sum tensor(-567.6884, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.8920, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0027],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0076, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(269731.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2793, 0.0055],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2345, 0.0146],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2308, 0.0171],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0121, 0.1212],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0121, 0.1212],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0121, 0.1212]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1729494.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2642.4878, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(35643.3125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(56.7937, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-289.9579, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  3.3252],
        [  3.3482],
        [  3.3458],
        ...,
        [-13.1636],
        [-13.1519],
        [-13.1513]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-934068.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(591.6161, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(591.6161, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.8839e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3343e-03],
        [-6.0055e-03,  1.8839e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3343e-03],
        [-7.0903e-03,  1.5058e-03, -1.8810e-02,  ..., -9.9275e-03,
         -1.5224e-09,  5.5664e-03],
        ...,
        [-6.0055e-03,  1.8839e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3343e-03],
        [-6.0055e-03,  1.8839e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3343e-03],
        [-6.0055e-03,  1.8839e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3343e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-123127.6719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3865, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.4480, device='cuda:0')



h[100].sum tensor(-9.0142e-07, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(82.3670, device='cuda:0')



h[200].sum tensor(-567.1969, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.9545, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0069],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0040],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(262504.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2782, 0.0096],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3411, 0.0072],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3772, 0.0031],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0110, 0.1220],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0110, 0.1220],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0110, 0.1220]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1692686.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2584.4236, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(37035.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(66.4073, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-271.8945, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.9991],
        [  1.8022],
        [  1.6558],
        ...,
        [-13.2651],
        [-13.2535],
        [-13.2527]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1039664.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4307],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(611.0690, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.4307],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(611.0690, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.8658e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3572e-03],
        [-6.4134e-03,  1.7173e-03, -1.6212e-02,  ..., -8.7060e-03,
         -5.3437e-10, -1.2607e-03],
        [-6.0055e-03,  1.8658e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3572e-03],
        ...,
        [-6.0055e-03,  1.8658e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3572e-03],
        [-6.0055e-03,  1.8658e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3572e-03],
        [-6.0055e-03,  1.8658e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3572e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-123054.6562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4696, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.3505, device='cuda:0')



h[100].sum tensor(-8.5102e-07, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(85.0753, device='cuda:0')



h[200].sum tensor(-567.3117, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.9723, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0081],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(265339.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0823, 0.0884],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1446, 0.0608],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3125, 0.0166],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0107, 0.1216],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0107, 0.1216],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0107, 0.1216]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1708994.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2634.8706, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(37350.2656, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(70.7739, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-278.5436, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.6115],
        [ -1.6751],
        [  0.8716],
        ...,
        [-13.3953],
        [-13.3860],
        [-13.3862]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1017441.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(672.0842, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(672.0842, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2827e-03,  1.7091e-03, -1.5711e-02,  ..., -8.4702e-03,
         -3.3885e-10, -2.5895e-03],
        [-6.0055e-03,  1.8141e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3735e-03],
        [-6.0055e-03,  1.8141e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3735e-03],
        ...,
        [-6.0055e-03,  1.8141e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3735e-03],
        [-6.0055e-03,  1.8141e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3735e-03],
        [-6.0055e-03,  1.8141e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3735e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-122610.8047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6824, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.1813, device='cuda:0')



h[100].sum tensor(-8.4465e-07, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.5701, device='cuda:0')



h[200].sum tensor(-567.6058, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.1647, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(269108.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1374, 0.0592],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0766, 0.0888],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0294, 0.1114],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0109, 0.1209],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0109, 0.1209],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0109, 0.1209]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1706455.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2630.5073, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(38385.5703, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(82.1682, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-241.8115, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.0002],
        [ -4.3079],
        [ -7.3805],
        ...,
        [-13.2864],
        [-13.3645],
        [-13.4001]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1038337.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2878],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(611.0099, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2878],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(611.0099, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2275e-03,  1.6848e-03, -1.5499e-02,  ..., -8.3706e-03,
         -2.5299e-10, -3.1405e-03],
        [-6.2781e-03,  1.6653e-03, -1.5693e-02,  ..., -8.4619e-03,
         -3.1063e-10, -2.6321e-03],
        [-6.0055e-03,  1.7707e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3717e-03],
        ...,
        [-6.0055e-03,  1.7707e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3717e-03],
        [-6.0055e-03,  1.7707e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3717e-03],
        [-6.0055e-03,  1.7707e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3717e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-123038.5469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4579, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.3478, device='cuda:0')



h[100].sum tensor(-6.9525e-07, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(85.0671, device='cuda:0')



h[200].sum tensor(-567.2955, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.9692, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0005],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0005],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264545.5625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2433, 0.0155],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1690, 0.0453],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0989, 0.0761],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0116, 0.1193],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0116, 0.1193],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0116, 0.1193]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1685105.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2686.5708, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39328.1328, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(78.3805, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-273.8227, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.7087],
        [  0.9452],
        [ -1.4036],
        ...,
        [-11.6182],
        [-12.8235],
        [-13.2243]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-947328.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(778.6202, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(778.6202, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-121700.3516, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0667, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.1241, device='cuda:0')



h[100].sum tensor(-7.9666e-07, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(108.4025, device='cuda:0')



h[200].sum tensor(-568.1367, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.7389, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(283705.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2458, 0.0233],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1928, 0.0378],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1115, 0.0699],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0123, 0.1182],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0123, 0.1182],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0123, 0.1182]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1799799.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2763.0691, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(37806.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(111.5859, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-243.5148, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.4761],
        [  0.9661],
        [ -1.6119],
        ...,
        [-13.3447],
        [-13.3295],
        [-13.2490]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-929504.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(558.0760, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(558.0760, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-123589.7422, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2636, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.8919, device='cuda:0')



h[100].sum tensor(-5.1580e-07, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(77.6975, device='cuda:0')



h[200].sum tensor(-567.0271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.1996, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(262760.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.6356e-02,
         1.1590e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.4001e-02,
         1.1712e-01],
        [0.0000e+00, 5.1700e-05, 0.0000e+00,  ..., 0.0000e+00, 1.2955e-02,
         1.1747e-01],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.2696e-02,
         1.1819e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.2696e-02,
         1.1819e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.2696e-02,
         1.1819e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1683602.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2711.6450, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(38358.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(96.4919, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-271.1544, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.3945],
        [-10.5241],
        [-10.6435],
        ...,
        [-13.4441],
        [-13.4369],
        [-13.4378]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1020515.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(568.2886, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(568.2886, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-123587.4844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3041, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.3657, device='cuda:0')



h[100].sum tensor(-4.7145e-07, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(79.1193, device='cuda:0')



h[200].sum tensor(-567.0830, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.7339, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(263014.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0357, 0.1071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0195, 0.1149],
        [0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0148, 0.1171],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0123, 0.1190],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0123, 0.1190],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0123, 0.1190]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1685053.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2721.7473, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39369.2500, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(115.0548, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-268.8989, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.6101],
        [ -8.5653],
        [ -9.5565],
        ...,
        [-13.4622],
        [-13.4554],
        [-13.4566]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-977208.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 660.0 event: 9900 loss: tensor(396.2472, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(584.2970, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(584.2970, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-123570.4141, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3620, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.1084, device='cuda:0')



h[100].sum tensor(-4.3252e-07, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(81.3480, device='cuda:0')



h[200].sum tensor(-567.1631, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.5715, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(263200.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0762, 0.0905],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1097, 0.0753],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1677, 0.0487],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0115, 0.1201],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0115, 0.1201],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0115, 0.1201]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1688957.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2705.1414, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(40633.5898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(140.3606, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-260.2004, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.4973],
        [  1.8873],
        [  2.3557],
        ...,
        [-13.5355],
        [-13.5284],
        [-13.5296]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-980508.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3469],
        [0.2426],
        [0.3691],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(654.0195, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3469],
        [0.2426],
        [0.3691],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(654.0195, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.9272e-03,  1.4237e-03, -1.8183e-02,  ..., -9.6331e-03,
         -7.3126e-10,  3.8847e-03],
        [-7.1696e-03,  1.3030e-03, -1.9113e-02,  ..., -1.0071e-02,
         -9.2366e-10,  6.3239e-03],
        [-6.7369e-03,  1.5183e-03, -1.7453e-02,  ..., -9.2897e-03,
         -5.8027e-10,  1.9705e-03],
        ...,
        [-6.0055e-03,  1.8821e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3860e-03],
        [-6.0055e-03,  1.8821e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3860e-03],
        [-6.0055e-03,  1.8821e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3860e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-123180.2812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6166, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.3432, device='cuda:0')



h[100].sum tensor(-4.2975e-07, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.0551, device='cuda:0')



h[200].sum tensor(-567.5149, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.2196, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0158],
        [0.0000, 0.0057, 0.0000,  ..., 0.0000, 0.0000, 0.0162],
        [0.0000, 0.0055, 0.0000,  ..., 0.0000, 0.0000, 0.0185],
        ...,
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(267109.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6087, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6528, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6644, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0102, 0.1222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0102, 0.1222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0102, 0.1222]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1712192.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2661.5142, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(40153.5977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(180.4210, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-237.2485, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.2383],
        [  1.1634],
        [  1.1495],
        ...,
        [-13.8044],
        [-13.7962],
        [-13.7970]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1079189.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3547],
        [0.3411],
        [0.3718],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(585.3971, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3547],
        [0.3411],
        [0.3718],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(585.3971, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.6913e-03,  1.4819e-03, -1.7278e-02,  ..., -9.2075e-03,
         -5.0487e-10,  1.5286e-03],
        [-6.6936e-03,  1.4807e-03, -1.7287e-02,  ..., -9.2117e-03,
         -5.0657e-10,  1.5518e-03],
        [-6.3285e-03,  1.6662e-03, -1.5886e-02,  ..., -8.5529e-03,
         -2.3780e-10, -2.1246e-03],
        ...,
        [-6.0055e-03,  1.8303e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3772e-03],
        [-6.0055e-03,  1.8303e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3772e-03],
        [-6.0055e-03,  1.8303e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3772e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-123797.8047, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3806, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.1595, device='cuda:0')



h[100].sum tensor(-3.4288e-07, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(81.5012, device='cuda:0')



h[200].sum tensor(-567.1887, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.6291, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0058, 0.0000,  ..., 0.0000, 0.0000, 0.0084],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0017],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0032],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(261897.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4106, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3250, 0.0059],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2974, 0.0050],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0098, 0.1237],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0098, 0.1237],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0098, 0.1237]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1685296., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2605.8882, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(40819.9297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(184.8497, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-236.4885, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.1286],
        [  2.5768],
        [  2.8908],
        ...,
        [-13.8986],
        [-13.8899],
        [-13.8906]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1148436.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4182],
        [0.6533],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(749.0883, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4182],
        [0.6533],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(749.0883, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-8.0170e-03,  6.7364e-04, -2.2364e-02,  ..., -1.1600e-02,
         -1.3727e-09,  1.4923e-02],
        [-6.8789e-03,  1.2651e-03, -1.7997e-02,  ..., -9.5459e-03,
         -5.9598e-10,  3.4461e-03],
        [-7.2199e-03,  1.0879e-03, -1.9306e-02,  ..., -1.0161e-02,
         -8.2870e-10,  6.8851e-03],
        ...,
        [-6.0055e-03,  1.7190e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3609e-03],
        [-6.0055e-03,  1.7190e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3609e-03],
        [-6.0055e-03,  1.7190e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3609e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-122349.2734, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9685, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.7539, device='cuda:0')



h[100].sum tensor(-3.8276e-07, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(104.2909, device='cuda:0')



h[200].sum tensor(-568.0011, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.1937, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0044, 0.0000,  ..., 0.0000, 0.0000, 0.0269],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0000, 0.0416],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0000, 0.0183],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(278209.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8224, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.9116, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8719, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0114, 0.1236],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0114, 0.1236],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0114, 0.1236]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1762093.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2633.1946, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39860.1367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(195.9927, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-195.0031, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.0532],
        [  1.0409],
        [  1.0331],
        ...,
        [-13.7661],
        [-13.7579],
        [-13.7587]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1038165.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(695.8319, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(695.8319, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0016, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-122716.6250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7758, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.2831, device='cuda:0')



h[100].sum tensor(-3.1090e-07, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.8764, device='cuda:0')



h[200].sum tensor(-567.7348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.4073, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(270268.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0433, 0.1102],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0192, 0.1205],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0143, 0.1227],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0135, 0.1236],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0135, 0.1236],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0135, 0.1236]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1712728.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2680.6956, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(38842.5977, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(193.3821, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-210.1154, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.1157],
        [ -8.5615],
        [ -9.9431],
        ...,
        [-13.7792],
        [-13.7710],
        [-13.7719]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-985889.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8315],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(755.0613, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.8315],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(755.0613, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  1.6836e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3573e-03],
        [-6.7931e-03,  1.2816e-03, -1.7668e-02,  ..., -9.3911e-03,
         -4.6041e-10,  2.6025e-03],
        [-6.0055e-03,  1.6836e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3573e-03],
        ...,
        [-6.0055e-03,  1.6836e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3573e-03],
        [-6.0055e-03,  1.6836e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3573e-03],
        [-6.0055e-03,  1.6836e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3573e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-122050.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9933, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.0310, device='cuda:0')



h[100].sum tensor(-2.9198e-07, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(105.1225, device='cuda:0')



h[200].sum tensor(-568.0355, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.5062, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0053, 0.0000,  ..., 0.0000, 0.0000, 0.0075],
        [0.0000, 0.0064, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0063, 0.0000,  ..., 0.0000, 0.0000, 0.0026],
        ...,
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(279147.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2452, 0.0312],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1569, 0.0657],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1426, 0.0704],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0147, 0.1243],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0147, 0.1243],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0147, 0.1243]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1744371.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2808.8618, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(37278.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(174.3224, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-364.6897, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.1229],
        [  2.5284],
        [  2.5436],
        ...,
        [-13.7679],
        [-13.7578],
        [-13.7578]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-990248.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(568.8191, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(568.8191, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-123358.4609, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3124, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.3903, device='cuda:0')



h[100].sum tensor(-1.8931e-07, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(79.1931, device='cuda:0')



h[200].sum tensor(-567.0945, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.7617, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(266071.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0157, 0.1244],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0157, 0.1244],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0158, 0.1242],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0155, 0.1248],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0155, 0.1248],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0155, 0.1248]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1671781.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2921.4524, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(37212.8633, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(121.6079, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-577.2178, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.3853],
        [-11.3677],
        [-11.2292],
        ...,
        [-13.6718],
        [-13.6838],
        [-13.6848]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-956915.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(613.4459, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(613.4459, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5163e-03,  1.5917e-03, -1.6606e-02,  ..., -8.8916e-03,
         -2.5469e-10, -1.7910e-04],
        [-6.0055e-03,  1.8264e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3560e-03],
        [-6.0055e-03,  1.8264e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3560e-03],
        ...,
        [-6.0055e-03,  1.8264e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3560e-03],
        [-6.0055e-03,  1.8264e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3560e-03],
        [-6.0055e-03,  1.8264e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3560e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-123476.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4827, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.4608, device='cuda:0')



h[100].sum tensor(-1.7334e-07, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(85.4063, device='cuda:0')



h[200].sum tensor(-567.3298, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.0967, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268762.6875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1867, 0.0490],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1376, 0.0716],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0568, 0.1071],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0157, 0.1260],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0157, 0.1260],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0157, 0.1260]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1701544.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2844.1592, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(36408.2578, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(123.4749, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-469.2418, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.0288],
        [ -1.7057],
        [ -5.2985],
        ...,
        [-13.7853],
        [-13.7725],
        [-13.7717]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-999678.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4358],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(640.0647, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4358],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(640.0647, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2015e-03,  1.8098e-03, -1.5398e-02,  ..., -8.3236e-03,
         -9.0083e-11, -3.3787e-03],
        [-6.6142e-03,  1.6252e-03, -1.6982e-02,  ..., -9.0684e-03,
         -2.7981e-10,  8.0800e-04],
        [-6.0055e-03,  1.8974e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3666e-03],
        ...,
        [-6.0055e-03,  1.8974e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3666e-03],
        [-6.0055e-03,  1.8974e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3666e-03],
        [-6.0055e-03,  1.8974e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.3666e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-123699.6719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5687, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.6958, device='cuda:0')



h[100].sum tensor(-1.5011e-07, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.1122, device='cuda:0')



h[200].sum tensor(-567.4487, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.4894, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 6.3145e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         7.4622e-03],
        [0.0000e+00, 7.1925e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         3.3720e-05],
        [0.0000e+00, 7.2460e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         8.0800e-04],
        ...,
        [0.0000e+00, 7.5898e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.5898e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.5898e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(269912.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3787, 0.0060],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2379, 0.0359],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1607, 0.0641],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0161, 0.1270],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0161, 0.1270],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0161, 0.1270]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1713909.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2776.7068, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(36106.6758, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(142.2776, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-385.6763, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.3162],
        [  1.2406],
        [ -1.0398],
        ...,
        [-13.8717],
        [-13.8581],
        [-13.8569]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1069073.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(640.6439, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(640.6439, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-124036.9922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.7226, device='cuda:0')



h[100].sum tensor(-1.2324e-07, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(89.1929, device='cuda:0')



h[200].sum tensor(-567.4595, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.5197, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(265810.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0169, 0.1270],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0169, 0.1270],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0170, 0.1268],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0167, 0.1274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0167, 0.1274],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0167, 0.1274]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1691232., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2691.3318, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(35377.2773, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(157.6707, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-297.7151, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.7581],
        [-11.7393],
        [-11.6767],
        ...,
        [-13.8785],
        [-13.8645],
        [-13.8632]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1031023.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 670.0 event: 10050 loss: tensor(429.1556, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(627.0079, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(627.0079, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-124779.1641, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5348, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.0900, device='cuda:0')



h[100].sum tensor(-9.6487e-08, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.2944, device='cuda:0')



h[200].sum tensor(-567.4018, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.8063, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(265011.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0182, 0.1265],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0182, 0.1265],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0183, 0.1263],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0180, 0.1269],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0180, 0.1269],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0180, 0.1269]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1707761.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2604.2896, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(34997.9766, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(168.3943, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-146.6281, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.7084],
        [-11.6687],
        [-11.5672],
        ...,
        [-13.8631],
        [-13.8492],
        [-13.8479]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1029390.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(782.6793, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(782.6793, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        ...,
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0054],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0054]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-124073.7969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1241, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.3124, device='cuda:0')



h[100].sum tensor(-9.2743e-08, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(108.9676, device='cuda:0')



h[200].sum tensor(-568.2162, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.9513, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(280394.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0317, 0.1203],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0522, 0.1115],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1217, 0.0811],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0192, 0.1260],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0192, 0.1260],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0192, 0.1260]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1791692.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2607.5496, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(34557.6367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(190.4027, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-112.6671, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.9672],
        [ -6.4062],
        [ -2.7531],
        ...,
        [-13.8561],
        [-13.8423],
        [-13.8411]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-987864.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(630.2289, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(630.2289, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  2.0966e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4550e-03],
        [-6.0055e-03,  2.0966e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4550e-03],
        [-6.2451e-03,  1.9961e-03, -1.5565e-02,  ..., -8.4022e-03,
         -7.8433e-11, -3.0251e-03],
        ...,
        [-6.0055e-03,  2.0966e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4550e-03],
        [-6.0055e-03,  2.0966e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4550e-03],
        [-6.0055e-03,  2.0966e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4550e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-125680.4297, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5338, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.2394, device='cuda:0')



h[100].sum tensor(-5.3567e-08, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.7429, device='cuda:0')



h[200].sum tensor(-567.4004, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.9748, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0003],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(267970.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0513, 0.1100],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1033, 0.0861],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1726, 0.0552],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0185, 0.1253],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0185, 0.1253],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0185, 0.1253]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1710236.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2458.6516, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(35534.7422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(114.0454, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-95.1042, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -5.8025],
        [ -2.5420],
        [  0.3169],
        ...,
        [-13.8685],
        [-13.8547],
        [-13.8535]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1018451.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2595],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(655.8695, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2595],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(655.8695, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.5483e-03,  1.8897e-03, -1.6728e-02,  ..., -8.9494e-03,
         -1.6261e-10,  9.5763e-06],
        [-6.5894e-03,  1.8726e-03, -1.6886e-02,  ..., -9.0236e-03,
         -1.7494e-10,  4.2651e-04],
        [-6.2294e-03,  2.0225e-03, -1.5505e-02,  ..., -8.3740e-03,
         -6.7082e-11, -3.2221e-03],
        ...,
        [-6.0055e-03,  2.1157e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4914e-03],
        [-6.0055e-03,  2.1157e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4914e-03],
        [-6.0055e-03,  2.1157e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.4914e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-125737.6562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6245, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.4290, device='cuda:0')



h[100].sum tensor(-3.6347e-08, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(91.3126, device='cuda:0')



h[200].sum tensor(-567.5257, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.3163, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0012],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0029],
        [0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0034],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(270051.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3341, 0.0101],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3591, 0.0046],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3396, 0.0085],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0165, 0.1247],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0165, 0.1247],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0165, 0.1247]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1698000.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2317.3916, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(37059.9141, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(66.0928, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-31.8605, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.6789],
        [  2.8741],
        [  2.7916],
        ...,
        [-13.9369],
        [-13.9234],
        [-13.9223]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1047024.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(497.1412, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(497.1412, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.7711e-03,  1.7646e-03, -1.7583e-02,  ..., -9.3514e-03,
         -2.0948e-10,  2.2451e-03],
        [-6.7167e-03,  1.7875e-03, -1.7374e-02,  ..., -9.2533e-03,
         -1.9460e-10,  1.6941e-03],
        [-7.2031e-03,  1.5822e-03, -1.9240e-02,  ..., -1.0131e-02,
         -3.2770e-10,  6.6233e-03],
        ...,
        [-6.0055e-03,  2.0878e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5126e-03],
        [-6.0055e-03,  2.0878e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5126e-03],
        [-6.0055e-03,  2.0878e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5126e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-127571.4062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.0370, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.0648, device='cuda:0')



h[100].sum tensor(-1.4276e-08, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(69.2139, device='cuda:0')



h[200].sum tensor(-566.7139, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.0114, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0075, 0.0000,  ..., 0.0000, 0.0000, 0.0025],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0100],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0139],
        ...,
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258338.4062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3744, 0.0077],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5030, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5874, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0165, 0.1239],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0165, 0.1239],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0165, 0.1239]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1658878.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2390.2715, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39129.6484, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(101.3254, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-77.6134, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.1524],
        [  1.7661],
        [  1.3926],
        ...,
        [-13.8089],
        [-13.7958],
        [-13.7949]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1069314.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(636.2589, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(636.2589, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        ...,
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0055],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-127011.5938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.5192, device='cuda:0')



h[100].sum tensor(-2.9220e-09, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(88.5824, device='cuda:0')



h[200].sum tensor(-567.4171, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.2903, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264157.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0581, 0.1014],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0679, 0.0996],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1256, 0.0764],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0158, 0.1240],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0158, 0.1240],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0158, 0.1240]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1692637., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2631.1316, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(38795.3320, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(207.1299, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-114.6017, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.8638],
        [ -4.5532],
        [ -1.6481],
        ...,
        [-13.8176],
        [-13.8049],
        [-13.8040]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1017837.1875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2920],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(622.7213, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2920],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(622.7213, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  2.0696e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5647e-03],
        [-6.2821e-03,  1.9499e-03, -1.5707e-02,  ..., -8.4690e-03,
         -6.2735e-11, -2.7640e-03],
        [-6.2020e-03,  1.9846e-03, -1.5400e-02,  ..., -8.3246e-03,
         -4.4585e-11, -3.5743e-03],
        ...,
        [-6.0055e-03,  2.0696e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5647e-03],
        [-6.0055e-03,  2.0696e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5647e-03],
        [-6.0055e-03,  2.0696e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5647e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-127721.0469, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5028, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.8911, device='cuda:0')



h[100].sum tensor(1.0682e-08, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(86.6976, device='cuda:0')



h[200].sum tensor(-567.3577, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.5820, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264056.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0773, 0.0945],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1326, 0.0683],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1855, 0.0436],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0146, 0.1239],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0146, 0.1239],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0146, 0.1239]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1691696.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2606.7280, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39585.8984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(231.0056, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-82.0652, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.6587],
        [ -1.5467],
        [  0.6820],
        ...,
        [-13.9081],
        [-13.8955],
        [-13.8947]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1128244.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2930],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(543.0565, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2930],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(543.0565, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2830e-03,  2.0295e-03, -1.5711e-02,  ..., -8.4707e-03,
         -5.7116e-11, -2.7584e-03],
        [-6.2128e-03,  2.0595e-03, -1.5442e-02,  ..., -8.3440e-03,
         -4.2671e-11, -3.4695e-03],
        [-6.4903e-03,  1.9409e-03, -1.6506e-02,  ..., -8.8447e-03,
         -9.9787e-11, -6.5803e-04],
        ...,
        [-6.0055e-03,  2.1480e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5699e-03],
        [-6.0055e-03,  2.1480e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5699e-03],
        [-6.0055e-03,  2.1480e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5699e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-128816.6875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2176, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.1951, device='cuda:0')



h[100].sum tensor(-31.8724, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(75.6064, device='cuda:0')



h[200].sum tensor(-566.9634, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.4138, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0004],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(260175.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2265, 0.0298],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2539, 0.0165],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1828, 0.0439],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0163, 0.1218],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0163, 0.1218],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0163, 0.1218]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1688292.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2609.8103, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(40019.1211, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(221.8958, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-126.1853, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  3.3747],
        [  3.0599],
        [  1.7645],
        ...,
        [-13.7207],
        [-13.7085],
        [-13.7078]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1023117.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.9639],
        [0.3210],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(686.3151, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.9639],
        [0.3210],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(686.3151, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.9184e-03,  1.7843e-03, -1.8147e-02,  ..., -9.6172e-03,
         -1.7007e-10,  3.6586e-03],
        [-6.3096e-03,  2.0497e-03, -1.5813e-02,  ..., -8.5186e-03,
         -5.6647e-11, -2.5087e-03],
        [-8.0079e-03,  1.3093e-03, -2.2326e-02,  ..., -1.1583e-02,
         -3.7306e-10,  1.4696e-02],
        ...,
        [-6.0055e-03,  2.1823e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5889e-03],
        [-6.0055e-03,  2.1823e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5889e-03],
        [-6.0055e-03,  2.1823e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5889e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-128030.8203, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7570, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.8416, device='cuda:0')



h[100].sum tensor(-57.8184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(95.5514, device='cuda:0')



h[200].sum tensor(-567.7087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.9093, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0265],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0204],
        ...,
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(271608.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2744, 0.0336],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5164, 0.0031],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5848, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0149, 0.1214],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0149, 0.1214],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0149, 0.1214]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1734932.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2641.1538, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39188.7148, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(286.5726, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-104.9271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.3904],
        [  1.4210],
        [  1.2484],
        ...,
        [-13.8688],
        [-13.8570],
        [-13.8565]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1073372.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2976],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(813.2690, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.2976],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(813.2690, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  2.1986e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5917e-03],
        [-6.5020e-03,  1.9722e-03, -1.6551e-02,  ..., -8.8659e-03,
         -8.3482e-11, -5.6049e-04],
        [-6.2202e-03,  2.1007e-03, -1.5470e-02,  ..., -8.3573e-03,
         -3.6091e-11, -3.4166e-03],
        ...,
        [-6.0055e-03,  2.1986e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5917e-03],
        [-6.0055e-03,  2.1986e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5917e-03],
        [-6.0055e-03,  2.1986e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5917e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-127279.0156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2164, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.7316, device='cuda:0')



h[100].sum tensor(-76.1917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(113.2264, device='cuda:0')



h[200].sum tensor(-568.3436, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.5518, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0002],
        ...,
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(280978.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1374, 0.0638],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1908, 0.0398],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2647, 0.0163],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0159, 0.1203],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0159, 0.1203],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0159, 0.1203]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1779529.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2669.8789, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(38205.8828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(285.8252, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-139.2771, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.1259],
        [  1.4075],
        [  2.7341],
        ...,
        [-13.8154],
        [-13.8037],
        [-13.8033]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-992776.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(549.4364, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(549.4364, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0022, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0022, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0022, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        ...,
        [-0.0060,  0.0022, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0022, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0022, -0.0146,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-129456.4531, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2294, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.4911, device='cuda:0')



h[100].sum tensor(-76.1915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(76.4946, device='cuda:0')



h[200].sum tensor(-566.9797, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.7476, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(262073.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0161, 0.1198],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0161, 0.1198],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0162, 0.1196],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0159, 0.1203],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0159, 0.1203],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0159, 0.1203]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1698967.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2626.5601, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39176.9297, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(251.2212, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-181.9894, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.1238],
        [-10.7673],
        [-10.0435],
        ...,
        [-13.8154],
        [-13.8037],
        [-13.8033]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1034085.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6211],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(630.9174, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6211],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(630.9174, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  2.2205e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5943e-03],
        [-6.5937e-03,  1.9434e-03, -1.6902e-02,  ..., -9.0314e-03,
         -8.8976e-11,  3.6868e-04],
        [-6.0055e-03,  2.2205e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5943e-03],
        ...,
        [-6.0055e-03,  2.2205e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5943e-03],
        [-6.0055e-03,  2.2205e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5943e-03],
        [-6.0055e-03,  2.2205e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.5943e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-128810.8750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5428, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.2714, device='cuda:0')



h[100].sum tensor(-86.1873, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(87.8387, device='cuda:0')



h[200].sum tensor(-567.4128, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.0108, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0007],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0004],
        ...,
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0089, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(274831.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1915, 0.0435],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1257, 0.0713],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1356, 0.0671],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0180, 0.1186],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0180, 0.1186],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0180, 0.1186]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1767324., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2453.2456, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39227.8984, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(68.7394, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-280.7576, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.5141],
        [  2.4313],
        [  2.9068],
        ...,
        [-13.6833],
        [-13.6718],
        [-13.6713]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-950246.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3457],
        [0.3203],
        [0.3201],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(617.3879, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3457],
        [0.3203],
        [0.3201],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(617.3879, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.0641e-03,  1.7789e-03, -1.8706e-02,  ..., -9.8802e-03,
         -1.4355e-10,  5.1065e-03],
        [-7.4320e-03,  1.6012e-03, -2.0117e-02,  ..., -1.0544e-02,
         -1.9344e-10,  8.8338e-03],
        [-7.3389e-03,  1.6462e-03, -1.9760e-02,  ..., -1.0376e-02,
         -1.8081e-10,  7.8908e-03],
        ...,
        [-6.0055e-03,  2.2903e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6187e-03],
        [-6.0055e-03,  2.2903e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6187e-03],
        [-6.0055e-03,  2.2903e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6187e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-129490.8594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4977, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.6437, device='cuda:0')



h[100].sum tensor(-87.8316, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(85.9551, device='cuda:0')



h[200].sum tensor(-567.3505, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.3029, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0065, 0.0000,  ..., 0.0000, 0.0000, 0.0335],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0264],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0280],
        ...,
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0092, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268749.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8330, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7783, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7330, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0153, 0.1188],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0153, 0.1188],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0153, 0.1188]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1736762.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2659.4607, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39088.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(313.5486, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-190.8802, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.9361],
        [  0.9521],
        [  0.9903],
        ...,
        [-13.9638],
        [-13.9520],
        [-13.9515]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1079692.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4817],
        [0.3882],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(603.3099, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4817],
        [0.3882],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(603.3099, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3732e-03,  2.1607e-03, -1.6056e-02,  ..., -8.6334e-03,
         -4.4517e-11, -1.9019e-03],
        [-6.4617e-03,  2.1172e-03, -1.6396e-02,  ..., -8.7932e-03,
         -5.5240e-11, -1.0046e-03],
        [-6.3732e-03,  2.1607e-03, -1.6056e-02,  ..., -8.6334e-03,
         -4.4517e-11, -1.9019e-03],
        ...,
        [-6.0055e-03,  2.3412e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6272e-03],
        [-6.0055e-03,  2.3412e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6272e-03],
        [-6.0055e-03,  2.3412e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6272e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-130138.2031, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4271, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.9905, device='cuda:0')



h[100].sum tensor(-81.8721, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(83.9951, device='cuda:0')



h[200].sum tensor(-567.2529, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.5663, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0012],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0012],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(267706., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2377, 0.0213],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2121, 0.0329],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1448, 0.0612],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0157, 0.1180],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0157, 0.1180],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0157, 0.1180]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1738128.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2679.9192, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39501.7344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(355.0952, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-184.5877, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  3.1940],
        [  1.8894],
        [ -0.8408],
        ...,
        [-13.7557],
        [-13.8427],
        [-13.9072]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1090976.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(771.6254, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(771.6254, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        ...,
        [-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0024, -0.0146,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-129222.8281, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0299, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.7995, device='cuda:0')



h[100].sum tensor(-69.6157, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(107.4286, device='cuda:0')



h[200].sum tensor(-568.0859, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.3729, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(281136.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0214, 0.1152],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0308, 0.1105],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0582, 0.0969],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0183, 0.1170],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0183, 0.1170],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0183, 0.1170]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1806644.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2670.5728, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39380.1562, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(352.7595, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-184.1342, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.6066],
        [ -9.7818],
        [ -8.4855],
        ...,
        [-13.8155],
        [-13.8032],
        [-13.8026]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-958294.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.4028],
        [0.5376],
        [0.4321],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(692.6974, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.4028],
        [0.5376],
        [0.4321],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(692.6974, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.3142e-03,  1.7146e-03, -1.9665e-02,  ..., -1.0331e-02,
         -1.2459e-10,  7.5608e-03],
        [-6.7963e-03,  1.9909e-03, -1.7679e-02,  ..., -9.3969e-03,
         -7.5287e-11,  2.3279e-03],
        [-6.9468e-03,  1.9106e-03, -1.8256e-02,  ..., -9.6686e-03,
         -8.9618e-11,  3.8489e-03],
        ...,
        [-6.0055e-03,  2.4127e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6623e-03],
        [-6.0055e-03,  2.4127e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6623e-03],
        [-6.0055e-03,  2.4127e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6623e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-130337.4844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7517, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.1377, device='cuda:0')



h[100].sum tensor(-52.7444, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(96.4399, device='cuda:0')



h[200].sum tensor(-567.7016, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.2433, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0000, 0.0261],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0274],
        [0.0000, 0.0077, 0.0000,  ..., 0.0000, 0.0000, 0.0151],
        ...,
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(272812.8438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6959, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6849, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5729, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0159, 0.1187],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0159, 0.1187],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0159, 0.1187]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1767908., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2671.7507, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39037.4688, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(348.3958, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-217.8252, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.1260],
        [  1.1713],
        [  1.4321],
        ...,
        [-14.0923],
        [-14.0796],
        [-14.0788]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1024795., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2467],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(753.2385, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2467],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(753.2385, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2492e-03,  2.2593e-03, -1.5581e-02,  ..., -8.4098e-03,
         -2.0404e-11, -3.2600e-03],
        [-6.2392e-03,  2.2651e-03, -1.5543e-02,  ..., -8.3916e-03,
         -1.9560e-11, -3.3614e-03],
        [-6.0055e-03,  2.4005e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.7128e-03],
        ...,
        [-6.0055e-03,  2.4005e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.7128e-03],
        [-6.0055e-03,  2.4005e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.7128e-03],
        [-6.0055e-03,  2.4005e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.7128e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-130292.4922, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9848, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.9465, device='cuda:0')



h[100].sum tensor(-33.1399, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(104.8687, device='cuda:0')



h[200].sum tensor(-568.0237, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.4109, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0053],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(278625., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3062, 0.0134],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1925, 0.0441],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1262, 0.0680],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0139, 0.1202],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0139, 0.1202],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0139, 0.1202]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1791988.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2634.7722, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39159.6523, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(369.0142, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-198.3468, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.6958],
        [  2.5926],
        [  2.4760],
        ...,
        [-14.4051],
        [-14.3920],
        [-14.3911]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1124785.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.7236],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(771.8608, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.7236],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(771.8608, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  2.3741e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.7368e-03],
        [-6.6909e-03,  1.9630e-03, -1.7274e-02,  ..., -9.2067e-03,
         -5.0103e-11,  1.1436e-03],
        [-6.2521e-03,  2.2262e-03, -1.5592e-02,  ..., -8.4149e-03,
         -1.8025e-11, -3.2615e-03],
        ...,
        [-6.0055e-03,  2.3741e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.7368e-03],
        [-6.0055e-03,  2.3741e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.7368e-03],
        [-6.0055e-03,  2.3741e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.7368e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-130501.3438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0381, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.8105, device='cuda:0')



h[100].sum tensor(-12.7146, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(107.4614, device='cuda:0')



h[200].sum tensor(-568.0973, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.3852, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0011],
        [0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0060],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0129],
        ...,
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0095, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(279154.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4523, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4055, 0.0026],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4422, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0142, 0.1201],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0142, 0.1201],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0142, 0.1201]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1807282.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2669.0049, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(38892.5859, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(367.3667, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-226.8136, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.2512],
        [  1.3388],
        [  1.3838],
        ...,
        [-14.4950],
        [-14.4858],
        [-14.4852]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1080670.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(670.0121, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.2510],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(670.0121, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2432e-03,  2.2038e-03, -1.5558e-02,  ..., -8.3989e-03,
         -1.5053e-11, -3.3656e-03],
        [-6.5236e-03,  2.0342e-03, -1.6633e-02,  ..., -8.9048e-03,
         -3.2807e-11, -5.5513e-04],
        [-7.7464e-03,  1.2942e-03, -2.1321e-02,  ..., -1.1111e-02,
         -1.1024e-10,  1.1703e-02],
        ...,
        [-6.0055e-03,  2.3477e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.7485e-03],
        [-6.0055e-03,  2.3477e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.7485e-03],
        [-6.0055e-03,  2.3477e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.7485e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-131499.4062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6915, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.0852, device='cuda:0')



h[100].sum tensor(6.7393, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.2816, device='cuda:0')



h[200].sum tensor(-567.6184, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.0563, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0084, 0.0000,  ..., 0.0000, 0.0000, 0.0047],
        [0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0255],
        [0.0000, 0.0066, 0.0000,  ..., 0.0000, 0.0000, 0.0242],
        ...,
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(273192.5312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4057, 0.0209],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6749, 0.0028],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.7833, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0164, 0.1187],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0164, 0.1187],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0164, 0.1187]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1775950.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2657.6499, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39277.8281, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(335.8958, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-267.2163, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.2493],
        [  1.0771],
        [  1.0042],
        ...,
        [-14.3691],
        [-14.3563],
        [-14.3556]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1010958.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(744.3934, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(744.3934, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  2.2602e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.7791e-03],
        [-6.0055e-03,  2.2602e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.7791e-03],
        [-6.0055e-03,  2.2602e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.7791e-03],
        ...,
        [-6.0055e-03,  2.2602e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.7791e-03],
        [-6.0055e-03,  2.2602e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.7791e-03],
        [-6.4397e-03,  1.9810e-03, -1.6311e-02,  ..., -8.7535e-03,
         -2.3590e-11, -1.4381e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-131433.5781, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.9413, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.5361, device='cuda:0')



h[100].sum tensor(-218.1113, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(103.6373, device='cuda:0')



h[200].sum tensor(-567.9635, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(38.9481, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(277008.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0374, 0.1089],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0210, 0.1161],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0185, 0.1171],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0413, 0.1079],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0992, 0.0843],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1736, 0.0551]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1785506.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2720.8706, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(38338.8438, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(357.6931, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-272.6450, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.5242],
        [ -9.5146],
        [-10.6316],
        ...,
        [-10.2095],
        [ -6.9081],
        [ -3.3366]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-944831.0625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 690.0 event: 10350 loss: tensor(813.4241, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.6284],
        [0.0000],
        [0.4326],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(546.9941, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.6284],
        [0.0000],
        [0.4326],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(546.9941, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-7.1118e-03,  1.3934e-03, -1.8888e-02,  ..., -9.9663e-03,
         -5.0948e-11,  5.2174e-03],
        [-7.6230e-03,  1.0438e-03, -2.0848e-02,  ..., -1.0889e-02,
         -7.4490e-11,  1.0314e-02],
        [-7.0007e-03,  1.4694e-03, -1.8462e-02,  ..., -9.7658e-03,
         -4.5830e-11,  4.1094e-03],
        ...,
        [-6.0055e-03,  2.1501e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.8113e-03],
        [-6.0055e-03,  2.1501e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.8113e-03],
        [-6.0055e-03,  2.1501e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.8113e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-133512.4688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2284, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.3778, device='cuda:0')



h[100].sum tensor(-419.9040, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(76.1546, device='cuda:0')



h[200].sum tensor(-566.9785, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.6198, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0043, 0.0000,  ..., 0.0000, 0.0000, 0.0400],
        [0.0000, 0.0052, 0.0000,  ..., 0.0000, 0.0000, 0.0267],
        [0.0000, 0.0045, 0.0000,  ..., 0.0000, 0.0000, 0.0359],
        ...,
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(261139.8594, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8722, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8357, 0.0000],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.8142, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0193, 0.1178],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0193, 0.1178],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0193, 0.1178]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1697610., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2700.0120, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39441.6094, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(371.3982, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-268.8199, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.9751],
        [  0.9713],
        [  0.9910],
        ...,
        [-14.4054],
        [-14.3906],
        [-14.3886]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1056046.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(705.9219, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(705.9219, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0058],
        ...,
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0021, -0.0146,  ..., -0.0080,  0.0000, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-132615.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8160, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.7512, device='cuda:0')



h[100].sum tensor(-602.9999, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(98.2811, device='cuda:0')



h[200].sum tensor(-567.7905, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(36.9352, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(273463.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0215, 0.1166],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0215, 0.1166],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0216, 0.1163],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0213, 0.1171],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0213, 0.1171],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0213, 0.1171]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1756371.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2808.9082, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(38375.3828, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(391.8855, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-265.1989, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.7556],
        [-11.4756],
        [-10.9295],
        ...,
        [-14.2880],
        [-14.2755],
        [-14.2749]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-932691.5625, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.3293],
        [0.2595],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(539.0309, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.3293],
        [0.2595],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(539.0309, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.3174e-03,  1.8964e-03, -1.5842e-02,  ..., -8.5328e-03,
         -9.8249e-12, -2.7074e-03],
        [-6.2513e-03,  1.9438e-03, -1.5589e-02,  ..., -8.4135e-03,
         -7.7419e-12, -3.3657e-03],
        [-6.5518e-03,  1.7285e-03, -1.6741e-02,  ..., -8.9557e-03,
         -1.7206e-11, -3.7456e-04],
        ...,
        [-6.0055e-03,  2.1199e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.8126e-03],
        [-6.0055e-03,  2.1199e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.8126e-03],
        [-6.0055e-03,  2.1199e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.8126e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-134413.6250, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.1967, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.0083, device='cuda:0')



h[100].sum tensor(-766.1426, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(75.0459, device='cuda:0')



h[200].sum tensor(-566.9346, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.2031, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0085, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258405.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1264, 0.0705],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2050, 0.0376],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2316, 0.0257],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0213, 0.1177],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0213, 0.1177],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0213, 0.1177]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1685761.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2774.2290, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(40925.0078, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(372.9458, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-301.3399, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -2.7497],
        [  0.4556],
        [  2.4580],
        ...,
        [-14.1904],
        [-14.1778],
        [-14.1771]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-929452.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(532.5537, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(532.5537, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  2.0549e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.8240e-03],
        [-6.0055e-03,  2.0549e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.8240e-03],
        [-6.2347e-03,  1.8847e-03, -1.5525e-02,  ..., -8.3835e-03,
         -5.7602e-12, -3.5445e-03],
        ...,
        [-6.0055e-03,  2.0549e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.8240e-03],
        [-6.0055e-03,  2.0549e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.8240e-03],
        [-6.0055e-03,  2.0549e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.8240e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-134954.1719, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.1730, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.7078, device='cuda:0')



h[100].sum tensor(-913.9414, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(74.1441, device='cuda:0')



h[200].sum tensor(-566.9019, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.8642, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0082, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258844.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0404, 0.1091],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0849, 0.0868],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1007, 0.0788],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0516, 0.1064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0207, 0.1194],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0180, 0.1206]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1676798.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2761.4551, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41871.0391, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(424.8746, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-213.4648, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.9217],
        [ -7.2165],
        [ -5.9119],
        ...,
        [ -7.4048],
        [-10.9822],
        [-13.1858]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1139517.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(784.9929, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(784.9929, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  2.0065e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.8177e-03],
        [-6.0055e-03,  2.0065e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.8177e-03],
        [-6.7276e-03,  1.4609e-03, -1.7415e-02,  ..., -9.2730e-03,
         -1.3952e-11,  1.3645e-03],
        ...,
        [-6.0055e-03,  2.0065e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.8177e-03],
        [-6.0055e-03,  2.0065e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.8177e-03],
        [-6.0055e-03,  2.0065e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.8177e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-133273.0312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.1067, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(36.4197, device='cuda:0')



h[100].sum tensor(-1049.6777, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(109.2897, device='cuda:0')



h[200].sum tensor(-568.1921, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(41.0723, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0014],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0014],
        ...,
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(278296.2812, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0792, 0.0967],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2039, 0.0477],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3013, 0.0200],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0501, 0.1078],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0183, 0.1213],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0155, 0.1226]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1755993.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2817.0574, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(41648.3086, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(482.6195, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-123.3836, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.3459],
        [ -0.9147],
        [  1.3992],
        ...,
        [ -7.3923],
        [-11.0313],
        [-13.2938]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1160451., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(816.4719, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(816.4719, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0058],
        ...,
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0058],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0000, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-133059.0312, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.2229, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(37.8802, device='cuda:0')



h[100].sum tensor(-1170.3224, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(113.6723, device='cuda:0')



h[200].sum tensor(-568.3527, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(42.7194, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0079, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(283350.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.4791e-02,
         1.2142e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.4791e-02,
         1.2142e-01],
        [0.0000e+00, 4.0147e-05, 0.0000e+00,  ..., 0.0000e+00, 1.9987e-02,
         1.1909e-01],
        ...,
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.4603e-02,
         1.2188e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.4603e-02,
         1.2188e-01],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 1.4603e-02,
         1.2188e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1773144.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2877.8372, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(40477.8125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(481.8021, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-123.1828, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.4442],
        [-10.5223],
        [ -8.8921],
        ...,
        [-14.5143],
        [-14.5017],
        [-14.5011]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1057983.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(566.4194, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(566.4194, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2630e-03,  1.7575e-03, -1.5634e-02,  ..., -8.4346e-03,
         -2.3726e-12, -3.1542e-03],
        [-6.5226e-03,  1.5756e-03, -1.6629e-02,  ..., -8.9031e-03,
         -4.7644e-12, -5.5583e-04],
        [-6.2651e-03,  1.7560e-03, -1.5642e-02,  ..., -8.4384e-03,
         -2.3918e-12, -3.1334e-03],
        ...,
        [-6.0055e-03,  1.9379e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.7317e-03],
        [-6.0055e-03,  1.9379e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.7317e-03],
        [-6.0055e-03,  1.9379e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.7317e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-135041.5625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.2790, device='cuda:0')



h[100].sum tensor(-1275.6931, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(78.8591, device='cuda:0')



h[200].sum tensor(-567.0822, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.6361, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0067, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0078, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264499.3125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 2.5393e-01,
         1.7812e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 2.4876e-01,
         1.9932e-02],
        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 2.0869e-01,
         3.3660e-02],
        ...,
        [0.0000e+00, 2.3443e-05, 0.0000e+00,  ..., 0.0000e+00, 1.4680e-02,
         1.2046e-01],
        [0.0000e+00, 2.3443e-05, 0.0000e+00,  ..., 0.0000e+00, 1.4680e-02,
         1.2046e-01],
        [0.0000e+00, 2.3443e-05, 0.0000e+00,  ..., 0.0000e+00, 1.4680e-02,
         1.2046e-01]], device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1680883.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2945.0183, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(39748.5898, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(442.6982, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-187.8110, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  3.1943],
        [  2.9765],
        [  1.6105],
        ...,
        [-14.2750],
        [-14.2632],
        [-14.2629]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-991281.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.5933]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(748.2438, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.5933]], device='cuda:0') 
g.ndata[nfet].sum tensor(748.2438, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2073e-03,  1.7543e-03, -1.5420e-02,  ..., -8.3341e-03,
         -9.7875e-13, -3.6883e-03],
        [-6.2017e-03,  1.7582e-03, -1.5399e-02,  ..., -8.3239e-03,
         -9.5129e-13, -3.7451e-03],
        [-6.4420e-03,  1.5934e-03, -1.6320e-02,  ..., -8.7576e-03,
         -2.1168e-12, -1.3343e-03],
        ...,
        [-6.0055e-03,  1.8926e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.7128e-03],
        [-6.5674e-03,  1.5075e-03, -1.6800e-02,  ..., -8.9839e-03,
         -2.7249e-12, -7.6528e-05],
        [-6.0055e-03,  1.8926e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.7128e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-133700.5156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0081, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(34.7147, device='cuda:0')



h[100].sum tensor(-1375.6350, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(104.1733, device='cuda:0')



h[200].sum tensor(-568.0558, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(39.1495, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0068, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0062, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(279441.9375, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1850, 0.0389],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1994, 0.0303],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1837, 0.0351],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0895, 0.0889],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1111, 0.0794],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1792, 0.0513]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1733335.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3022.6917, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(36620.1406, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(468.3967, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-59.1754, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ 3.2304],
        [ 3.1429],
        [ 3.0003],
        ...,
        [-7.5388],
        [-5.2396],
        [-4.1194]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1058285.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(591.1495, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(591.1495, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.2192e-03,  1.7167e-03, -1.5466e-02,  ..., -8.3556e-03,
         -1.9303e-13, -3.5264e-03],
        [-6.2192e-03,  1.7167e-03, -1.5466e-02,  ..., -8.3556e-03,
         -1.9303e-13, -3.5264e-03],
        [-6.0055e-03,  1.8578e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6771e-03],
        ...,
        [-6.0055e-03,  1.8578e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6771e-03],
        [-6.0055e-03,  1.8578e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6771e-03],
        [-6.0055e-03,  1.8578e-03, -1.4647e-02,  ..., -7.9700e-03,
          0.0000e+00, -5.6771e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-135375.3438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3947, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.4264, device='cuda:0')



h[100].sum tensor(-1461.2605, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(82.3021, device='cuda:0')



h[200].sum tensor(-567.2082, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.9301, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0074, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(266121., device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1059, 0.0722],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1157, 0.0672],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1063, 0.0714],
        ...,
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0155, 0.1187],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0155, 0.1187],
        [0.0000, 0.0007, 0.0000,  ..., 0.0000, 0.0155, 0.1187]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1673942.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3061.3342, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(35177.1250, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(472.9211, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-93.5591, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -2.9992],
        [ -2.2705],
        [ -2.9744],
        ...,
        [-14.1977],
        [-14.1938],
        [-14.1984]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1044630.8125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(544.6637, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(544.6637, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        ...,
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0056],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0000, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-135956.7344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2198, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.2696, device='cuda:0')



h[100].sum tensor(-1539.6699, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(75.8301, device='cuda:0')



h[200].sum tensor(-566.9666, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.4978, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000e+00, 6.8708e-03, 0.0000e+00,  ..., 0.0000e+00, 1.9121e-12,
         0.0000e+00],
        [0.0000e+00, 7.3242e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.3242e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        ...,
        [0.0000e+00, 7.3242e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.3242e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00],
        [0.0000e+00, 7.3242e-03, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,
         0.0000e+00]], device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264157.6562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1226, 0.0677],
        [0.0000, 0.0011, 0.0000,  ..., 0.0000, 0.0538, 0.0990],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0341, 0.1085],
        ...,
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0145, 0.1189],
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0145, 0.1189],
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0145, 0.1189]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1659321.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3051.7256, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(34943.5000, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(438.0031, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-88.4434, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.9851],
        [ -3.3558],
        [ -4.6032],
        ...,
        [-14.0680],
        [-14.0572],
        [-14.0572]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-977568., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 700.0 event: 10500 loss: tensor(435.4497, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(662.4711, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(662.4711, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0001, -0.0056],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0001, -0.0056],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0001, -0.0056],
        ...,
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0001, -0.0056],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0001, -0.0056],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0001, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-135292.1406, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6458, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.7353, device='cuda:0')



h[100].sum tensor(-1612.6812, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.2317, device='cuda:0')



h[200].sum tensor(-567.5551, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.6618, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        ...,
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0005, 0.0000],
        [0.0000, 0.0072, 0.0000,  ..., 0.0000, 0.0005, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(276498.3438, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0130, 0.1191],
        [0.0000, 0.0047, 0.0000,  ..., 0.0000, 0.0130, 0.1191],
        [0.0000, 0.0051, 0.0000,  ..., 0.0000, 0.0131, 0.1189],
        ...,
        [0.0000, 0.0011, 0.0000,  ..., 0.0000, 0.0227, 0.1147],
        [0.0000, 0.0021, 0.0000,  ..., 0.0000, 0.0194, 0.1164],
        [0.0000, 0.0036, 0.0000,  ..., 0.0000, 0.0145, 0.1188]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1731602.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3044.8831, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(34600.8906, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(446.0024, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-46.5237, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.3652],
        [-11.3568],
        [-11.3173],
        ...,
        [-10.9304],
        [-11.1553],
        [-11.9175]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-948496.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(583.9957, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(583.9957, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0023, -0.0055],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0023, -0.0055],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0023, -0.0055],
        ...,
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0023, -0.0055],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0023, -0.0055],
        [-0.0060,  0.0018, -0.0146,  ..., -0.0080,  0.0023, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-136142.6562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3716, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.0945, device='cuda:0')



h[100].sum tensor(-1675.7415, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(81.3061, device='cuda:0')



h[200].sum tensor(-567.1763, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.5558, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0070, 0.0000,  ..., 0.0000, 0.0094, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        ...,
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0093, 0.0000],
        [0.0000, 0.0071, 0.0000,  ..., 0.0000, 0.0093, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268361.5938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0731, 0.0879],
        [0.0000, 0.0002, 0.0000,  ..., 0.0000, 0.0476, 0.1012],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0278, 0.1109],
        ...,
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0098, 0.1206],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0098, 0.1206],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0098, 0.1206]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1655989.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3071.4707, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(35129.6953, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(667.4646, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(44.0878, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -5.8437],
        [ -6.8338],
        [ -8.0733],
        ...,
        [-14.4272],
        [-14.4177],
        [-14.4182]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1069368.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(545.8679, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(545.8679, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0045, -0.0055],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0045, -0.0055],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0045, -0.0055],
        ...,
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0045, -0.0055],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0045, -0.0055],
        [-0.0060,  0.0017, -0.0146,  ..., -0.0080,  0.0045, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-136630.7812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2292, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.3255, device='cuda:0')



h[100].sum tensor(-1732.9470, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(75.9978, device='cuda:0')



h[200].sum tensor(-566.9796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.5609, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0181, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0181, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0181, 0.0000],
        ...,
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0181, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0181, 0.0000],
        [0.0000, 0.0069, 0.0000,  ..., 0.0000, 0.0181, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(266221.0625, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0019, 0.0000,  ..., 0.0000, 0.0498, 0.1016],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0181, 0.1156],
        [0.0000, 0.0048, 0.0000,  ..., 0.0000, 0.0071, 0.1205],
        ...,
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0069, 0.1213],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0069, 0.1213],
        [0.0000, 0.0038, 0.0000,  ..., 0.0000, 0.0069, 0.1213]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1636619., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3145.6680, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(35177.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(904.3632, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(117.1226, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.0026],
        [ -8.8726],
        [-10.5764],
        ...,
        [-14.7704],
        [-14.7622],
        [-14.7633]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1146446.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(599.2334, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(599.2334, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0025, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0025, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0025, -0.0055],
        ...,
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0025, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0025, -0.0055],
        [-0.0060,  0.0020, -0.0146,  ..., -0.0080,  0.0025, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-136608.2656, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4172, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.8014, device='cuda:0')



h[100].sum tensor(-1785.9087, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(83.4275, device='cuda:0')



h[200].sum tensor(-567.2394, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.3530, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        ...,
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0100, 0.0000],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0100, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268584.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0078, 0.1192],
        [0.0000, 0.0037, 0.0000,  ..., 0.0000, 0.0078, 0.1192],
        [0.0000, 0.0041, 0.0000,  ..., 0.0000, 0.0083, 0.1188],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0204, 0.1147],
        [0.0000, 0.0016, 0.0000,  ..., 0.0000, 0.0109, 0.1184],
        [0.0000, 0.0031, 0.0000,  ..., 0.0000, 0.0076, 0.1197]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1645178.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3132.6445, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(32945.8125, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(538.8167, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(27.2216, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.8875],
        [-10.1392],
        [ -9.0151],
        ...,
        [ -9.3791],
        [-10.8957],
        [-12.4650]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1009411., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(768.8475, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(768.8475, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0012, -0.0055],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0012, -0.0055],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0012, -0.0055],
        ...,
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0012, -0.0055],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0012, -0.0055],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0012, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-135744.5938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0443, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.6707, device='cuda:0')



h[100].sum tensor(-1835.8301, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(107.0419, device='cuda:0')



h[200].sum tensor(-568.1060, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.2276, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        ...,
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0050, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(280131.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0023, 0.0000,  ..., 0.0000, 0.0079, 0.1193],
        [0.0000, 0.0024, 0.0000,  ..., 0.0000, 0.0089, 0.1188],
        [0.0000, 0.0028, 0.0000,  ..., 0.0000, 0.0115, 0.1172],
        ...,
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0078, 0.1198],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0078, 0.1198],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0078, 0.1198]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1708846., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3173.6484, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(29682.6426, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(304.0236, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-17.4683, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-10.9951],
        [-10.3272],
        [ -9.3403],
        ...,
        [-14.4780],
        [-14.4712],
        [-14.4728]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-919169.9375, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(649.3375, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(649.3375, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0012, -0.0055],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0012, -0.0055],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0012, -0.0055],
        ...,
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0012, -0.0055],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0012, -0.0055],
        [-0.0060,  0.0023, -0.0146,  ..., -0.0080,  0.0012, -0.0055]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-136725.1875, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5964, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.1260, device='cuda:0')



h[100].sum tensor(-1833.7095, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(90.4032, device='cuda:0')



h[200].sum tensor(-567.4870, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.9746, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0090, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        ...,
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0050, 0.0000],
        [0.0000, 0.0091, 0.0000,  ..., 0.0000, 0.0050, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(270227.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0779, 0.0903],
        [0.0000, 0.0012, 0.0000,  ..., 0.0000, 0.0253, 0.1115],
        [0.0000, 0.0026, 0.0000,  ..., 0.0000, 0.0105, 0.1179],
        ...,
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0078, 0.1198],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0078, 0.1198],
        [0.0000, 0.0017, 0.0000,  ..., 0.0000, 0.0078, 0.1198]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1660875.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3133.5542, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(30311.8594, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(288.3269, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-29.3381, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -4.0849],
        [ -7.2882],
        [ -9.3467],
        ...,
        [-14.4002],
        [-14.0894],
        [-13.0099]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-964295.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.3572],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(673.1945, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.3572],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(673.1945, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0065,  0.0022, -0.0166,  ..., -0.0089,  0.0008, -0.0004],
        [-0.0063,  0.0023, -0.0159,  ..., -0.0086,  0.0008, -0.0021],
        [-0.0060,  0.0025, -0.0146,  ..., -0.0080,  0.0007, -0.0056],
        ...,
        [-0.0060,  0.0025, -0.0146,  ..., -0.0080,  0.0007, -0.0056],
        [-0.0060,  0.0025, -0.0146,  ..., -0.0080,  0.0007, -0.0056],
        [-0.0060,  0.0025, -0.0146,  ..., -0.0080,  0.0007, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-137275.5938, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7003, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.2328, device='cuda:0')



h[100].sum tensor(-1876.5354, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.7247, device='cuda:0')



h[200].sum tensor(-567.6306, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.2228, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0086, 0.0000,  ..., 0.0000, 0.0032, 0.0015],
        [0.0000, 0.0093, 0.0000,  ..., 0.0000, 0.0030, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        ...,
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0028, 0.0000],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0028, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(267645.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2660, 0.0257],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1791, 0.0546],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0901, 0.0891],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.1222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.1222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0069, 0.1222]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1651962.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3086.5854, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(27255.9531, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(142.7826, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-31.1596, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.0330],
        [ -0.1196],
        [ -3.4027],
        ...,
        [-14.8450],
        [-14.8367],
        [-14.8378]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1117100.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.8540],
        [0.8237],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(608.6814, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.8540],
        [0.8237],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(608.6814, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.7857e-03,  2.0734e-03, -1.7636e-02,  ..., -9.3777e-03,
         -3.6791e-05,  2.3388e-03],
        [-6.8143e-03,  2.0552e-03, -1.7746e-02,  ..., -9.4295e-03,
         -3.5846e-05,  2.6305e-03],
        [-6.7857e-03,  2.0734e-03, -1.7636e-02,  ..., -9.3777e-03,
         -3.6791e-05,  2.3388e-03],
        ...,
        [-6.0055e-03,  2.5681e-03, -1.4647e-02,  ..., -7.9700e-03,
         -6.2500e-05, -5.5967e-03],
        [-6.0055e-03,  2.5681e-03, -1.4647e-02,  ..., -7.9700e-03,
         -6.2500e-05, -5.5967e-03],
        [-6.0055e-03,  2.5681e-03, -1.4647e-02,  ..., -7.9700e-03,
         -6.2500e-05, -5.5967e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-138401.8125, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4562, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.2397, device='cuda:0')



h[100].sum tensor(-1913.4657, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(84.7429, device='cuda:0')



h[200].sum tensor(-567.2932, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(31.8474, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0080, 0.0000,  ..., 0.0000, 0.0000, 0.0140],
        [0.0000, 0.0081, 0.0000,  ..., 0.0000, 0.0000, 0.0132],
        [0.0000, 0.0094, 0.0000,  ..., 0.0000, 0.0000, 0.0035],
        ...,
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(265456.1562, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3476, 0.0222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3452, 0.0225],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2310, 0.0528],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0074, 0.1232],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0074, 0.1232],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0074, 0.1232]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1659798.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3019.7693, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(24918.6367, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-19.7415, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-50.6271, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.0238],
        [  0.8341],
        [ -0.9755],
        ...,
        [-15.0126],
        [-15.0040],
        [-15.0050]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1174947.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(670.9254, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(670.9254, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-6.0055e-03,  2.5681e-03, -1.4647e-02,  ..., -7.9700e-03,
         -6.2500e-05, -5.5967e-03],
        [-6.0055e-03,  2.5681e-03, -1.4647e-02,  ..., -7.9700e-03,
         -6.2500e-05, -5.5967e-03],
        [-6.5185e-03,  2.2427e-03, -1.6613e-02,  ..., -8.8957e-03,
         -4.5593e-05, -3.7824e-04],
        ...,
        [-6.0055e-03,  2.5681e-03, -1.4647e-02,  ..., -7.9700e-03,
         -6.2500e-05, -5.5967e-03],
        [-6.0055e-03,  2.5681e-03, -1.4647e-02,  ..., -7.9700e-03,
         -6.2500e-05, -5.5967e-03],
        [-6.0055e-03,  2.5681e-03, -1.4647e-02,  ..., -7.9700e-03,
         -6.2500e-05, -5.5967e-03]], device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-137860.8594, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.7035, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(31.1276, device='cuda:0')



h[100].sum tensor(-1914.6879, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(93.4088, device='cuda:0')



h[200].sum tensor(-567.6349, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(35.1041, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(268824.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0511, 0.1041],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1299, 0.0720],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1758, 0.0516],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0074, 0.1232],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0074, 0.1232],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0074, 0.1232]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1665046.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3035.1970, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(24769.5156, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-14.9877, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-44.8652, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -5.5312],
        [ -1.8598],
        [  0.7767],
        ...,
        [-15.0126],
        [-15.0040],
        [-15.0050]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1158603.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(638.9379, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(638.9379, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0007, -0.0056],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0007, -0.0056],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0007, -0.0056],
        ...,
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0007, -0.0056],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0007, -0.0056],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0007, -0.0056]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-138506.8438, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.5704, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(29.6435, device='cuda:0')



h[100].sum tensor(-1948.3796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(88.9553, device='cuda:0')



h[200].sum tensor(-567.4511, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(33.4305, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(264136.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0092, 0.1222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0092, 0.1222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0093, 0.1220],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0955, 0.0829],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0805, 0.0903],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0508, 0.1043]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1641197.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3094.5674, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(21704.5137, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-120.9746, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-118.5772, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-12.2360],
        [-12.2130],
        [-12.1299],
        ...,
        [ -5.0036],
        [ -5.2252],
        [ -7.2308]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1019784.2500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 710.0 event: 10650 loss: tensor(337.8653, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(527.3337, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(527.3337, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0014, -0.0057],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0014, -0.0057],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0014, -0.0057],
        ...,
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0014, -0.0057],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0014, -0.0057],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0014, -0.0057]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-139780.9062, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.1547, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.4656, device='cuda:0')



h[100].sum tensor(-1977.2000, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(73.4174, device='cuda:0')



h[200].sum tensor(-566.8765, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.5911, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(256308.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0106, 0.1217],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0106, 0.1217],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0151, 0.1198],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0105, 0.1222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0105, 0.1222],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0105, 0.1222]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1608457.6250, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3061.4634, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(20109.6445, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-213.5779, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-146.3411, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.8681],
        [-11.2537],
        [-10.0605],
        ...,
        [-14.8283],
        [-14.8209],
        [-14.8223]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1002930.8750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(551.9369, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(551.9369, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0019, -0.0057],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0019, -0.0057],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0019, -0.0057],
        ...,
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0019, -0.0057],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0019, -0.0057],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0019, -0.0057]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-140035.3750, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2568, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.6071, device='cuda:0')



h[100].sum tensor(-2005.5769, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(76.8427, device='cuda:0')



h[200].sum tensor(-567.0178, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.8784, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(256578.0938, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0124, 0.1211],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0124, 0.1211],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0125, 0.1209],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0123, 0.1216],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0123, 0.1216],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0123, 0.1216]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1608344.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3077.9512, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(19108.0039, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-234.7957, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-137.9040, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.8570],
        [-11.9914],
        [-12.0197],
        ...,
        [-14.8102],
        [-14.8031],
        [-14.8046]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-993469.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(725.2535, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(725.2535, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0024, -0.0057],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0024, -0.0057],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0024, -0.0057],
        ...,
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0024, -0.0057],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0024, -0.0057],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0024, -0.0057]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-139095.7812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8957, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(33.6481, device='cuda:0')



h[100].sum tensor(-2034.0244, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(100.9725, device='cuda:0')



h[200].sum tensor(-567.9005, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.9467, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(269624.1875, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0624, 0.1010],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0289, 0.1144],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0136, 0.1206],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0134, 0.1213],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0134, 0.1213],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0134, 0.1213]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1664512.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3101.5283, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18383.8867, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-218.9447, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-84.3628, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -5.5414],
        [ -8.4610],
        [-10.2252],
        ...,
        [-14.8310],
        [-14.8238],
        [-14.8253]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-979536.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(571.7766, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(571.7766, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0027, -0.0146,  ..., -0.0080, -0.0028, -0.0057],
        [-0.0060,  0.0027, -0.0146,  ..., -0.0080, -0.0028, -0.0057],
        [-0.0060,  0.0027, -0.0146,  ..., -0.0080, -0.0028, -0.0057],
        ...,
        [-0.0060,  0.0027, -0.0146,  ..., -0.0080, -0.0028, -0.0057],
        [-0.0060,  0.0027, -0.0146,  ..., -0.0080, -0.0028, -0.0057],
        [-0.0060,  0.0027, -0.0146,  ..., -0.0080, -0.0028, -0.0057]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-140629.7812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3220, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.5276, device='cuda:0')



h[100].sum tensor(-2053.6277, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(79.6049, device='cuda:0')



h[200].sum tensor(-567.1078, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.9164, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0108, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(254990.5156, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0415, 0.1092],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0161, 0.1192],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0132, 0.1202],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0130, 0.1209],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0130, 0.1209],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0130, 0.1209]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1588390.3750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3064.9395, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(18393.6719, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-256.6716, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-96.6759, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.4616],
        [ -8.7705],
        [ -9.8219],
        ...,
        [-14.8217],
        [-14.8146],
        [-14.8161]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1023823.6250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(569.9995, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(569.9995, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062,  0.0027, -0.0155,  ..., -0.0083, -0.0034, -0.0036],
        [-0.0060,  0.0028, -0.0146,  ..., -0.0080, -0.0032, -0.0057],
        [-0.0064,  0.0025, -0.0160,  ..., -0.0086, -0.0035, -0.0021],
        ...,
        [-0.0060,  0.0028, -0.0146,  ..., -0.0080, -0.0032, -0.0057],
        [-0.0060,  0.0028, -0.0146,  ..., -0.0080, -0.0032, -0.0057],
        [-0.0060,  0.0028, -0.0146,  ..., -0.0080, -0.0032, -0.0057]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-140873.0625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3256, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.4451, device='cuda:0')



h[100].sum tensor(-2074.0195, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(79.3575, device='cuda:0')



h[200].sum tensor(-567.1127, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.8235, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(256526.6406, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1540, 0.0570],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2152, 0.0339],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2458, 0.0282],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0110, 0.1210],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0110, 0.1210],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0110, 0.1210]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1601877., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3063.2114, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17966.5781, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-280.4211, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-81.6494, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.1999],
        [  2.6509],
        [  2.8651],
        ...,
        [-14.7981],
        [-14.7914],
        [-14.7931]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1004840.6875, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(570.7578, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(570.7578, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0029, -0.0146,  ..., -0.0080, -0.0036, -0.0057],
        [-0.0060,  0.0029, -0.0146,  ..., -0.0080, -0.0036, -0.0057],
        [-0.0060,  0.0029, -0.0146,  ..., -0.0080, -0.0036, -0.0057],
        ...,
        [-0.0060,  0.0029, -0.0146,  ..., -0.0080, -0.0036, -0.0057],
        [-0.0060,  0.0029, -0.0146,  ..., -0.0080, -0.0036, -0.0057],
        [-0.0060,  0.0029, -0.0146,  ..., -0.0080, -0.0036, -0.0057]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-141205.1562, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3107, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.4803, device='cuda:0')



h[100].sum tensor(-2092.2925, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(79.4631, device='cuda:0')



h[200].sum tensor(-567.0922, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.8631, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0115, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(252990.4844, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0091, 0.1212],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0101, 0.1207],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0125, 0.1194],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0090, 0.1217],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0090, 0.1217],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0090, 0.1217]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1572819., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3003.2319, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17836.2422, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-280.0212, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-47.4842, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[-11.6933],
        [-11.1341],
        [-10.1835],
        ...,
        [-14.8627],
        [-14.8553],
        [-14.8567]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1042913.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.8413],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(591.1602, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.8413],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(591.1602, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0068,  0.0023, -0.0177,  ..., -0.0094, -0.0047,  0.0024],
        [-0.0060,  0.0029, -0.0146,  ..., -0.0080, -0.0039, -0.0057],
        [-0.0068,  0.0023, -0.0177,  ..., -0.0094, -0.0047,  0.0024],
        ...,
        [-0.0060,  0.0029, -0.0146,  ..., -0.0080, -0.0039, -0.0057],
        [-0.0060,  0.0029, -0.0146,  ..., -0.0080, -0.0039, -0.0057],
        [-0.0060,  0.0029, -0.0146,  ..., -0.0080, -0.0039, -0.0057]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-141252.9844, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3858, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(27.4269, device='cuda:0')



h[100].sum tensor(-2109.2334, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(82.3036, device='cuda:0')



h[200].sum tensor(-567.1959, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.9306, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0104, 0.0000,  ..., 0.0000, 0.0000, 0.0060],
        [0.0000, 0.0088, 0.0000,  ..., 0.0000, 0.0000, 0.0168],
        [0.0000, 0.0101, 0.0000,  ..., 0.0000, 0.0000, 0.0094],
        ...,
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0116, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(255792.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4158, 0.0148],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5012, 0.0008],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4390, 0.0165],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0078, 0.1218],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0078, 0.1218],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0078, 0.1218]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1591374.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3002.7798, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17419.6641, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-265.2284, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-15.5028, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  1.0799],
        [  0.9819],
        [  0.9941],
        ...,
        [-14.8737],
        [-14.8778],
        [-14.8898]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1073148.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.2717],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(539.1897, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.2717],
        [0.0000],
        [0.0000],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(539.1897, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064,  0.0026, -0.0160,  ..., -0.0086, -0.0046, -0.0022],
        [-0.0063,  0.0027, -0.0156,  ..., -0.0084, -0.0045, -0.0031],
        [-0.0060,  0.0029, -0.0146,  ..., -0.0080, -0.0042, -0.0058],
        ...,
        [-0.0060,  0.0029, -0.0146,  ..., -0.0080, -0.0042, -0.0058],
        [-0.0060,  0.0029, -0.0146,  ..., -0.0080, -0.0042, -0.0058],
        [-0.0060,  0.0029, -0.0146,  ..., -0.0080, -0.0042, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-141856.4688, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.1866, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(25.0157, device='cuda:0')



h[100].sum tensor(-2123.0422, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(75.0680, device='cuda:0')



h[200].sum tensor(-566.9208, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(28.2114, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0110, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0112, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0114, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(250334.3906, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2317, 0.0365],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1267, 0.0735],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0716, 0.0941],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0076, 0.1217],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0076, 0.1217],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0076, 0.1217]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1564558.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-2989.8247, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(17133., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-243.7136, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(6.8030, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  0.5010],
        [ -1.2721],
        [ -3.5859],
        ...,
        [-14.5403],
        [-14.8752],
        [-14.9556]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1120522.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(573.3307, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(573.3307, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0028, -0.0146,  ..., -0.0080, -0.0045, -0.0058],
        [-0.0060,  0.0028, -0.0146,  ..., -0.0080, -0.0045, -0.0058],
        [-0.0060,  0.0028, -0.0146,  ..., -0.0080, -0.0045, -0.0058],
        ...,
        [-0.0060,  0.0028, -0.0146,  ..., -0.0080, -0.0045, -0.0058],
        [-0.0060,  0.0028, -0.0146,  ..., -0.0080, -0.0045, -0.0058],
        [-0.0060,  0.0028, -0.0146,  ..., -0.0080, -0.0045, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-141612.0625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3215, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.5997, device='cuda:0')



h[100].sum tensor(-2137.1995, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(79.8213, device='cuda:0')



h[200].sum tensor(-567.1071, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.9978, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0113, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(254001.2188, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0935, 0.0871],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0514, 0.1023],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0806, 0.0893],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0085, 0.1204],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0085, 0.1204],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0085, 0.1204]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1583599.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3007.5122, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(16481.8652, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-223.9310, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(22.2167, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.8487],
        [ -2.8311],
        [ -1.8842],
        ...,
        [-14.8821],
        [-14.8745],
        [-14.8759]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1086136.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(501.5388, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(501.5388, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0028, -0.0146,  ..., -0.0080, -0.0047, -0.0058],
        [-0.0060,  0.0028, -0.0146,  ..., -0.0080, -0.0047, -0.0058],
        [-0.0060,  0.0028, -0.0146,  ..., -0.0080, -0.0047, -0.0058],
        ...,
        [-0.0060,  0.0028, -0.0146,  ..., -0.0080, -0.0047, -0.0058],
        [-0.0060,  0.0028, -0.0146,  ..., -0.0080, -0.0047, -0.0058],
        [-0.0060,  0.0028, -0.0146,  ..., -0.0080, -0.0047, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-142005.7812, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.0674, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(23.2689, device='cuda:0')



h[100].sum tensor(-2147.8796, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(69.8261, device='cuda:0')



h[200].sum tensor(-566.7559, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(26.2415, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0107, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0111, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(251528.9688, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0973, 0.0809],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1249, 0.0696],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0959, 0.0812],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0098, 0.1181],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0098, 0.1181],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0098, 0.1181]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1579675.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3049.4463, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15679.2344, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-237.6912, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-33.9496, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -2.9252],
        [ -1.6596],
        [ -2.0743],
        ...,
        [-13.7484],
        [-14.4272],
        [-14.6072]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-983287., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
epoch: 0 batch 720.0 event: 10800 loss: tensor(419.0888, device='cuda:0', grad_fn=<DivBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.0000],
        [0.4016],
        [0.5322],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(667.4298, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.0000],
        [0.4016],
        [0.5322],
        ...,
        [0.0000],
        [0.0000],
        [0.0000]], device='cuda:0') 
g.ndata[nfet].sum tensor(667.4298, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0064,  0.0024, -0.0161,  ..., -0.0087, -0.0054, -0.0019],
        [-0.0068,  0.0021, -0.0176,  ..., -0.0093, -0.0059,  0.0020],
        [-0.0070,  0.0019, -0.0186,  ..., -0.0098, -0.0062,  0.0047],
        ...,
        [-0.0060,  0.0027, -0.0146,  ..., -0.0080, -0.0049, -0.0058],
        [-0.0060,  0.0027, -0.0146,  ..., -0.0080, -0.0049, -0.0058],
        [-0.0060,  0.0027, -0.0146,  ..., -0.0080, -0.0049, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-140615.7969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.6722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(30.9654, device='cuda:0')



h[100].sum tensor(-2162.1360, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(92.9221, device='cuda:0')



h[200].sum tensor(-567.5917, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(34.9212, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0020],
        [0.0000, 0.0083, 0.0000,  ..., 0.0000, 0.0000, 0.0114],
        [0.0000, 0.0073, 0.0000,  ..., 0.0000, 0.0000, 0.0228],
        ...,
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0109, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(263686.0312, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2612, 0.0386],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4526, 0.0110],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.6022, 0.0000],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0115, 0.1167],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0115, 0.1167],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0115, 0.1167]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1633342.8750, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3136.8289, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(15104.1240, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-189.4629, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(8.9678, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -0.3100],
        [  0.7358],
        [  0.9264],
        ...,
        [-14.6139],
        [-14.6071],
        [-14.6087]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-938191.1250, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(619.5704, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(619.5704, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0062,  0.0024, -0.0154,  ..., -0.0083, -0.0054, -0.0037],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0051, -0.0058],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0051, -0.0058],
        ...,
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0051, -0.0058],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0051, -0.0058],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0051, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-140934.2344, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.4963, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(28.7449, device='cuda:0')



h[100].sum tensor(-2170.9709, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(86.2589, device='cuda:0')



h[200].sum tensor(-567.3486, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(32.4171, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0100, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0105, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(262705.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1416, 0.0594],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1047, 0.0760],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0487, 0.1005],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0122, 0.1177],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0122, 0.1177],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0122, 0.1177]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1633121.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3313.8345, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14955.4277, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-129.0947, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(223.8249, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.1231],
        [ -3.3300],
        [ -6.2534],
        ...,
        [-14.8510],
        [-14.8435],
        [-14.8449]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-998759., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(531.4753, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(531.4753, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0068,  0.0019, -0.0175,  ..., -0.0093, -0.0063,  0.0019],
        [-0.0063,  0.0024, -0.0156,  ..., -0.0084, -0.0056, -0.0032],
        [-0.0063,  0.0024, -0.0156,  ..., -0.0084, -0.0056, -0.0032],
        ...,
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0053, -0.0058],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0053, -0.0058],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0053, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-142077.7969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.1691, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(24.6578, device='cuda:0')



h[100].sum tensor(-2177.9753, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(73.9940, device='cuda:0')



h[200].sum tensor(-566.8965, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(27.8078, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0045],
        [0.0000, 0.0087, 0.0000,  ..., 0.0000, 0.0000, 0.0045],
        [0.0000, 0.0099, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(251927.8125, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2861, 0.0228],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2721, 0.0258],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1652, 0.0558],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0107, 0.1197],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0107, 0.1197],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0107, 0.1197]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1598388., device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3032.2878, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14602.4385, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-122.5906, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(18.2546, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[  2.5397],
        [  2.0840],
        [  0.2757],
        ...,
        [-15.0920],
        [-15.0836],
        [-15.0848]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1059403.5000, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(710.2693, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(710.2693, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0053, -0.0058],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0053, -0.0058],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0053, -0.0058],
        ...,
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0053, -0.0058],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0053, -0.0058],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0053, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-140658.5156, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.8196, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(32.9529, device='cuda:0')



h[100].sum tensor(-2181.6323, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(98.8864, device='cuda:0')



h[200].sum tensor(-567.7953, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(37.1626, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0098, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(269284.9062, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0003, 0.0000,  ..., 0.0000, 0.0353, 0.1085],
        [0.0000, 0.0006, 0.0000,  ..., 0.0000, 0.0180, 0.1159],
        [0.0000, 0.0009, 0.0000,  ..., 0.0000, 0.0131, 0.1179],
        ...,
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0157, 0.1177],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0328, 0.1108],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0814, 0.0913]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1691385.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3094.9844, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(14400.8174, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-94.2085, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(46.0072, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -7.6577],
        [ -9.7444],
        [-10.9432],
        ...,
        [-13.2297],
        [-11.1775],
        [ -8.5103]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1005844.7500, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(771.9618, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(771.9618, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0055, -0.0058],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0055, -0.0058],
        [-0.0065,  0.0021, -0.0166,  ..., -0.0089, -0.0061, -0.0007],
        ...,
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0055, -0.0058],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0055, -0.0058],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0055, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-140454.2969, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-615.0602, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(35.8151, device='cuda:0')



h[100].sum tensor(-2190.9639, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(107.4754, device='cuda:0')



h[200].sum tensor(-568.1279, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(40.3905, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0097, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0096, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0103, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(271842.1250, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0891, 0.0845],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1548, 0.0574],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1930, 0.0414],
        ...,
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0089, 0.1209],
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0089, 0.1209],
        [0.0000, 0.0014, 0.0000,  ..., 0.0000, 0.0089, 0.1209]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1699556.7500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3040.9866, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13851.1816, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-97.2668, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(24.0200, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -1.1456],
        [  0.7609],
        [  1.9925],
        ...,
        [-15.1555],
        [-15.1555],
        [-15.1615]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-984238.3750, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(560.8965, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(560.8965, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0056, -0.0058],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0056, -0.0058],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0056, -0.0058],
        ...,
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0056, -0.0058],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0056, -0.0058],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0056, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-142365.5625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.2776, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.0228, device='cuda:0')



h[100].sum tensor(-2193.7212, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(78.0901, device='cuda:0')



h[200].sum tensor(-567.0464, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(29.3472, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(258472.0781, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0358, 0.1071],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0287, 0.1110],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0316, 0.1097],
        ...,
        [0.0000, 0.0019, 0.0000,  ..., 0.0000, 0.0076, 0.1203],
        [0.0000, 0.0019, 0.0000,  ..., 0.0000, 0.0076, 0.1203],
        [0.0000, 0.0019, 0.0000,  ..., 0.0000, 0.0076, 0.1203]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1655789.2500, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3006.9143, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13956.8164, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-109.8088, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-7.8975, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -6.4790],
        [ -6.2971],
        [ -5.6064],
        ...,
        [-15.2254],
        [-15.2167],
        [-15.2178]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1026190.3125, device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)



input node feature: 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet].sum tensor(578.2606, device='cuda:0')



input graph: 
g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)}) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>) 
g.ndata[nfet].shape torch.Size([101940, 1]) 
g.ndata[nfet] tensor([[0.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]], device='cuda:0') 
g.ndata[nfet].sum tensor(578.2606, device='cuda:0')
param0_0.shape torch.Size([256])
param.data[:, 0].shape torch.Size([256])



h after the first convolutional layer: 
 tensor([[-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0057, -0.0058],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0057, -0.0058],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0057, -0.0058],
        ...,
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0057, -0.0058],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0057, -0.0058],
        [-0.0060,  0.0026, -0.0146,  ..., -0.0080, -0.0057, -0.0058]],
       device='cuda:0', grad_fn=<AddBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(-142325.5625, device='cuda:0', grad_fn=<SumBackward0>)



h[:, 0].sum tensor(-614.3285, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[0] tensor(26.8284, device='cuda:0')



h[100].sum tensor(-2200.4722, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[100] tensor(80.5076, device='cuda:0')



h[200].sum tensor(-567.1167, device='cuda:0', grad_fn=<SumBackward0>)

g.ndata[nfet].sum() * conv1.weight[200] tensor(30.2557, device='cuda:0')



h1 after relu, the first updating, and another relu: 
 tensor([[0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        ...,
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0102, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h.shape torch.Size([101940, 256]) 
h.sum tensor(257462.9531, device='cuda:0', grad_fn=<SumBackward0>)



h2 after the second convolutional layer: 
 tensor([[0.0000, 0.0005, 0.0000,  ..., 0.0000, 0.0163, 0.1136],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0331, 0.1064],
        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0729, 0.0899],
        ...,
        [0.0000, 0.0022, 0.0000,  ..., 0.0000, 0.0071, 0.1183],
        [0.0000, 0.0022, 0.0000,  ..., 0.0000, 0.0071, 0.1183],
        [0.0000, 0.0022, 0.0000,  ..., 0.0000, 0.0071, 0.1183]],
       device='cuda:0', grad_fn=<ReluBackward0>) 
h2.shape torch.Size([101940, 128]) 
h2.sum tensor(1645105.5000, device='cuda:0', grad_fn=<SumBackward0>)



h2[0].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param0_2).sum() + bias0 tensor(-3035.4568, device='cuda:0', grad_fn=<AddBackward0>)



h2[100].sum tensor(13967.9219, device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param50_2).sum() + bias50 tensor(-84.5627, device='cuda:0', grad_fn=<AddBackward0>)



h2[200].sum tensor(0., device='cuda:0', grad_fn=<SumBackward0>)

(h1.sum(axis=0) * param100_2).sum() + bias100 tensor(-11.4391, device='cuda:0', grad_fn=<AddBackward0>)



g Graph(num_nodes=101940, num_edges=1097790,
      ndata_schemes={'nfet': Scheme(shape=(1,), dtype=torch.float32), 'h1': Scheme(shape=(256,), dtype=torch.float32), 'h2': Scheme(shape=(128,), dtype=torch.float32), 'h3': Scheme(shape=(64,), dtype=torch.float32)}
      edata_schemes={'efet': Scheme(shape=(1,), dtype=torch.float32)})



 output, 
h5 tensor([[ -8.7381],
        [ -6.1995],
        [ -2.8926],
        ...,
        [-15.1715],
        [-15.1648],
        [-15.1694]], device='cuda:0', grad_fn=<AddBackward0>) 
h5.shape torch.Size([101940, 1]) 
h5.sum tensor(-1042959., device='cuda:0', grad_fn=<SumBackward0>) 
g.edata[efet] tensor([[1.],
        [1.],
        [1.],
        ...,
        [1.],
        [1.],
        [1.]], device='cuda:0', grad_fn=<CatBackward0>) 
g.edata[efet].shape torch.Size([1097790, 1]) 
g.edata[efet].sum tensor(1097790., device='cuda:0', grad_fn=<SumBackward0>)
Traceback (most recent call last):
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/./TrainingBha.py", line 79, in <module>
    outi = net(batcheddglgraph, featbatch).reshape(BatchSize, 6796)#.to('cpu')#.type(torch.LongTensor)  # Perform a single forward pass.
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/MLTracking/otherparticles/pppipi/DGLpppipiGcnReNewestweight7N2/ModelBha.py", line 57, in forward
    h3 = self.conv3(g, h2)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/hpcfs/bes/mlgpu/hoseinkk/Miniconda3/envs/dgl1/lib/python3.9/site-packages/dgl/nn/pytorch/conv/graphconv.py", line 450, in forward
    rst = rst + self.bias
RuntimeError: CUDA out of memory. Tried to allocate 26.00 MiB (GPU 0; 31.75 GiB total capacity; 27.19 GiB already allocated; 11.75 MiB free; 27.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

real	1m44.395s
user	0m38.668s
sys	0m18.594s
